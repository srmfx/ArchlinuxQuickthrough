{USE LICENSE}
{S-INDEX, [INDEX], INDEX SYNTAX}
{S-CHANGELOG}
{TEST LATER, SEARCH LATER, NEW FROM BELOW, REGEX ON NVIM}
{OPTIMIZATION, TWEAK, PERFORMANCE, [PERFORMANCE]}
{IMPROVEMENT, RECOMMENDED, PERFORMANCE, QUALITY, SPEED, WARNING, LINKS TO CHECK}

NOTES:
	MAN PAGE SAVES LIVES
	MAN PAGE SAVES LIVES
	MAN PAGE SAVES LIVES
	MAN PAGE SAVES LIVES

PERSONAL NOTE:
	These were mainly annotations from back when I was studying archlinux on my own before I even joined University.
	When I joined University, I kept using and learning archlinux at my own pace and I kept doing this so even after I finished uni.

	Because of that, some of the stuff in here is very very old and I haven't had the time to double check everything is 101% correct. 
	Some tools and commands described may cause permanent damage to your linux distro or system.

	You need to be careful when using anything described in this document, read the 'USE LICENSE' carefully.
	You're on your own from this point forward.

	{ Author: https://github.com/srmfx/ || Github Page: https://github.com/srmfx/ArchlinuxQuickthrough }

USE LICENSE:

1 - I'm not accountable on users making mistakes when using one or more commands causing harm to their systems when trying to follow instructions using any document or text file provided here;

2 - The user is responsible for double-checking every single tool, command, script or file that he wants or needs to use;

3 - The user has full-responsibility over his own PC and Operating System;

4 - Always read the man pages before executing one or more linux tools described in the document, making sure they're the exact ones you need to use;

5 - Always double check any script, configuration files or tool commands provided here, making sure they're completely safe to use on your end;

6 - If you modify and share any file, document or text parts without directly providing a link to this github page, you take full responsibility upon any harm or damage being caused to someone else's PC;

7 - If you modify or distribute to other people, you must share them an unmodified copy of this use license so that they're able to understand what they're dealing with;

8 - At this point, as a linux user you completely understand that some linux tools when misused can completely wipe off your system, leaving it in a unrecoverable state and that you're the only one responsible for it;

9 - Most important, if you don't agree with this 'use license', then you'll not use, modify nor distribute any of the contents provided here neither in it's full form nor sub-parts.

---
========================================S-INDEX========================================
--LINUX:
	USING RAMDISK UTILITY IN LINUX | ZICRORAM | [PERFORMANCE]
		CREATING .ZICRORAM FILES
			RULES FOR IDENTATION AND COMMENTS IN .ZICRORAM FILES
		ADDING PROGRAMS/DATA TO RAM DURING SYSTEM STARTUP USING ZICRORAM
		WORKAROUND FOR FIREFOX IN RAMDISK

	BASIC LINUX CONCEPTS
		WHAT IS THE LINUX KERNEL
	DIFFERENCE BETWEEN BASH, SH AND SOURCE LINUX COMMAND
	HOW TO USE MANUAL PAGES
		HOW TO USE TLDR
	DO NOT EVER USE LINUX WITHOUT READING THIS
	INSTALL THERMALD
	CUSTOM KERNELS
	CALCULATING SCREEN / MONITOR DPI

	USEFUL LINKS
	THE DEVIL
		THE DEVIL2
	
	---IMPORTANT HEALTH ADVICE
		REDUCING BLUE LIGHT TO HELP DAILY COMPUTER USAGE / UV LIGHT
			PERFECT SETTINGS FOR NVIDIA USERS

	---BASIC INSTALLATION INFO:
		CREATING A BOOTABLE ARCHLINUX INSTALLATION MEDIA
		CONNECTING TO THE INTERNET
		HOW TO INSTALL GRUB / BOOTING FROM INSTALLATION MEDIA / CREATING INSTALLATION MEDIA
			TUTORIAL INSTALLING ARCHLINUX ON USB DISK / EFI INSTALLATION / UEFI INSTALLATION
		DEPLOYING ALSA | SETTING UP AUDIO
		SETTING KEYBOARD LAYOUT ( NEWER 2023 )
			SETTING UP KEYBOARD | KEYBOARD LAYOUT CONFIGURATION ( OLD 2022 - UPDATED, SOME PARTS MANDATORY )
		MOUNTING DISKS, EXTERNAL DISKS, USB DISK / PEN DRIVES
			MOUNTING HARD DISKS
			MOUNTING .CUE AND .BIN FILES USING CDEMU
			MOUNTING .CUE AND .BIN FILES USING FUSEISO AND FUSERMOUNT
			MOUNTING .ISO FILES WITH MOUNT
		ENABLING FAST COMMIT OPTION ON EXT4 FILESYSTEMS || PERFORMANCE
		THINGS TO PAY ATTENTION WHEN USING LINUX
		INCREASING LINUX PERFORMANCE | OPTIMIZATION
			USING A DIFFERENT MEMORY ALLOCATOR(MALLOC) FOR LAUNCHING PROGRAMS
				SCRIPT FOR RUNNING PROGRAMS WITH MIMALLOC
				TROUBLESHOOTING PRELOAD
					32BIT APPLICATIONS DO NOT WORK WITH PRELOADED MIMALLOC
					MAKING SURE APPLICATIONS ARE RUNNING WITH PRELOADED LIBRARY
					ALTERNATE WAY TO CHECK APPLICATIONS ARE RUNNING WITH MIMALLOC
					DO NOT WASTE TIME ON MISTAKES
					RUNNING STEAM GAMES WITH LD_PRELOAD
				RUNNING STEAM GAMES WITH LD_PRELOAD
					USING OPENBLAS LIBRARY
				SECURING YOUR CODE/PROGRAM FROM LD_PRELOAD INJECTIONS
				EXAMPLES OF KNOWN PROGRAMS THAT USE LD_PRELOAD
			FASTER LOCATE, USE MLOCATE
			WHY WAYLAND IS BETTER AND FASTER / UNDERSTANDING WAYLAND
				STACKING WINDOW MANAGERS VS COMPOSITE WINDOW MANAGERS
				FURTHER IMPROVING PERFORMANCE ON LINUX WITH WESTON
				MAKING A WESTON AUTO-LAUNCHER FOR A GIVEN PROGRAM
			INSTALLING POSSIBLE MISSING FIRMWARES
			IMPROVING FIREWALLD PERFORMANCE
			OPTIMIZING ZRAM
			ENTROPY ON KERNEL 6.0
			ENABLING RAM (A-)XMP / DOCP PROFILE
			ABOVE 4GB DECODING (SAM / RBAR)
			DISABLING COMPOSITION OR ENABLING UNREDIRECTION
			USING A WAYLAND DESKTOP ENVIRONMENT
			KDE PLASMA BALOO FILE INDEXING
			DO NOT USE FERALINTERACTIVE GAMEMODE
			NON-STOCK / CUSTOM KERNELS
			IMPROVE CLOCK_GETTIME THROUGHPUT
		INSTALLING YAY / AUR REPOSITORY
		WHAT IS DBUS
			INSTALLING DBUS-BROKER FOR PERFORMANCE


	LIST OF LIGHTWEIGHT OSES

	ARCH LINUX POST-INSTALL INTRODUCTION 2020.1-T
		HOW TO SURVIVE ON LINUX (LONGER THAN EVERYONE ELSE)
		HOW TO FIND OUT THE REASON A PROGRAM DOESN'T WORK ON LINUX | SYSTEM FREEZE
			KEEPING AN EYE ON UPDATED PACKAGES

		PULSEAUDIO BASICS(OLD)
			RESTART PULSEAUDIO SERVICES
			OPTIMIZING PULSEAUDIO FOR PERFORMANCE

		USING PIPEWIRE INSTEAD OF PULSEAUDIO (2024)
			INTRODUCTION
				ALSA AND OSS DRIVERS
				PULSEAUDIO AND JACK
				PIPEWIRE AND WIREPLUMBER
			EXPLAINNING CARD PROFILES
				ANALOG X DIGITAL
			TROUBLESHOOTING AUDIO / AUDIO CRACKLING / AUDIO STUTTERS ON PIPEWIRE/WIREPLUMBER
				MICROPHONE GARBLED AUDIO INPUT
				MICROPHONE UNMUTED BUT STILL GIVES NO AUDIO OR GIVES GARBLED MICROPHONE INPUT
				MICROPHONE INPUT/SOURCE AUDIO MUTE OR CRACKLING ON FEW APPLICATIONS
				MODIFYING MONO AUDIO MICROPHONES TO PLAY AUDIO ON BOTH SIDES WITH EASY EFFECTS
				SOUND OUTPUT CRACKLING / AUDIO STUTTERS WITH EASY EFFECTS + PIPEWIRE
					ELIMINATING AUDIO STUTTERS DUE TO USING EASY EFFECTS
					ELIMINATING AUDIO STUTTERS FROM  PIPEWIRE/WIREPLUMBER
					SETTING UP QUANTUM ON A PER APPLICATION BASIS
			TROUBLESHOOTING VOLUME NOT SET TO DEFAULT
				USE ALSACTL FOR RESTORING CARD TO DEFAULT VALUES
					AUTO INITIALIZING ALSACTL RESTORE FOR ALL USERS
					TROUBLESHOOTING ALSACTL RESTORE FOR PULSE SERVER
				MAKING SURE AUDIO CHANNELS DEFAULT TO THE SAME VOLUME
				USER BIND MOUNT HOME DIRECTORIES
			TROUBLESHOOTING DAEMON NOT STARTING
			SETTING UP DEFAULT AUDIO DEVICE
				SETTING PULSE_SERVER TO UNRESTRICTED MODE
				SELECTING AN CARD PROFILE
				SETTING THE DEFAULT SINK/SOURCE DEVICE
				PERSISTENCE / PERMANENT SETTINGS
					SETTING PULSE_SERVER TO UNRESTRICTED MODE
					SELECTING A CARD PROFILE
						WHAT IS PRO-AUDIO? || SPECIAL TOPIC
						DEFAULT AUDIO SETTINGS DIRECTORY
					SETTING THE DEFAULT SINK/SOURCE DEVICE
					SETTING DEFAULT VOLUME FOR DEFAULT SINK/SOURCE DEVICE
					PERSISTENCE / PERMANENT SETTINGS
					REMOVING OLD CONFIGURATION SETTINGS
					TESTING DEFAULT SETTINGS
					RECOMMENDATIONS
				DISABLING BLUEZ(BLUETOOTH) FOR PERFORMANCE
					VERIFYING PERFORMANCE BOOST
				VERIFYING WHICH AUDIO DRIVER IS RUNNING
				INCREASING SOUND RESAMPLING QUALITY
				TROUBLESHOOTING AUDIO NOT WORKING PROPERLY || SYSTEMD ONLY
			SETTING UP EASYEFFECTS
				AUDIO EQUALIZATION AND SOUND EFFECTS FOR PIPEWIRE
				SETTING UP AUDIO SINK AND SOURCE FOR CAPTURING SOFTWARE/DEVICES
				TESTING OUTPUT/INPUT DEVICES IN OBS STUDIO
				MONO INPUT DEVICES IN OBS STUDIO
				PERFECT AUDIO VOLUME SETTINGS
				SETTING UP EASYEFFECTS AS DEFAULT AUDIO SOURCE
					SETTING UP FOR ZSH
					SETTING UP FOR GNOME DESKTOP
			PIPEWIRE TOOLS
			ALSA PERMISSIONS
			REDIRECTING GPU HDMI OUTPUT TO REGULAR SPEAKER/HEADSET
			SETTING UP PRO-AUDIO PROFILE || PERFORMANCE
			AUDIO TEST || SPEAKER TEST
				FETCHING VALID ALSA CARD NAMES
				FETCHING UNUSED DEVICES
				CONNECTING OUTPUT SOUND DEVICE TO VIRTUAL INPUT DEVICE
			SHARING AUDIO WITH MULTIPLE LOGGED USERS ON THE SYSTEM
				ALTERNATE OPTION
			CREATING VIRTUAL INPUT/OUTPUT AUDIO DEVICE
			CONNECTING OUTPUT SOUND DEVICE TO VIRTUAL INPUT DEVICE
			PLAYING DESKTOP MUSIC TO INPUT DEVICE AND USING MICROPHONE AT THE SAME TIME | REDIRECTING DESKTOP AUDIO AS INPUT DEVICE
			VOICE MODULATION THROUGH CARLA
			TROUBLESHOOTING
			INSTALLATION STEPS
			USING PIPEWIRE AS AUDIO RECORDER
				USING PIPEWIRE FOR RECORDING MICROPHONE AUDIO  
				USING PIPEWIRE TO RECORD DESKTOP AUDIO
				USING PIPEWIRE TO RECORD AUDIO FROM SPECIFIC APPLICATION
				USING PIPEWIRE FOR TESTING AUDIO FILE 

		SECURITY
			SECURITY CONCERN
			LINUX FIREWALLING WITH IPTABLES | NFTABLES
				BASIC CONCEPTS
				OPTIONAL TOOLS FOR IPTABLES
				HOW DOES IPTABLES WORK | VERY IMPORTANT
				USEFUL COMMANDS || EXTENDED REGEX USING GREP
				USEFUL LINKS
				LISTING ALL FILTERS CREATED FOR IPTABLES
				BLOCKING MULTIPLE PORTS
				CREATING BASH SCRIPTS TO SIMPLIFY RULE CREATION
				ADDING IP ADDRESSES TO IPTABLES FILTER
				IPTABLES LOGGING
				FORMATTING IPTABLES LOG ON A HUMAN READABLE FORMAT
				DROPPING PACKETS USING IP ADDRESS RANGE, USE THE 'IPRANGE' MODULE
				DROPPING PACKAGES TO/FROM SPECIFIC PORTS
				DELETING A RULE
				CREATING A NAT RULE TO TRANSLATE ALL TRAFFIC FROM THE `192.168.0.0/24` SUBNET TO THE HOST'S PUBLIC IP
				DROPPING PACKAGES FROM UNWANTED SOURCES
				BLOCKING ALL INCOMING AND ONGOING CONNECTIONS
				SAVING AND PERSISTING RULES ON DISK FILE:
				RESTORING RULES FROM DISK FILE
				CREATING CHAINS TO FACILITATE RULE APPLICATIONS AND LOGGINGS || VERY IMPORTANT
					HOW DOES IPTABLES WORK || ADVANCED
				HOW TO BLOCK BY LINK ADDRESS / HOW TO BLOCK BY MAC ADDRESS
				HOW TO BLOCK IP BY WEBSITE / DOMAIN NAME
				REGULATING INTERNET ACCESS THROUGH UDP/TCP CONNECTIONS BY TIME
				LIMITING CONNECTIONS FROM A SINGLE HOST
				SEEING HOW MANY TIMES A RULE HAS BEEN HIT
				ASSURING AND VERIFYING A GIVEN IPTABLE MODULE HAS BEEN LOADED
				IP RANGE BLOCK USING CIDR IP ADDRESS NOTATION
				CUSTOM IPTABLES RULES
					ALLOWING MINIMUM INTERNET WORK
					MY OWN FILTERS
				LISTING CONNECTED IP ADDRESSES AND CURRENT OPEN DOORS ON THE SYSTEM
				ENABLING IPTABLES TO BOOT UP WITH THE SYSTEM
				ENABLING LOOPBACK INTERFACE TO BE LISTENED FOR PACKAGES
				GOING INTO DETAILS
				FIREWALLD VS IPTABLES
			ENABLING FIREFOX ENCRYPTED CLIENT HELLO (ECH)
			CLAMAV ANTI-VIRUS
				ENABLING CLAMAV SERVICES
				TESTING CLAMAV TOOLS
				CLAMAV CONFIGURATION FILES
					ALLOW FILE DESCRIPTOR PASS | ALLOWING CLAMAV TO SCAN DIFFERENT FILE SYSTEM TREES
					ENABLE LOGGING FOR DETECTED INFECTED DATA
					CHANGING QUARANTINE DIRECTORY FOR REAL-TIME ACCESS SCAN
					SETTING UP DIRECTORIES FOR REAL TIME SCANNING
					EXCLUDING DIRECTORIES FROM REAL TIME SCANNING
					TURNING ON REAL-TIME SCANNING ON NEW, MOVED OR RENAMED FILES
				CLAMAV UTILITIES
			NETWORK MONITORING
			FIREWALLING
			HOW TO CREATE SECURITY KEYS / KEYFILES ON LINUX
			CLEANING JOURNALCTL
			ENCRYPTING DIRECTORIES WITH FSCRYPT
				INFORMATION TO CONSIDER | 2024
				INSTRUCTIONS & ENABLING FSCRYPT ENCRYPTION ON EXT4
				ENCRYPTING NEW DIRECTORY WITH FSCRYPT
				ENCRYPTING DIRECTORIES WITH SECURITY KEY INSTEAD OF PASSWORD | 2024
				ADDING/CREATING NEW FSCRYPT PROTECTOR | CHANGING FSCRYPT PASSWORD
				CLEANING JOURNALCTL
				MORE INFO ON FSCRYPT
				FSCRYPT VS LUKS OR DM-CRYPT ON LOOP DEVICE
			CREATING A LUKS ENCRYPTED LOOP DEVICE USING DM-CRYPT
				RESIZING A LUKS ENCRYPTED LOOP-DEVICE USING DM-CRYPT
				HOW TO KEEP CRYPTSETUP / LUKS / DM-CRYPT DEVICE SAFE
				CRYPTSETUP / DM-CRYPT KEYS MANAGEMENT
		ADJUSTING HARDWARE & SYSTEM CLOCK USING HWCLOCK
			USING TIMEDATECTL FOR SETTING TIMEZONE (SYSTEM CLOCK ONLY)
			USING SYSTEMD-TIMESYNCD
			USING HWCLOCK
		AVOIDING HEATING UP YOUR CPU TOO MUCH
			INSTALL THERMALD
		INSTALLING XORG
		INSTALLING AND CUSTOMIZING SDDM
		BUILDING INITRAMFS
			CUSTOMIZING INITRAMFS FOR A SPECIFIC KERNEL
			OPTIMIZING BOOTUP
		TROUBLESHOOTING 2022.3
			RESTORING A BROKEN ARCHLINUX INSTALLATION
			DELETING A FILE DOES NOT FREE UP SPACE ON DISK
		---TROUBLESHOOTING 2023.1
			STEAM NOT WORKING | NVIDIA XCONFIG
		RECOMMENDED ARCHLINUX PACKAGES
			RECOMMENDED ARCHLINUX PACKAGES / RECOMMENDED PACKAGES
		GOOD APPLICATIONS DESCRIPTIONS
		XORG/X11 STARTUP ORDER
		LINUX DESKTOP ENVIRONMENT COMPOSITION
		SYSTEM BOOT INSTRUCTIONS FOR ARCHLINUX
		VERY USEFUL HINTS - VERY IMPORTANT
			CHECKING BOOT TIMES - VERY IMPORTANT
		AUR & ALA REPOSITORY - VERY IMPORTANT
		SOMETHING ABOUT MOUNTING DISKS OR DRIVES
		INCREASING SSD LIFETIME

	---VERY BASIC:
		USING RAMDISK UTILITY IN LINUX | ZICRORAM | [PERFORMANCE]
			CREATING .ZICRORAM FILES
				RULES FOR IDENTATION AND COMMENTS IN .ZICRORAM FILES
			ADDING PROGRAMS/DATA TO RAM DURING SYSTEM STARTUP USING ZICRORAM
			WORKAROUND FOR FIREFOX IN RAMDISK

		USING CDPARANOIA TO EXTRACT AUDIO FROM .BIN .CUE FILES | EXTRACTING AUDIO FROM CD-ROM CONTAINNING CDDA LAYER
		HOW TO USE KEXEC FOR EXECUTING A FAST SYSTEM REBOOT

		[INDEX: COMPRESSING AND UNCOMPRESSING FILES & DATA]
			CREATING FILE ARCHIVES USING AR
			DIFFERENCE BETWEEN GNU TAR, 7Z, BZIP2, GUNZIP, ZIP, COMPRESS AND BZZ
			CREATING ARCHIVE FILES USING GNU TAR, LIBARCHIVE, BSDTAR, PAX
				COMPRESSION ALGORITHM TOOLS
					GZ CHEAT SHEET
						RUNNING/EXECUTING .GZ COMPRESSED BINARY FILES - [PERFORMANCE]
					XZ CHEAT SHEET
					ZSTD CHEAT SHEET
					BROTLI CHEAT SHEET
					LZ4 CHEAT SHEET
					OTHER COMPRESSION ALGORITHM TOOLS
				ARCHIVING TOOLS / ARCHIVE TOOLS
					TAR CHEAT SHEET
						TAR DATA COMPRESSION
							MANUAL TAR COMPRESSION 
							AUTOMATIC TAR COMPRESSION 
					DAR CHEAT SHEET
						READ THIS BEFORE USING DAR
						LIST OF DAR FEATURES
						BASIC DAR EXAMPLES
							ADDING DATA TO DAR ARCHIVE
								USING DAR ARCHIVING WITHOUT COMPRESSION
								GENERATING HASH FILE FOR DAR ARCHIVE DURING CREATION
								USING STANDARD DAR COMPRESSION
									USING DAR WITH LZ4 COMPRESSION
									USING DAR WITH ZSTD COMPRESSION
								USING MULTITHREADING DAR COMPRESSION 
								CREATING DAR ARCHIVE WITH ENCRYPTION AND PASSWORD
								SPLITTING/SLICING ARCHIVE INTO SEVERAL FILES
									PAUSING ARCHIVING PROCESS FOR EACH SLICE CREATED
										PAUSING ARCHIVING PROCESS AFTER A GIVEN NUMBER OF SLICES HAVE BEEN CREATED
								ADDING/EXCLUDING DATA FROM DAR ARCHIVING USING A LISTING FILE
							LISTING DAR FILES
							MERGING DIFFERENT DAR ARCHIVES
								MERGING TWO DIFFERENT DAR ARCHIVES
								MERGING A THIRD DAR ARCHIVE
							MAKING CHANGES TO AN EXISTING DAR ARCHIVE
								CHANGING COMPRESSION ALGORITHM FROM EXISTING DAR ARCHIVE
								CHANGING ENCRYPTION ALGORITHM AND PASSWORD FROM EXISTING DAR ARCHIVE
								CREATING BACKUP SLICES FROM AN EXISTING DAR ARCHIVE
								ADDING NEW FILES AND CONTENTS TO AN EXISTING DAR ARCHIVE
								MAKING OTHER TYPES OF CHANGES TO EXISTING DAR ARCHIVE
							INTEGRITY TEST FOR DAR ARCHIVE
							REPAIRING A DAMAGED/CORRUPTED DAR ARCHIVE
								ISOLATING THE CATALOGUE
								REPAIRING DAR ARCHIVE
									LISTING REPAIRED FILES FROM DAR ARCHIVE
							COMPARING DAR ARCHIVES
								COMPARING TWO DIFFERENT DAR ARCHIVES
								COMPARING ARCHIVE WITH DATA ON THE FILESYSTEM
							EXTRACTING DATA FROM DAR ARCHIVE
								EXTRACTING ALL CONTENTS FROM DAR ARCHIVE INTO CURRENT FOLDER
								EXTRACTING SINGLE CONTENT FROM DAR ARCHIVE INTO CURRENT FOLDER
								EXTRACTING DAR ARCHIVE USING PASSWORD
						SOME DAR LINKS
					7Z CHEAT SHEET
						PROPERLY UNCOMPRESSING/DECOMPRESSING SEVERAL FILES USING '7Z' AND 'AWK'
					ZIP CHEAT SHEET
				GNU TAR EXAMPLES
					EXAMPLE 0 - EXTRACTING TAR DATA ON A GIVEN DIRECTORY
					EXAMPLE 1 - CREATING, UPDATING AND APPENDING TO TAR ARCHIVE
						EXAMPLE 1.0 - EXTRACTING ARCHIVES FROM UNTRUSTED SOURCES
						EXAMPLE 1.1 - DELETING FROM TAR ARCHIVE
						EXAMPLE 1.2 - AUTO COMPRESSING TAR ARCHIVE
						EXAMPLE 1.3 - EXTRACTING A CHOSEN FILE WITHIN TAR ARCHIVE
						EXAMPLE 1.4 - SPLITTING A LARGE FILE INTO MANY DIFFERENT FILES | SPLITTING A FILE INTO DIFFERENT CHUNKS USING TAR
						EXAMPLE 1.5 - EXTRACTING SPLIT FILES USING TAR
				IMPORTANT LINKS ABOUT COMPRESSION
					IMPORTANT TAR RESOURCE LINKS
				COMPRESSION ALGORITHMS/FORMATS FOR GNU TAR

		HOW TO USE THE 'LESS' COMMANDLINE TOOL
		USING CUPS FOR HANDLING PRINTERS AND PRINTING DOCUMENTS ON LINUX
			CUPS - THE COMMON UNIX SYSTEM PRINTER
			ENABLING CUPS TO FIND OUT AVAILABLE PRINTERS FOR USE | ENABLING AND STARTING AVAHI-DAEMON.SERVICE
			INSTALLING AND ENABLING 'CUPS' AND 'CUPS-PDF'
			VERIFYING YOUR USB PRINTER IS CONNECTED
			VERIFYING YOUR PARALLEL PORT PRINTER IS CONNECTED
			ADDING A NETWORK PRINTER
			ADDING A LEGACY PRINTER
			VIEWING PRINTER INK LEVELS
			CUPS MANAGEMENT THROUGH WEB INTERFACE
			SETTING UP DEFAULT PAGE SIZE FOR CUPS
			SETTING UP DEFAULT PRINTER FOR CUPS USING 'LPADMIN'
			DISPLAYING DEFAULT SYSTEM PRINTER USING 'LPSTAT'
			DISPLAYING ALL AVAILABLE SYSTEM PRINTERS USING 'LPSTAT'
			LISTING ALL QUEUED PRINT JOBS USING 'LPSTAT'
			CANCEL ALL QUEUED PRINT JOBS USING 'LPRM'
			CANCEL ONLY CURRENT QUEUED JOB USING 'LPRM'
			USING 'LP' FOR PRINTING DOCUMENTS ON COMMANDLINE
				PRINTING A FILE
				PRINTING FILE USING A KNOWN PRINTER
				PRINTING A KNOWN NUMBER OF COPIES USING THE DEFAULT PRINTER
				PRINTING SPECIFIC PAGES USING THE SYSTEM DEFAULT PRINTER
				PRINTING OUTPUT OF A COMMAND
				DISPLAYING DEFAULT SYSTEM PRINTER USING 'LPSTAT'
			IMPORTANT LINKS

		THE VERY QUICK INTRODUCTION TO GIMP
			SAVING FILES WITH BEST COMPRESSION
			USING THE PATH TOOL FOR CREATING MASKS AND CHANNELS
				CONVERTING A CHANNEL AND PATHS TO SELECTION FOR EDITING
				EDITING AN ALREADY EXISTING PATH
			SAVING SELECTION AS A NEW CHANNEL
				MAKING/LOADING SELECTION FROM EXISTING CHANNEL
			CREATING A LAYER MASK | APPLYING LAYER MASK | APPLYING MASK TO LAYER
			SELECTION EDITOR
			DUPLICATING LAYERS
				DELETING A LAYER MASK
			APPLYING MASK
			CREATE NEW LAYER GROUP
			TRANSFORM TOOLS
			BETTER FREEDOM WHEN COLORIZING
			SOME GOOD FILTERS
			MORE OPTIONS
				INCREASING SHADOW HIGHLIGHTS
				POSTERIZE
				COLOR TO ALPHA
				ROTATE COLOR
				CHANNEL MIXER
				RGB CLIP
				OTHER EFFECTS
			OTHER OPTIONS ON GIMP'S GUI ( OLD )

		USING FFMPEG - THE UNIVERSAL MEDIA CONVERTER
			RECORDING MICROPHONE AUDIO USING FFMPEG
			EXTRACTING AUDIO FROM VIDEO
			TRIM A VIDEO FROM ONE POINT TO ANOTHER
			CONVERTS IMAGES INTO EITHER ANIMATED .GIF OR .MPG
			SAVE VIDEO AS AN ANIMATED .GIF
			RECORDING SCREEN USING FFMPEG
				RECORDING SCREEN ALONG WITH AUDIO INPUT/SOURCE DEVICE
			CONVERTING VIDEOS INTO A MORE COMPRESSED FORMAT
			CONVERTING AUDIO INTO A MORE COMPRESSED FORMAT
			CONVERTING BOTH VIDEO AND AUDIO TO MORE COMRPESSED FORMAT
			BATCH CONVERSION OF MULTIPLE VIDEOS INTO COMPRESSED FORMAT
			MORE LINKS

		THE ONION WEB / TOR WEB
			HOW TO USE TOR
			LINKS
		DISABLING OOM(OUT-OF-MEMORY) KILLER
		CLEANING SWAP MEMORY
		SETTING UP STRETCHED RESOLUTION FOR NVIDIA WITHOUT USING GAMESCOPE
		ADDING NEW SUPPORTED/UNSUPPORTED SCREEN RESOLUTION | NVIDIA ONLY
		DIFFERENCE BETWEEN 'ENV' AND 'EXPORT'
		SUSPEND X HIBERNATE X HYBRID
			SUSPENDING SYSTEM / SLEEP SYSTEM
			HIBERNATING
			HYBRID SUSPENDING AND HIBERNATING
			SUSPEND THEN HIBERNATE
			TROUBLESHOOTING NVIDIA GPUS
		HOW TO FETCH FILE DESCRIPTORS
		ACCESSING THE UNDERLAYING BIND MOUNT POINT
		GETTING INFO ON MEMORY USAGE
		DISABLING HUGEPAGE FOR PERFORMANCE
		DIFFERENCES BETWEEN SOFTLINKS/SYMLINKS, HARDLINKS AND REFLINKS
		HOW TO USE LSOF
		CUSTOMIZING ZPROFILE | ZSH PROFILE
		RUNNING APPLICATIONS AS ANOTHER USER || MULTIPLE USERS
		MANAGING USERS
			RESETTING USER PASSWORD || PASSWORD RECOVERY
		SOME FIND COMMANDS
		BASIC SECURITY CONCERNS
			PACNEW FILES - (ARCHLINUX ONLY) -
			USER HISTORY
			SUDO DATA
			BIND MOUNTING AS DATA OBFUSCATION ON THE SYSTEM
				EXAMPLE 1
				EXAMPLE 2
			USING LOOP DEVICES AS DATA OBFUSCATION
			USING DM-CRYPT TO ENCRYPT LOOP DEVICES
			USING USB DEVICES TO BOOT THE SYSTEM
			SANDBOXING APPLICATIONS WITH FIREJAIL
				FIREJAIL INTRODUCTION
				IMPROVING PERFORMANCE BY FIREJAILING APPLICATIONS
				FIREJAIL COMMANDS
				USEFUL TOOLS FOR USING IN FIREJAIL
				CORE TECHNOLOGY LINUX NAMESPACES
				USEFUL FIREJAIL EXAMPLES
					USING FIREFOX ANONYMOUSLY WITH FIREJAIL
				FIREJAIL GAMESCOPE WORKAROUND
				FIXING NVIDIA ISSUE ON FIREJAIL WHEN RUNNING SOME PROGRAMS LIKE STEAM
				SETTING UP FIREJAIL TO USE PIPEWIRE SERVER AUDIO DEVICE WITH A DIFFERENT NETWORK DEVICE
				GETTING FILES/DIRECTORIES FROM FIREJAIL CONTAINERS
			INTRO TO FLATPAK
				DEFAULT FLATPAK SETTINGS
				INSTALLING FLATPAK APPS
				UPDATING INSTALLED FLATPAK APP
				RUNNING FLATPAK PROGRAMS
				UNINSTALLING FLATPAK PROGRAM
				LISTING ALL FLATPAK APPLICATIONS INSTALLED
				SHARING THE HOST FILESYSTEM
				GRANTING ACCESS TO A FILESYSTEM, FOLDER OR DIRECTORY
				GRANTING READ-ONLY ACCESS TO A FILESYSTEM, FOLDER OR DIRECTORY
				RESETING PERMISSIONS TO DEFAULT
				BLOCKING ACCESS TO A FILESYSTEM, FOLDER OR DIRECTORY
				DISPLAYING ALL USER EDITED PERMISSIONS
				MISC LINKS
				FLATPACK SOURCES
			SYMLINKING/BIND MOUNT CACHE AND CONFIGURATION DATA TO FSCRYPTED FOLDER | LIST OF UNSAFE DIRECTORIES
			FURTHER INSTRUCTIONS ON LINUX DATA OBFUSCATION
			WHY NOT ENCRYPT THE ENTIRE HOME DIRECTORY?
			ADVANCED OBFUSCATION
			PROTECTING THE FSCRYPT KEY ON A REMOVABLE DISK
		PROPERLY USING 'RENAME' FOR RENAMING FILES
			UTIL-LINUX 'RENAME' VERSION
			PERL 'RENAME' VERSION
		PROPERLY USING 'CP' FOR COPYING FILES
		PROPERLY USING 'MV' AND 'AWK' FOR MOVING FILES AND SUB-DIRECTORIES
			WHY USE 'MV' AND 'AWK' INSTEAD OF GRAPHICAL USER INTERFACE?
		PROPERLY USING 'RM' FOR REMOVING FILES
		PROPERLY UNCOMPRESSING/DECOMPRESSING SEVERAL FILES USING '7Z' AND 'AWK'
			DECOMPRESSING ALL COMPRESSED FILES IN A GIVEN DIR
			BATCH RENAMING FILES AND DIRECTORY
			RENAMING ALL ZIP FILES
		CUSTOMIZING YOUR ZSH EXPERIENCE
			INSTALLING ZSH
				INSTALLING CUSTOM THEMES
			SETTING UP ZSH AS YOUR DEFAULT SHELL
			CLONING ZSH CONFIGURATION FOR OTHER USERS
			CHANGING ZSH THEME | INSTALLING POWERLEVEL10K | PERFORMANCE
				CLONING POWERLEVEL10K FOR OTHER USERS
			CHANGING ZSH THEME | INSTALLING OH-MY-ZSH
				RECOMMENDED OH-MY-ZSH TOPIC READINGS
				TROUBLESHOOTING ZSH AND OH-MY-ZSH INSTALLATION
				RECOMMENDED OH-MY-ZSH THEMES
			USING BOTH POWERLEVEL10K AND OH-MY-ZSH FOR EXTRA FEATURES | PERFORMANCE | RECOMMENDED
				CLONING POWERLEVEL10K + OH-MY-ZSH CONFIGURATION FOR OTHER USERS
				MY CUSTOM POWERLEVEL10K + OH-MY-ZSH CONFIGURATION FILE
			MY CUSTOM ZSHRC CONFIGURATION FILE
			MY CUSTOM POWERLEVEL10K ZSHRC CONFIGURATION FILE
			MY CUSTOM OH-MY-ZSH ZSHRC CONFIGURATION FILE
			LOADING ZSH MODULE
			USING ZSH WITH VI/VIM KEYS
		MKINITCPIO NVIDIA CONFIGURATION 2024
		CREATING KEY BINDINGS
			CREATING MODULAR KEYBINDINGS
				CREATING AN ALERT WINDOW
					CREATING A TEMPORARY 4 SECONDS WINDOW
			SETTING UP THINGS FOR STREAMING
				SETTING UP VIRTUAL AUDIO DEVICE
				SETTING UP OBS-STUDIO
			CREATING MACROS
				MACRO PERSISTENCE
			CREATING MAPS USING HWDB
				READING INPUTS FROM KEYBOARD AND OTHER DEVICES
				FETCHING ACTUAL DEVICE NAME
				CREATING DEFAULT MAPPING
				CREATING CUSTOM EDITED MAPPING
				ENABLING A GIVEN CREATED MAPPING
		INTRODUCTION TO KERNEL MODULES CONFIGURATION (2023)
		ABOUT MAN PAGES
		FINDING FILES THAT HAVE BEEN ACCESSED OR MODIFIED IN LESS THAN A FEW HOURS
		PROPERLY VERIFYING DISK ERRORS ON LINUX
			IN SUMMARY - VERY IMPORTANT
			PREDICTING DISK DEATH
			IN-DEPTH ANALYSIS OF A HARD DISK / HARD DRIVE WITH SMARTCTL
				CHECKING USAGE PERCENTAGE FOR SSD DISKS
				CHECKING FOR ERRORS ON DISK
				READING AND UNDERSTANDING ATTRIBUTE COLUMNS ON SMARTCTL -A
				IN SUMMARY - VERY IMPORTANT
				AUXILIARY COMMANDS
		FINDING OUT AVAILABLE RESOLUTIONS
		INSTALLING POSSIBLE MISSING FIRMWARES
		FINDING OUT PACKAGE OWNER OF A GIVEN FILE
		LISTING ALL FILES OWNED BY A GIVEN PACKAGE
		HOW TO BUILD PACKAGES MANUALLY
		CHANGING MAKEPKG.CONF FOR BUILDING APPLICATIONS FROM SCRATCH
			LISTING OPTIMIZATION FLAGS FOR GCC THAT CAN BE USED IN MAKEPKG.CONF
			LISTING WHICH OPTIMIZATION FLAGS ARE ENABLED AT WHICH LEVEL
			RECOMMENDED CFLAGS
			FINDING OUT WHICH OPTIMIZATION FLAGS A BINARY HAS BEEN COMPILED WITH IN C++
				VERY HELPFUL SOURCE
			MY CUSTOM MAKEPKG.CONF CFLAG AND CXXFLAG CHOICE
			RUST FLAG TUTORIAL
			CMAKE FLAG TUTORIAL
			IMPROVING PYTHON PACKAGES PERFORMANCE
			HOW TO COMPILE YOUR PROGRAMS THROUGH YAY AND PACMAN FOR PERFORMANCE GAINS
			SOMETHING ABOUT COMPILE, DEBUG, TEST AND RELEASE

	---BASICS:
		FINDING OUT IF COMMAND IS A BINARY EXECUTABLE OR A SHELL BUILT IN COMMAND
		SETTING UP SYSTEM(GLOBAL) AND USER SERVICES USING SYSTEMD
		WHAT IS DBUS
			INSTALLING DBUS-BROKER FOR PERFORMANCE
		PROCESS(RUNNING APPLICATION OR PROGRAM) MANAGEMENT AND CONTROL
			SUSPENDING & PAUSING PROGRAMS(PROCESSES) AND RESTARTING THEM
			FORCE SHUTDOWN FROZEN PROCESSES
			CONTROLLING SHELL PROCESSES AT RUNTIME
			FETCHING PIDS ON A LINE-PER-LINE STYLE
			RETRIEVING PIDS IN A SINGLE ENTIRE LINE
			LISTING A PROCESS MEMORY MAP
			LISTING A PROCESS WORKING DIRECTORY
			LISTING OPEN FILES AND THEIR RELATED PROCESS
			OTHER TOOLS
		KILLING A FROZEN APPLICATION
			KILLING APPLICATIONS / EXTERMINATING AN APPLICATION PROCESS
		CUSTOMIZING GRUB THEME
		MAKING SURE DATA HAS BEEN WRITTEN TO DISK
		LINUX EXECUTABLE BINARY FILE / ELF
			LINUX SHARED LIBRARY / DLL
			READING INFORMATION FROM ELF TYPES
		AVOIDING HEATING UP YOUR CPU TOO MUCH
		MANUALLY INSTALLING ARCHLINUX PACKAGES
			FINDING FILETYPES IN A GIVEN DIRECTORY
			CREATE DESKTOP SHORTCUT
		---PACKAGE MANAGERS:
			FEDORA PACKAGE MANAGER | RPM, DNF AND YUM PACKAGE MANAGER
				DNF COMMANDS
				RPM COMMANDS
			FEDORA FUSION REPOSITORY
			GENTOO PACKAGE MANAGER
				EMERGE COMMANDS
			DEBIAN PACKAGE MANAGER
				APT COMMANDS
			FREEBSD PACKAGE MANAGER
			XBPS PACKAGE MANAGER
			PACMAN COMMANDS | ARCHLINUX PACKAGE MANAGER
				USEFUL CHAIN PACMAN COMMANDS
				OPTIMAL CUSTOM PACMAN.CONF CONFIGURATION
				GPME ERROR NO DATA || INVALID OR CORRUPTED DATABASE (PGP SIGNATURE) || PERFORMANCE
				BLACKLISTING PACKAGE AND PACKAGE GROUPS FROM PACMAN UPDATE
		FUN LINUX STUFF
		INSTALLING UNICODE CHARACTERS
		DISK SCHEDULERS / I/O SCHEDULERS
		SUDO BASICS
			SUDO - SUDOERS TUTORIAL
				ADDING USER TO SUDOERS LIST
					TROUBLESHOOTING ADDING USER TO SUDO
				RUNNING PROGRAMS AS ROOT WITH SUDO
				SUDO / GKSUDO OR GKSU COMMAND OPTIONS
			EXECUTING PROGRAMS AS A DIFFERENT GROUP IN THE SYSTEM
			IPTABLES
				REVOKING INTERNET PERMISSION FROM AN EXISTING GROUP
		CREATING AND MOUNTING LOOP DEVICES
			CREATING RAW IMAGES
			RESIZING EXISTING RAW IMAGES | RESIZING LOOP DEVICES
			RESIZING EXISTING QCOW IMAGES
				ABOUT SPARSE FILES
				ABOUT RAW DISK IMAGES
				ABOUT QCOW2 DISK IMAGES
			EXPERIMENTING DIFFERENT FILESYSTEMS AND PARTITION TABLES WITHOUT USING A REAL DISK
			SAFELY INSTALLING OPERATING SYSTEMS WITHOUT OVERWRITTING THE BOOTLOADER
			INSTALLING OPERATING SYSTEMS WITHOUT VIRTUAL MACHINE
		LINUX TOOLS
			HARDWARE INFO / SYSTEM INFO COMMANDS
			SYSTEM PROCESS COMMANDS
			HARD DISK COMMANDS
			NETWORK / NETWORK SECURITY COMMANDS
			MONITORING TOOLS
			CHECKING DEVICE TEMPERATURE
		DIFFERENCE BETWEEN BASH AND SOURCE COMMANDS
		DIFFERENCE BETWEEN TCL AND BASH SCRIPT
		UNDERSTANDING PAM SYSTEM
		IMPORTANT ENVIRONMENT VARIABLES AND OPTIONS
		WHAT DOES THE 2>&1 MEANS IN BASH SCRIPTING
		MODIFYING LINUX KEY MAPS | KEYBOARD MAPPINGS
		CREATING MAPS USING HWDB
			READING INPUTS FROM KEYBOARD AND OTHER DEVICES
			FETCHING ACTUAL DEVICE NAME
			CREATING DEFAULT MAPPING
			CREATING CUSTOM EDITED MAPPING
			ENABLING A GIVEN CREATED MAPPING
		WINE
			ABOUT WINEPREFIX
			SETTING UP WINEPREFIXES
			USING A WINEPREFIX
			ABOUT WINETRICKS
			ABOUT PLAYONLINUX
			HOW TO ENABLE DXVK ON WINE-GE
			RUNNING PROGRAMS WITHIN AN ALREADY EXISTING WINE OR PROTON PREFIX

		KERNEL OPTIONS - NEW: VERY IMPORTANT: reduces keyboard input delay from 50~120ms to fixed 36ms increases performance
			NVIDIA PERFORMANCE TRICK

		SHORTCUT/HOTKEYS
			COMMON TEXT EDITOR SHORTCUT KEYS
			TERMINOLOGY/YAKUAKE/SOME TERMINALS SHORTCUT/HOTKEYS KEYS
				KDE KONSOLE HOTKEYS
			ZSH VI KEYS
			AWESOME WM SHORTCUT/HOTKEYS
			MOZILLA FIREFOX HOTKEYS
				ENABLING CARET MODE  IN FIREFOX
			BROWSER/OPERA HOTKEYS 2020
			VIVALDI HOTKEYS
			WEECHAT HOTKEYS 
			TEXT EDITOR HOTKEYS

		MOZILLA FIREFOX TIPS
			USING AND MANAGING DIFFERENT PROFILES ON MOZILLA
			MOZILLA FIREFOX HOTKEYS
			ENABLING CARET MODE  IN FIREFOX
			WORKAROUND FOR FIREFOX IN RAMDISK

		--BASICS PART 2
			ENABLING VSYNC FOR NVIDIA
			PLAYING VIDEOS ON THE TERMINAL / VIDEOS COMMAND LINE
			FETCHING EVERY SINGLE FRAME FROM VIDEO
			SETTING UP SYSTEM LANGUAGE | SYSTEM LOCALE (NOT MANDATORY FOR KEYBOARD LAYOUT)
				CHANGING SYSTEM LOCALE PROTON AND WINE

		--OTHER STUFF
			WHAT IS THE LINUX KERNEL
			HOW TO COMPILE YOUR OWN LINUX KERNEL

	LINUX GENERAL TOPICS
		HOW TO DOSBOX-X
			MACHINE TYPES DOSBOX-X CAN RUN
			MOUNTING DISK IMAGES
				HOW TO USE FDI FILETYPES ON DOSBOX-X
				MOUNTING .HDI FILETYPES ON LINUX
				UMOUNTING DISK
				UMOUNTING AND MOUNTING A 2ND DISK AT THE SAME TIME
				ALTERNATE OPTION TO MOUNTING .FDI FILES ON LINUX
			RUNNING DOSBOX-X AS PC-9800
			SAVING AND LOADING STATES
			HOTKEYS
			OPTIMIZING DOSBOX-X
			RUNNING 4DOS INSTEAD OF COMMMAND.COM FOR DOSBOX-X
			RUNNING PROGRAMS ON THE HOST'S SYSTEM
			AUTOMOUNTING DRIVES ON LAUNCH
			ABOUT CONFIGURATION FILE
		LINUX EXT4 DEFRAG
		LINUX EXT4 REPAIRING
		USING PROGRAMS AS ANOTHER USER
		HOW TO SYSTEMCTL
		HOW TO USE MANUAL PAGES
		HOW TO USE INFO PAGES
		HOW TO CHECK GNOME SETTINGS
		HOW TO CREATE A SHORTCUT
		HOW TO START NVIDIA FAN
		USING PS, DBUS-MONITOR, TOP, PKILL, PGREP TO HANDLE SYSTEM & USER PROCESSES:
		INCREASING LINUX PERFORMANCE
			SETTING CPU SCALING GOVERNOR TO MAXIMUM PERFORMANCE
		INCREASE STEAM PERFORMANCE
			SOLVING AUDIO RUNNING UNDERRUN OCCURRED ISSUE | CRACKLING SOUNDS & LOW FPS ON WINE-GE:
			DECREASING/CAPPING/LIMITING FPS IN GAMES
		I/O SCHEDULER
			TURN OFF I/O SCHEDULER
		SETTING MAXIMUM CPU PERFORMANCE FOR PROCESSES/SERVICES/PROGRAMS ON LINUX
		PRELOADING PROGRAMS INTO MEMORY
		MAKING YOUR SYSTEM BOOT UP FASTER
		DAILY CHECKING YOUR DISK HEALTH WITH SMARTCTL AND GIT
		USING PROGRAMS AS ANOTHER USER
		CREATING A ZFS PARTITION
		CHECKING WHICH MOUNT OPTIONS ARE ACTIVE
		INTRODUCTION TO INITRAMFS
		TIPS ON READING JOURNALCTL AND DMESG
		CLEANING CACHE
		UDEV RULES
			I/O SCHEDULER
			MY CUSTOM UDEV RULE
		TELLING PROGRAMS TO INIT WITH THE SYSTEM
		USING ZRAM AS SWAP DEVICE
			THE DIFFERENCE BETWEEN ZRAM AND ZSWAP
			CREATING ZRAM DEVICE
			THE THREE ZRAM FILES
				ZRAM MODULES-LOAD.D FILE
				ZRAM MODPROBE FILE
				ZRAM UDEV FILE
		SWAP, CACHE, RAM, ZRAM, ZSWAP, ZCACHE, TMPFS, KERNEL, FURTHER NOTES
		GNOME TWEAK SETTINGS FOR PROGRAM STARTUP
	
	LINUX GENERAL TOPICS 2
		ABOUT EXT4 FILESYSTEM FEATURES
			EXT4 EXTENTS
			DELAYED ALLOCATION | SPECIAL TOPIC
			VERIFYING SIZE OF DIRTY BLOCKS
			HOW OFTEN DIRTY BLOCKS ARE GETTING FLUSHED TO THE DISK
			HOW TO CHANGE DIRTY BLOCK EXPIRE TIME
			DELAYED ALLOCATION CAN LEAD TO POTENTIAL DATA LOSS | SPECIAL TOPIC
			WHAT ARE THE MAIN ADVANTAGES?
			VERY IMPORTANT NOTE | SPECIAL TOPIC
			FS-VERITY
			CONTENT TO READ LATER
		EXT4 FILESYSTEM ENCRYPTION
			DATA AT REST ENCRYPTION
		ENCRYPTING FILES WITH GPG / OPENPGP
			MORE ABOUT ENCRYPTION
		---KERNEL THINGS
			LISTING WHICH KERNEL MODULES AND FEATURES YOU HAVE AVAILABLE IN YOUR SYSTEM
			TURNING OFF SLEEP MODE FROM USB DEVICES
			GREPPING DEVICES FROM DMESG(KERNEL)
			CHECKING WHICH KERNEL MODULES ARE LOADED FOR WHICH DEVICES
			LISTING ALL LINUX KERNEL MODULES CURRENTLY IN USE
			DISABLING HPET FOR INCREASED GPU USAGE
			ENABLING NOHZ FOR PERFORMANCE
			SETTING CPU SCALING GOVERNOR TO MAXIMUM PERFORMANCE
			FINDS ALL CONTROL RELATED VARIABLES IN /PROC/SYS
			ENABLING IOMMU
		INSTALL THERMALD
		VERIFYING THERE'S SOMETHING WRONG WITH PROCESSES USING POWERTOP
		LOADING PROGRAM COMMANDS AS SOON AS POSSIBLE AT BOOT TIME

	HARDWARE GENERAL TOPICS 1
		DISK SCRUB

	WINDOW MANAGER
		AWESOMEWM

	INTRODUCTION TO SHELL SESSIONS
		CREATING ALIAS ON LOCAL BASH
		SHELL TROUBLESHOOTING
		VIRTUAL TERMINAL(VT) / VIRTUAL CONSOLE(VC)

	EXPLAINNING THE BIN FOLDER
	HOW TO MAKE KDE RUN FAST

	LINUX TOOLS
		WHAT'S VERBOSE MODE
		INTERACTIVE MODE
		ENCRYPTING FILES WITH GPG / OPENPGP
			MORE ABOUT ENCRYPTION
		ENCRYPTING FILES WITH OPENSSL
		HOW TO SYSTEMCTL
		DD COMMAND

	WHAT'S PKGBUILD
		ABOUT MAKEPKG

	BASIC LINUX TUTORIAL
	PACMAN, THE MAIN ARCHLINUX PACKAGE MANAGER
		ADVICE ON PACKAGE MIRRORS
		WHAT IS PACKAGE?
		PACKAGE STATUS
			OUT-OF-DATE PACKAGES
			ORPHANED PACKAGES
		LOOKING FOR SIMILAR PACKAGES ON THE PACKAGE MANAGER REPOSITORY
		CHECKING PACMAN HISTORY
		HOW TO RECOVER LOST PACKAGES HINTS AND TIPS
		PREVENTING DESTROYING YOUR SYSTEM PACKAGES
		BLACKLISTING PACKAGE AND PACKAGE GROUPS FROM PACMAN UPDATE
		AUR & ALA REPOSITORY
		UNOFFICIAL ARCHLINUX USER REPOSITORIES | 2024
		OPTIMAL CUSTOM PACMAN.CONF CONFIGURATION
		PACKAGE MANAGER LOG || THE IMPORTANCE OF /VAR/LOG/PACMAN.LOG
		WHAT TO DO WHEN PACMAN BREAKS || INSTALLING PACKAGES WITHOUT PACMAN
	ABOUT MASKED SERVICES
	BASH SCRIPT EXAMPLE

	LEARNING CHROOT

	ABOUT PORTLAND PROJECT
		ABOUT XDG MIME APPLICATIONS - IMPORTANT
		ABOUT XDG-UTILS - IMPORTANT

	COMPILING C, C++ CODE ON LINUX
		RUNNING A C++ PROGRAM AGAINST GDB
		HOW TO COMPILE YOUR PROGRAMS THROUGH YAY AND PACMAN FOR PERFORMANCE GAINS

	--PERMISSION SETTINGS
		USER AND GROUPS CREATION AND OWNERSHIP
		DIFFERENCE BETWEEN SUDO, SU AND RUN0
		TURNING OFF / DISABLE SPECIFIC LINUX BINARY && COMMANDS
		BACKUP FILE PERMISSIONS BEFORE CHANGING THEM
		RESTORE FILE PERMISSIONS

	MOUNTING A TMPFS FILESYSTEMS ON FSTAB
		MOUNTING DISK/FILE/DIRECTORY IN MEMORY
		MAKING STEAM WORK WITH TMPFS FOR GAMES - VERY OLD - 
		MORE ABOUT RAMDISKS FILESYSTEMS

	DIFFERENCES BETWEEN SOFTLINKS/SYMLINKS, HARDLINKS AND REFLINKS

	DEFAULT DIRECTORIES AND DEFAULT FILES
		SOME DIRECTORIES BACK FROM WHEN I WAS LEARNING
			SYSTEM LOGS

	SYSTEM ENVIRONMENT VARIABLES
		PERMANENT USER ENVIRONMENT VARIABLES
		SETTING UP SHELL ENVIRONMENT VARIABLES
		PRINTING ENVIRONMENT VARIABLES

	--MY PERSONAL TOPICS
		FINDING DKMS MODULE STATUS
		FINDING OUT-OF-TREE LOADED KERNEL MODULES
		LOOKING IF A CERTAIN FEATURE IS ENABLED IN THE SYSTEM
		LIMITING SYSTEM RESOURCES ON A USER BASIS
		RUNNING PROGRAMS WITHIN AN ALREADY EXISTING WINE OR PROTON PREFIX
		ADVANCED REGEX PART 1
			A LITTLE  HISTORY ON REGEX
			ADVANCED REGEX WITH GREP
				MATCHING NULL CHARACTERS
				PERL-STYLE PATTERN EXTENSIONS
				LOOKAHEAD AND LOOKBEHIND ASSERTIONS
				MATCHING NULL CHARACTERS
				SOME GREP EXAMPLES USING PCRE
				ONCE-ONLY SUBPATTERNS
					RECURSIVE PATTERNS
				GREP AND OTHER APPLICATIONS
		ADVANCED REGEX PART 2
			WORD BOUNDARIES
			RECURSIVE PATTERNS
			CHARACTER CLASS INTERSECTION
			CHARACTER CLASS SUBTRACTION
			ALTERNATION
			OPTIONAL ITEMS
			REPETITION OPERATOR OR QUANTIFIER
				REPETITION LIMITATION
			BACKREFERENCES
			BRANCH RESET GROUPS
			FREE SPACING MODE
			MODE MODIFIERS
			ATOMIC GROUPS
			RECURSION
			IMPORTANT LINKS
		ADVANCED REGEX PART 3
			DIFFERENCE BETWEEN SED AND GREP
			ALTERNATE OPTION FOR SED
			SED SYNTAX
			SOME SED COMMANDS
			FETCHING SAME-LINE FIELDS
			FIND, AWK GREP AND UGREP || GOOD AND BAD EXAMPLES
			FINDING ALL 'PASSWORD' OCCURRENCES IN ALL FILES IN A GIVEN DIRECTORY
			LOOKING FOR SIMILAR PACKAGES ON THE PACKAGE MANAGER REPOSITORY
			STRIPPING OUT TEXT WITH TR
		WHY WAYLAND IS BETTER AND FASTER / UNDERSTANDING WAYLAND
		HOW TO COMPILE YOUR PROGRAMS THROUGH YAY AND PACMAN FOR PERFORMANCE GAINS
		SETTING CPU SCALING GOVERNOR TO MAXIMUM PERFORMANCE
		INCREASING EMULATOR PERFORMANCE
		FILE UPLOAD
		MISC LINKS
			AI / ARTIFICIAL INTELLIGENCE
		THINGS TO KNOW
		KDENLIVE DARK THEME SETTINGS
		INCREASING KDENLIVE PERFORMANCE
		ENLIGHTENMENT DESKTOP
		LAST KNOWN WORKING NVIDIA DRIVERS(GTX 1050ti)
		MY BOOTUP COMMANDS
		MY CUSTOM I3 CONF
		MY CUSTOM GDM CONF
		MY CUSTOM SUDOERS CONFIG
		MY CUSTOM UDEV RULE
		MY CUSTOM MODULES-LOAD.D
		RECOMMENDED ARCHLINUX PACKAGES
		HOW TO INSTALL GRUB / BOOTING FROM INSTALLATION MEDIA / CREATING INSTALLATION MEDIA
			TUTORIAL INSTALLING ARCHLINUX ON USB DISK / EFI INSTALLATION / UEFI INSTALLATION
		POWER SUPPLY / STABILIZER
			SOMETHING ABOUT POWER STABILIZERS
			SOMETHING ABOUT LOW WATTAGE AND POWER SUPPLY
		CUSTOMIZED KERNEL MICROCODE + INITRAMFS
		DIFFERENCE BETWEEN API AND FRAMEWORK
	
	--GENERAL HINTS
		IMPROVING STEAM 
		DECREASING/CAPPING/LIMITING FPS IN GAMES
		HOW TO CHECK GNOME SETTING

---SOME WINDOWS STUFF:
	SOME WINDOWS UTILITIES
	RUNNINNG COMMAND AS ADMINISTRATOR
	SOME WINDOWS 11 HOTKEYS
	SOME WSL CONFIGURATION FILES | WSL SETTINGS
	MANAGING WINDOWS ACCOUNTS
		WINDOWS REMOTE ACCOUNTS
		LOCAL WINDOWS ACCOUNTS
		CONVERTING REMOTE USER ACCOUNT INTO LOCAL USER ACCOUNT
	SOME WINDOWS PERFORMANCE IMPROVEMENT
		ENABLING DRIVERS FOR YOUR GPU
		DISABLING XBOX GAME BAR
		ENABLING GAME MODE
		DISABLING GAME DVR
		INSTALLING DIRECT-X
		DISABLE ENERGY ECONOMY
		ENABLE DIRECT PLAY
		DISABLE HYPER-V
		DISABLE CORTANA AND CO-PILOT
		DISABLING PAGE FILE
	SOME NVIDIA PERFORMANCE IMPROVEMENT
	SOME LIFE QUALITY IMPROVEMENTS ON WINDOWS
		ENABLING HDR(HIGH DYNAMIC RANGE) ON WINDOWS
		ENABLING DSR(DYNAMIC SUPER RESOLUTION) FOR NVIDIA CARDS ON WINDOWS

	---SOME INTRODUCTION TO WSL AND ARCHLINUX
		INSTALLING ARCHLINUX DISTRIBUTION IN WSL
			WSL DEFAULT WINDOWS LOCATION FOR LINUX DISTROS
			LISTING ALL DISTRIBUTIONS INSTALLED IN WSL
		REMOVING A WSL DISTRIBUTION
		ADDING AND USING A NEW REGULAR USER
			ADDING A NEW USER TO SUDOERS LIST
		LOGIN AS ANOTHER USER IN A WSL LINUX DISTRO
		BASIC USER AND GROUP MANAGEMENT
			CREATING A NEW USER
			MODIFYING EXISTING USER
			CREATING A NEW GROUP
			MODIFYING AN EXISTING GROUP
		SETTING UP SEATD ON YOUR ARCHLINUX SYSTEM | PERFORMANCE
		SETTING UP TIME AND DATE ON THE LINUX SYSTEM
		SETTING UP SYSTEM LOCALE IN A WSL LINUX DISTRO
		SETTING UP TEXT FONT FAMILIES IN THE SYSTEM
			LOOKING FOR AND FINDING NEW FONT FAMILIES AND FONT TYPES FOR INSTALLATION
				LOOKING FOR HANDWRITTEN FONTS FOR INSTALLATION
			INSTALLING NEW FONTS ON THE SYSTEM
			LISTING USER INSTALLED FONTS ON THE SYSTEM
		LISTING USER AND GROUP ENTITIES ON THE LINUX DISTRO
		A FEW LINUX BEGINNER COMMANDS
		SETTING UP NEW DEFAULT STARTER USER FOR WSL DISTRO
		RECOMMENDED ARCHLINUX PACKAGES FOR WSL
			ARCHLINUX REPOSITORY
				INSTALLING UTILITIES
					INSTALLING UTILITIES
					INSTALLING FONTS
				AUR(ARCHLINUX USER REPOSITORY)
				BASICS ON HOW TO USE PACMAN
		STARTING DESKTOP ENVIRONMENT FROM WSL USING ARCHLINUX
		RUNNING CLAMAV ANTI-VIRUS IN WSL
		SETTING UP A FIREWALL WITH IPTABLES/NFTABLES IN WSL
		ENABLING DBUS-BROKER IN WSL
		MOUNTING DEVICES IN WSL
			FETCHING/LISTING ALL HARD DISKS AVAILABLE USING WINDOWS POWER SHELL
			MOUNTING DEVICES ON WSL
				BARE MOUNTS
					BARE MOUNTING DEVICES
					BARE MOUNTING WSL .VHDX IMAGES
			COPYING AND MOVING FILES FROM WITHIN WSL
		INSTALLING ZSH AND POWERLEVEL10K FOR WSL DISTROS | PERFORMANCE | RECOMMENDED
		CREATING AN ENVIRONMENT VARIABLE FOR WSL LINUX DISTRO
			PRACTICAL EXAMPLE 1 - STORING THE SCRIPT AS A STRING IN AN ENV. VARIABLE
			PRACTICAL EXAMPLE 2 - POPULATING /ETC/ENVIRONMENT USING SHELL FUNCTION/METHOD
				FULL SCRIPT 1-1
			LOADING THE NEWLY CREATED ENVIRONMENT VARIABLES
			TESTING THE NEWLY ADDED VAR
		ALLOWING AUDIO TO BE USED BY USERS OTHER THAN ROOT
		THE VERY BASICS ABOUT FILESYSTEM USER AND GROUP OWNERSHIP
			LISTING OWNERSHIP AND PERMISSIONS OF A GIVEN FILE OR DIRECTORY USING 'GETFACL'
			LISTING USER PERMISSIONS, MASK AND OTHERS USING 'LS'
			CHANGING USER-OWNER AND GROUP-OWNER OF A GIVEN DATA USING 'CHOWN'
			CHANGING FILE MODE BITS | CHANGING PERMISSIONS/RIGHTS FOR A GIVEN DATA USING 'CHMOD'
			GIVING EXTRA PERMISSION FOR OTHER USERS AND GROUPS IN A FILE/DIRECTORY USING 'SETFACL'
				PRACTICAL 'SETFACL' EXAMPLES
					ADDING NEW USER-OWNER AND GROUP-OWNER TO A GIVEN FILE/DIRECTORY
					MODIFYING CURRENT USER-OWNER AND GROUP-OWNER TO A GIVEN FILE/DIRECTORY
					REMOVING AN EXISTING ACL
					MODIFYING MASK OF A GIVEN/DIRECTORY FILE
			SHARING SINGLE DIRECTORY WITH ANOTHER USER | RESTRICTING SOME PERMISSIONS WHILE GIVING ACCESS | RECOMMENDED
		COMPILING WSL KERNEL
			HOW TO SELECT THE RIGHT KERNEL VERSION FOR YOU

	---SOME CMD(COMMAND PROMPT) AND WINDOWS POWER SHELL STUFF
		MANAGING ALIASES IN WINDOWS POWER SHELL
			LISTING ALL AVAILABLE ALIASES
			CREATING AN ALIAS IN WINDOWS POWER SHELL
			CREATING ALIAS FOR A COMMAND
			UPDATING ALIASES
			HOW TO PERMANENTLY SET ALIASES
				LISTING ALL PROFILES AVAILABLE
				CREATE THE FILE USING NVIM OR NOTEPAD
				ALLOWING SCRIPTS TO BE EXECUTED BY POWERSHELL

--MISCALENEOUS:
	MONITORING LOG FILES IN REAL TIME
		MULTITAIL COMMAND – MONITOR MULTIPLE LOG FILES IN REAL TIME
		MULTITAIL COMMAND - USING LNAV FOR THE SAME TASK ABOVE
		MULTITAIL COMMAND - USING LESS FOR THE SAME TASK ABOVE
	DIFFERENCE BETWEEN SUDO, SU AND RUN0
	RUNNING COMMANDS MULTIPLE TIMES IN LINUX | ITERATIVE LOOPS | ITERATION
	FRAME INTERPOLATION
	HTOP HOTKEYS
	TOP HOTKEYS
	WEECHAT HOTKEYS
	VIEWING PICTURES ON COMMANDLINE WITHOUT XORG USING MPV
	USING MPV TO PLAY ANIMATED DESKTOP BACKGROUNDS VIDEOS ON X11
	USING MPLAYER TO PLAY ANIMATED DESKTOP BACKGROUNDS VIDEOS ON X11
	TCP OFFLOAD AND WHAT IT IS || NETWORK OFFLOAD
		CHECKING CURRENT OFFLOAD
		DISABLING OFFLOAD || PERFORMANCE ADVICE || SPEED
		PERMANENT SETTINGS
	DISABLING IPV6 TEMPORARILY
	USING GAMECONQUEROR  ON GAMES
	NSCDE HOTKEYS
	PAL VS NTSC GAMES
	IMPROVING GRAPHICS ON LINUX
	USING OPTIMAL GRAPHICAL ENVIRONMENT VARIABLES FOR GAMES	
		OPTIMAL /ETC/ENVIRONMENT CONFIGURATION FOR NVIDIA
		OPTIMAL /USR/SHARE/VKBASALT/VKBASALT.CONF

	NVIM/VIM TUTORIAL 2020.2-T EDITION 1(NEWER)
		SETTING PERMANENT COLORSCHEMES FOR NVIM
		WINDOW COMMANDS / WINDOW SHORTCUT KEYS
		ADDING VIM CLIPBOARD FUNCTION
			ENABLING CLIPBOARD TEMPORARILY
			ENABLING CLIPBOARD PERMANENTLY ON NVIM
			TROUBLESHOOTING ISSUES NVIM
			CLIPBOARD TOOLS
			SETTING UP NVIM CLIPBOARD FOR COPY AND PASTING IN 2025
			MANUALLY SETTING UP NVIM CLIPBOARD
			AUTOMATICALLY SETTING UP NVIM CLIPBOARD 
				***: XORG CLIPBOARD TOOL
				***: WAYLAND CLIPBOARD TOOL
			CLIPBOARD MANAGER
			NVIM BINARIES & CUSTOMIZATION PACKAGES
		CHANGE ENCODING IN A FILE
		VIM NEW 2020.2-T VIM TUTORIAL EDITION 2(old, edition 1 better!), MADE ON WINDOWS
		VIM HINTS 2021.1-T
		VERY OLD NVIM TUTORIAL
			REGULAR EXPRESSIONS / REGEX ON NVIM

	GIT TUTORIAL 2020
		SOMETHING ABOUT GIT COMMIT AND GIT PHYSICAL STORAGE METHOD
			DELTA ENCODING
			GIT DELTA ENCODING
		ADDING FILES TO GITIGNORE
			HOW TO IGNORE FILES FROM BEING TRACKED/UNTRACKED
			FIXING UNTRACKED FILES
		CREATING GIT ALIASES
		ADVANCED GIT
		GIT OAUTH TOKEN | VERY IMPORTANT 2023
		GIT GPG - SIGNING GITHUB COMMIT LOCALLY AND REMOTELY | VERY IMPORTANT(2024):
		GIT TROUBLESHOOTING
			CLEANING COMMIT HISTORY FROM MAIN BRANCH
				CLEANING REFERENCE LOGS
	TUTORIAL AWESOME-WM
	TUTORIAL DWM / I3-WM 2020
	NEW QEMU TUTORIAL 2020.2-T
	CAPABILITY-BASED SECURITY SYSTEMS
	ORLOV BLOCK ALLOCATOR
	ABOUT COBOL
	ABOUT FILESYSTEMS
	ABOUT BTRFS
	ABOUT KERNEL & VIDEO CARDS
		NOTES FOR FUTURE 2020.2-T

	

	UNDERSTANDING RPCS3 | PS3 EMULATOR
	UNDERSTANDING PCSX2-QT
		VKBASALT ISSUE
		MEMORY CARD

	UNDERSTANDING PROTON
		PROTON COMMANDLINE ARGUMENTS | VKBASALT | GAMESCOPE | IMPROVING GAME EXPERIENCE | PERFORMANCE
			UPSCALING STEAM INTERFACE
		RUNNING GAMES ON NON-VULKAN GPU
		COMPILING PROTON-GE
		STEAM GAMES NOT WORKING FOR UNKNOWN REASON
		HINTS AND TIPS FOR STEAM
		SOLVING AUDIO RUNNING UNDERRUN OCCURRED ISSUE | CRACKLING SOUNDS & LOW FPS ON WINE-GE
		SOME GAMES HAVE ISSUES RUNNING IN FULL SCREEN / GAME BLACK SCREEN ISSUE
		WHAT HAPPENS TO CORRUPT SAVE DATA / CORRUPT PREFIX


	
	SOLVING STEAM FILE PICKING ISSUE / STEAM FILE PICKER NOT WORKING

	PERFECT OBS STUDIO CONFIGURATION AND SETTINGS / PERFECT LIVE STREAMING CONFIGURATION
	IMPROVING VULKAN GAMES EXPERIENCE / VASTLY IMPROVING GRAPHICS FOR VULKAN BASED GAMES
	
	UNDERSTANDING WEECHAT

--LINUX TROUBLESHOOTINGS:
	REMOVING READ ONLY MODE FROM HARD DISK || 2024
	BACKTRACKING PULSEAUDIO ISSUES
	RANDOM FREEZES ON SOME PROGRAMS/GAMES | ZRAM CULPRIT
	TESTING MICROPHONE ON LINUX
	FORCING FSCK TO BE RUN ON DISKS AT BOOT
	DELETING A FILE DOES NOT FREE UP SPACE ON DISK
	FIXING WRONG DISK SIZE
	SYSTEM NOT BOOTING / BOOTING FROM INSTALLATION MEDIA
	AUDIO VOLUME NOT SET CORRECTLY 
	FIXING RESOLUTION IN MPV
		MORE MPV OPTIONS
	HOW TO START NVIDIA FAN
	LOGIN LOCKDOWN
	STARTING DHCP SERVICE FOR FETCHING IP CONFIGURATION FROM ROUTER
	UNABLE TO LOGIN ON USER ACCOUNT
	SETTING KEYBOARD LAYOUT ( NEWER 2023 )
		SETTING UP KEYBOARD | KEYBOARD LAYOUT CONFIGURATION ( 2025 - UPDATED, SOME PARTS MANDATORY )
		FIXING KEYBOARD LAYOUT ( NEWER 2023 )
			FIXING SDDM AUTOMATICALLY CHANGING KEYBOARD LAYOUT TO AMERICAN LAYOUT AT BOOT
			EACH DESKTOP ENVIRONMENT HAVE THEIR OWN KEYBOARD SETTINGS
	SOME PROGRAMS NOT RUNNING
	GRUB RECOVER MODE / FIX MISSING GRUB BOOT ISSUE
	AKONADI ERROR ON KDE DESKTOPS
	NVIDIA TROUBLESHOOTING
		REVERTING DOWNGRADING NVIDIA DRIVERS ON ARCHLINUX || NVIDIA DRIVERS || TROUBLESHOOTING XORG
			INSTALLING NVIDIA DRIVERS LOCALLY
		LISTING DETAILED NVIDIA INFORMATION
		ENABLING VSYNC FOR NVIDIA
		TROUBLESHOOTING: XORG STOPS WORKING FOR NO REASON
			TROUBLESHOOTING STARTX AND XINIT FOR NVIDIA
		NVIDIA PERFORMANCE TRICK
			SETTING UP 'HIGH PERFORMANCE' OPTION FOR OPENGL
			NVIDIA NIS (NVIDIA IMAGE SCALER)
			ENABLING NVIDIA MULTITHREAD OPENGL
				NVIDIA OPENGL PERFORMANCE TWEAK
			DISABLING HPET FOR INCREASED GPU USAGE
		NVIDIA FAILING | CREATING ZRAM FOR NVIDIA
		RECREATING NVIDIA XCONFIG
		ADDING NVIDIA MODULE REFERENCE
		NVIDIA KERNEL 6 ISSUE
		NVIDIA AND WAYLAND 2023 | WESTON
		INCREASING PERFORMANCE FOR WAYLAND 2023
		NVIDIA WAYDROID IMPROVEMENT 2023
		STARTING WAYLAND AT BOOT FOR NVIDIA GPUS
		WESTON BACKENDS
		WESTON XWAYLAND MODULE
		FIGURING OUT WHICH WESTON BACKEND IS BEING USED
		ADDING NEW SUPPORTED/UNSUPPORTED SCREEN RESOLUTION | NVIDIA ONLY
		SETTING UP STRETCHED RESOLUTION FOR NVIDIA WITHOUT USING GAMESCOPE

--ARCHLINUX TROUBLESHOOTING:
	GPU ISSUE IN LINUX
	UPDATING ARCHLINUX GPG KEYS / UPDATING ARCHLINUX PGP SIGNATURE KEYRINGS
	SYSTEM BOOT INSTRUCTIONS FOR ARCHLINUX
	PACMAN LOCKED DOWN / PACMAN LOCKDOWN
	REMOVE CACHED PACKAGES ON ARCHLINUX

	ADDING 3rd PARTY CUSTOMIZED OR STOCK KERNELS PART 1
	NVIDIA KERNEL HOOKS / INSTALLING CUSTOMIZED & STOCK KERNELS PART 2
		CHECKING WHETHER KERNEL MODULES HAVE BEEN INSTALLED AFTER REBOOT

	FIXING KEYBOARD LAYOUT ( NEWER 2023 ):
		FIXING SDDM AUTOMATICALLY CHANGING KEYBOARD LAYOUT TO AMERICAN LAYOUT AT BOOT
		EACH DESKTOP ENVIRONMENT HAVE THEIR OWN KEYBOARD SETTINGS

--OLD STUFF:
	OLD KERNELS
========================================END INDEX ========================================
========================================INDEX SYNTAX / SYNTAX CONVENTION========================================
-REMOVE			#PARTS THAT NEED TO BE REMOVED BEFORE UPLOADING .
NOT ON INDEX		#Topics that haven't been set on Index.
S-INDEX			#When queried/searched it will show the the beginning of this document Index.
END INDEX		#Indicates the End of this document Index.
SYNTAX INDEX		#When queried/searched it Will display this section here.
[INDEX: ...TEXT...]	#This will indicate a topic or group of topics that can be found in this document INDEX.

IMPORTANT		#INDICATES VERY IMPORTANT NOTES ACROSS THE DOCUMENT.
VERY IMPORTANT		#Same thing as above.
NOTE:			#Will indicate minor notes throughout the document.

WARNING			#INDICATES VERY DANGEROUS COMMAND THAT COULD MAKE YOUR COMPUTER DEFFECTIVE.

TODO LATER		#THINGS THAT NEED TO BE DONE IN THIS DOCUMENTATION.
CHECK LATER		^ Same as Above.

CHECK			#INDICATES INFORMATION THAT NEEDS TO BE CHECKED / RE-CHECKED TO MAKE SURE IT WORKS OR IS NOT A MISTAKE.

[code]
	...		#MEANS EVERYTHING INSDIE THIS TAG IS A CODE.
[/code]

[file]
	...		#INDICATES THE CONTENTS OF A FILE.
[/file]

-T			#MARKUP TAG I USE TO READ THINGS LATER.

=======			#WILL USUALLY SEPARATE UNRELATED SECTIONS FROM ONE ANOTHER.

-------			#WILL SEPARATE SUB-SECTIONS WHO ARE STILL RELATED TO EACH OTHER OR STILL RELATED TO THE MAIN SECTION
			Note: Throughout multiple parts of this document, I'll use multiple lines '-----' to indicate complete different sections.


^			#The '^' character will be usually used to describe the upper line content. This is only meant for readibility.


New Syntax(2023):
	1(A) and 2(A) Describe completely different steps that do not need to be followed in an order.
1(B) describes a follow up step to 1(A), which may or may not be followed by the user(optional step) in order to procede.
1.2(A), if it existed, would describe a mandatory follow up step to 1(A) if nothing else procedes it like 1.1(A).


------------------------------
========================================END INDEX SYNTAX============================================
========================================METADATA / META-DATA / META DATA:===========================
====================================================================================================

====================================================================================================
========================================S-CHANGELOG=================================================
====================================================================================================

2025 - May:
	--Day 30:
		SETTING UP NVIM CLIPBOARD FOR COPY AND PASTING IN 2025
		---SOME WINDOWS STUFF
		---SOME INTRODUCTION TO WSL AND ARCHLINUX
			CHANGING ZSH THEME | INSTALLING POWERLEVEL10K | PERFORMANCE
		---SOME CMD(COMMAND PROMPT) AND WINDOWS POWER SHELL STUFF


2025 - April:
	--Day 21:
		FRAME INTERPOLATION
		USING AND MANAGING DIFFERENT PROFILES ON MOZILLA
		SYMLINKING/BIND MOUNT CACHE AND CONFIGURATION DATA TO FSCRYPTED FOLDER | LIST OF UNSAFE DIRECTORIES
		BATCH CONVERSION OF MULTIPLE VIDEOS INTO COMPRESSED FORMAT
		[INDEX: COMPRESSING AND UNCOMPRESSING FILES & DATA]

2025 - March:
	--Day 30:
		CHANGING SYSTEM LOCALE FOR PROTON AND WINE

	--Day 15:
		USING MPV TO PLAY ANIMATED DESKTOP BACKGROUNDS VIDEOS ON X11

2024 - December:
	-- Day 22:
		TROUBLESHOOTING AUDIO CRACKLING / AUDIO STUTTERS ON PIPEWIRE/WIREPLUMBER
		TCP OFFLOAD AND WHAT IT IS || NETWORK OFFLOADING


2024 - November:
	-- Day 20:
		REVERTING DOWNGRADING NVIDIA DRIVERS ON ARCHLINUX || NVIDIA DRIVERS || TROUBLESHOOTING XORG
	-- Day 04:
		NVIDIA OPENGL PERFORMANCE TWEAK
		SUSPEND X HIBERNATE X HYBRID

2024 - October:
	-- Day 30:
		FIREJAIL

	-- Day 29:
		ENABLING CARET MODE  IN FIREFOX

2024 - September: 
	-- Day 26:
		--NEW BASH TUTORIAL 2020.2-T EDITION 1 - ---

====================================================================================================
============================================END OF STACK===================================================
====================================================================================================

#Metadatas can be used by the future system that will search/download handbooks from an official git repository
#that will contain all trusted user handbooks, the metadata structures will provide information about the handbook's content,
#so that the user can actually decided whether to download it or not upon the available content information and not just it's title
#Personal Note: NOT ALL CONTENTS ARE INCLUDED ON THE INDEX
#Personal Note2: NOT ALL CONTENTS ARE INCLUDED ON THE META-DATA

{Title: My 1st Linux & Archlinux User Handbook}

{Description: My Personal Handbook for when I was still dealing with learning Archlinux, 
It contains a lot of beginner hints on how I dealt with my first archlinux environment.  
I'll also be leaving hints on linux basic tips other than just archlinux, but most of them will be aimed for the archlinux user.}

{Contents: Archlinux, Nvidia, Kernels, Grub, Systemd, Linux, Intermediate Linux, Beginner Linux, Bash, Tutorials, Hints
Systemctl, Zsh(future-work), I3-WM, DWM, git, gnome, softlinks/symbolic links, hardlinks, tmpfs / temporary file system / ram, steam,
vim / nvim }

{Date: 2021-03-06}

{ Author: https://github.com/srmfx/ || Github Page: https://github.com/srmfx/ArchlinuxQuickthrough }
------------------------------
==================================================END-METADATA====================================
==================================================OLD TODO LIST====================================
OLD TODO LIST: 

Write something about daemon.conf

CHECK LATER:
	MONITORING TOOLS

ESP = EFI SYSTEM PARTITION

-0 Talk about ACLs and Extended Attributes

-1 - Write something about weechat
	Explain how to safely connect and use Internet Relay Chat through tsl and ssl
		Adding a server
		Joinning channels on different servers
0 - What's the difference between megabytes, mebibytes, kilobytes, kibibytes?
	^ Type "$free --help" for more info
2 - add more text/topics to:
		WRITE ABOUT HOW CIRCUIT LUBE CAN SAVE SUPPOSEDLY BROKEN STABILIZER OR PSU
		WRITE SOMETHING ABOUT CPU DELID
			https://www.youtube.com/watch?v=zHPkviUv5BM - My CPU dropped *20 DEGREES* after my first delidding 
			https://www.youtube.com/watch?v=Gm2AgdGLFSs - How to Delid a CPU for Better Performance 
			https://www.youtube.com/watch?v=9bfEgVGYXow - CPU Delidding Tutorial - i7 4790k (Vice method) & Liquid Metal
				^ Required Items:
					1 - Isopropyl Alchohol
					2 - Mini Vice(Deliding Tool)
					3 - Thermal Paste(TIM - Thermal Interface Material)
					4 - Liquid Metal(Conductonaut)
					5 - Microfiber cloth
		WRITE SOMETHING ABOUT ATX, MICRO-ATX, MINI-ITX, EATX AND LIST EACH OF THEIR DIFFERENCES
			https://www.youtube.com/watch?v=U4wHRubdDmc
		WRITE SOMETHING ABOUT MAKEFILES: $MAN MAKE
		WRITE SOMETHING FILESYSTEM HIERARCHY STANDARD
			https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard


		WRITE SOMETHING ABOUT IWD/IWCTL
		WRITE SOMETHING ABOUT VIM EX MODE
		EXPLAIN WHY SMALL FILES ARE TRANSFERRED AT 36~45 Mbps RATES on HARD DISKS WHILE LARGER FILES 
			^ ARE TRANSFERRED AT FASTER RATES OF 125~500 Mbps EXPLAIN ABOUT ZSTD, LZE and COMPRESSION RATE
			^ Done: The hard-drive transfers different chunks of blocks at different speeds,
			if a file is too bog, it'll be transferred in much larger chunks of data,
			small size files will be transferred in smaller chunks of data.
			larger chunks of data usually can be transfered at faster speeds.
			while contiguous small blocks of data will be transferred at slower speeds.

			That means 1 file of 10GB will be transferred faster than 500 files of 10GB total when summed.
			so even if they're the same size, the 1 file will be faster copied.
		EXPLAIN A LITTLE ABOUT FILESYSTEMS(Hammer(Pseudo Filesystems), BTRFS, Ext4, NTFS, ZFS, OpenZFS(Pseudo-Filesystems)
		EXPLAIN ABOUT VIRTUAL FILESYSTEMS(AVFS, GVFS/VFS, etc)
		PSUDO FILES: sysfs / procfs
		CUSTOMIZED KERNEL MICROCODE + INITRAMFS
		BASIC SHELL TUTORIAL SECTIONI ISN'T A SHELL TUTORIAL
		TALK ABOUT HOW TROUBLESOME NVIDIA DRIVERS CAN BE

		*New:
		TALK ABOUT XINIT, STARTX AND THE IMPORTANCE OF PACMAN -Qi KNOWING PACKAGE DEPENDENCIES AND OPTIONAL DEPENDENCIES
		UPDATE RECOMMENDED PACKAGES
			Add: Konqueror
			Add: Veracrypt
		TALK ABOUT RUNLEVEL, LOGLEVEL, RESTORE AND RECOVER OPTIONS ON /ETC/GRUB.CFG
		TALK ABOUT HINTS ON HOW TO RECOVER SYSTEM FROM FUTURE PROBLEMS BY CREATING AN EXTRA BOOTABLE PARTITION/DISK THAT HAS INSTALLATION MEDIA FILES STORED IN IT

3 - CHECK ALL TODO LIST THROUGH OUT THE DOCUMENTS;

==================================================END-OLD-TODOLIST====================================
DOCUMENT:
-----------------------

-----------------------
nproc			#Checks how many cores are available on the current system,
			doesn't show many are being used.
lscpu			#Lists how many CPUs are available, and how many are being used
-----------------------
program settings are located in  /usr/share/ or /usr/share/etc/ or /usr/share/
~/.config

Some program settings can be found in: ~/.local/share
Other settings can be found in: ~/.cache/

========================================
SOMETHING ABOUT MOUNTING DISKS OR DRIVES (VERY OLD)
/etc/fstab is the configuration file for auto-mounting disks and drives on your system,
you can use mount, gparted or gnome-disks for setting it up.

It's important to note that only admins can make modifications to /etc/fstab due to the filesystem access permissions.
if you run 'ls -la /etc/fstab' you'll see only root has permissions to touch it.

If you booted your system into a Graphical User Environment, you can run either gparted or gnome-disks as admin, 
for auto-mounting options of hard disks:
	$ sudo gnome-disks
		or
	$ sudo gparted

These programs will automatically setup the /etc/fstab configurtion file.

========================================
========================================
SOME PROGRAMS NOT RUNNING:
Check if you have access permission on /usr/bin/ as follows: 
ls -la /usr/bin/program_name

Permissions needs at least to be: "-rwxr-xr-x" for the given program name you want to open.
your username should also appear when using 'ls -la'.
-----------------------------------------------------
================================================================================
ABOUT MASKED SERVICES
systemctl unmask gdm	#WARNING - only use mask/unmask if you know EXACTLY what you're doing

You can mask a service. This is like disabling a service, but on steroids. It not only makes sure that service is not started automatically anymore, but even ensures that a service cannot even be started manually anymore. This is a bit of a hidden feature in systemd, since it is not commonly useful and might be confusing the user. But here's how you do it:


1 - use this to start or stop a service:
	This will start / stop a service, but will not persist throughout multiple system boots.
	# systemctl start <service_name>
		or
	# systemctl stop <service_name>

2 - use this to enable or disable a service throughout multiple system boots:
use systemctl enable | disable in order to automatically start a service everytime on boot
	# systemctl enable <service_name>
		or
	# systemctl disable <service_name>

3 - use this to check status of a service:
	# systemctl status <service_name>


Note: In all cases mentioned above, if you're using 'zsh' you can press TAB Key for auto-completing the service name.
-----------------------------------------------------
================================================================================
--
systemctl daemon-reload
--
---------------------
========================================
DOWNLOADING VIDEOS FROM YOUTUBE:

youtube-dl --extract-audio --audio-format mp3 <url>	      #Downloads youtube video mentioned on URL as an .MP3 audio-only file
youtube-dl <url>					      #Downloads url as a video
---------------------
========================================
SYSTEM BOOT INSTRUCTIONS FOR ARCHLINUX:
After installing archlinux base system, 
you'll be prompted to install Video-Drivers, Xorg Server, Display Managers and Desktop Environments from the arch wiki page;
After installing a desktop environment you might find yourself on the Bash Shell after logging in, not sure what to do next.

These are the commands you should use to boot your OS from the BASH Shell:

# systemctl start firewalld				#Starts the firewall service
# systemctl start dhcpcd				#Starts DHCP service for Dynamic Host Configuration Protocol
								(Fetches Automatic IP from Router using the DHCPCD service)
then finally:							
# systemctl start sddm				#Starts SDDM Display Manager
	or
# systemctl start gdm				#Starts GDM Display Manager

then you can login and use your Desktop Environment of Choice
----------------------
========================================
---no idea what this is, it was just sitting here---
"telinit 3" followed by "telinit 5"
---------------------
Firewall Instructions - Very Old - 
	VERY IMPORTANT: Prefer iptables over firewalld, avoid using this.
# systemctl start firewalld
# firewalld
# firewall-config

---OBS FIREWALL:
Linux Kernel Firewall Config has two tabs for setting door open: 
1 - Runtime
2 - Permanent

Changes made to the permanent tab will be kept forever,
while runtime will just keep the settings until the system or the firewall is restarted, then it'll procede to load the default permanent settings.
--------------------
----------------
Links:
Persistent USB Live archlinux
https://wiki.archlinux.org/index.php/Install_Arch_Linux_on_a_removable_medium
----------------
This will activate the Login Manager Permanently, avoid use unless you don't want to boot into CLI Anymore on archlinux!
systemctl enable sddm.service
systemctl enable NetworkManager.service
--
yay list <package_name>
--
sudo pacman -Sc    #Removes all unused packages
sudo blkid         #shows all available disks for mount/unmount

sudo mv /etc/X11/xorg.conf /etc/X11/xorg.conf.bck1
------
chattr 				#ALLOWS TO SET IMMUTABLE FILES OPTION 
watch -n -n1 nvidia-smi 	#runs nvidia-smi every 1sec, refreshing it on terminal
htop  				#shows memory usage
------
================================================================================
UNABLE TO LOGIN ON USER ACCOUNT:
--known Su / Sudo Issue:
sometimes you won't be able to login into your user account if you miss it too many times!
there will be a cooldown time that you'll need to wait in order to login again.
if you try to login from terminal, you'll never know the reason, use this as admin to reset the lock:

	1 - Login as system administrator:
		$ su

	2 (Alternative) - Login as system administrator using 'sudo':
		$ sudo -iu root

	3 - Use 'faillock' to reset the lock:
		# faillock --reset --user <username>

		Note: remember to exit 'root' when you're done:
			# exit

	4 (Optional) - Checking faillock status
		$ faillock
------
================================================================================
GNOME TWEAK SETTINGS FOR PROGRAM STARTUP:
	On Gnome, go to TWEAKS, and then set your application to run on boot
	at Startup Applications!
------
$ pwd         		#shows current directory
  or
$ echo $PWD

$ echo $OLDPWD		#shows previous directory

mount all avaiable disks:
	# mount -a 	#this will only mount disks setup on /etc/fstab

----------------------------------------
================================================================================
SETTING KEYBOARD LAYOUT ( NEWER 2023):
FIXING KEYBOARD LAYOUT ( NEWER 2023 ):
Mandatory Step:
	setxkbmap -model abnt2 -layout br -variant abnt2
	^ Mandatory: add this line to /etc/X11/xinit/xinitrc as the final line
	also add this if using sddm as Display Manager:

	[file: /usr/share/sddm/scripts/Xsetup]
			localectl set-x11-keymap br abnt2 abnt2
			setxkbmap -model abnt2 -layout br -variant abnt2
	[/file]
	^ VERY IMPORTANT: 2nd line may not be needed anylonger
	VERY IMPORTANT 2: also read these 2 updated topics: 
		SETTING UP KEYBOARD | KEYBOARD LAYOUT CONFIGURATION ( 2025 - UPDATED, SOME PARTS MANDATORY )
				AND
		FIXING SDDM AUTOMATICALLY CHANGING KEYBOARD LAYOUT TO AMERICAN LAYOUT AT BOOT

	
	VERY IMPORTANT: the file "/usr/share/X11/xkb/rules/base.lst" contain a list of all supported keyboard layouts, sections are listed in !variant, !model, !layout, !rules, !options

To make changes permanent to x11:
localectl [--no-convert] set-x11-keymap layout [model [variant [options]]]
	or
localectl set-x11-keymap br abnt2 abnt2
	and
localectl set-keymap br-abnt2.map.gz

USING THE ONE FROM BELOW WORKS:
localectl set-keymap --no-convert keymap
KEYMAP:br-abnt2.map.gz


^ although, it first might be needed to remove these 
files and reboot the PC:
rm /etc/vconsole.conf
rm /etc/X11/xorg.conf.d/00-keyboard.conf

SETTING UP SYSTEM LANGUAGE | SYSTEM LOCALE (NOT MANDATORY FOR KEYBOARD LAYOUT)

	1(Optional) If you want, you can check available locale settings:
		$ vi /etc/locale.gen

		1.1(Optional) If you have a missing /etc/locale.gen, you can re-download it by installing glibc package:
			# pacman -S glibc

		1.2(Optional) You can list pre-generate locales:
			$ localectl list-locales
				or
			$ locale --all-locales

	2 -  Set locale using:
		VERY IMPORTANT: whenever you set a new locale, it'll define it as the main locale settings,
		so if you want en_US.utf-8 as the main locale, you have to type it for last:

		# localectl set-locale LANG=ja_JP.utf-8
		# localectl set-locale LANG=pt_BR.utf-8
		# localectl set-locale LANG=en_US.utf-8

		2.1 - (Alternative and Optional) You can edit locale straight from the /etc/locale.gen file
		Simply uncomment each locale you want by removing the '#' character from the beginning of each line
			$ vi /etc/locale.gen

	3 - Finally, generate a new locale.conf file:
		This will simply generate a new /etc/locale.conf file based on the choices you have made in step #2 above:
			# locale-gen

	4 - CHANGING SYSTEM LOCALE FOR PROTON AND WINE
		This is here, since some windows games may require japanese locale in order to work:

		4.0 - (Maybe optional) If you haven't done it already, generate a locale for 'ja_JP.utf-8'

		4.1 - (Maybe optional) Generate a new /etc/locale.conf for 'ja_JP.utf-8'
			# locale-gen

		4.2 - Install japanese fonts for wine
			$ winetricks cjkfonts

		4.3 - Launch wine:
			$ LC_ALL=ja_JP.UTF-8 LANG=ja_JP.UTF-8 wine ./game.exe

--explanation: - VERY IMPORTANT - 
The problem with the keyboard layout on terminal happens because
there is no default application to create keyboard settings for all SHELLS Programs in the system.

It's required that the user sets the keyboard layout configuration for each one of their SHELL Programs
on their Global Configuration Files, as opposed to set it on their local configuration files!

for ex.: BASH has local configuration files set for each user on ~/.bashrc,
that means each user in the system will have their own .bashrc file with their own settings, environment variables, etc

It's possible, however, to set a global configuration file for all BASH User Shells in the system at:
	/etc/profile.d/<filename>.sh

all files with .sh will be recognized by bash(on that folder) as a bash script and therefore loaded up to the tlc interpreter
as soon as a BASH SHELL Session is executed.

!!CONFIGURATION FOR OTHER SHELL PROGRAMS LIKE ZSH MIGHT VARY!!!

SOLUTION: Append these lines at the end your ~/.bashrc file:
		localectl set-x11-keymap br abnt2 abnt2
		localectl set-keymap br-abnt2.map.gz

	^VERY IMPORTANT: as of 2023, this is not needed anylonger, only use this if you go through problems	

once you save, you might need to: source ~/.bashrc
this will apply changes immediatly.

Type ctrl+alt+F3 to test it!!!
----------------------------------------------
EACH DESKTOP ENVIRONMENT HAVE THEIR OWN KEYBOARD SETTINGS:
they're usually set to US Keyboards - NEEDSCHECK

gnome offers an easy way: gnome-settings
you can set: Portuguese(Brazil) for an abnt2 settings.

Personal Note: I followed all Archlinux Wiki steps to properly set Brazillian Keyboard and it still didn't work.

----EXPLANATION: -- VERY IMPORTANT --
The problem can probably solved by using xdg-utils to set a global default layout keyboard configuration for all Desktop Environments 
Available in the system.
----------------------------------------------
================================================================================
PACMAN, THE MAIN ARCHLINUX PACKAGE MANAGER:
	ADVICE ON PACKAGE MIRRORS
		In /etc/pacman.conf you can chose mirrors which pacman will use
		for fetching requested packages. 

		Advise to not enable 'testing' nor 'staging' repositories!
		in the past, AUR Packages updates did fetch packages from testing as dependencies when doing package 
		upgrading, this thus led to some packages from 'core' to stop working because other packages who dependended 
		on an older(more stable) version were now forced to use the now broken updated package(s) from 'testing'.

		VERY IMPORTANT: Avoid Testing and Staging repositories!

	WHAT IS PACKAGE?
		A Package may contain a program, library or tool which is usually explicitely installed by the user, the system
		or be installed as dependency for another program. The package contains all instructions 
		on how the tool/program/library can be installed and how it should be configured on the given system.

		Since operating systems differ from one another, each of them have their own PACKAGE MANAGER.

	PACKAGE STATUS
		
		OUT-OF-DATE PACKAGES
			These are packages in which the tools/programs they contain are getting updated/maintained
			by the actual developers, but the package maintainer isn't updating nor maintainning it
			for very long periods of time.

			If the package is broken and doesn't work, then go to "https://aur.archlinux.org/", 
			search for the package and flag it as 'out-of-date',
			doing as such will help the package maintainer to get notified about the issue.

			Note that if a package is 'orphaned' on the repositories, the chances of it getting fixed
			is very small.


		ORPHANED PACKAGES
			A Package can be both orphaned on the system it's installed and also on the repositories.

		When a package is orphaned on the system, it means that the package has been installed as a 
		dependency for another package AND not explicitly installed by neither the user nor the system, 

		The 'orphaning' can happen when a package has removed all it's dependency it had with another package and 
		no other system packages depends on the given package anymore.

		In the other hand, when a package is orphaned on the repositories, it means that the tool/program
		contained in the package isn't being maintained anylonger. The package can still be found in the repos,
		it can still be installed, but it's unwise to do so since it may contain serious bugs or security breaches.

		Typing 'yay --show -s' will list remote orphaned packages on the system.
		Typing 'yay -Qtdt' will list local orphaned packages on the system.


		It'd be wise to remove locally orphaned packages, since they have zero-use:
			$ yay -Rns  $(yay -Qtdq);
			^ do not mistake -Qtdq with -Qtdt !!!
			^ this has to be ran multiple times manually to ensure all 
			^ local orphaned packages are fully removed.

		As for 'remote orphaned packages', they have to be removed manually!
			yay -Rns <package_name>

		it's wise to remove both from the system, but you may wait for a 'remote orphaned package'
		to either rott or to be brought back to life.
			
		You can check how long a package has been 'remote orphaned' by looking at the 'Last Update' date:
			pacman -Qi <package_name>
				or
			yay -Qi <package_name>

		The longer a 'remote orphaned' package has been without updates, 
		the worse it is to have it installed on the system.

	CHECKING PACMAN HISTORY
		$cat /var/log/pacman.log

To check directories being used for pacman type: $pacman -v
those directories are important, specially log directories and cache directory, 
since it allows you to re-install previous working packages when they break.

	Conf File : /etc/pacman.conf
	DB Path   : /var/lib/pacman/
	Cache Dirs: /var/cache/pacman/pkg/  
	Lock File : /var/lib/pacman/db.lck
	Log File  : /var/log/pacman.log

HOW TO RECOVER LOST PACKAGES HINTS AND TIPS
	1 - Use pacman -Qkk to find out which packages are broken:

		# pacman -Qqk > /home/<your_username>/pacman.qqk.log	#Lists only packages whose files are missing files, it uses -q for quite mode.
									^ This has to be executed as root, to check for all files in the system.
				or
		# pacman -Qkk
	
	2 - When executing and redirecting to a file using:
		# pacman -Qqk > /home/<your_username>/pacman.qqk.log	#Lists only packages whose files are missing files, it uses -q for quite mode.
									^ This has to be executed as root, to check for all files in the system.
					or
		# pacman -Qkk > ~/pacman.log.qkk 


	When using -Qkk instead of -Qqk :
		It's important to understand that messages will be left as warnings as a hint to which packages and which problems they are having,
	these warning messages however never get into the file ~/pacman.log.qkk through redirection, they're only left at the terminal screen, it's important to read them, because pacman.log.qkk will not list which problems the packages present; for example: usually permission mismatch isn't an issue, but missing or corrupt files are. Example: 

		warning: libisoburn: /usr/bin/xorriso-tcltk (No such file or directory)
		warning: libplist: /usr/lib/python3.10/site-packages/plist.so (No such file or directory)
		warning: libproxy: /usr/lib/python3.10/site-packages/libproxy.py (No such file or directory)

	3 - (For -Qkk only) You can parse files using REGEX once th output of pacman -Qkk has been redirected to a file:
		$ cat ~/pacman.log.qkk | less

		3.1 - Then press '/' and type:
			[0-9] altered

	4 - Properly recovering packages with missing files:
		# pacman -Sy --overwrite '*' <package_name>
			or
		$ yay -Sy --overwrite '*' <package_name>

PREVENTING DESTROYING YOUR SYSTEM PACKAGES
	Do not partial upgrades or partial downgrade any packages that have not been explicitely installed by the user,
you can find whether a package has been explicitely installed by the user by doing: 
	$pacman -Qi <package_name>

The following line should be found:
	Install Reason  : Explicitly installed

If a package has not been explicitly installed, avoid partially upgrading or downgrading them.
The reason is some other software or package likely depends on that version, partially downgrading or upgrading it 
may break important parts of your system.

This is why some users recommend new users to always make full-system updates instead of partial upgrades.
However, if a conflict has appeared during a full-system upgrade, pacman will auto-resolve if partial upgrade is possible and
if it's not possible it will block the user from following with a system update until the package conflict is resolved.

Please note that only in the above case it is possible to partially upgrade your system.
Avoid using tutorials/instructions on the web that tell you to remove/downgrade packages that have not been explicitely 
installed by the user, these instructions will likely not work for you and will leave your system in an even worse condition.

------
USEFUL CHAIN PACMAN COMMANDS:

# pacman -Qkk | cut -f1 -d: | sort -u | grep python | xargs sudo pacman -Syu				#Fetches all packages with name python in it

$ cat ~/pacman.qkk.log | cut -f1 -d: | sort -u | grep python | xargs sudo pacman -Sy 			#Re-Downloads all python packages found in the system

$ pacman -Si lutris | grep Depends.On.*: | grep -Po 'python\S+' | xargs sudo pacman -S --noconfirm  	#Re-Downloads all lutris dependencies related to Python Packages

# pacman -Qkk 2>/dev/null | grep -P '(?<!\s0 altered file)' | xargs echo				#Fetches all packages that have Altered Files and Prints on the screen
													 ^ Files in the system

$ yay -Si archey4 | grep Depends.On | cut -f2 -d: | xargs sudo pacman -Sy --noconfirm --overwrite '*' 

$ pacman -Qi ranger | awk '/^Optional Deps/,/^Required By/' | grep -Po '(^\s+\K[^\s:]+|^Opt[^:]+: \K[^\s:]+)' | xargs echo  #Fetches all optional packages for a given package - WORKS

	other tips 2022.3:
		$pacman -sS python | grep -P "installed"	#Lists all installed python packages(useful for re-installing if needed)
		# pacman -Qkk | grep -P "[1-9] altered file"	#Lists all altetered packages in a given system(useful for re-installing if needed)

-----
Effectively Download All Broken Packages:
Solution A:
	pacman -Qk 2>/dev/null | grep -v ' 0 missing files' | cut -d: -f1 |
	    while read -r package; do
		sudo pacman -S "$package" --overwrite "*" --noconfirm
	    done
Solution B:
	pacman -S "$(pacman -Qk 2>/dev/null | grep -v ' 0 missing files' | cut -d: -f1)" --overwrite "*" --noconfirm
-----
-----
LOOKING FOR SIMILAR PACKAGES ON THE PACKAGE MANAGER REPOSITORY
	Meant for archlinux only, but can likely be adapted for others
		$ pacman -sS awk | grep -C 3 -P -i "\bawk\b|\bawk.*"
		^ ugrep can be used in place of 'grep' when installed on the system.
		^ looks for packages named 'awk', then greps/ugreps for alternative options of the same package
		^ GOOD REGEX, May return uwanted but still related results.
	

		$ yay -sS grep | ugrep -P -i -C 3 "\bgrep\b|\bgrep.*|(?<=\w)grep\b"
		^ uses 'yay' package manager(AUR Repository)
		^ BETTER REGEX for searching related packages.
----
------------------------------
PACMAN COMMANDS
//YAY COMMANDS: ONLY AVAILABLE ON YAY
yay -P --news --news		#Shows all news from archlinux official website
	or
yay -Pww

yay -P --stats			#Shows package status

//-S: SYNC DATABASES(REMOTE/ONLINE)
$pacman -Ss <Regex_Expr> 	#Searches repositories(sync databases) for given regex expression(package name & description) 
	or(same)
$pacman -sS <Regex_Expr>	#Searches repositories(sync databases) for the given regex expression(package name & description) 
					^ -Ss enforces a regex search for either name or package description.
					^ -S only enforces a package name search, so exact name needs to be given.

#pacman -S <package name>	#Installs package
#pacman -Sw			#Downloads package and any related dependency, but DOES NOT INSTALLS ANYTHING.
					^ Use -U for installing the downloaded packages located on /var/cache/pacman/pkg
					^ Check man page for the proper pacman cache directory.
$pacman -Si			#Displays information available REMOTE PACKAGES
$yay --show -s			#Show System Wide Package Status(VERY GOOD)
#pacman -Rns $(pacman -Qtdq)	#Helps keeping the System Clean from orphaned unused packages by removing them from system
$yay -Rns $(yay -Qtdq)		#Same, but includes AUR Packages on the list as well.
				^VERY IMPORTANT:
					BECAREFUL not to use 'Qtdt' instead of 'Qtdq', since those mean actual different 
					things.
				^VERY IMPORTANT: 
					These commands have to be run multiple times in order to clean up everything.

#pacman -Syu [package name] 	#Updates mirrors and systems packages, installs any package in [package name]
#pacman -Syu --noconfirm	#Force pacman to say 'yes' to all questions
#pacman -Syyuw			#Downloads updated packages for installing at another time
#pacman -Sy			#Updates mirror
#pacman -Sy <package_name>	#Updates mirror and installs package
#pacman -Su			#Updates system without updating mirrors
				^ this can result in unupdated packages, using -Syu guarantees mirror database gets update
				and gets the most recent packages

#pacman -Scc			#Removes all cached packages from pacman
#pacman -Syu $(cat <filename>)	#Installs all packages listed in a given text file.

#pacman -Sf <package_name>	#Does the same as --overwrite for old pacman versions. Installs package and overwrite all existing files

#pacman -S <package_name> --overwrite <'<path>'>		#Resolves conflicting files when installing a given package
							ex.: pacman -S nvidia --overwrite '/usr/lib/*'
								^ overwrites all conflicting files that exists in /usr/lib/.
							ex.2: pacman -S nvidia --overwrite '/usr/shared/*' --overwrite '/usr/lib/*'
								^ overwrites all conflicting files that exists in both 
								  directories.
							ex.3: pacman -S nvidia --overwrite '*'
								^ use this, most effective

#pacman -S --noconfirm <package_name>			#Installs package without confirmation

------------------------------
------------------------------
//-R: FOR REMOVING PACKAGE
#pacman -Rns $(pacman -Qtdq)	#Removes orphaned unused packages | REMEMBER THIS!
#pacman -R <program name> 	#removes package
#pacman -Rns <program name>	#Removes packages along with it's dependencies(if not being used by other packages| REMOVES OPTIONAL PACKAGES)
#pacman -Rns $(pacman -Qtdq)   	#Removes outdated/unused packages ?
				^ Required after removing package without dependency removal in 'pacman -R'
#pacman -Rnu <package_name>	#Removes packages without breaking dependencies, leaving the important ones on the system
				^ VERY USEFUL for removing useless packages.


------------------------------
------------------------------
//-Q: LOCAL DATABASE
$pacman -Q <program name>  	#searches for installed packages(exact name search)
$pacman -Qo <program name> 	#searches and checks if given package is a dependency of some other packager
$pacman -Qqo "<path_to_data>"	#Lists all  affected packages for a given file/directory
				^ Example:
					pacman -Qqo "/etc/mkinitcpio.d/"
				^ VERY IMPORTANT.

$pacman -Qqdt			#Lists all packages installed as dependencies but no longer required by any packages
				^ same as 'pacman -Qtdq'
$pacman -Qdt			#Lists Orphan Packages
$pacman -Qi <package_name>	#Gives information on the given package name
				^ Also lists it's dependency packages along with other informations
$pacman -Qikk <package_name>	#Searches Locally Installed packages, gives information on it and checks if local package files have been
				^ Installed Correctly.
$pacman -Q <package_name>	#Searches for exact package name
$pacman -Qe			#Lists packages specifically installed by the user.
$pacman -Qe >> <filename>	#Creates a given file that contains all packages installed by the user
$pacman -Qs <Regex_Expr>	#Searches for given regex expression(Package Name & Description) in the local database
				^ Searches local packages
$pacman -Qo <uri_to_file>	#Searches installed packages for owner(s) of the given file(s) or directory.
				^ ex.: $pacman -Qo /etc/X11
				^ VERY IMPORTANT: very useful when you want to know what a given file in your hard disks belongs to.
				^ check -Fx, works the same for sync databases, but is not supported by YAY Package Manager yet.
$pacman -Qg <group_name>	#Displays all packages that are actual members of the given group.
$pacman -Ql <package_name>	#List all files owned by a given package
				^ check -Fl, works the same, but for sync databases.

# pacman -Qqk 						#Lists only packages whose files are missing files, it uses -q for quite mode.
							^ This has to be executed as root, to check for all files in the system.
# pacman -Qkk						#Checks all packages integrity
							^ Specifying 'k' twice will perform detailed checking(permission, file size, modification time)
# pacman -Qkk | grep "No such file or directory"	#Allow retrieving only packages that have been accidently deleted



------------------------------
------------------------------
//-U ALLOWS TO INSTALL PACKAGES FROM LOCAL SYSTEM FILES(FROM PACMAN/AUR/ALA REPOSITORY)
#pacman -U <local_uri_to_package>	#Installs package from a local URI
					^ ex.: #pacman -U ~/Downloads/package/<package_name>
					^ Downloads any required dependency from 'sync' database.
#pacman -Uw				#Downloads package dependencies from 'sync' database, but does not install anything - NEEDS TO CHECK.


------------------------------
------------------------------
//-F SEARCHES FILES IN THE SYNC DATABASES:
2023 note: This isn't yet supported by YAY Package Manager!!!

#pacman -Fy				#Updates file database
$pacman -F <filename>			#Queries 'sync' database for packages that own the given filename/directory
					^ VERY IMPORTANT: very useful to know if there are other packages that use the same file,
					this can help solving conflict issues when/if different packages are using the same config file.

$pacman -Fx <regex_expr>		#Interprets query as regex-expressions.			       ^ Lists which packages owns the given folder/directory/file - shows package ownership of files/directory

$pacman -Fl <package_name>		#Lists all files owned by given package name
					^ Check -Ql, works the same, but for local database. 



CHECK MAN PAGE FOR MORE INFO!

------------------------------
------------------------------
ABOUT PACMAN:
--------------PACMAN -R:
Removes a package by checking if any other packages depends on it,
if they do, removal will fail; after dependecy removals, 
you should manually remove all the unused packages due to it's removal from the systemd mentioned below:

In order to remove all unused/outdated packages, use:
pacman -Rns $(pacman -Qtdq)   	#Removes outdated/unused packages

-------------PACMAN -Q:
Usually runs against the local repository, can list local installed packages.

-------------PACMAN -S:
Runs against a remote repository, usually the mirrors set in /etc/pacman.conf
used to install / fetch information from remotely located packages
-----------------
Create a List of Installed Packages:

 $ pacman -Q
Get Detailed Information about Packages:

$ pacman –Qi bash
Calculate the Total Number of Packages Installed in Your System:

$ pacman -Q | wc –l
Export a List of Installed Packages:

$ pacman -Q > packages.txt
-----------------
FILES:
	/etc/pacman.conf	#Allows to add/remove repos, beware there isn't a way to add AUR Repository in here.
				#Yay package manage is required for the AUR repository
				#Hint.: you can enable testing and multilib-testing repos
				#Repository EXTRA is only available on AUR


		VERY IMPORTANT: Advise to not enable 'testing' nor 'staging' repositories,
		in the past, AUR Packages updates did fetch packages from testing as dependencies when upgrading, 
		thus this led to some packages from 'core' to stop working because other packages who dependended on
		an older(more stable) version were now forced to use the now broken updated package(s) from 'testing'.


-----------------
GPME ERROR NO DATA || INVALID OR CORRUPTED DATABASE (PGP SIGNATURE) || PERFORMANCE
	Sometimes the Signature Database may be corrupted, if so follow the following steps:

	0) sudo -iu root
		or
	   su -- root

	1) pacman-key --init & pacman-key --populate archlinux

	2) mv /var/lib/pacman/sync /var/lib/pacman/sync-damaged

	3) pacman -Syu

VERY IMPORTANT: For some reason, this made pacman faster, if you notice a slow pacman you can use this from
time to time.
-----------------
BLACKLISTING PACKAGE AND PACKAGE GROUPS FROM PACMAN UPDATE:
1 - Open the following file for edit:
	$ sudo nvim /etc/pacman.conf

2 - Add/Uncomment IgnorePkg line or IgnoreGroup
 #IgnorePkg =
 IgnorePkg = vlc
 #IgnoreGroup =

3 - If you want to ignore whole groups of packages, you can just add them to IgnoreGroup instead:
ex.:	IgnoreGroup = plasma-desktop

4 - Save the file!
5 - DONE!!!

Alternatively:  $ sudo pacman -Syu --ignoregroup=plasma-desktop
				or
		$ sudo pacman -Syu --ignore=vlc


This is important, 'cus sometimes one or other package will break your system when updated!


-----Note.: my custom ignore pkg 2022: 
IgnorePkg   = nvidia-utils nvidia nvidia-dkms nvidia-settings lib32-nvidia-utils lib32-opencl-nvidia libxnvctrl opencl-nvidia zfs-dkms

---------------------------------------------
================================================================================
HOW TO BUILD PACKAGES MANUALLY

mkdir .programdir
cd .programdir/
git clone http://xxx.xxx/program.git
chown -R username:users program/
cd program/
makepkg -si

#makepkg -si can't run as sudo
------------------------------------------------
2020.1-T:
VERY OLD NVIM TUTORIAL:----------------------------
VIM Plugin Directory:
VIM has it's plugin directory located in ~/.vim/plugin
both directories need to be created manually after installing VIM
Plugins need to be executed with their file names: hjkl.vim   COMMAND: :HJKL

VIM Hotkeys:
	todolist:
		How to do case-sensitive and insensitive searches
		How to do tab traversing

w - skips a word
e - jumps to the last character of the current word
ge - moves to the last character of the previous word
b - goes back 1 word
l - jump skip a letter
*By default, VIM Considers non-alpha characters as full words
*you can also use their CAPS LOCK version - which allows for skipping non-alpha characters in a single word
	example, try it: non-alpha - non-alpha
*you can also combine these above with numbers as in 9l to jump-skip 9 letters

	Cursor Movement:
		HJKL - Left, Down, Up, Right respectively 
		^note.: Both HJKL and WEB are the most important things in VIM.
		        They're the ones responsible for turning life easier with VIM.

		2020.2 ----------------
		^F or ^D - Moves Forward Full-screen or Half-screen
		^B or ^U - Moves back Full-screen or Half-screen

	Copy and Paste:
	"+y  or "*y   - Copies selected text to the
			clippboard #only works in Neovim and Gvim
		"*y   - Copies selection to X11 Clippboard,
			which can then be pasted with <MiddleMouse> click

	File Info:
		^G - Information on current working file

	REDO / UNDO:
		ctrl+r   - Redoes
		u	 - Undoes

	Cursor History:
		ctrl+i	#Moves to previous cursor position   
		ctrl+o  #Moves to next cursor position
		:jump   #Shows list of all current jumpings/cursor positioning 
			^ press w for a list of commands

		^It's a recursive list, so the current cursor position has position #0 on the list
		 the previous has position #1, so always start with ctrl+o when you want to go back to
		 a previous position in the document and use cltr+i for returning to the previous current
		 position you were.

	Open File Commands:
	:o file_name - opens a file(closes the current one)
	:tabnew file_name - opens file on a new tab
	gt or gT - Switches tab from next to previous tab 
	:tabfirst - switches to the first tab
	:tablast - switches to the last tab
	:tabn - switches to the next tab
	:tabp - switches to the previous tab
	:tabnew /home/<username>/  - will open a list of path directory

	Press M to go to Midscreen:
		M - midscreen

	Automated Printing:
	30i  #allows - character to be print 30 times - WORKING in current VIM, check bellow

	Search Single Character:
	3fq or 3Fq    f or F allows you to find next/previous character, combining it with a number to make 
	it find the 3rd next or previous 'q' character 

	Special inserts:
	:r [filename]	Insert the file [filename] below the cursor
	:r ![command]	Execute [command] and insert its output below the cursor

	Setting Bookmarks:
	'{a..Z} - Navigate through bookmars
	:marks - Lists all bookmarks available
	m{a..Z} - creates bookmark at cursor position
	Obs.: if having 2 files open, you must save all modified files that have not yet been saved
	because VIM uses a single bookmark file for all textfiles, thus requiring that to be done.
	Otherwise, VIM will promptly ask you whether you want to open those files from swap or original
	document. - becareful when doing this.

	Obs.: Each Terminal has their own Bookmark Files, and Each Vim version such as nvim or gvim do
	too as well.
		
	Modify Text: - 2020.2
	vgu - lower case all selected characters
	vgU - upper case all selected characters
	~~ - Switch Camel Case
	> - Shift right
	< - Shift left
	! - Filter through an external command - NO IDEA

	^ * Use these side by side with Visual Mode for benefits

	Insert Text:	
	a	Insert text after the cursor
	A	Insert text at the end of the line
	i	Insert text before the cursor
	o	Begin a new line below cursor
	O	Begin a new line above cursor

	Finds Word Under Cursor:
	#    #Previous word
	*    #Next Word
	:noh  #To clear highlight search results

	Moving Between BEGIN and END:
	gg    #moves to beginning of the file
	G    #moves to the end of the file
	3G   #moves to line 3 of the file

	Copies Current Line:
	yy - Copies current line
	y - Copies N words on N positions, Example: y2b - Yanks 2 previous words
	"xyy - Copy the current lines into register x
	p - paste storage buffer after current line
	P - paste storage buffer before current line
	"xp - paste from register x after current line
	"xP - paste from register x before current line
	v2jy - selects 2 lines of text from cursor position and Yanks it

	Pastes Copied Text:
	p - Pastes Copied Text

	Combining HJKL with numbers:
	3k - moves the cursor UP 3 times
	3l - moves the cursor LEFT 3 times
	...  and so on

	Repeats Text:
		30i-	#changes NORMAL mode to INSERT mode and after ESC has been pressed
			It just repeats all the contents


	Search and Replace KEYS:
	:noh - to clear search highlights results
	/search_text - search document for search_text going forward
	?search_text - search document for search_text going backward
	n - move to the next instance of the result from the search
	N - move to the previous instance of the result
	:%s/original/replacement  - Search for the first occurrence of the string “original” and replace 
				    it with “replacement”
	:%s/original/replacement/g - Search and replace all occurrences of the string “original” with 
				     “replacement”
	:%s/original/replacement/gc - Search for all occurrences of the string “original” but ask for 
				     confirmation before replacing them with “replacement”

	The i after the g makes the search case insensitive. An I would make the search case sensitive. 
	The g makes the substitution global in the line. Without the g, it performs the substitution once 
	per line.

	Searching for Text:
		use / character and type TEXT to find that specific word, then ENTER Key, 
	*regex can also be used*
	also use n and N to move between Next and Previous after pressing ENTER.

	Finding the Next/Previous typen Letter - ON A SINGLE LINE -:
		'fA' will position the cursor at the next 'A' occurence
		'FA' will position the cursor at the previous 'A' character occurrence.
	Use ; and , to repeat last f or F operation.

	Append:
		press capsed 'A' to apend to the end of line.
	
	Replacing Words
		2rb - will replace the 2 next characters with the 'b' letter.
	Capsed 'R' will turn on REPLACE MODE for replacing entire text.
	
	Removing a Character:
		2x - will remove the next 2 characters

	Inserting New Line:
		Press uncapsed 'o' to create a new line right below the cursor.
		Press capsed 'O' to create a new line right above the cursor.
		Note to self: Both of them change VIM to INSERT mode just right after.

	Deleting Characters:
		Press 'x' to delete characters, can be combined with words
	Capsed X allows previous character to be deleted, while normal X deletes current 
	character on cursor.

	Go to Matching Parenthesis:
		Press % character to go to matching ( ) or { } or [ ] in a given line.
	
	Deleting Entire Words:
		Press 'd' to delete entire words, it's also allowed to combine it with numbers and 
	HJKL and WEB words(Capsed or Uncapsed). 

	For example, combining '3dw' will delete the next 3 words.
	while '3dw' allows you to delete the previous 3 words.
	Another example: '2dk' allows for deletion of the previous 2 lines.
	It also copies the content, so you can paste somewhere else using 'p'.

	Examples:	
		4dge - allows the 4 previous word to be deleted
		4db - " "
		dw - deletes single word
		db - deletes previous word
		dd - deletes an entire line without Switching to Insert Mode		
		D - deletes line from Current Cursor Position without Switching to Insert Mode
		S - deletes an entire line, switching to Insert Mode

	*Note to Self: using CAPSED 'D' will delete the rest of the entire line,
	combining uncapsed 'd' it with numbers + HJKL will allow for deletion of multiple lines
	from the current cursor point.

	Repeating the Previous Command:
		just press .

	Example: type d2w to delete the next 2 words, then keep pressing '.' to delete the rest 
	of the entire text.

	[n] [ MOVEMENT]	:
		Allows you to execute X Different Movement Y times.

	Clear Current Line; Change to Insert Mode:
		Just Press 'S'

	ci [MOVEMENT]
		No IDEA

	VIM MODES:
		INSERT MODE - Press i or Insert
		NORMAL MODE - Just Press ESC
		COMMAND MODE - From Normal Mode, Just press ':' character
		REPLACE MODE - press capsed R or Insert, r for single character replacement
		VISUAL MODE - press v or V or ^v, which is short for ctrl+v
				v   #visual mode for text selection
				V   #visual line mode for text line selection
				^v  #visual block mode for block text selection

		DIFF MODE - See below in advanced section
		TERMINAL MODE - :terminal

	Visual mode is used to Select Text for Yanking(Copy/Pasting) within VIM.

--------FINAL BASIC NOTES/HINTS:            -----------------------------
	*the mark ' is used everytime you switch modes, 

	*todolist: :help bookmarks
	
	*Press CTRL+k on command mode, then press a key to find it's command code

	*Everytime you want to copy something into command-mode, do this:
		1. Go into visual mode: v
		2. Yank/Copy: y
		3. Go into command mode: :
		4. Press ctrl+r and shift+"
	^that will past the last content you yanked(or deleted) from the register: "
	this might seem like nothing, but is very useful in VIM, mainly for copy/pasting your custom mappings.

	*Use #  and 'ctrl+w r' in command mode to get the last previous buffer path/directory
	*use % and 'ctrl+w r' in command mode to get current buffer path/directory

	*There's nothing better than VIM's Help System, just type in:
		:help :       #Gets help on all commands available in command mode
		:help gT      #Gets help on all available tab commands
		:help tabnew 

		*remember the word coming after :help needs effectively to be a command.

	*Take advantage of buffers terminal and multiple windows when studying something
	from school/university, for example, you can open documents/images from a terminal buffer
	at the same time as you're reading content in another buffer, all you have to is press
	:bprev :bnext (:bp or :bn) to go to previous to next buffer. With multiple windows
	you can do multi-task work at the terminal, for example: while you open a picture
	at one terminal window, the other is free to do stuff like copying, removing, or 
	even openning another program - check more on windows and buffer section.
	^The same can be done if you want to watch movies, read chat from IRSSI, and listen to music
	all at the same time.

	*When typing a command like :buffers or :registers press 'w' for a list of commands

	*Make a Mapping to automatically read and convert registers typen into TERMINAL MODE 
	that way you don't need to switch from TERMINAL MODE to NORMAL MODE and then back to 
	TERMINAL MODE again.

	:tnoremap <Esc> <C-\><C-n> #Makes Custom ESC key to leave Terminal mode - Optional

	:map <F2> v"*y<C-w>n:put <C-r>"<CR><END>v<HOME>"*y:bdelete!<CR>i<BS> <MiddleMouse>
	^ The command above pastes the content of a register into the terminal mode

	*It's possible to print last known register content with ""p
	*!todo-list: figure out how to get out of terminal mode using map
	*!todo-list: make a mapping for doing multiple commands
	Note to Self: CTRL-SHIFT+S is a terminal key, it doesn't exist in VIM!
	Because of that you need to use "*y  instead of "+y register, and use <MiddleMouse> to paste 
	content.

	CTRL+R in command-mode ':' allows you to type a letter to pick a register content

	*VIM Command Mode allows commands to be pipelined like this:
			Ex.: :tabnew | :terminal   #opens a new tab and then opens the terminal
			     :tabnew | terminal    # same as above
			     :vertical sb | terminal #opens a new terminal buffer on the same window
			     :sb | terminal  or <C-w>n or (CTRL-w n)

	*Note that whenever you delete something by mistake, you can just UNDO or REDO at your own
	volition with ctrl+r and u keys	

	* VIM uses a register called "" to store deleted/yanked(copied) content 
	so if you ever missplace a text, don't worry about it: just keep typing and when you're done
	delete it with 'dd', if you ever need to delete multiple lines in a row - without 
	cursor positioning - you can just press a number first and type 'dd'as in: 
			example:
				2dd
				2dj
				2dk

	* Pressing '.' in normal-mode allows last operation to be repeated.
	   Example: try switching to visual-mode, select and increase indentation with '>' 
	            for the next 4 lines:
					testtest
					testtest
					testtest
					testtest
						

	^Note.: Movement(WEBL) and Cursor Positioning(HJKL) aren't covered by '.' repeat operand.
	       In this case a macro/mapping might need to be created to cover this up.

	*Use Visual Mode to Select Text before making any other operations.
        This way Visual Mode can be used to easily select multiple blocks/lines of text,
		example: 
			v2j - selects 2 lines of text from cursor position

	*Use ^i  and ^o  to move between previous and next cursor position in case
	you move the cursor to wrong positioning by accident.

	*remember WEBL can relieve you from using CTRL + ARROW to move between words.
	but can only be used in NORMAL MODE.

	*capsed 'D' allows you to delete multiple lines, relieving you from VISUAL MODE
	*uncapsed 'd' allows you for deletion of multiple words, relieving you from VISUAL MODE
	*^ both of them are capable of deleting words/lines from the cursor point.
	*^ 'd' can be combined with HJKL(Movement) or WEB.

	*Append 'A' allows you to insert text at the end of line, relieving you from 
	cursor positioning.
	
	*Capsed 'S' is similar to Append 'A', erasing an entire line, but remaining at the beginning at
	INSERT MODE, relieving you from cursor positioning.
	
	*Capsed 'D' is also similar to both of them, but it deletes lines from the CURRENT CURSOR POSITION

		^The difference between 'D', 'S', and 'dd' is: 
			'D' deletes entire FROM Current Cursor Position.
			'S' deletes entire line REGARDLESS of Cursor Position, AND switches to INSERT MODE.
			'dd' deletes entire line REGARDLESS of Cursor Position, AND DOESN'T change to
				INSERT MODE.

	*You can Use REGISTERS(See advanced VIM), for example, Copy & Pasting full directories path 
	into VIM and it's TERMINAL Mode.

	*VIM Allows for Abbreviation of comands:
		example: 
			:bdelete <buffer name/number>    equals to  :bd <name/number>
			:abbreviate     equals to ab
			:bnext - which stands for Buffer Next, abbreviates to :bn

		*Further abbreviations can be created for VIM Insert and Command Modes 
		check :help abbreviate-local also Check VIM Advanced Section for more info

	*You can either use Local/Global Registers and Abbreviations for copy&pasting full directory paths
	check advanced VIM.

	*In Command Mode, you can press <tab key> for auto-completing a directory path and also
	picking up a file. <tab key> also helps setting attributes to variable content like colorscheme 

	*remember lower-case HJKL to move - can be combined with letters WEB and also Numbers
	*Example: 10j will jump 10 lines below.

	*Under Normal/Command/Insert/Diff/Terminal Mode: upper-case commands and lower-case commands have
	different meanings.

	* :w to save
	* :w {new file name} - save file with new name
	* :q to quit
	* :q! to quit without saving
	* ctrl+r to REDO
	* U to UNDO
	* ZZ quits, and writes file if modified

	*Remember that you can use CAPSLOCK to simplify some tasks, instead of pushing down SHIFT
	Example: ^U   and   ^D  to go up half-screen and down half-screen, both of which would
	normally require you to press both CTRL and SHIFT at the same time.

	**Yanking text with y2b can be better than switching to VISUAL MODE or holding down shift
	Example: 2yw   and    4Y

	*Listing all Data from buffers, mappings, abbreviations, bookmarks, registers, macros:
		:map
		:ab[breviation]
		:buffers
		:marks
		:registers or :reg   - Notice that Macros are just stored as Registers
		:jump 		     # vim currently sets a list of cursor position
		:tabs		     # Shows all tabs and windows available


	*About Buffers: whenever you open a new document, it gets saved in the buffer in the exact order
	you've opened them. So if you close the last window, and want to open them the same window:
		type in :sblast  #this will allow you to open the last buffer you've opened.
				 and thus will save you time from looking the buffer list for it's name
				 or number.

	*Also About Buffers: You can simplify your life by simply typing
		:b tex*   instead of   :b text.txt   to open a buffer  
		the same goes around if you want to open a terminal buffer, just type :b term* instead
		this will save you both life and time from having to check the buffer list on :buffers

	*Typing full path of a directory and pressing enter, will open vim directory mode

	*Reasons to use VIM: VIM Keys are used/shared amongst many linux/windows/mac OS applications
	
	*VIM Many registers can be used to store path directories that otherwise would have to be fully
	typen into the terminal, you can do that by creating a MAPPING on the .vimrc file
	the mapping will then precede store directory path values into the register.
		- read more on registers and maps
		- vimrc file location can be located with :help vimrc

	*VIM Registers can also be used in git to save HEAD IDs, that way you don't need to check
	them one by one with reflog.

	*A Great Way to use VIM Bookmark system, is at the :terminal mode.
	for example.: Before you type something like '7z --help' to list all available 7zip commands
	you can bookmark where you are at, so when list of commands gets printed on screen
	you can just go back with '<bookmark letter> instead of scrolling the screen.
	^The same can be done with a MAN Page, using global bookmarking instead.
	^ ctrl + i and ctrl + o   can also help in case you forget to add a bookmark
	^Note.: All mentioned above can still be done even after typing clearing the screen with
	'clear' on bash inside VIM terminal mode.

	*You can use VIM buffers to your advantage, for example, if you need to open a temporary document
	you can just open it using :e <file>, then once you're done reading you can go back to the previous
	document typing :bufferprevious or simply :bprev, and then if you want to re-open that document
	just type :bnext - Check more hints on buffer section

	*You can use VIM Registers - Specifically Named Registers(A Type of Register) - to permanently
	GLOBALLY save piece of Text like full path directory names.

Vim Commands:
	!<terminal command>
	:terminal  - once the shell is open, you can go to TERMINAL Mode by pressing 'i'
		     ^\  ^n to get out of TERMINAL Mode
		     TERMINAL mode allows you to input normal console shell commands.

	:r! <command>    #prints shell output in the current file
	
	*note you can run emacs in CLI mode inside VIM: 
		emacs -nw
		^ It is recommended to use spacemacs for that
	
	:bdelete!   -  also deletes a terminal

Source: https://www.openvim.com/sandbox.html
*Check ~/Downloads/Tutorial  for more tips
Vim Plugins: https://vimawesome.com/

Note to Self:
	ctrl+v ctrl+m to copy ^M special ascii code for removal in :%s/text/replacement
	Need to find out how to remove <95> characters

Advanced Hints:
	After every and each paragraph, use 'a' instead of 'i' to append at the current cursor;
	Use ( and ) to travel through text blocks;

	To create a page break, while in the insert mode, press the CTRL key
	And l. ^L will appear in your text and will cause the printer to start a new page;
	

	^F (CTRl F) Forward screenful

	^B Backward screenful

	^f One page forward

	^b One page backward

	^U Up half screenful

	^D Down half screenful


	: 10,25 w temp
	write lines 10 through 25 into a new file named temp. Of course, other line
	numbers can be used. (Use :f to find out the line numbers you want;
	Example: Try copying this entire VIM Tutorial to a new file
	
	nG or :n Cursor goes to the specified (n) line;

	To have absolute line numbering, you can use the :set number command. To not show line numbers, you 
	use the :set nonumber command;

	To set Relative numbering, type :set relativenumber. To put it back to Absolute numbering, type
	:set norelativenumber;	

	You might prefer to always have the Relative line numbering, but it’s hard to always set it when 
	1starting Vim. That’s where the Vim configuration file is useful. In the terminal at your home 
	directory, type:
		vim .vimrc

	Then you set:
		set number
		set relativenumber
		set hlsearch

	Now, every time you open Vim it will have the mixed Absolute and Relative line numbering mode set 
	with all search results highlighted as well. Highlighted search is useful in the next section. 
	There is much more you can do with the .vimrc file, but that will have to wait for another tutorial;

	Advanced VIM Tutorial:

		Bookmarks:	
	ma – Creates a bookmark called a
	`a – Jump to the exact location (line and column) of the bookmark a
	‘a – Jump to the beginning of the line of the bookmark a
	:marks – Display all the bookmarks
	:marks a – Display the details of the bookmark with name a
	`. – Jump to the exact location (line and column) where the last change was performed
	‘. – Jump to the beginning of the line where the last change was performed
	:delmarks {bookmark-name}


		*OBS.: Numbers are used to store Open File History
			Lower-case letters are used for Local Bookmarking
			Upper-case letters are used for Global Bookmarking
	
	source: https://www.thegeekstuff.com/vim-101-hacks-ebook/

	Advanced Search:
		:vimgrep Jarvis *.txt     - searches for Jarvis string in all text files
		use :cn and :cN    to go previous and next result

	Swap File:
		:swapname - Shows current swap file name

	Registers:
		:registers - Shows all registers already saved by VIM globally

		Default Registers

				% - Name of the current file

				# - Name of the alternate file for the current window

				: - Most recently executed command

				. - Contains the last inserted text

				" - Last used register

			Example:
				"%p - pastes current file name
				"#p - pastes alternate file for current window
				":p - pastes most recently executed command
				""p - pastes text stored at last used register
					*** 2020.2 ^ although, it's much better to just use p in normal mode

		Copying Text to Register:
			"<register-name><command>
			example: "ayy    - Copies line of text into register 'a'
				 "a4Y    - Copies 4 lines of text into register 'a'
				 "a4yw   - Copies 4 words into register 'a'
				 Y      - Yanks/Copies line of text
				 y      - Yanks/Copies character 
				 yw     - Yanks/Copies word 
				 and so on...

		Paste Text From Register:
			example: "ap - Pastes text from register 'a'
					  p  - Pastes text after line
					  P  - Pastes text before line

		Register Types:
			Unamed Registers:
				Unamed register is denoted by “”. Vim stores deleted or copied text
			in this register.

			Named Registers:
				It's possible to use 26 named registers - a to z and A to Z
			by defaul VIM doesn't uses/touches these registers.

			Registers are Permanently Saved Globally. - NEEDS TO CHECK

			Numbered Registers:
				It's possible to use 0 to 9 named registers. Vim fills these registers
			with text from Yank and Delete Command.

				0 Register - Contains the text from the most recent yank command
				1 Register - contains the text deleted by the most recent delete or change
					     command

				^ 2023: the number registers actually work as a queue.

			Default Regiters:
				Check above at the beginning of this section

	Cursor Positioning:
		Ctrl + o      - Jump back to the previous position inline
		Ctrl + i      - Jump to the next position inline
		:jumps	      - Vim currently keeps a list of all cursor positions

	Multiple Files:
		:e! <filename>	  - Creates a new file
		:e  <filename>    - Edits existing text file as a new buffer on same window
		:edit <filename>  - ^ Same as above
		:e <tab key>      -  Lists all files available in a directory and open on current buffer
		:edit <tab key>   -  Lists all files available in a directory and opens new tab
		 ^ :edit or simply :e  do the same thing 
		 ^ :e <tab key> allows to both open non-existing buffers and current buffers 


		:sb | e <filename>   # to edit an existing file on the same tab in a new window
		:sb | b <filename>   # to edit an existing buffer on the same tab in a new window

		:sb | e! <filename>  #to create a new file on the same tab in a new window
			^ctrl+w and ctrl+s  are better alternative to the :sb commando to split window
			alternatively you can use ctrl+n to create a new window.

		Multiple Buffers:
			:b <tabkey> - lists all currently open buffers and it's path/directory,
				      and allows for selection
			:b   - opens a buffer on same window, if it doesn't exist, then creates a new one
			:add - add file into new buffer
			:bN  - switch to Nth buffer; note, you can also use name of buffer here.
				example: :b text.txt
					 :b tex*      

			:bnext - switch to next buffer
			:bprevious - Move to the previous buffer in buffer list
			:buffers - List all buffers
			:bfirst - moves to first buffer
			:blast - moves to last buffer
			:ball - Loads all buffers
			:[N]bmod [N] - Goes to Nth modified buffer at the same window 

			Same goes for tab commands:
				:tabnext  or :tabn
				:tabprev  or :tabp
				etc...



		*Adding an 's' as a prefix will allow to open the buffer on a new window
		*Note.: New Window isn't a new Tab
		*Example: sblast

		*Note.: When you OPEN/CREATE a file or go into terminal mode, a buffer is created and kept alive
		even after you've closed it with :close, to remove a buffer type
		
		Change Buffer Window:
			You can use :below, :above, :left, :right, :vertical, :horizontal, :bottom,
		:top in combination with some buffer commands above. Example:

				:below sb <buffer name/id>
				:above sb <buffer name/id>
				:below vertical sb <buffer name/id>
				:botright vertical sb <buffer name/id>
				:botleft vertical sb <buffer name/id>
				:topright vertical sb <buffer name/id>
				:topleft vertical sb <buffer name;/id>

				:vertical sb | terminal
				:below sb | terminal  #creates a new terminal window+buffer
				:above sb | terminal  #creates a new terminal window+buffer
				etc
				
				try this:
					:tabnew | terminal   #1st open a new terminal on a new tab
					:sb | terminal       #2nd open a new buffer on the same window
					:vertical sb | terminal #3rd Switch to bottom window first
							        before doing this 3rd step
								press ctrl+w j.
				
				^This type of window setup is great if you want to read something on the
				top most window and do terminal stuff like music/video on the terminals
				below. if you need to read something on the top, you can just press
					Example: ctrl+w _  #maximizes current window height
				
				and once you're done reading you can: 
					Example: ctrl+w =  #set sizes to normal
					
			WINDOW COMMANDS / WINDOW SHORTCUT KEYS
			Other Commands:
				ctrl+w q        # closes window
				ctrl+w w        # SWITCHES window vertically(windows must exist)
				ctrl+w v        # vertical split - CREATES A NEW WINDOW VERTICALLY
				ctrl+w s        # splits horizontally - CREATES A NEW WINDOW HORIZONTALLY
				ctrl+w n	# opens new window with a new empty file
					
				ctrl+w h/j/k/l  # HJKL movements for switching between window

				ctrl+w =	# Makes windows equally in size
				ctrl+w [N] +	# Increases current window height By N Lines
				ctrl+w [N] -    # Decreases current window height by N Lines
				ctrl+w [N] <	# Decrease current window width by N Lines
				ctrl+w [N] >	# Increase current window width By N lines
				ctrl+w _ 	# Sets current window to it's maximum height	
				ctrl+w |	# Sets current window to it's maximum width
				ctrl+w [N] ^    # no idea????????

				ctrl+w T        # Moves current window to a new tab
						*only does that if there are more than 2 window

				ctrl+w t    #Moves cursor to top-left window
				ctrl+w b    #Moves cursor to bottom-right window

				ctrl+w H/J/K/L     # Moves current window left, down, up, right
						   All of them makes the current window occupy the width/height at 
						   it's maximum size.
						   ^This makes it possible to set whether a window occupy full width or height,
						   while the other two share the same width/height.

				:mod[e]		   # Detects the screen size and redraws the screen
						   ^ very important when windows bugs and stuff disappear

				ctrl+w gf	   # opens file under the cursor in a new tab
				^ VERY IMPORTANT AND USEFUL

		type :help window   for details on how to control windwos

		Removing a Buffer:
			:bdelete <name/number>    - Removes a given buffer by it's name or number
			:3,5bdelete               - Will remove buffer from 3 to 5(including 3 and 5)
			:bdelete name1.txt name2.txt  - Will remove given list of buffers by it's names
			:33,34bd 

		*VIM Also allows you to use the abbreviated forms:
			:sblast   as :sbl
			:bnext    as :bn

		Multiple Windows:
			:new <file> - Open new Window
			:new <file> - Open file in new window
		*HINT use <tab key> when openning existing files

	
	Showing Difference Among Files - DIFF MODE:

		Similar to UNIX diff command we can use Vim to show diff in much better manner.
	It will show diff in colorful manner. In this chapter, we will discuss following items: 

		Show differences between files
		Activate and deactivate diff mode
		Navigation in diff mode
		Applying changes from diff window
	
	$ vimdiff <file> <file>
	$ vim -d <file> <file>

	Purple Color indicates common text among files, Red color indicates differences

	If you're already in VIM you can use:
		:diffsplit <filename>
			or
		:vert diffsplit <filename> - to open in vertical*

		Applying/Updating Changes:
			For instance, if you are in left window and you want to take change from right 
		window to CURRENT window then you will execute above command.

			:diffget - To apply change from adjacent diff window to CURRENT diff windows
				   execute. 

			:diffput - To apply change from CURRENT diff window to adjacent diff window execute.

		Jump to Previous and Next Change:
			[c   and    ]c    - Previous and Next change	

		Navigation in Diff MODE:
			Navigation in diff mode is little bit different. For instance, when you scroll text 
		from one window then text from adjacent window also scrolled. This is called scrollbind. 
		To enable this use:

			:set scrollbind
			:set noscrollbind

		Editing file in Diff MODE:
			If you edit file in this mode then use following command to update diff:
				:diffupdate

		
	Switching Between Vertical and Horizontal Window:
		Ctrl + w Ctrl + w

	Remote File Editing in VIM:
		With VIM it's possible to open a Remote Text File:
			$ vim scp://user@server.com/filepath
				or
			$ vim scp://jarvis@remote-server.com//tmp/message.txt

		If you're already inside vim, you can use:
			:Nread    and    :Nwrite   to either load or save files remotely
		
		In addition to SCP, Vim supports following protocols: 
			FTP
			SFTP
			HTTP (read-only)
			rsync

	Fold Activation and Deactivation:
		To Activate fold use following command:
			:set foldenable
			:set foldmethod = indent

		To deactivate fold use following command:
			:set nofoldenable
			^*VIM Global syntax for turning off :set commands

		To fold/unfold code, go to any Method/Funcion/Procedure and type:
			zo   and   zc  - for folding and unfolding	
			zM   and   zR  - for folding and unfolding all

	Macro Start/Stop/Play Actions:

		q{macro-key} - Starts recording a macro
			       you can see list of actions by executing :register
			       press 'q' again to stop recording
			       In this step you can perform any Vim actions like: cut, copy, delete, 
			       replace and so on.

		@{macro-key} - Plays recorded macro
			       to play recorded macro N times do: 10@{macro-key)

	Advanced Tricks:
		Ctrl + p    - Word Completion
		:set spell   - turns on misspelled words
		:set nospell - turns of misspelled words
		
		=%           - Indents code according to current cursor line-character
		
		Example - try it -:
		{
		unindented line
				unindented line 2
			unindented line 3
		unindented line 4
		}

	
		Configuration Files:
			Prints System Path:
				:echo $PATH
				
			Global Vim Configuration File:
				:echo $VIM    - print Global vimrc file for Configuration

			Local Vim Configuration File:
				Local vimrc will be located under user’s home directory. 
			For instance, on Linux it will be under /home/<USER> directory while on 
			Windows it will be under C:\Documents and Setting\<USER>/ folder

			Note that, on Windows file name will be _vimrc whereas on Linux it will be .vimrc.

			Setting Font:
				:set guifont = courier
					
			Changing Color Scheme:
				:colorscheme murphy
					or
				:colorscheme ron
					or
				:colorscheme darkblue

			Set Spell:
				:set spell
				:set nospell
				:set spelllang = <language>
				   example - :set spelllang = de     
				   	     sets spell language for german

			Set Number:
				:set number
				:set nonumber

			Set Abbreviations:
				Global Abbreviations:

				Abbreviations can be used for auto-correcting common mistakes:
					:abbr teh the
				Now if you type teh, by mistake, it will auto-correct by itself.
		
				Creating a Local Abbreviation method:
					:abb <buffer> FF for (i = 0; i < ; ++i)
					^ Type in <buffer> literally speaking
						
				*Type in :help abbreviate-local for more info
				
				Deleting Abbreviation:
					:una[bbreviate] - Deletes an abbreviation			
							you can write 'unab' to keep it short

				Listing all Abbreviations:
					:ab - lists all abbreviations


				*OBS.: Adding an 'i' or 'c'on the first column defines abbreviation 
				for either insert mode or command mode.
					Example: 
						:cab FD /home/<username>/Desktop/

					^ Default creates abbreviation for both Insert and Command mode

				also check :help map-local

			Setting Mappings:
				The main difference from Abbreviations, is that abbreviation is meant
			to insert Strings of Text, Mappings are associated with full-actions as pressing
			F10 to create a new buffer or a new window.

				:cmap handles command-line mappings.
				:imap handles insert-only mappings.
				:map maps keys in normal, visual, and operator-pending mode.
				:map! maps keys in Vim's command and insert modes.
				:nmap maps keys in normal mode only.
				:omap maps keys in operator-pending mode only.
				:vmap maps keys in visual mode only.
				
				Example:
					:map! <F10> <ESC>:tabnew<CR>
					:map  <F10> <ESC>:tablast<CR>:tabnew<CR>	
					:map  <F9>  <ESC>:tablast<CR>:tabnew<CR>:terminal<CR>

				*Maps can also be created locally with <buffer> option
				check :help map-local for more info

				Listing all Mappings:
					:map

				Unmapping:
					:unmap <F10>	

				More Examples:
					Mappings for HTML Tags:
						imap <F2> <p>
						imap <F3> <strong>
						imap <F4> <em>
						imap <F5> <code>
						imap <F6> <a href="
						imap <S-F6> ">
						imap <F7> <blockquote>
						imap <S-F2> &lt;
						imap <S-F3> &gt;

				Function Call:
					You could add the following in .vimrc file:

						function ToggleHLSearch()
						       if &hls
							    set nohls
						       else
							    set hls
						       endif
						endfunction

					^Finally you could create a mapping to Call Function:
						:nmap <silent> <C-n> <Esc>:call ToggleHLSearch()<CR>.

					In this example, <silent> isn't a key -- it's to tell Vim not 
					to print a message when it runs the command. Then, the key shortcut
					that is mapped to the action, and the action that will be called.


				Mapping Notations:
					We've covered the notation for some of the special keys already, 
				such as Escape (<Esc>), Enter (<CR>), and function key notation 
				(<F1>, <F2>, etc.). For the most part, the notation is pretty intuitive, 
				but not always.

					For example, if you want to set a mapping for Ctrl-Esc, you'd 
				use <C-Esc>. If you want to use Shift-F1, you'd use <S-F1>. For Mac users,
				if you want to set a mapping for the Command key (the weird symbol-thingy
				that PC users scratch their heads over), you'd use <D>. Note that Alt and
				Meta are the same, and you can use <M-key> or <A-key>.

					You can also combine several keys if you want to emulate the Emacs 
				hand-cramp style of key mappings. For example, if you want to run a command 
				using Shift-Alt-F2, you could use map <S-A-F2> command.

			Set Status Line:
				:set laststatus=0    - to disable it
				:set laststatus=2    - to enable it

			Word Completion:
				ctrl + p    - Will bring list of auto completion words - 2020.2

			Syntax Highlight:
				:syntax on   - will turn on color for syntax in programming languages
				:syntax off   - will turn it off

			Auto Indent:
				:set autoindent - Sets auto indent
				:set smartindent - Sets smart indent
				:set cindent     - Auto indent C Code

			Executing Single Shell Terminal Command:
				:!<command>

				:terminal   - for multiple shell commands

		*Protip: you can also type any of these commands in : mode
		and then press <tab> to bring a list of options on those commands
		*Note: Pressing enter will activate that option
		*Try: :colorscheme <tab>

	Final Considerations:
		Mapping X Macro X Abbreviations
			While Macro and Maps can do the same thing, Maps are limited to how many keys
		you have freely available to each mode(Insert, Command, Normal, etc), which means 
		they should never overlap the already existing commands in any of these modes.

		Macros in the other hand, are set to registers from a-z and A-Z and as such have no 
		limitations. Macros are also permanently kept in your VIM application once they are created,
		requiring no updates to your local .vimrc configuration file.(Just as Bookmarks)

		Abbreviations and Mappigns need to be kept inside .vimrc file, requiring modification of
		this file - Remember that .vimrc file exists for each local user on the system, a global
		.vimrc file is also available - thus making this task a more complicated one.

		Abbreviations should be reserved for KISS commands, and auto-correction of words.
		For example: 
				abbr ab abbr   
			#So instead of typing 'abb' or 'abbr', all it takes now is to type 'ab'


		Remember that to execute a macro, it's still required for you to type '@' operand
		select a register and press ENTER key.

		^ In case you wanna keep things at KISS level, use Mappings for these cases; Mappings will
		allow you to assigne a single key or a simple combination of keys to execute multiple 
		commands:
			:map <F10> <ESC>:tablast<CR>:tabnew<CR>:terminal<CR>

		so now instead of having to press @ + select a register + enter key, you can simply press
		F10 on your keyboard to open a new terminal shell tab at the last tab.
		
		^ This is also why it's important to learn and keep using the original VIM Commands.
		making new mappings wouldn't be possible without them. So don't go out making MAPS for all
		sorts of commands like assigning :w to <F2>, or you'll just eventually forget all original
		commands. Only use mappings when it's really benefficial.
		


	Making First Plugin:
		Write a function that pastes new data into a buffer, then counts how many line exists
	then join all of the lines in a single one, finally copies it to the clippboard using 
	"+y

	The following command does that for a single line:
		:map <F8> <ESC><CR>:tabnew<CR>p<ESC><END>:j10<CR>x<S-V>"+y:bdelete!<CR>gT 		
		
		^Joins 10 lines with :j10, does not erases a white space(for directory path yanking!), 
		and goes back to the current tab with 'gT'

		:map <F8> <ESC><CR>:tabnew<CR>p<ESC><END><S-j>x<S-V>"+y:bdelete!<CR>gT

		^ This one erases a white space, but only works for a single line
		^Make a new script that copies full path directly from terminal

	https://vim.fandom.com/wiki/Mapping_keys_in_Vim_-_Tutorial_(Part_2)

	Advanced Tab Commands:
		type in :help tabnew for a list of extra commands
	
		ctrl+w gf       	#Creates a new tab and edit the file name under the cursor 
					example, try it: /home/<username>/scripts/automount.sh

		:tabclose or :tabc      #Safely closes a tab, if it has no hidden or modificated windows
					:tabc!  also works for closing all modified windows in current 
					tab.
	 	:tabnew 
		:tabfirst 
		:tablast

		:tabs			#Shows all tab pages, shows > for current tab, and 
					+ for modified windows in a tab

		:tabonly		#Kills all tabs except current one
					can be kept short with :tabo! to forcibily closes all other 
					tabs			

		:tabn [N]	        #Moves to next tab [N] after current one
					^Should be used the same way as :tabm

		:tabp [N]		#Moves to previous [N] tab before the current one
					^Should be used the same way as :tabm

		:tabm [N]		#Moves/Position tab to [N] from first tab 
					short for :tabmove and can also be used as
							:tabm +[N]    #Moves tab from current position
							:tabm -[N]    #Moves tab from current position
---
REGULAR EXPRESSIONS / REGEX ON NVIM:
Regular Expressions: :g/^\n\</+;'}-j 
I understand how the above works, but I can't find justification 
for it in Vim's help.  I'd break it down as follows: 

  :g            Begin "global" command 
    /^\n\</       Find blank line followed by start of word 
      +             Go forward one line to start of paragraph 
        ;             Set cursor to above-calculated start of range 
	  '}            Move to blank line after paragraph 
	    -             Go backward one line to end of paragraph 
	      j             Join lines in paragraph delimited by the range 

	      But looking at ``:help :g``, I see: 

	        :[range]g[lobal]/{pattern}/[cmd] 

	
	VISUAL MODE:
		Little was said about visual mode, but here are some hints:
			The objects that can be used are:
				aw	a word (with white space)			|v_aw|
				iw	inner word					|v_iw|
				aW	a WORD (with white space)			|v_aW|
				iW	inner WORD					|v_iW|
				as	a sentence (with white space)			|v_as|
				is	inner sentence					|v_is|
				ap	a paragraph (with white space)			|v_ap|
				ip	inner paragraph					|v_ip|
				ab	a () block (with parenthesis)			|v_ab|
				ib	inner () block					|v_ib|
				aB	a {} block (with braces)			|v_aB|
				iB	inner {} block					|v_iB|
				at	a <tag>asxs </tag> block (with tags)		|v_at|
				it	inner <tag> </tag> block			|v_it|
				a<	a <> block (with <>)				|v_a<|
				i<	inner < > block					|v_i<|
				a[	a [ assas ] block (with [])			|v_a[|
				i[	inner [] block					|v_i[|
				a"	a double quoted string (with quotes)		|v_aquote|
				i"	inner double quoted string			|v_iquote|
				a'	a single quoted string (with quotes)		|v_a'|
				i'	inner simple quoted string			|v_i'|
				a`	a string in backticks (with backticks)		|v_a`|
				i`	inner string in backticks			|v_i`|



VIM Modeline allows each file to carry their own configuration settings inside of them.
Not recommended since it's insecure.

	My Own Mappings:

			Document:
				note.: " is used to bookmark last known position before a given instruction is executed,
					for example, a bookmark is set everytime you use ml for listing directories.
					that way you can just go back to where you previously were with '" command.

					q is a bookmark used for song playing, it bookmarks the last song/video you have
					set to play.

				ALT+TAB #Only works in terminal-mode, it allows for path auto-completion - NOT DONE YET
				mi   	#Copies content between "(double quotes) from cursor position to terminal
				mc   	#CD into a directory and Lists all files and directories
				md   #CD .. from a directory and Lists all files
				mp   #Plays music file from cursor position in MVP Cli mode
				mv   #Plays video file from cursor position in VLC
				ms   #Stops song
				ml   #Lists all files and folders in a directory
				mL   #Lists all files and folders in a directory and sub-directories
				cl   #Erases any terminal input without changing mode

				mC   #Allows to terminal 'cd ' into a branch directory - DONE 
				     using terminal 'ls -R' to list subdirectories
				mV   #same as above for 'mv' command - DONE
				mP   #same as above for 'mV' command - DONE
				mI   #same as above, for the 'mi' command - DONE
				rM   # same as above for 'rm' command - DONE
				cP   # same as above for 'cp' command - DONE
				mF   # same as above for 'mf' command - DONE
			        zL   # same as above for 'zl' command - DONE
				mD   # same as above for 'md' command - DONE
				zP   # same as 'zp'	
				zL   # same as 'zl'

				OBS.: Important note - 
					these keys are overriding global bookmarkings
					must switch from SHIFT to alt key.

				ESC  #Quits terminal mode

				Untested ones:
					rm  #Removes file at cursor position - OK
					cp  #Adds file at cursor position to be copied over - OK
					mf  #Adds file at cursor position to be moved - OK
					zp  #Adds file at cursor position to be added in zip file - 7z error
					zu  #unzips file inside a folder - Needs checking on the 7z command - WORKS
					zU  #unzips file in current directory - Needs Checking on the 7z Command - WORKS
					zl  #lists files currently in a zip file - OK

					^All these commands should be used with 'mi' for selecting more 
					than one value into them


		#ONLY WORKS ON WINDOWS	
		Creates directory path traversal commands: 
			:nmap mC m"vi""2y?/.<CR>vi""1yicd "<C-\><C-n>"1pi/<C-\><C-n>"2pi"<CR><C-\><C-n>:noh<CR>:<ESC>
			:nmap mc m"vi""*yicd "<MiddleMouse>i"; ls -a -Q --group-directories-first<CR><C-\><C-n>
			:nmap md m"icd ..; ls -a -Q --group-directories-first<CR><C-\><C-n>	
			:nmap mD m"icd ..; ls -a -Q -R --group-directories-first<CR><C-\><C-n>:noh<CR>:<ESC> 
			:nmap mi vi""*yi "<MiddleMouse>i"<C-\><C-n> 
			:nmap mI vi""2y?/.<CR>vi""1yi "<C-\><C-n>"1pi/<C-\><C-n>"2pi"<C-\><C-n>:noh<CR>:<ESC>
			:nmap ml G<END>m"ils -a -Q --group-directories-first<CR><C-\><C-n>
			:nmap mL G<END>m"ils -a -Q -R --group-directories-first<CR><C-\><C-n>
				^G<END>allows a mark to be set in the last command line instruction
				 for later reference.
		File Operations:
			:nmap rM vi""2y/.<CR>vi""1yirm -rf "<C-\><C-n>"1pi/<C-\><C-n>"2pi"<CR><C-\><C-n>:noh<CR>:<ESC>
			:nmap rm vi""*yirm -rf '<MiddleMouse>i'<CR><C-\><C-n>
			:nmap cp vi""*yicp '<MiddleMouse>i' <C-\><C-n>
			:nmap cP vi""2y/.<CR>vi""1yicp "<C-\><C-n>"1pi/<C-\><C-n>"2pi" <C-\><C-n>:noh<CR>:<ESC>
			:nmap mf vi""*yimv '<MiddleMouse>i' <C-\><C-n>
			:nmap mF vi""2y/.<CR>vi""1yimv "<C-\><C-n>"1pi/<C-\><C-n>"2pi" <C-\><C-n>:noh<CR>:<ESC>
			:nmap zp vi""*yi7z a '<MiddleMouse>i' <C-\><C-n>
			:nmap zu vi""*yi7z x '<MiddleMouse>i' <C-\><C-n>
			:nmap zU vi""*yi7z e '<MiddleMouse>i' <C-\><C-n>
			:nmap zl vi""*yi7z l '<MiddleMouse>i'<CR><C-\><C-n>
			:nmap zL vi""2y/.<CR>vi""1yi7z l "<C-\><C-n>"1pi/<C-\><C-n>"2pi"<CR><C-\><C-n>:noh<CR>:<ESC>
		MISC.:
			:nmap cl i<END><C-u><C-\><C-n>

		Creates a key that reads current cursor line, and ouputs it on a video player inside the terminal window:

				Use these new ones instead:
         :nmap mP msmqvi""2y?/.<CR>:noh<CR>:<ESC>vi""1yimpv --no-video --loop "<C-\><C-n>"1pi/<C-\><C-n>"2pi"<CR><C-\><C-n>`q
		:nmap mp msmqvi""*yimpv --no-video --loop "<MiddleMouse>i"<CR><C-\><C-n>`q
		:nmap mv msmqvi""*yivlc "<MiddleMouse>i" <CR><C-\><C-n>`q
		:nmap mV msmqvi""2y?/.<CR>:noh<CR>:<ESC>vi""1yivlc "<C-\><C-n>"1pi/<C-\><C-n>"2pi"<CR><C-\><C-n>`q
		:nmap ms mqi<C-z><C-\><C-n>`q
					^ Remember to use `q instead of 'q



				Note.: Songs not playing is caused by VIM, if you take too long to press key combinations
				it doesn't counts as a mapping execution. - CONFIRMED

				Note.: Update other commands with the solution below
				Note.: There's a chance :<ESC> a the end of MPV's command might be - SOLVED
				cancelling the instruction, thus not playing any sound
				
					^SOLUTION: clear the : command as soon as search is finished. - SOLVED

				Note.: Find alternative solution to stop song instead of using <C-z>, for both vlc and mpv

				Note.: Mapping a 1st and 2nd key combination solves the problem of multiple instructions - SOLVED
				getting executed before the other is finished in bash terminal-mode. - SOLVED
					^ Since the terminal doesn't do those in a specific order, VIM does. - SOLVED


				Note.: In Terminal-Mode, when bookmarking, VIM Simply 'memorizes' line and column,
				nothing else; what happens here, is that the xterm terminal has a maximum buffer size
				for the terminal, once data hits it's maximum size, lines starts being 'popped'
				and bookmarking simply becomes useless.
					^ Find out how to increase xterm maximum buffer
					There's a work around for this tho, you can add in a register the
					full filename, and then reverse search it, setting the cursor where it was before.
					instead of using bookmarks.
						^Don't do this, because files with equal names on different paths would
						break it.

					^Solution.: Type CLEAR in bash when terminal buffer does that. - YET TO BE IMPLEMENTED
				
			todolist: use a register different than " for mp, mv, mV and ms, - DONE - SOLVED
			         this one is being used by mc, md, mi, ml - SOLVED

			todolist: vim uses register ", it records when u've switched buffers

			note.: do not mistake <C-U> with <C-u> the first one half-scrolls page up, 
				the second one erases content inside a terminal.
				
				These older commands do not scroll back the screen - USE THE ONE ABOVE -:
			:nmap mp vi""*yi<C-z>mpv --no-video --loop "<MiddleMouse>i"<CR><C-\><C-n>:noh<CR>:<ESC> - Updated
			:nmap mv vi""*yi<C-z>vlc "<MiddleMouse>i" <CR><C-\><C-n>:noh<CR>:<ESC> - Updated
			:nmap ms i<C-z><C-\><C-n>

			-------
				These 2 first commands DEPRECATED will be kept for history
			:nmap mp v/\.<CR>e"*yimpv --no-video --loop *'<MiddleMouse>i'*<CR><C-\><C-n>:noh<CR>:<ESC>
			:nmap mv v/\.<CR>e"*yivlc *'<MiddleMouse>i'*<CR><C-\><C-n>:noh<CR>:<ESC>

			-------

			*Note.: Use /\. to match a . character in search mode	
					filename name.mp3

				\.   #Searchs for the first occurrence of the character .
				      can be used with other characters

				Further, ls -Q can be used to sorround filenames with double quotes "
				In visual mode i" can be used to select text inside a string.
				todolist: ls -R can be useful later - DONE
				todolist: make a command that clears input from terminal mode
				todolist: make a command to execute any given string in a terminal.

		Pastes the content of a register inside a terminal, by reading a single letter inside the terminal
		without leaving the terminal mode:
			:map <F2> v"*y<C-w>n:put <C-r>"<CR><END>v<HOME>"*y:bdelete!<CR>i<BS> <MiddleMouse><C-\><C-n>
				or							(DEPRECATED)
			:tmap <F2> <C-\><C-n>vyi<BS><C-\><C-n>:put <C-r><S-"><CR>i
		       	
			^First mapping is old, but will be kept for history reasons

		Maps a ESC key to leave terminal-mode:
			:tmap <ESC> <C-\><C-n>

	Todolist:

		Manual MPV:
			https://mpv.io/manual/master/
			https://www.maketecheasier.com/mpv-cli-media-player/
			https://www.linux.com/news/linux-thumbnail-viewers

		BugFix: - FIX'D
			When you need to execute multiple instructions at bash, and there's a need to wait for a 
			process to finish first.
			simply map a key for the first instruction, and a second one for the second one.

		Feature 12:
			Make a mapping that copies filenames directly to the + register

		Feature 11:
			Make a terminal operator that auto-complete path

		Feature 10:
			Make a special map for storing current full path(directory+path) in a register

		Feature 9:
			Make a confirmation message for commands like mi
		saying that the path has been copied, always scrolling back to it's position with '" bookmark.

		Feature 7:	
			Integration with I3-Dmenu.
			Find out how to press SYSTEM KEY
		
		Feature 6:
			Open programs from their name at cursor position.

		Feature 5:
			Allow features to be turned off
			Like disabling GIT Integration.
			Let the user know through a confirmation message.
			Allow the user to show a list of enabled/disabled features.

		Feature 4:
			Make commands available only in Terminal Buffer

		Feature 3: 
			Git Integration

		Feature 2:
			Create a configuration file that stores time/music for playing.
				mpv --start=<time>     (<time> = hh:mm:ss

			That file must have full path of music at the first line.
			Copy/Pasting the full directory path require to JOIN All lines in case they're in different lines.

		
		Feature 1:
		Make a portable player that allows you to pick songs from one window, and play it on another window;
		everytime a new music/video is input, it will stop the music on the 'song window' and play a new music.
		check if it's possible to tag buffers and move to windows according to the buffer ID, 
		this will make the system more modular, since users can open and resize as many tabs as they want. 
		also keeping the 'song window' to a minimum size.

		:help window
			line 409	

		Alternative Solution: Just open the buffer of the song window - still needs taggin
		It's possible however to use :bnext +[N] and -[N]  and [N]
		By oppening it in a specific order, it's possible to always reach the same buffer.
		as long as the person doesn't delete the buffer, it never changes order
		even if they delete the window with :close,
		only throwing the tab away can kill that order.


		:note it's possible to to create an unlisted buffer, so it doesn't appear as a buffer in the :buffers
		but still can be used as a buffer

		Alternative Solution 2: Just open the buffer in a new tab and close it - still needs tagging

		Alternative Solution 3: Copy/Paste from :buffers the name of the 'song terminal'
					make an unlisted buffer with it's number
					check the unlisted buffer and copy it's value
					:sb <buffer name/id>
					close the new window
					input command to song player
					done
		
		try: :help terminal
		:vsplit term://top #check later

		:e [++opt] <filename>
http://www.vimregex.com/

NVIM/VIM:---------
----------NVIM/VIM TUTORIAL 2020.2-T EDITION 1(NEWER)----------
======================2020-T======================
VERY IMPORTANT:
	From terminal type:
	nvim -S <session_name>     #opens session out of box with vim

Note.: Note that 'vi' is included in most UNIX operational systems on a default base.

VERY IMPORTANT:
	0 - (2023) Use ncdu for navigating through directories and checking file sizes

	1 - Vim can set default commands for shell sessions inside each VIM SESSION File,
	edit your chosen vim session file with care!

	2 - You can open nvim inside nvim safely now, just use ctrl+c to get out of insert mode(or any other mode).

	ctrl+w gf	   # opens file under the cursor in a new tab
			   ^ Useful for oppening files/directories on terminal mode.

	3 - if you ever user ':mksession! <session_file>' for saving sessions, you can edit the same file
	and set custom options such as 'colorscheme habamax' or 'colorscheme ron'; this way each session
	can have it's own colorscheme!

	4 - shift + 3 or shift + # will select cursor text for search in the file.
	can be used in visual mode too.

NORMAL MODE HOTKEYS:
	ctrl+w		#Erases a single word
	shit+w		#Moves cursor a single word

TERMINAL MODE HOTKEYS:
	ctrl+\+ctrl+n	#Exits Terminal Mode

VIM TERMINAL SCROLL:
	1. VIM Terminal can be scrolled by switing to terminal mode using: <C-w> <C-N>
	2. NVIM Terminal can be scrolled by pressing ESC

	3. both can be scrolled up using <C-f> <C-u> or page-up and page-down.

VERY USEFUL 2023:
	$nvim ./source/**/*(D-.)   #Opens all files in a given ./source/ directory as well as all it's subdirectories
	
	$find -L ./source/ -type f -exec nvim {} +		#Same as above, opens all files in a given ./source/ dir

VERY USEFUL:
	<system_key> + esc	   #?
	:!<shell_comand>	   #Very useful if you don't plan on oppening a :terminal tab
	:r!<shel_command>	   #Pastes terminal output into current window/buffer

	:mksession <session_name>  #creates a session called by a given name

	:source <session_name>     #loads a session
		or
	nvim -S <session_name>	   #same as above, but used before nvim gets open.

	:history		   #shows the search history 

	^r			  #COMMAND-MODE: Allows to copy register content to command mode
	
	% and #			  #These registers contain the current and previous filepath that have been acessed in VIM.
				  they can be used with ^r

	q 			 #NORMAL-MODE: can be used to record macros, these macros can act as aliases for one or multiple instructions
				 they'll also work for terminal-commands.
	
	^\ + ^n			 #ESCAPE KEYS
	   or
	^\ + ^[
	   or
	 <esc>
	   or
	^c			#Same as above, added on 2022 version.


	%, #, "			#Three important registers, the first 2 stores current and previous directory paths used by nvim;
				The third registers stores information from nvim's default clipboard selection.

	alt+<any_vim_keys>	#Holding down The ALT key allows the user to activate the INSERT MODE from any other VIM MODE: 
				command-mode, normal-mode, visual-mode, block-mode...

	2gT			#Changes current tab to the 2nd previous tab
	3gt			#Changes current tab to the LITERAL 3rd tab
	gt or gT		#Changes to the previous or next tab
	g<Tab>			#Go to previous (last accessed) tab page.
	:tabs			#Shows list of tabs

	^- or ^+		#Decreases/Increases font-size

	shift+#			#Searchs word on entire document
					^VERY USEFUL WHEN CODING, you can just search an entire method/function/procedure call!
						TODO LATER: Search Selection from visual mode


	NEW COMMANDS:
	:sb buffer_name		#Opens named buffer on the same window as a completely new window
	:sb e <file_path>       #Same as above, but opens new file instead

	:new			#Creates a new window by splitting the screen horizontally
	:vnew			#Creates new window by splitting it vertically
	:sp or :hsplit		#Splits window horizontally using the same buffer
	:vsp or :vsplit		#Splits window vertically using the same buffer

		^ The commands above can be mixed with: +, ., -, $, 0
			+: opens window after the current one on it's axis alignment(horizontal or vertical)
			-: same, but opens before the current one.
			.: opens on top of current one
			$: opens at the end of the current one(as last window)
			0: opens at the beginning(as first window)

	:e src/**/F*b*r.cpp	#VIM ALLOWS USE OF REGEX! This is similar to the auto-complete function found in ZSH!!!! :)

	:windo diffthis		#Turns on difference | Shows difference between files on the same window
	:windo diffoff		#Turns off difference | Turns the above off

	SEARCH LATER:
		1 - Is it possible to name tab pages?
		2 - Is it possible to name buffers?
		

	nvim -d <file1> <file2> 	#Shows differences between two files - VERY IMPORTANT

	^i  and  ^o			#Moves between next and previous buffer in a given window - VERY USEFUL
					^ Also moves between previous / next cursor position in a document

	qa				#Records all keyboard action into register 'a'. Press 'q' again to stop recording
	@a				#Plays all keyboard action from register 'a'.
					^ Very useful for doing same task multiple times, ex.:
						Add a .dot character at the end of all lines below:
							line 1 line 1 line 1
							line 2 line 2 line 2
							line 3 line 3 line 3
					Instruction: press 'qa' to start recording, then press '$' to go the end-of-line character,
						    finally press 'a' for appending and 'q' to stop recording. finally go to next-line and press '@a' for playing the recording.

					Exercise: now try adding '...' to each line:
							line 1 line 1 line 1
							line 2 line 2 line 2
							line 3 line 3 line 3
					
					VERY IMPORTANT(NEW): 
						You can also use macros as shortcut for accessing specifics parts of text, tabs, window, buffers, files, etc.

					ex1.: qA 200G q						#Creates a macro for key A that goes to line 200
												 ^ Press @A for executing the command on any file.

					ex2.: qB :vnew | e ~/ <press_enter_key> q		#Creates a macro for key B that opens the given file/folder structure

					ex3.: qW <tab_number_here>gt q				#Creates a macro for key W that goes to <tab number>
					
					ex4.: qU ^w10h q					#Goes to rightmost buffer on the same window
						ex5.: qI ^w10j q				^ same as above, but bottommost
						ex6.: qO ^w10k q				^ same as above, but topmost
						ex7.: qP ^w10l q				^ same as above, but rightmost

	:find <directory>		#Display files in tree structure

	i^x^f				#Displays list of files inside Insert Mode
	or ^x^f				ex.: type the command described in the following path on insert mode:
							/home/fmrs/InstructionA
					obs: you can use ^x^f n-consecutive times to traverse the whole directory structure to find what you want.

	cw				#Changes word, try it:
						ex.: word1, word2, word3
					also works for: cb, cge, 
	
	dip				#Deletes inside paragraph
	diw				#Deletes word from having your cursor anywhere
	di"				#Deletes content inside double-quotes character
	di'				#Deletes content inside single-quote character
	di(				#Deletes content inside parantheses
	di{				#Deletes content inside { } character
	di[				#Deletes content inside [ ] character

	f				#Find given character and move cursor to it
	F				#Find previous given character and move cursor to it
	p				#Find given character and move cursor to it
	T				#Find previous given character and move cursor to it(not-inclusive)
	cf				#Changes all text from current cursor to given character
	ct				#Changes all text from current cursor to given character(not including the given character)
	c/				#Changes all text from current cursor to the given search string

	ds"				#Deletes surrounding quotes
	cs"				#Changes surrounding quotes
						Ex.: Try it: "text text text text text text"

	zz				#Centralizes screen on cursor
	ZZ				#Save and Quit
	:waq				#Save all and Quit


	v$y				#Selects & yanks whole line including \n(newline) character.
	y$				#Yanks whole line, not including \n character.

obs.: ^ stands for <ctrl> key. 

------
NEW 2022.2:
1) Text Concatenation in VIM:
	1.1 - Select Text in Visual Mode
	1.2 - Press "ay to copy text into 'a' register
	1.3 - Select Another Text in Visual Mode
	1.4 - Press "Ay to copy concatenate text content into 'a' register
	1.5 - Paste whole content using: "ap

2) Switches tab, same as gt or gT:
	2.1 - <system_key> + <ctrl> + <page-up | page-down> 
------
NEW 2021.2:
When either in command mode(:) or search mode(/) you can partially type the name of a 
command and use <up> and <down> key for searching similar commands in the history list.

------
NVIM REGEX:

/home\/user\/Documents\/		#Searches for /home/user/Documents string in a given document

/\ccase insensitive			#Case insensitive search for the given string

/ProjectMEMRAM\/[^source]		#Searches for ProjectMEMRAM/ string but excluding any results that includes source directory.
					ex.: 1) /ProjectMEMRAM/cake_code	2) /ProjectMEMRAM/source
					
					^ doing this search will exclude 2nd from being returned and include 1st

/[0-9]					#Searches for numbers only, by excluding characters range
/[^0-9]					#Excludes numbers from the search
/flavo*					#Will search for flavors, flavor, flavours, flavour (try it)
/flavo...				#Will search for flavo + any 3 next characters that follow it

------

ABOUT ROOT AND VIM SESSIONS:
	When oppening files, you should always open them as full paths instead of just using '~' character,
this will help ROOT user when he wants to load your session files!
Very Important: using ~<username> also helps in achieving this solution
			:e ~username/filepath.txt
			
ABOUT VIM MKSESSION:
When working under linux, it's important to save session for openning files like logs, grub.cfg, pacman.conf, xorg.0.log,
sudoers, etc... because of this it's VERY USEFUL to save sessions that can be later accessed when needed.

For that, use vim as root, open the files you might need, save session. Life Quality Improvement Achieved!
you can check for sessions in your root folder.

WHEN MAKE SESSIONS, it's important to open NVIM from the path/directory you want to work,
that way when you load your saved session, it'll open NVIM and use the $PWD associated with it when it was open.
WHEN SAVING SESSIONS, in this very case, it's required to type full path name of your saved sessions: 
	:mksession! ~/devweb
		as opposed to
	:mksession! devweb
otherwise devweb session will be on the $PWD path associated when NVIM was saved!! - VERY IMPORTANT

VERY IMPORTANT: sessions can also be used as workspaces, since they'll load on top of the current working space,
saving sessions can be used as a modular working spaces. 

For example.: let's say you're working in java and a c++ project.
you can save a session for oppening all your java work related tabs, files, windows, etc.
you can also save a session for the same related tabs, files, windows you're also working on the C++ project.
AND you can finally save a session for later loading both sessions at once.

This allows you to save multiple sessions for working with multiple files in the same project, since NVIM supports buffers
oppening the same files in more than one tab/windows won't hurt the project, since they refer to the same buffer.
-------------------------
STILL USEFUL HINTS:
#1 - Create a Session with all documents open as buffers,
and only make tabs and windows for the more important ones,
this will make it possible to type the document's name instead of the bookmark
as in :b <buffer_name>  instead of memorizing the actual mark

example: :tabnew | b memo*
this will create a new tab for the buffer memo!

#2 - VISUAL MODE "SHORTCUT" - YANK

Instead of using visual mode to yank text into a register, you can do this:
"ay$      #Yanks the entire line into register 'a'
"Ay$	  #Yanks the entire line into register 'a', by appending the content as a new line into this register
"+y3W	  #Yanks 3 words into system's clipboard

If you had to use Visual Mode, you'd have to do this instead for the 1st command:
v$"ay    #Yanks the entire line into register 'a'

Not only it's a much larger command, it also requires visual confirmation that you're actually in visual mode, or else it won't work.

#3 - REGISTERS
All registers are globally exposed in the program, regardless of what sessions you are in,
there are no local registers.

"ay$      #Yanks the entire line into register 'a'
"Ay$	  #Yanks the entire line into register 'a', BY APPENDING the content as a new line into this register

To summon it up, you can use capsed letters to make vim append content instead of re-writting a register's content to anew.
--------------------------
VIM SEARCH & REPLACEMENT TUTORIAL:
https://www.linux.com/training-tutorials/vim-tips-basics-search-and-replace/

To match one or more, use the \+ quantifier. A search for /abc\+ will match abc, but not abby or absolutely. 
For zero or one, use \=, which would match abc, abby, and absolutely.

Vim can be even more precise, and will allow you to specify an exact number or range. The syntax for this is \{0,10}, 
where the search would match 0 to 10 instances of the character.


:8,10 s/search_text/replacing_text/g 		#Replaces text from line 8 to 10
:%s/search/replace/g				#Replaces text on the entire text

/2020.*-T    or /\c2020.*-T 			# \c adds Case Insensitive Search -  VERY USEFUL
						# Searches for any string that contains 2020 FOLLOWED by -T substring
						# The .dot character tells to find 2020 followed by any -T string.

/text$						# Finds all lines that ends with text
/^text						# Finds all lines that begins with text
/text.$						# Finds all lines that ends with text$, followed by $ 
						ex.: textblack

/text$$  					#Finds all lines that ends with (same as above) text$ 
						 ex.:    text$

/text.*$					# Finds all lines that ends with text, regardless if it's followed by another string - HOW? CHECK LATER
						 ex.: textBlack
------------------------------
Matching lines with White Spaces:
To match whitespace, use the \s operator. If you wanted to find empty lines that contain nothing but 
whitespace, you could use ^\s.*$(TOO OLD-DOESNT WORK). This will match lines with whitespace, but no other characters — it won’t 
match empty lines without white space. By contrast, the \S operator will match non-whitespace characters.

/^\s*$ 						#Matches lines that contain empty blank character spaces
						#Only counts 1 Blank Space
						#\s fetches blank character spaces 

/\s*$						#Same as above more efficient, counts any blank spaces

/^\s.\s$					#Fetches all lines that begin and end with white space
---------------------------------------
	CONFIRMATION TO AVOID DAMAGE DURING TEXT REPLACEMENT:
You may also wish to be asked for confirmation before Vim makes a substitution. To do this, add the confirm (c) 
option to the end of the search and replace command: :%s/search/replace/gc. When you run this search, 
Vim will give you a prompt that looks something like this:

replace with foo (y/n/a/q/l/^E/^Y)?

The “y” and “n” are self-explanatory, but what about the rest? To tell Vim to go ahead and replace all 
instances of the matched string, answer with a. If you realize that you don’t really want to make the changes, 
you can tell Vim to quit the operation using q. To tell Vim to make the current change and then stop, use l, for last.

^E and ^Y allow you to scroll the text using Ctrl-e and Ctrl-y.

------------------------------
	WHERE U LAND:
To land on the last character in the matched string, rather than the beginning, add an /e to your search:

/Debian/e

That will place the cursor on the n rather than the D. Vim also allows you to specify a cursor offset by line, or from the 
beginning or end of the string. To have the cursor land two lines above a matched string, for example, use /string/-2. 
To place the cursor two lines below the string, use /string/+2.

To offset from the beginning of the string, add a /b or /s with the offset that you want. For example, to move three 
characters from the beginning of the search, you’d use /string/s+3 or /string/b+3 — “s” for “start” or “b” for “begin.” 
To count from the end of the string, use /e instead, so /string/e-3 will place the cursor on the third character from 
the last character of the matched string.
-----------------------------------
VIM REGISTES:
Sr.No	Register & Description
1	% Name of the current file
2	# Name of the alternate file for the current window
3	: Most recently executed command
4	.  Contains the last inserted text
5	“ Last used register
Example: "%p  #Pastes current file path
Example2.: :let @+=@%   #Lets register + be equal to register %'s current content

TEXT REPLACEMENT:
:%s/text/replacement_text/g search through the entire document for text and replace it with replacement text.
:%s/text/replacement_text/gc search through the entire document and confirm before replacing text.

INTERNAL VARIABLES:
:help internal-variables

--------------------------------
VIM SCRIPT CHEATSHEET:
https://devhints.io/vimscript  #Introduction to Vim Scripts

let l:name = "John"   #defines local variable name and attributes "John" to it

SCRIPT EXAMPLE:
function! SuperTab()
  let l:part = strpart(getline('.'),col('.')-2,1)
  if (l:part =~ '^\W\?$')
      return "\<Tab>"
  else
      return "\<C-n>"
  endif
endfunction

MAPPING TO FUNCTION:
imap <Tab> <C-R>=SuperTab()<CR>


Note.: You can either put this in a script (script.vim) and run it (:source script.vim), 
or you can type the commands individually in normal mode as :let and :echo.
-------------------------------

gF  #very useful when openning other source-code linked libraries

:mksession [My_session.vim]    - Creates a Session named My_session.vim
:source teste                  - Loads a Session named teste

------------------
Starting up VIM: 
	vim -S [session_name]          - will load the given session
------------------

A Session saves all tabs/windows/buffers
Loading from one will bring them back

To list all sessions available, go to terminal and type: dir
all files without an extension or under the .vim are sessioNS You might have created

Whenever openning a new session, you should close all buffers with :%bdelete!

:%bdelete - Closes all buffers
:%bdelete! - Forcibly closes all Buffers
:tabonly - Kills all tabs, but keeps the buffers
:only - Closes all windows, keeps buffers and tabs opens
:xa[ll] - Kills all buffers, closes VIM


:find [filename]   -  finds all file that has/starts/ends with [filename]

--END VIM TUTORIAL---
---NEW DD Tutorial 2020.2-T ---------
#1 Backups /dev/target_partition to of(output file):
dd if=/dev/target_partition of=/home/user/partition.image

--NEW QEMU TUTORIAL 2020.2-T ----------
You can try qemu with different firmware other than defaul SeaBoot:
edk-ovmf is a pretty stable firmware for qemu

Android x86:
https://osdn.net/projects/android-x86/

1 - CREATING A DISK IMAGE:
	QEMU-IMG:
qemu-img create -f <file_format> <image_name> <image_size>  #BECAREFUL WHEN COPYING & PASTING INTO TERMINAL AS SUDO, IT'LL FORMAT ANY IMAGE U'VE PREVIOUSLY MADE BEFORE!

ex.: qemu-img create -f qcow2 test-img.qcow2 5G
	or
qemu-img create -f raw vm_macine.raw 5G
	or
archlinux: qemu-img create -f raw archlinux-install.raw 5G #DO NOT USE THIS TWICE

^ This Create a disk image for Virtual Machine using qemu-img

2 - INSTALL AN OPERATING SYSTEM IN DISK IMAGE:
	qemu-system-x86_64 -cdrom <iso_image> -cpu host -enable-kvm -m <mem_size> -smp <core_numbers> -drive file=<disk_image>,format=raw
where,

-cdrom is for iso_image,

-cpu host is to emulate the host processor. There is a list of supported architectures available – qemu-system-x86_64 -cpu ?

-enable-kvm starts QEMU in KVM mode,

-m is for memory (RAM),

-smp is to specify the number of cores a VM could use.

For instance,

example.:
	qemu-system-x86_64 -cdrom /path/to/iso_image  -cpu host -enable-kvm -m 2048 -smp 2 -drive file=/path/to/disk_image/test-img.qcow2,format=qcow2 
		or
qemu-system-x86_64 -cdrom /path/to/iso_image  -cpu host -enable-kvm -m 8046 -smp 2 -drive file=/path/to/disk_image/test-img.raw,format=raw

ARCHLINUX:
archlinux step #1: this creates a new raw disk image that has 5GB as Total Disk Size
	$ qemu-img create -f raw "archlinux-disk.raw" 5G

archlinux step #2: this will boot installation media using the 
	$ qemu-system-x86_64 -device usb-ehci,id=ehci -device usb-host,bus=ehci.0,hostbus=2,hostaddr=7 -cdrom ./archlinux\ installation\ media.iso -cpu host -enable-kvm -m 8046 -smp 2 -drive file=./archlinux-disk.raw,format=raw

Alternate: 
qemu-system-x86_64 -device usb-ehci,id=ehci -device usb-host,bus=ehci.0,hostbus=2,hostaddr=5 -cdrom ./archlinux\ installation\ media.iso" -cpu host -enable-kvm -m 8046 -smp 2
^The alternate option above only loads the USB as DISK


ABOUT USB:
	OBS.: type in: lsusb to find out your hostbus and hostaddr(device number).
	alternative: lsusb -t

END ARCHLINUX---
--------------------------------------------------------------
-------------------------------------------
***BOOT ORDER OPTION:
	-boot once=d    #boots from iso first, very important if you want to boot into firmware from the Installation media!

***USING GPU ACCELERATED GRAPHICS:
virtio-vga / virtio-gpu is a paravirtual 3D graphics driver based on virgl. Currently a work in progress, supporting only very recent (>= 4.4) Linux guests with mesa (>=11.2) compiled with the option gallium-drivers=virgl.

To enable 3D acceleration on the guest system select this vga with -device virtio-vga-gl and enable the opengl context in the display device with -display sdl,gl=on or -display gtk,gl=on for the sdl and gtk display output respectively. Successful configuration can be confirmed looking at the kernel log in the guest(vm machine): 
	$dmesg | grep drm 

***USING UEFI ON QEMU
	1. Use the package manager and download & install ovmf firmware
	2. add this when initiating your VM using qemu: -bios /usr/share/ovmf/OVMF.fd

***ABOUT ARCHLINUX INSTALLATION:
	Use fdisk /dev/sdX to create GPT Table and EFI and EXT4 partition, followed by mkfs.ext4 /dev/sdX
	in both steps /dev/sdX must not be mounted!

***MOUNTING QEMU IMAGE ON CURRENT SYSTEM
	0. It's important to not try to boot QEMU while doing this, or permanent data damage will be caused to the image.
		0.1 - To avoid mistakes, backup your IMG File!
		0.2 - Remember to umount the disk/partition once you're finished with all steps!

	1. type "fdisk -l <image_file>"
		ex.: fdisk -l archlinux_vm.raw
	
	OUTPUT PRODUCED:
		Disk archlinux_vm.raw: 125 GiB, 134217728000 bytes, 262144000 sectors
		Units: sectors of 1 * 512 = 512 bytes
		Sector size (logical/physical): 512 bytes / 512 bytes
		I/O size (minimum/optimal): 512 bytes / 512 bytes
		Disklabel type: gpt
		Disk identifier: 6D98EB5D-7D4F-2441-92A9-DFBDB5120BE6

		Device              Start       End   Sectors   Size Type
		archlinux_vm.raw1    2048   1048576   1046529   511M BIOS boot
		archlinux_vm.raw2 1050624 262141951 261091328 124.5G Linux filesystem
	
	2. From the given output, extract "Sector Size" and the Start Sector of the partition you want to mount.
		2.1 - Multiply Secotr Size by the Start Sector of the partition
			ex.: 512x1050624 = 537919488
	
	3. Mount the partition as follows
		3.1 - #mount--mkdir -o loop,offset=537919488 archlinux_vm.raw /mnt/archlinux_vm
	
	4. Final Step: Once you're done remember to umount the disk with: 
		4.1 - #umount /mnt/archlinux_vm

	5. Remember Step #0 before booting your virtual QEMU machine!

1ST ALTERNATIVE TO MOUNTING QEMU IMAGE:
	BASH SCRIPT:
		[script]
			#!/bin/bash
			# usage: qemu-mount {imagefile}
			# 1st argument: QEMU raw image file

			if [ $# -ne 1 ] ; then
			  echo 'usage: qemu-mount imagefile'
			  echo 'Mounts a QEMU raw image file to /tmp/dos'
			  exit 1
			fi

			start=$( fdisk -l -o Device,Start ${1} | grep "^${1}1" | gawk '{print $2}' )
			sectors=$( fdisk -l ${1} | grep '^Units: sectors of' | gawk '{print $(NF-1)}' )
			offset=$(( $start * $sectors ))

			[ -d /tmp/dos ] || mkdir /tmp/dos
			sudo mount -o loop,offset=$offset ${1} /tmp/dos
		[/script]


2ND ALTERNATIVE TO MOUNTING QEMU IMAGE:
	1. Install libguestfs or libguestfs-tools
	2. guestmount --add image.img --mount /dev/sda1 /tmp/dos

***CREATING AN ETHERNET DEVICE | TAP NETWORKING WITH QEMU:
	Normally QEMU just shares the host's network with the guest/virtual machine,
however, it's possible to create a virtual network device so that the virtual machine appears to be a different computer in the network! This is desirable if you want your virtual machines to be able to talk to each other, or if you want other machines on your LAN to be able to talk to the virtual machines.

Use the command below when initializing your virtual machine:
	-device virtio-net,netdev=network0 -netdev tap,id=network0,ifname=tap0,script=no,downscript=no,vhost=on

note: vhost=on is only meant for increasing performance of the virtual machine.
-------------------------------
-------------------------------
CM IMG:
qemu-system-x86_64 -cdrom "cm-x86_64-14.1-r4.iso" -cpu host -enable-kvm -m 8046 -smp 2 -drive file="disk_img.raw",format=raw

-------------------------------
QEMU VERY IMPORTANT
ctrl+alt+g			#Returns the mouse/cursor out of the VM Operating System.
-------------------------------
Android IMG(new 2022.3):
	qemu-system-x86_64 -cdrom ./android\ installation\ media.iso -cpu host -enable-kvm -m 4G -smp 2 -drive file=./android\ disk\ img.raw,format=raw
^ NEW NUBANK ACCOUNT


	qemu-system-x86_64 -cdrom ./android\ installation\ media.iso -cpu host -device AC97  -device virtio-vga-gl -display sdl,gl=on -enable-kvm -m 8G -smp 2 -drive file=./android\ disk\ img.raw,format=raw
^ Same as above, has Hardware GPU Acceleration

	Archlinux:
		qemu-system-x86_64 -cdrom ./archlinux\ installation\ media.iso -cpu host -enable-kvm -m 4G -smp 2 -drive file=./archlinux\ disk\ img.raw,format=raw


	qemu-system-x86_64 -cdrom ./archlinux-2022.11.01-x86_64.iso -cpu host -device AC97  -device virtio-vga-gl -display sdl,gl=on -enable-kvm -m 8G -smp 2 -drive file=./disk_image.raw,format=raw,if=virtio,aio=native,cache.direct=on
^ Same as above, has Hardware GPU Acceleration
--------------------------
Android IMG(old):
	qemu-system-x86_64 -cdrom "android_installation_media.iso" -cpu host -enable-kvm -m 8046 -smp 2 -drive file="some_android_disk_img.raw",format=raw
^ WORKS!

^ Same as above, but has VIRTIO ENABLED and AUDIO: | source: https://android.stackexchange.com/questions/231158/step-by-step-guide-to-build-androidx86-with-virtual-box-guest-os-integrations:
	qemu-system-x86_64 -cdrom "androi_installation_media.iso" -cpu host -device AC97 -vga virtio -display sdl,gl=on -enable-kvm -m 8046 -smp 2 -drive file="android_disk_raw_image.raw",format=raw

qemu-android -cpu host -enable-kvm -m 8046 -smp 2 -drive file="android_disk_img.raw",format=raw

qemu-android -hda "some_android_raw_file.raw" -boot d

qemu-android -cdrom "some_android_file.iso"

Void Linux ISO INSTALLATION:
qemu-system-x86_64 -boot once=d -cdrom "some_void_linux_file.iso" -cpu host -enable-kvm -m 4046 -smp 2 -drive file="/dev/sdd",format=raw 

	^ -boot once=d	boots only one time from installation media!

	^ VERY IMPORTANT: In order to install an OS in any given partition, you'll need to provide the whole disk
	  instead of a single partition!

Void Linux RUN:(old)
qemu-system-x86_64 -boot once=d -cdrom "some_iso_image.iso" -cpu host -enable-kvm -m 4046 -smp 2 -drive file="/dev/sdd4",format=raw

VOID LINUX(NEW):
qemu-system-x86_64 -boot once=d -cpu host -enable-kvm -m 4046 -smp 2 -drive file="/dev/sda",format=raw

Ubuntu:
qemu-system-x86_64 -boot once=d -cpu host -enable-kvm -m 4046 -smp 2 -drive file="/dev/sda",format=raw

Ubuntu 2:
qemu-system-x86_64 -boot once=d -cpu host -device AC97 -vga virtio -display sdl,gl=on -enable-kvm -m 8046 -smp 2 -drive file="/dev/sda",format=raw

Untested:
qemu-system-x86_64 -boot c -enable-kvm -smp 4 -device virtio-vga,virgl=on -cpu host -m 8046 -hda -display sdl,gl=on file="some_raw_img.raw",format=raw

FREEBSD (2023):
qemu-system-x86_64 -cpu host -device AC97 -vga virtio -display sdl,gl=on -enable-kvm -m 8046 -smp 2 "freebsd.cqow2"
^ GPU VIRTIO AND SOUND ENABLED. | Works

-----
-----MORE ABOUT QEMU:

1 - LOADING A LINUX SYSTEM THAT HAS NO BOOTLOADER:
	1.2 - You'll need an installation that currently has grub.cfg on /boot/grub/
	1.3 - You'll need to add your new OS Entry on that grub.cfg
	1.4 - Then you'll summon the grub.cfg through qemu:

qemu-system-x86_64 -boot once=d -cdrom "some_void_linux.iso" -cpu host -enable-kvm -m 4046 -smp 2 -drive file="/dev/sda",format=raw 


Very Important: /dev/sda is the disk that you can actually boot into!
Follow STEP 2 below in case you have your OS installed in a different disk!!! VERY IMPORTANT

2 - LOADING MULTIPLE DISKS IN A SINGLE VIRTUAL MACHINE:
	2.1 - Add the disk containning the new OS into to the above qemu command:

qemu-system-x86_64 -boot once=d -cdrom "some_void_linux.iso" -cpu host -enable-kvm -m 4046 -smp 2 -drive file="/dev/sda",format=raw -drive file="/dev/sdd4",format=raw


VERY IMPORTANT: note that sdd4 is the actual partition where the new OS is located!
VERY IMPORTANT2: REQUIRES OS PARTITION TO BE MOUNTED!
----
KNOWN ISSUES:
	WHEN DISK WHERE THE RAW IMAGES BELONGS TO ARE FULL, QEMU WILL AUTO-PAUSE ITSELF WITHOUT GIVING ANY WARNING! - Old Issue not sure it's still there

------------------------------
================================================================================
--NEW BASH TUTORIAL 2020.2-T EDITION 1 - ---
VERY IMPORTANT:
Note.: Note that 'vi' is included in most UNIX operational systems on a default base.

USEFUL:
	v$ and v0 are very useful for selecting text in visual mode!
	xdg-mime query default inode/directory   	  #Determines default file explorer on per-user basis
	xdg-mime default pcmanfm.desktop inode/directory  #Sets default File Explorer on per-user basis
							  #inode/directory indicates what's the default application is for.
	NOTE.: The XDG MIME Applications specification builds upon the shared MIME database and desktop entries to provide 
		default applications.
	In Gnome go to search, type: Tweaks  #Set startup applications

KNOWN ISSUES:
	In PcmanFM, go to Edit > Preferences > Advanced, Terminal option is case-sensitive,
	setting a wrong information will likely block applications from being open from PcmanFM.

VERY USEFUL:
whereis <program_name>       			#gives location of installed program
						#how to find a package/program directory

dbus-run-session & <program_name>             	#Launches instance of a given program from terminal
					        #one which won't close upon console/terminal being terminated
						^ Using operator '&' and executing 'disown'  would have the same effect.

dbus-launch & <program_name>		        #Does the same as above
dbus-<TAB_KEY><TAB_KEY>	     


Note.: It's also possible to show a list of available commands/directories/paths:
	ls /<TAB_KEY><TAB_KEY>    #also works with other commands as echo $<TAB_KEY><TAB_KEY>

====.bashrc: 
CREATING ALIAS ON LOCAL BASH:
	echo 'alias ytmp3="youtube-dl --extract-audio --audio-format mp3"' >>  ~/.bashrc

DBUS--:
In computing, D-Bus (short for "Desktop Bus") is a software bus, inter-process communication (IPC), and remote procedure 
call (RPC) mechanism that allows communication between multiple processes running concurrently on the same machine.
D-Bus was developed as part of the freedesktop.org project, initiated by Havoc Pennington from Red Hat to standardize 
services provided by Linux desktop environments such as GNOME and KDE.

example.: using dbus-launch and dbus-run-session it's possible to launch/request another program's service
and to fetch it's return value into the current executing application. It's also possible to send data from the 
currently executing application into another application assynchronously from the terminal; this same program
processing the received data can also answer with status data to the caller application.
with d-bus it's possible to send/receive data synch and asynch.

The dbus-daemon plays a significant role in modern Linux graphical desktop environments.
------------------
	TEST DISK TUTORIAL 2020.2-TS
		try testdisk /dev/sdc7 instead of /mnt/sdc7!

	7z TUTORIAL 2020.2-TS

7z x NetbeansProject1.7z -oanothertest -pmypassword   #-o ouputs extraction to a directory called anothertest
						      #-p sets the password for extraction
New Form:
7z a -t7z directory1				#Directory1 are the files to be compressed

exemple 2:
       7z a -mhe=on -pmy_password archive.7z a_directory
              add all files from directory "a_directory" to the archive "archive.7z" (with data and header archive encryption on)

For more info:
man 7z

-------------------
================================================================================
GRUB RECOVER MODE / FIX MISSING GRUB BOOT ISSUE:
From grub recover mode shell, type:

	[code]
		set prefix=(hd0,msdos1)/boot/grub/
		insmod normal
		normal
	[/code]

note:
ls /	#will list all available bootable disks/partitions
	#if set preset fails, then disk isn't bootable - CHECK

ls (hd0,msdos1)/boot/grub   ##Checks if disk/partition is bootable from grub
			    ##It won't be bootable if no files exist, specially grub.cfg

GRUB PROBE--:
grub-probe -t efi_hints /media/srmf/E0069AC0069A96DE/ 
grub-probe -t bios_hints /media/srmf/E0069AC0069A96DE/
grub-probe -t baremetal_hints /media/srmf/E0069AC0069A96DE/

^ Both commands will show the current system grub naming-convetion for the desired mounted hard-drive in /media/E0069AC... etc
grub-probe needs to be used because when editing /boot/grub/grub.cfg, because grub1 and grub2 have different naming convetions!

This can help adding new OS Entries in Grub Menu at /boot/grub/grub.cfg
It is always a good idea to make a grub backup file at the same folder or other place to avoid this kind of situation.

----------------------
================================================================================
COMMANDS:
ECHO--:
echo $PATH	#Prints Path Variable, allows added programs to run on bash from any directory
		#And also to be called by any other programs 

echo $PWD	#Shows Current Bash Path
echo $DIRSTACK  #SAME AS ABOVE?

echo $_		#Shows Previously used Linux Tool 

echo $<TAB_KEY><TAB_KEY>     #Lists all Environment Variables 


DF, DU--:
df -h		#shows disk space available and used, h - human readable
		#ex.: df -h /mnt/sda1

df -ha		#Shows all property from df in human readable language(uses MB instead of B)
du -hs 		#shows disk usage per file/folder, h - stands for human readable, s - stands for simple
		#ex.: du /home/Downloads/


CP, RM--:
cp -r <Path>    #Recursively removes all files and sub-directories in a given path PERMANENTLY!
rm -r <Path>	#Recursively removes all files and sub-directories in a given path PERMANENTLY! 
			NEVER <TAB> <ENTER> AT THE SAME TIME ON THIS COMMAND!
			ALWAYS CHECK TWICE AFTER <TAB> KEY IS PRESSED!!!!!!!!!!!!!!!!!!!!!!1

LS--:
ls -a <Path>    #Shows all hidden files in a given path
ls -hs <Path>   #Better alternative to du -hs, if you want to check on particular files/folders
ls -l  		#Lists permission for files, and Directory/File type, also shows how much files exists
		#Ex.: -rwxrwxrwx 30
			- #indicates file type
			1st rwx  #indicates user access
			2nd rwx  #indicates group access
			3rd rwx  #indicates other access
			Number.: 30  #indicates how much files exists
		
		'r' stands for read, 'w' stands for write, 'x' stands for execute.
		file types can be indicated by:
			'-' for file, 
			'd' for directory, 
			'i' for link(as in symlink, softlink, hardlink),
			'b' file block
			'c' special character file
			'p' channel
			's' socket

ls -lhs	        #Shows permission, file/directory, and also makes it human readable and simple
ls -lahs        #The same as above, but shows hidden files/paths

CHMOD--:
chmod u=rw,g=rx,o=--- test.txt   #Sets user, group and other users permission for text.txt
			      	 #Symbolic mode for defining permissions

chmod 644 test.txt            #Same as above - Octal Notation Method
				      #R(EAD) has the value of 4
				      #W(RITE) has the value of 2
				      #(E)X(ECUTE) has the value of 1
				      #NO PERMISSION has the value of 0

chmod u+rw test.txt	      #Appends new permissions to pre-existing ones
chmod g+r test.txt	      #Does the same as "chmod 644 test.txt" Above - Note.: Appends read access to current user group
chmod o+r test.txt

chmod a+w test.txt	      #Appends write permission to ALL: users groups and others

CHOWN--:
chown [user_name] [file_name] 

CHMOD--:
cgrp [group_name] [file_name]
--------------------------------
================================================================================
UPDATING ARCHLINUX GPG KEYS / UPDATING ARCHLINUX PGP SIGNATURE KEYRINGS:

	pacman -S archlinux-keyring  			#Updates GPG Keys
			or
	pacman -Syy archlinux-keyring 

	Note.: It's always a good idea to update mirrors with pacman -Syu before downloading archlinux-keyring

	Alternatively:
		#gpg --refresh-keys;
		#pacman-key --init && pacman-key --populate archlinux;
		#pacman-key --refresh-keys;

	note.: you can safely ignore errors at this point.

	pacman -Syy  #Updates Pacman Mirrors
	pacman -Syu  #Updates System

if packages are conflicting during install, try overriding the install with the new installation by
listing all conflicting packages


FIXING GPGME ERROR: NO DATA ON ARCHLINUX
	Simply use this command:
		# rm -r /var/lib/pacman/sync/ 

If it still doesn't work, try changing/upgrading the mirrorlist at /etc/pacman.d/mirrorlist
to a known working one.

FIXING BROKEN PACKAGES FROM A SECONDARY LINUX INSTALLATION
	Both 'arch-chroot' and 'pacstrap -K' can be used to install
packages on a broken linux.

--------------------------------------
================================================================================
IFCONFIG / IP ADDR:
these commands can be used to find your IP Address in the network
However a Few years back, ifconfig was the favorite way to know IP address in Linux. 
Unfortunately, ifconfig command has been deprecated.(NOT TO BE MISTAKEN WITH ifcfg)

	ALTERNATIVE OPTION(archlinux):
		you can use 'archey4' to see your IP Address.

PARTX -s /dev/sdc:
PARTX -L:
You can use partx -l command to list the partitions in order to find the used space and can subtract from total space. You shall notice that but all numbers are in 512-byte sectors.
-------------------------------------
================================================================================
REMOVE CACHED PACKAGES ON ARCH 2020.1-T:
The storage directory for pacman downloaded packages is /var/cache/pacman/pkg/.  The old and uninstalled packages are not automatically removed and you need to deliberately do the cleaning manually. You can choose to clean all cached packages or specific.

sudo pacman -Sc   # removes all cached packages that aren't currently installed
                  # keeps locally installed packages

REMOVE ALL THE CACHED PACKAGES BY EMPTYING THE CACHE FOLDER
It is possible to clear everything inside the /var/cache/pacman/pkg/ directory using below command:

sudo pacman -Scc

Note.: This also prevents from reinstalling a package directly from the cache folder in case of need, thus requiring a new download. It should be avoided unless there is an immediate need for disk space.
-------------------
================================================================================
BASH SCRIPT EXAMPLE-T:
.bashrc is a file used for using bash auto-scripts

This is an example of bash script:

#!/bin/bash

media_path=/run/media/<username>

#mydisks and partitions
sdb1=/dev/sdb1
sdb2=/dev/sdb2
sdb5=/dev/sdb5
sdb6=/dev/sdb6
sdb7=/dev/sdb7
sdb8=/dev/sdb8
sda1=/dev/sda1
sda2=/dev/sda2
sda5=/dev/sda5

if [ $1 == '-mount' ] 
then
	#MOUNTS SDB1
	$(mkdir $media_path/sdb1)
	$(mount $sdb1 $media_path/sdb1/)

	#MOUNTS SDB2
	$(mkdir $media_path/sdb2)
	$(mount $sdb2 $media_path/sdb2/)

	#MOUNTS SDB5
	$(mkdir $media_path/sdb5)
	$(mount $sdb5 $media_path/sdb5/)

	#MOUNTS SDB6
	$(mkdir $media_path/sdb6)
	$(mount $sdb6 $media_path/sdb6/)

	#MOUNTS SDB7
		$(mkdir $media_path/sdb7)
		$(mount $sdb7 $media_path/sdb7/)


	#MOUNTS SDB8
	$(mkdir $media_path/sdb8)
	$(mount $sdb8 $media_path/sdb8/)

	#MOUNTS SDA1
        $(mkdir $media_path/sda1)
        $(mount $sda1 $media_path/sda1/)

	#MOUNTS SDB2
        $(mkdir $media_path/sda2)
        $(mount $sda2 $media_path/sda2/)

	#MOUNTS SDA5
        $(mkdir $media_path/sda5)
        $(mount $sda5 $media_path/sda5/)
elif [ $1 == '-umount' ]
then
	$(umount $sdb1)
	$(umount $sdb2)
	$(umount $sdb5)
	$(umount $sdb6)
	$(umount $sdb7)
	$(umount $sdb8)
	$(umount $sda1)
	$(umount $sda2)
	$(umount $sda5)
fi
---------------------------
================================================================================
SYSTEMCTL---:
systemctl start <service_name> #Archlinux
service start <service_name>   #Unbuntu, Debian

PACMAN LOCKED DOWN / PACMAN LOCKDOWN:
 rm /var/lib/pacman/db.lck      #in case pacman/yay gets lockdown

SUDO---:
sudo su				#logs in as root/sudo user
sudo -iu <user_name> 		#login as that username
su <user_name>			#logs in as another username


MORE TUTORIAL:
You can pipeline certain programs into another programs, example:
du -h / | nvim

VIM:
INSIDE vim you can tell vim to write a command's result into a buffer:
	1 - Create a buffer, write it to disk
	2 - Go to VIM command mode by pressing ':'
	3 - type in: !<command>  > [buffer_name]
	4 - the buffer should be automatically refreshed!

EXTRA COMMANDS:
archlinux-java status
------------------------------
================================================================================
----VIM----------------------VIM
--VIM NEW 2020.2-T VIM TUTORIAL EDITION 2(old, edition 1 better!), MADE ON WINDOWS--

USEFUL HINTS 2021-T:
	set title	#sets a buffer name

VERY IMPORTANT:
Note.: Note that 'vi'(NVIM predecessor) 
is included in most UNIX operational systems on a default base.


WHEN WORKING WITH VIM AND BASH:
1 - you can copy current bash path by typing: echo $PWD  or echo $DIRSTACK
	into a register.
2 - <C-R> will allow you to paste register content into command mode

3 - Finnaly:
	:e <C-R>[register]/filename.txt  
		#this will allow you to open a file/directory from the current path you want

VIM:
INSIDE vim you can tell vim to write a command's result into a buffer:
	1 - Create a buffer, write it to disk
	2 - Go to VIM command mode by pressing ':'
	3 - type in: !r <command>  > [buffer_name]
	4 - the buffer should be automatically refreshed!

Abrir configurações vim:
	:e ~/.vimrc   #~ = System Path ? - CHECAR

DICA:
Utilize NVIM e GVIM da seguinte forma:
	Utilize NVIM para trabalhar com os documentos do dia a dia.
	Utilize GVIM para mexer com os arquivos não tão frequentes, de pouca importancia.
Motivo.: GVIM e NVIM mantém historico de arquivos únicos,
sendo assim, se você fechar um arquivo no GVIM, este pode ser recuperado pelo histórico.
Isso salva tempo, pois não há necessidade de: 
	1 - Nem navegar por todos os diretórios
	2 - Nem criar bookmarks pra arquivos não-cotidianos,
	pois os bookmarks já são criados e armazenados nos marks de 0 à 10.
	(ver dica 1.SP)

	Opcionalmente você também pode criar Bookmarks sem se preocupar de sobreescrever
	marks já existentes Na Apliciação VIM Alternativa de sua escolha.

	TODO.: Pesquisar se é possível criar áreas de trabalho especificas para cada 
	instancia VIM. Isso resolveria o problema de usar VIMs diferentes.

	1 - Start VIM from a different path
	2 - How to load different .vimrc file for different working directory

HOW TO DELETE LAST WORKING BUFFER?

https://stackoverflow.com/questions/18932012/how-to-load-different-vimrc-file-for-different-working-directory

1.SP - OPENED FILE HISTORY IS MAINTENED BY MARKS {0..10}:
	'0 - Opens last opened file
	'1 - Opens 2nd opened file
	etc..

Check existing ones by typing :marks in command mode.

MOVES TO THE FILE NAME UNDER THE CURSOR:
	gf - #moves to the filename under the cursor.
Note.: File needs to be on path.

EDITS DE PREVIOUSLY MODIFIED FILE: 
CTRL+6
CTRL+SHIFT+6
CTRL-^    Edit the alternate file.  Mostly the alternate file is
          the previously edited file.  This is a quick way to
          toggle between two files.  It is equivalent to ":e #",
          except that it also works when there is no file name.

	If there already was a current file name, then that one becomes the alternate file name. It can be used with "#" on the command line |:_#| and you can use the |CTRL-^| command to toggle between the current and the alternate file. However, the alternate file name is not changed when |:keepalt| is used. An alternate file name is remembered for each window.


:help CTRL-^

EXTRA COMMANDS:
:write ++enc=utf-8 russian.txt    	# Change Encoding in a File
:colorscheme <tabkey>     		# Change themes or colorscheme

:colorscheme quiet


CHANGING ENCODING IN A FILE THROUGH NVIM | NVIM UTF-8
:write ++enc=utf-8 russian.txt    	# Change Encoding in a File

SETTING PERMANENT COLORSCHEMES FOR NVIM
You can set themes, colorschemes, etc on /usr/share/nvim/ by CREATING A FILE with a prefix called .vim
example:

[file: ~/.config/nvim/init.vim ]
	colorscheme murphy
[/file]

It's also possible to set color scheme on vim session files:

	1 - First save your session with mksession
		:mksession ~/my_nvim_session.nvim

	2 - then edit the file
		$ nvim ./<vim_session_file>
	
	3 - add the following line:
		[file: ./<vim_session_file>.nvim] 
			colorscheme murphy
		[/file]

-END NEW VIM TUTORIAL--
------------------------------
================================================================================
--- BASIC LINUX TUTORIAL --- 

When typing 'man <program name>' you can always press w to check a list of commands

pwd         			# shows current full path directory - also works for VIM
ls --group-directories-first 	#Group directories together
ls -R				#shows all files recursively
ls -a      # shows all hidden files and directories
ls -Q	    # shows all files/directories in double quotes
cd [path]
cd ..       # recursive movement within current directory
cd /        # moves to root directory
cd ~/       # moves to user space directory (aka.: /home/<username>/)
ls -a -R        # shows all hidden folders/paths
rm -rf      # removes all files and folder
		NEVER <TAB> <ENTER> AT THE SAME TIME WHEN USING THIS COMMAND
		ALWAYS CHECK TWICE BEFORE PRESSING ENTER!!!!!!!!!!!!!!!!!!!!!!!1

cp <file> <copy>	    # copies a file
mv <source> <destination>   # moves a file
!!	#Executes the previous command in bash
	*same as up-enter cycle but can be used for scripting

watch -n 1 bsh ~/scripts/disk_space.sh    #prints disk space each 1 second

	---7Zip---
		7z --help    # Shows all commands 
		7z e <filename> # Uncompresses file without creating a directory
		7z x <filename> # Uncompresses file creates a directory
		7z a <filenames> # adds files to archive
		7z l <filename> # lists all files inside 

you can play videos in bash using mpv - Media Player Video
and also songs, with: mpv --no-video <filename>
		      mpv --no-video --loop <filename>

alternatively you can use cvlc, which stands for cli vlc
cvlc mode only works for mp3.

--- BASIC LINUX TUTORIAL END ---
------------------------------
================================================================================
--- GIT TUTORIAL 2020-T ---
BEFORE YOU START:
	Correct -> HEAD IS A POINTER TO YOUR CURRENT WORKSPACE LOCATION
	That is HEAD will always point to the current branch you're working at.
MASTER is the current/main branch you'll be using for your project.
COMMITS are literal reference points for each commited changes you've made in your project. 
File Update/Insertion/Removal (also, 'git add' and 'git rm') inside the MASTER Branch blocks 
you from switching to other branches, until modifications are actually COMMITED - that's unless you're
in other branches.

VERY IMPORTANT: Take note that GIT uses a global buffer for implementing non-commited changes made to files called 
INDEX, so when you move between branches/heads, you're taking the INDEX to whatever branches you are moving from/to,
until you 'git commit -m "Commit Name"' those changes, only then it turns these changes from global
to local modifications within that specific branch(aka.: where your current HEAD is point at). - NOT CHECKED/UNCHECKED

	Further More.: Index is a view of your working directory that is ready for commit. 
	It can be seen as a pre-commit state and is not as simple as a "list of files". 
	When you do git add, the file (with the change) is added to the index and the newer 
	changes will not be see until you add them too.

Note.: INDEX/Staging Area are the same thing! - CHECKED(2023)

Also, note that GIT is a DVCS - Distributed Version Controlling Software, that means each user has
it's own repo and it's own working area. VCS - Versionning Control Software only has 1 main repository,
and each user has it's own working area.

Git was made by Linus Torvalds
read more about Checking WORKING AREA, STAGING AREA, HEAD

VERY IMPORTANT(2023): Staging Area are the files being staged by GIT! The Working Area is the Working Directory. ex.: when one uses 'git add <file>' it will add files/directory to the staging area.

	THEREFORE The Staging area and Working Area are different things!
-----------------
BEFORE YOU CONTINUE:
	On linux, you can get easy git documentation by typing: 
		$man git-log

	^ this will allow you check manual pages on git log command, which would be the same as doing: 
		$git help log


There's also a good GIT GUI Client called git-tower: https://www.git-tower.com/


	GIT EXERCISES: https://gitexercises.fracz.com/committer/ny4
----
EXERCISES SOLUTION:
	Exercise 7: git rebase hot-bugfix
		manual: https://www.benmarshall.me/git-rebase/
			https://git-scm.com/docs/git-rebase
-----------------
Working Area, Staging Area, Head Area:
seus repositórios locais consistem em três "árvores" mantidas pelo git. a primeira delas é sua Working 
Directory que contém os arquivos vigentes. a segunda Index que funciona como uma área temporária e 
finalmente a HEAD que aponta para o último commit (confirmação) que você fez.


Working Area, Staging Area, Repository:

	The Working Area:
		It's the Area that currently holds all your physical files you're currently working with in the current branch.

	The Staging Area:
		It's a Memory Buffer Area that holds all file modifications/insertions you've done 
	so at the moment. Moving to a different branch will also move the Staging Area with the same
	modified/inserted files from the branch before. Which will need to be either commited or dropped.


	The Repository Area:
		Whenever you type 'git commit -m "MyCommit"' you're applying the changes in the staging
	area to the the Repo. -- NEEDS TO CHECK

To summon it up, this is how it goes:
	GIT ADD will add new files created in the 'Working Area' to the 'Staging Area'
	GIT COMMIT will add changes made to the 'Staging Area' to the 'Repository Area'

	GIT CHECKOUT -- file will remove changes made to the Working Area FROM the Staging Area 
		and UNDO them - BEFORE THEY ARE COMMITED

	GIT RESET <HEAD/BRANCH> will undo changes made to the Working Area from the Repository Area 
	AFTER they've ALREADY been commited to the Repository Area


Example: typing 'git diff' will show all diferences between the Working Area and the Staging Area
	 'git diff --staged' will show differences between The Staging Area and the Repo Area.
	 'git diff <id1> <id2>' will show differences between Different Repos 

	 git difftool is also an alternate option.

Importante Notes:
	On Git branches are just a reference for a commit

	---IMPORTANT GIT LOG NOTES----
	git log --onoline --decorate
	git log --graph --abbrev-commit --oneline

	*you can type cat .git/HEAD   to show where you actually are
        *HEAD == WHERE YOU ARE

	*switching branches will switch the files at the Workspace Area
	*git log   #shows all commits - check
	*git log --all #shows commits that are ahead of that branch
		        normally 'git log' only shows commits done up to that moment 
			that the branch was located.
	*git log --oneline --graph  #shows divergence between branches. 
			  example: when you have made modifications/merges in different branches
			  this will make git show divergences/merges between branches.

	git log --graph

	*Fixing Merge Conflicts:(GIT MERGE & GIT MERGE --ABORT)
		What happens when you make modifications on the same file at different branches and then
	try to merge? git will warn you about conflict, and SET THE WORKING AREA with the conflictuous 
	file(s) - note.: if merge runs without conflict, there is neither files to be added nor commits - 
	then you can just open the file in a normal text editor and edit it to
	fix the conflicts.

	After that's done you can finally add the modifications done into the Working Area
	to the Staging Area with 'git add <filename>' and then commit the 
	modifications to the Repository Area with ' commit -m <"commitname"> '.

	Alternatively you can drop modifications with 'git merge --abort' to abort merge.
	The second syntax ("git merge --abort") can only be run after the merge has resulted in conflicts. git merge --abort will abort the merge process and try to reconstruct the pre-merge state

	FILE STATUS LIFECYLE:
		There are 4 File Status:

			1 - Untracked
				When files are added to the Working Area, but are not being Tracked.
			using 'git add <filename>' will start tracking them, adding them to the 
			Unmodified Status.

			When files are Untracked, git will not touch them when moving between branches or commits!
			they'll be left untouched by git's engine! - VERY IMPORTANT

			2 - Unmodified
				Files are kept as unmodified status after being tracked, 
			editing a file will turn them to Modified Status.

			Renaming/Moving a file will set them as have been deleted.
			And the Renamed/Moved file will be set as untracked.

			Same thing happens if files get Renamed/Moved AFTER being Modified.

			*After the files have been moved/rename, undoing changes will require the
			'old' file to be 'git checkout -- <filename>' and the new files to be manually
			deleted.

			3 - Modified
				After files are being tracked and get some modification through editing,
			they receive a Modified Status, these files will require to be added to the
			Staging Area with 'git add <filename>', thus turning them into the Staged Status.

			4 - Staged
				Staged Status is the final step in the File Status Lifecycle of GIT
			before they are finally commited to the Repository, thus making permanent changes.
			to move a file from the Staging Area(staged status), all that's needed is to make
			a commit 'git commit -m <"commitname">'.

http://thomascgreen.com/tech/?p=7
https://latedreamer.blogspot.com/2016/11/working-directory-staging-area.html

-----------------------------
*Note, some of the description bellows need to be updated with the Staging Area Eplanation

git --help  or man git
git help config   #gets help about configuration
git config --global user.name "Your Name"
git config --global user.email "Email Address"
git config --list
git config --global core.editor nvim     #sets nvim as main text editor for git

git init    #Initilizaes empty Git repository in .git/
		creates a .git folder inside the workspace directory - creates a git repository
		    ex.: ~/MyGitProject/      by typing 'git init' makes it starts a new repo on it
					      on a hidden .git folder

git add <filename>	       #This command updates the INDEX using the current content found in the working tree
				^ before commiting those changes.
			       #A 2nd 'git add' to an already added file, allows to save unmodified
			         ^ files onto the repo.
git add *		       #Recursively adds all files to the INDEX(Staging Area)
git add .		       #Same as above 
git add Documentation/\*.txt   #Adds all .txt files under that directory to the INDEX(Staging Area)
git add git-*.txt	       #Adds all files that start with git- and ends with .txt to the INDEX(Staging Area)
		(note.: all files under the Staging Area will need to be commited)

git restore --staged B.txt     #Removes B.txt from the Staging Area

git ls-remote		       #List references in a remote repository along with the associated commit IDs

ADDING FILES TO GITIGNORE
	--- HOW TO IGNORE FILES FROM BEING TRACKED/UNTRACKED ---

1 - Create a file called .gitignore,
2 - Add the following content as follows:
	*.o
	*.jar
	*.exe
	libraries/
3 - then add to the Staging Area: git add .gitignore
4 - finally commit it: git commit -m "IgnoringFiles"

OBS: when adding files in .gitignore file, know that it doesn't accepts './' for pointing to current directory
as the first character(s).

If you run into issues, you may have to read the following topic:
	HOW TO FIX UNTRACKED FILES

Source: https://git-scm.com/docs/gitignore

ADVANCED COMMANDS:
	git log --graph --abbrev-commit --oneline --all --reflog     #SHOWS ALL COMMITS
--------------------------------------------------------
--------------------------------------------------------
SOMETHING ABOUT GIT COMMIT AND GIT PHYSICAL STORAGE METHOD
	Whenever a commit is made, git will make a completely new snapshot of each and every single file that has been updated and/or changed.

New snapshots will not be created for files that have not been changed, instead git will keep a reference to those files from previous commits where it has been changed, saving disk space in this case.

Other VCSs will keep a reference only to changes that have been made to the files instead of keeping a completely new snapshot each time files are updated - This is called Delta Encoding - ; but as explained this is not the case with git.

	DELTA ENCODING
		Delta encoding is a way of storing or transmitting data in the form of differences (deltas) between sequential data rather than complete files; more generally this is known as data differencing. Delta encoding is sometimes called delta compression, particularly where archival histories of changes are required (e.g., in revision control software). 

^ Source: https://en.wikipedia.org/wiki/Delta_encoding

	GIT DELTA ENCODING
		Git does uses delta encoding, but differently.
Git does not keep per-file history, when it wants to do delta compression, it takes each blob, selects some blobs that are likely to be similar (using heuristics that includes the closest approximation of previous version and some others), tries to generate the deltas and picks the smallest one. This way it can (often, depends on the heuristics) take advantage of other similar files or older versions that are more similar than the previous. The "pack window" parameter allows trading performance for delta compression quality. The default (10) generally gives decent results, but when space is limited or to speed up network transfers, git gc --aggressive uses value 250, which makes it run very slow, but provide extra compression for history data.

^ Source: https://stackoverflow.com/questions/8198105/how-does-git-store-files
--------------------------------------------------------
--------------------------------------------------------
--------------------------------------------------------
Study Source: https://www.w3schools.com/git/

git status             #Shows modified files, unmodified files, tracked, untracked, and staged files
git log                #Shows all commits that have been done
git log --oneline      #Shows 1 commit per line
git log --oneline master..HEAD 		# Lists Commmits done by a single branch - VERY IMPORTANT/VERY USEFUL
					Usually, git log shows all commits including the commits that have been
					done by another branch when you merge. HOWEVER, sometimes it's useful
					to list commits that have been done by a single branch.
					ex.: branch 'shareware' usually merges into branch 'master',
					so typing 'git log' will show both commits done by shareware and master.
					^ VERY IMPORTANT, VERY USEFUL.
git log --graph --abbrev-commit --oneline	#Same
git log --graph --abbrev-commit --oneline --all		#Same, better

git show								#For commits, it shows log messages and textual diff.
git show --color-moved-ws=ignore-all-space --color-moved --stat -p	#Improved git-show


git reflog	       #Shows all commits, INCLUDING branch merge commits - VERY IMPORTANT

git reflog             #Shows all Heads

git branch	       #Shows all branches
git branch <branch_name>		#Creates a new branch on current HEAD.
git branch -c <new_branch>		#Creates a new branch BY CLONING the current branch. 
git branch -m <new_branch_name>		#Renames the current branch to a new name.
git branch -d <branch_name>		#Deletes Branch / Removes Branch
git branch -d -f <branch_name>		#Force Deletes Branch in case of issues.	

git restore --source <branch_name> -- <filename> 	#Restores/Retrieves/Copy a single file from another branch
git checkout -b <branch> <commit_id> #Creates branch from current commit or commit_id

git commit                  	    #Opens default text editor for both: naming commit and choosing 
			    	    files to commit

git commit -a 			    #Commit current work with an editor
git commit -m "Commit 1"    	    #Commmits all currently added files, and names commit as "Commit 1"
git commit -m "Commit 1" File1.txt  #Commits changes made to File1.txt, names commit as "Commit 1"    
git commit [filename] -m "message"  #Commits a single file in the current branch 
					^ you can check which files can be commited using git status

git commit -a -m "Commit Message"   #Git adds all modified files and creates a new commit / Makes a new commit

Git fixup (Haven't used this myself yet):
	$ git add ...                           # Stage a fix
	$ git commit --fixup=a0b1c2d3           # Perform the commit to fix broken a0b1c2d3
	$ git rebase -i --autosquash a0b1c2d3~1 # Now merge fixup commit into broken commit
						^ To avoid breaking the history of colleagues
						^ you should only change the branch’s history, e.g. via rebase,
						^ if the commits haven’t been pushed yet, or if you’re working
						^ on a dedicated remote branch.

git commit --amend		    #Renames the previous commit
					READ MORE ON IF YOU NEED: 
					"CHANGING THE MESSAGE OF A COMMIT THAT YOU'VE ALREADY PUSHED TO YOUR REMOTE BRANCH"

git commit --amend -m "Message"	    #Creates New Commit and Amends the previous one at the same time
					^ NOT GOOD, this will automatically commit any files on staged area.

git commit -a --amend		    #Git adds all file modifications and edits the last commit
					^ NOTE VERY Useful.

git commit -m "Message" <filename>  #Creates a commit point for one single file, even if there are multiple changes
				    on the staging/working area.

git commit <filename> -m "Message"  #Commits changes to a given file.
					^ VERY IMPORTANT: you can auto-complete directory/filenames using <tab> key
					^ ex.: git commit ./source/Main<tab><tab>
					^ will auto-complete to: ./source/MainComponentes/StaggedFile.cpp

git add -f 			    #Adds file by force to staging area.
				    ^ VERY USEFUL/VERY IMPORTANT: normally files aren't added with 'git add' 
				    if they're on the .gitignore list, so this is where it's useful.

git add -p			    #Goes into patch mode, this allows git to splits larger hunks code change 
				    ^ into smaller hunks of code changes in order to ALLOW MICRO-COMMITS inside a 
				    ^ single/multiple files that might've had too many changes.
				    ^ WARNING: giving an 's' answer will apply the staging area into the newly 
				    ^ made previous commit. DO NOT USE THIS FOR MICRO-COMMITS!

git add -p <filename>		    #Same as above, but for a single given file.

git restore <filename>		    #Restores git working tree: reverses/undoes any change made to the working/staging area
					^ VERY IMPORTANT: you can auto-complete directory/filenames using <tab> key
					^ ex.: git restore ./source/Main<tab><tab>
					^ will auto-complete to: ./source/MainComponentes/StaggedFile.cpp

git stash			    #Saves current working tree instead of reversing/undoing it, and stashes it away.
git stash pop			    #Opens stash within current working tree for commiting.
git stash list
man git-stash

					source: 
		http://git-scm.com/book/en/v2/Git-Tools-Rewriting-History#Changing-Multiple-Commit-Messages


git status -uall --ignored  	#Lists both ignored and untracked files
				^ VERY USEFUL/VERY IMPORTANT: Use this before doing any of the 'git clean' commands
				to avoid permanent data loss.

git rm <filename> 	    #removes a file from being Tracked AND DELETES IT SAFELY
git rm -f <filename>        #forcefully removes a file from being checked AND DELETES IT
git rm --cached <filename>  #removes a file from being checked, retainning the original file
			    ^remove a file from being tracked with --cached flag,  will also require 
			    file to be physically removed.

git clean -f 		    #Permanently deletes Untracked Files only, keeps directory trees intact
			    ^ doesn't deletes files in .gitignore list, as they're not considered UNTRACKED.
git clean -f -d		    #Permanently deletes Untracked Files and Directory, keeps nothing.
git clean -f -X		    #Permanently deletes Ignored Files.
			    ^ VERY USEFUL, after compiling something you can just use this to clean those
			    unwanted files that have been added to .gitignore.
			    ^ VERY IMPORTANT: Consider committing all changes first to avoid deleting unwanted files.
			    			$ git commit -a -m "done"

git clean -f -x		    #Permanently deletes both Ignored and non-ignored Untracked files
			    ^ VERY IMPORTANT: Use with caution, non-reversible action, prefer using 'git thrash' instead
			    ^ -f stands for 'force' not 'file', by default git doesn't allows the use of git-clean without
			    the use of -f; this is done in order to protect the user from unwarely deleting important files
			    on his git repository.

git ls-files --others --exclude-standard		#Lists Untracked Files
							^ this doesn't takes in consideration files that have been added 
							^ to .gitignore, since they're not considered UNTRACKED
							^ prefer using 'git status -uall --ignored' instead.

git trash		    #Moves untracked files to .trash directory
			    ^ 'git trash' doesn't exists and needs to be set as a git alias.
			    Read More ON: CREATING GIT ALIASES
			    ^ Requires the user to create a .trash folder and add it to the .gitignore file
			    ex.: $echo ".trash" >> ./.gitignore

git mv <filename>	    #Moves/renames files within git

git switch <branchname>	    #Switches to a different branch

git checkout - 		    #Goes back to where previous HEAD was set
git checkout -- <filename>  #Discard changes from being made in the commit AND UNDOES them, 
			    setting them to their original previous state 
			    ^- Use in case File has been Modified/Updated
				Checkout only works for undoing changes in the staging area.
git checkout <branch>	    #Sets HEAD into given branch

git checkout <commit_id>   		  #Goes back to the previous commit changes!
git checkout -b <new branch> <commit_id>  #Turns all current work into a new Branch without commiting
git branch -d <branch>			  #Deletes Branch / Removes Branch

git revert <commit_id>		#Pulls out any modification made by a given commit in such a way that it would be like
				if the commit has never happened. Avoid using this in public/remote repos.
				^ VERY IMPORTANT: Just removes the work done in the given commit_id from 
				^ the actual working tree(staging area) atomically and makes a new commit it.

git reset <commit_id>		#Resets the working space/staging area to that of the given commit
					^This reset also sets all of the staging area from the given commit 
				before the commit was realized. changes that have been made to the given 
				commit, will also have to be "recommited" if you intend on keeping those changes ONLY.
				always checkout the changes with git diff or git difftool after reset.

				^ VERY IMPORTANT: Always use git reset --hard <commit_id> if you want to entirely revert
				to a given commit/branch.

git reset --hard <commit_id>	#Undoes commits, Reverses Current Working Space to the one in the Given Commit State
				    ^ if file has been forcibily removed, git checkout -- <filename> is needed.
				    NOTE.: Checkout can't discard nor undo changes that already have been commited
					'git reset' undoes changes made permanently - Need to check

git reset --soft HEAD~1		#Undoes commit to the previous commit/HEAD state.
				       ^ Does not touch the index file or the working tree at all 
				(but resets the head to <commit>, just like all modes do). This leaves all 
				your changed files "Changes to be committed", as git status would put it.

git reset --hard HEAD~1		#Undoes commits and sets it to the previous commit_ID/HEAD state
					Resets the index and working tree. Any changes to tracked 
				files in the working tree since <commit> are discarded.

git reset --mixed HEAD~1	#DEFAULT BEHAVIOR: Resets the index but not the working tree (i.e., the changed files 
				are preserved but not marked for commit) and reports what has not been updated. 
				This is the default action.  If -N is specified, removed paths are marked 
				as intent-to-add (see git-add(1)).

git reset --keep HEAD~1		#Resets index entries and updates files in the working tree that are different 
				between <commit> and HEAD. If a file that is different between <commit> and HEAD has 
				local changes, reset is aborted.

				in other words, reset only occurs if same existing files have no difference between 
				them in the different commit_ID/HEAD States.

git reset --merge HEAD~1	#Resets the index and updates the files in the working tree that are different 
				between <commit> and HEAD, but keeps those which are different between the index 
				and working tree (i.e. which have changes which have not been added). If a file 
				that is different between <commit> and the index has unstaged changes, reset is 
				aborted.

			       In other words, --merge does something like a git read-tree -u -m <commit>, 
			       but carries forward unmerged index entries.

	(note.: check man pages for actual examples -> $man git-reset)	

--	GIT STASH COMMANDS:
		<ID_NUMB> is stash@{2}

git stash list		    #Lists all saved stashes
git stash show <ID_NUMB>    #Shows what data has actually been changed when the command 'git stash' was invoked,
			    just displays name-only.
			    ^ VERY IMPORTANT, since it allows you to track which changes have been actually made
			    if you forgot about when you actually saved the stash.
			    From git-stash man Page: Show the changes recorded in the stash entry as a diff 
			    between the stashed contents and the commit back when the stash entry was first created.

git stash -p <ID_NUMB>      #Same as above, but shows what has been changed in Patch Form(BOTH ARE VERY USEFUL).

git stash                   #Saves all uncommited work, for committing later on any branch/head.
git stash -u		    #Saves/Stashes uncommited work, including untracked files that are not on stage.
			    ^ DOESN'T STASHES IGNORED FILES added to ./.gitignore list.
git stash --all		    #Stashes everything, including untracked/unstaged files and ignored files.

git stash pop <ID_NUMB>     #Applies the given stash for commiting in the current branch, 
			    ^ and removes the given stash from the list.
			    ex.: 
			    		git stash pop stash@{2}

git stash apply <ID_NUMB>   #SAME AS ABOVE, but DOES NOT remove stash from the list, useful if you want to apply
			    same changes to another branch.

git stash drop <ID_NUMB>   #Removes a given stash
git stash clear		    #Removes all stored stashes, WARNING: irreversible
			
---

git diff stash		    #Checks difference between Working Area and Stash
git difftool stash	    #Same as bove, but lists file differences side-by-side, vim/nvim style

git branch				    #Lists all branches in existance
git branch <branchname>			    #Creates a new branch
git branch <branchname> <old_branch>	    #Creates new branch from old_branch commits
git branch -d <branchname>		    #Deletes a branch
git branch -df <branchname>			#Force deletes a branch
git switch <branchname>			    #Switches to a different branch
git switch --orphan <new branch>	    #Creates an empty orphan branch with no relation to any other branch.
					    ^ VERY IMPORTANT: This branch can't merge any other branches and vice-versa!

git push <remote-name> <local-branch-name>:<remote-branch-name>		#Creates new branch on remote repository based on an
									^ already existing local branch.
									^ VERY IMPORTANT: do not use :<remote_branch_name> 
									alone, always specify <local_branch>: or else
									the remote_branch will be erased if it 
									already exists!

									ex.: git push projectmram shareware:shareware
git push origin					#Pushes the current branch to remote
git push --force origin				#Force pushes current branch to remote
						Note: Useful when cleaning commit history on non-public branches.

git reset HEAD [filename|branchname]  		#Undoes file or branch deletion
				       
git checkout <branchname>	     		#Switches current branch to <branchname>
git checkout -b <branchname>	     		#Creates new branch with all the current branch commits and 
						  ^ automatically switches to it.
git checkout -b <branchname> <old_branchname> 	#Creates a new branch called new_feature with master branch commits 
						  ^ and switches to it.
git checkout --orphan <new_branch_name>		#Creates an orphan empty branch, containning no commits nor log history.

git diff					    #Shows difference between Working Area and Current Branch.
git difftool					    #ALTERNATE OPTION, but lists file differences side-by-side, vim/nvim style
git diff --name-only				    #Same as above, only displays filenames that had been changed.
git diff <branch origem> <branch destino>	    #Shows difference between two different branches.
git diff stash					    #Shows difference between Working Area and Stash.
git diff <branch_id>				    #Shows difference between Working Area and given branch_id.
git diff <filename>				    #Shows difference for a given file between Working Area and Current Branch. 

git merge <branchname>      #Commits new modifications from <branchname> to the CURRENT actual branch
			    you're working with, ex.: git checkout master  
			    			      git merge newbranch
					^Will merge all modifications from newbranch to master branch,
					It's now possible to safely delete newbranch if not needed.

git merge <New Commit ID>    #Merges New Commit into Currrent Branch 
			     Note.: You can only Merge Commits
       				^ Warning: Running git merge with non-trivial uncommitted changes is discouraged: while possible, 
				it may leave you in a state that is hard to back out of in the case of a conflict.


git merge --abort 	     #The second syntax ("git merge --abort") can only be run after the merge has resulted in conflicts. 
			      ^ git merge --abort will abort the merge process and try to reconstruct the pre-merge state.
			      warning: avoid using with uncommited changes on INDEX!

git merge --continue 	     #The third syntax, can only be used when merge conflicts has appeared.
git mergetool 		     #Executes tool for resolving merging conflicts manually
				^Requires merge tool to be set and configured.

git rebase <commit|branch>   #Merges ALL OF THE COMMITS from a given branch into the current one! - READ PRACTICAL EXAMPLE BELOW
				Rebase will present conflicts one commit at a time whereas merge will present them 
				all at once. It is better and much easier to handle the conflicts but you shouldn’t 
				forget that reverting a rebase is much more difficult than reverting a merge if 
				there are many conflicts.

				Rebase Golden Rule: Never use rebase on a public/shared branch, only use rebase 
				if you're the only developer of a given branch!

				 ^public branches are branches that other people might have checked out. If you’re 
				 developing a branch on your own and not sharing it with anyone, you could rebase it 
				 to keep the branch up to date with respect to the main branch.

				Moreover, the main branch can move forward with a fast-forward merge rather than 
				a regular merge commit. Rebasing rewrites history, and anyone having branches that 
				were checked out of the history you just unmade will be sad, angry or worse. 
				That’s one reason you can’t just push rebased branch on GitHub 
				(unless you force it and sacrifice a kitten). So just say no.  Rebasing private 
				branches is perfectly fine, and in fact often done when squashing or rearranging 
				commits, cleaning up a branch before going public with it, or just updating 
				long-running feature branch (go easy on the last one, though).

				source: https://www.benmarshall.me/git-rebase/

				PRACTICAL EXAMPLE: 
					summary: git merge turns all commits from a given branch into a single commit on the merging branch.
				Pretend that branch B has +11 different commits, that means when master merges 'Branch B' it'll turn 
				all those 11 commits into a single commit on the Master branch, while branch B remains unchanged after this.

				'git rebase' in the other hand will bring all +11 commit history into the master branch, in front of the master branch, 
				preserving the history of all 11 commmits. This makes it easier bug solving a new feature that has been completely 
				or partially developed on a different branch.

				However, you should always use 'git merge' whenever you have no intention on keeping the log history of the commmits.

git rebase --onto <base_branch> <branch_1> <branch_2> 	#Rebases branch_1 and branch_2 onto base_branch 
git rebase --onto topicA~5 topicA~3 topicA 		#Removes Commits from 3rd(excluded) to 5th(included) from 
							being rebased. *included: 5th gets removed.
							This is useful if F and G were flawed in some way, 
							or should not be part of topicA. Note that the argument 
							to --onto and the <upstream> parameter can be any valid 
							commit-ish. - NEEDS TO CHECK

git rebase --continue   #Continues rebase after conflicts have been solved
git rebase --abort	#Aborts & Reverts rebase in case of conflict

-- HOW TO DROP MODIFICATIONS FROM STAGING AREA --
git restore --staged B.txt		#Removes B.txt from the Staging Area
git restore <filename>			#Also works as of date: 2021

-- HOW TO REVERT COMMITS --
Once you have made an accidental commit in your LOCAL REPO by mistake,
it's possible to revert it by checking the last commits you've made with:
	git log

then pick the commit ID you want to revert to and do a checkout in it:
	git checkout <commit_id>

if you're working on only 1 branch, you'll need to make a spare one: 
	git branch backup


finally delete the branch that has been commited by accident:
	git branch -d <branch_committed_as_accident>    
	 ^ note.: this isn't a good idea, because it'll erase all that branch's history

recreate it:
	git branch <branch_commited_as_accident>


Now your accidental commit should be reverted.
--------------------------
--------------------------
CREATING GIT ALIASES
	
	CREATES AN ALIAS FOR LISTING BOTH UNTRACKED AND IGNORED FILES:
		$ git config --global alias.all '!echo "This is a user git-alias | Outputting:" && git status -uall --ignored'

	CREATES AN ALIAS FOR LISTING UNTRACKED FILES THAT HAVE BEEN IGNORED BY RULES ON .GITIGNORE:
		$ git config --global alias.ignored '!echo "This is a user git-alias | Outputting:" && git ls-files --others --ignored --exclude-standard'
	
	CREATES AN ALIAS FOR LISTING TRACKED FILES:
		$ git config --global alias.tracked '!echo "This is a user git-alias | Outputting:" && git ls-files -c'

	CREATES AN ALIAS FOR LISTING UNTRACKED FILES:
		$ git config --global alias.untracked '!echo "This is a user git-alias | Outputting:" && git ls-files --others --exclude-standard'

--------------------------
--------------------------
FIXING UNTRACKED FILES
	When adding files to be ignored on .gitignore, it's possible for gitignore to fail in case
the file has been added to the staging area before it was added on .gitignore by accident.

To fix this, you can either remove the file by hand from the staged area, or do a full system cleanage:

1) Commit all your current work, or else you'll lose it:
	$ git commit -a -m "Committing everything"

2) Remove all files from staging area:
	$ git rm -r --cached .

3) Re-add all files again to staging area, this time taking .gitignore in consideration:
	$ git add .

4) Commit untracked files change:
	$ git commit -m "Fixed untracked files"

Here is the complete script:
	$ git commit -a -m "Committing everything"

	$ git rm -r --cached .
	$ git add .
	$ git commit -m "fixed untracked files"

--------------------------
--------------------------
CHANGING THE MESSAGE OF A COMMIT THAT YOU'VE ALREADY PUSHED TO YOUR REMOTE BRANCH

If you've already pushed your commit up to your remote branch, then - after amending your commit locally (as described above) - you'll also need to force push the commit with:

git push <remote> <branch> --force
# Or
git push <remote> <branch> -f

WARNING: force-pushing will overwrite the remote branch with the state of your local one. If there are commits on the remote branch that you don't have in your local branch, you will lose those commits.

WARNING: be cautious about amending commits that you have already shared with other people. Amending commits essentially rewrites them to have different SHA IDs, which poses a problem if other people have copies of the old commit that you've rewritten. Anyone who has a copy of the old commit will need to synchronize their work with your newly re-written commit, which can sometimes be difficult, so make sure you coordinate with others when attempting to rewrite shared commit history, or just avoid rewriting shared commits altogether.
--------------------------
--------------------------
ADVANCED GIT:
	man pages and tldr are the best place for checking out git command examples,
for example: 

	$ man git-log 						#The bottom-page displays a small list of examples for git-log commands.
	$ tldr git-log						#Requires tldr Installed

----GIT LOG:

       git log --graph --abbrev-commit --oneline --all --reflog     #SHOWS ALL COMMITS

       git log --no-merges 					#Show the whole commit history, but skip any merges

       git log v2.6.12.. include/scsi drivers/scsi		#Show all commits since version v2.6.12 that changed any file in - VERY IMPORTANT
       								^ the include/scsi or drivers/scsi subdirectories

       git log --since="2 weeks ago" -- gitk 			#Show the changes during the last two weeks to the file gitk. 
       								^ The -- is necessary to avoid confusion with the branch named gitk

       git log --name-status release..test 			#Show the commits that are in the "test" branch but not yet in the - IMPORTANT
       								^ "release" branch, along with the list of paths each commit modifies.

       git log --follow builtin/rev-list.c 			#Shows the commits that changed builtin/rev-list.c, including those commits 
       								^ that occurred before the file was given its present name.

       git log --branches --not --remotes=origin 		#Shows all commits that are in any of local branches but not in any of remote-tracking 
       								^ branches for origin (what you have that origin doesn’t). - IMPORTANT

       git log master --not --remotes=*/master 			#Shows all commits that are in local master but not in any remote repository master branches. - IMPORTANT

       git log -p -m --first-parent 				#Shows the history including change diffs, but only from the “main branch” perspective, 
       								^ skipping commits that come from merged branches, and showing full diffs of changes introduced 
								by the merges. This makes sense only when following a strict policy of merging all topic branches 
								when staying on a single integration branch.

       git log -L '/int main/',/^}/:main.c 			#Shows how the function main() in the file main.c evolved over time. - VERY IMPORTANT

       git log -3 						#Limits the number of commits to show to 3.

----GIT LOG(2):

    git log -p <directory/file>					#Show the history of a particular file or directory, including differences. - VERY IMPORTANT

    git log --stat						#Show an overview of which file(s) changed in each commit. - VERY IMPORTANT

    git log --oneline --graph					#Show a graph of commits in the current branch using only the first line of each commit message. IMPORTANT

    git log --oneline --decorate --all --graph			#Show a graph of all commits, tags and branches in the entire repo. - IMPORTANT

    git log -i --grep search_string				#Show only commits whose messages include a given string (case-insensitively).

    git log -n number --author=author				#Show the last N commits from a certain author. - VERY IMPORTANT

    git log --before="2017-01-29" --after="2017-01-17"		#Show commits between two dates (yyyy-mm-dd). - VERY IMPORTANT

---- GIT COMMIT

    git commit -m "message"  					#Commit staged files to the repository with a message.
    git cherry-pick master					#Commits only the files changed on master's ^HEAD
    								^ picks up only files from a single commit.

    git commit --file path/to/commit_message_file 		#Commit staged files with a message read from a file.

    git commit -a -m "message" 					#Auto stage all modified files and commit with a message.

    git commit -S -m "message" 					#Commit staged files and [S]ign them with the GPG key defined in `~/.gitconfig` - VERY IMPORTANT

    git commit --amend 						#Update the last commit by adding the currently staged changes, changing the commit's hash. - VERY IMPORTANT

    git commit path/to/file1 path/to/file2 			#Commit only specific (already staged) files.

    git commit -m "message" --allow-empty 			#Create a commit, even if there are no staged files. - VERY IMPORTANT

---
-- GIT REMOTE --
git remote					#lists all local and remote repos
git remote rm <repo>				#deletes repo
git remote add origin <path_directory>     	#creates local repo on pre-existing path
						 ^ example: git remote add origin ~/gittest/REPO/

git remote add origin https://github.com/user/some_git.git 		#creates a remote repo  called origin

git remote [-v | --verbose]
git remote add [-t <branch>] [-m <master>] [-f] [--tags | --no-tags] [--mirror=<fetch|push>] <name> <url>
git remote rename [--[no-]progress] <old> <new>
	ex.: git remote projectmraam projectzicroraam

git remote remove <name>
	ex.: git remote projectmraam

git remote set-head <name> (-a | --auto | -d | --delete | <branch>)
git remote [-v | --verbose] show [-n] <name>
git remote prune [-n | --dry-run] <name>
git remote [-v | --verbose] update [-p | --prune] [(<group> | <remote>)...]
git remote set-branches [--add] <name> <branch>...
git remote get-url [--push] [--all] <name>				#Retrieves URL for a given remote git repo
	ex.: git remote get-url projectzicroram

git remote set-url [--push] <name> <newurl> [<oldurl>]			#Updates/Changes URL for a given remote git repo
	ex.: git remote set-url https://www.github.com/userA/RepositoryA https://www.github.com/userA/RepositoryB

git remote set-url --add <name> <newurl>
git remote set-url --delete <name> <url>

-----
git clone <remote path/name>	#Creates a local repo based on a remote repository
				    example: 
					git clone https://github.com/marcelojo/artigo_git.git


git push <remote path/name>    #Pushes current branch modifications into remote one
git push -u <repo> <branch>  #Sends modifications from branch to remote repository
			     example: 
			     		git push -u origin master

git pull <repo_name>		#Incorporates changes from a remote repository into the current branch. 
				^ In its default mode, git pull is shorthand for git fetch followed by git merge 
				FETCH_HEAD.  More precisely, git pull runs git fetch with the given parameters 
				and calls git merge to merge the retrieved branch heads into the current branch. 
				With --rebase, it runs git rebase instead of git merge.
					ex.: git pull origin

Git pulling one single file | git pull on a single file:
	$ git fetch --all
	$ git checkout origin/master -- path/to/file

  ---MISC COMMANDS---
  	git config color.ui true              #sets colored output
	git config format.pretty oneline      #exibir logs apenas uma linha por commit
	git add -i			      #fazer inclusões interativas
	gitk				      #interface padrão do git
	git tag 1.0.0 1b2e1d63ff	      #rotulando um commit ID

				   
  ---GIT HEAD COMMANDS---
  OBS.: Note that both MASTER and HEAD are pre-existing branches - RE-EXPLAIN
  so in order to switch back to master do: git checkout master
	You also can't move from HEADs to BRANCHES when making changes without COMITTING them 
	first on the master branch. you can, however, if you are in another branch. - RE-EXPLAIN

  After merging different branches, it's necessary ADD or REFUSE each changes and make a new commit in 
  case you have accepted any changes.

  	git reflog                      #Lists all history of head movements
	git checkout HEAD^<number>      #Goes to the previous COMMIT by 0 or 1
	git checkout HEAD~<number>      #Goes to the previous COMMIT by Nth - Needs Check
	git checkout HEAD@{<number>}	#Goes to Nth Head - check rflog
	git checkout <head ID> 		#Goes to the head ID
	git branch <new-branch-name> 35e6ab4  #Creates a new branch with all modifications from head ID
	^ it differs from git checkout -b "new branch", because 'checout -b' doesn't requires ID, and
	it 'checkout -b' not only creates a new branch, but also switches you to it whereas 'branch'
	doesn't do that. also, what 'branch' does is simply copy one branch/head to another.
	'checkout -b' is useful when you've made modifications to the working area, and want to bring
	modifications to a new branch, instead of modifying the original branch.



Whenever you want to make a modification to the master branch, do the following: - REVIEW
	git checkout HEAD~<number>           #ID Number of commit
	... make modification to files...
	git add <filename>	             #Adds Modification to current head
	git commit -m <new commit name>      #Makes a new commit with all the added modifications
	git checkout master                  #Switches to Master Branch
	git merge <New Commit ID>	     #Merges New Commit into Master
					     Note.: You can only Merge Commits


*Note.: Whenever you make modifications to a HEAD's Files, you need to either ADD modification
or UNDO them either with: 1 - git reset HEAD <filename>    #same as below, but for undoing deletion
			      git checkout -- <filename>   #for undoing changes
				or
		          2 - git add <filename>        #for confirming file modification/insertions
		And then finally commit those changes with 'git commit -m "New Commit" [filename]',
		only then you'll be able to switch branches/head


*Note: You can save Head IDs on different VIM Registers, so you don't need to 'git reflog' all the
time.

git reflog example:
35e6ab4 (HEAD) HEAD@{0}: checkout: moving from 0a028cb8ff4af74ec2cdc0d4c953b015121dc81a to HEAD@{20}

^ HEAD@{0}   - HEAD Number on List History
	       everytime there's a head movement, there will also be a new head number added at the end
	       of the list.

^ Checkout: Moving from XYZ to HEAD@{20}   #Shows the operation that has been done, and the movement.
-------------------------
 * GIT OAUTH TOKEN | VERY IMPORTANT 2023:
 *		1) Go to Your Personal Account Settings on github
 *		2) Go to Developer Settings > Personal Access Tokens
			https://github.com/settings/personal-access-tokens
 *		3) Generate a token accordingly 
 *		4) Copy it to clipboard & save it somewhere
 *		5) If not done yet, create a repo by Following A)
 *		6) Mandatory: Make your first git pull using: 
 *			git pull <repo_name> <branch_name> --force --allow-unrelated-histories
 *
 *		7) add all new files and commit the changes:
 *			git add * and git commit -a -m "Commit reason"
 *		8) Do a git push by folllowing steps on B)
 *
 * 		MISC:
 *			A) Adding a repo:
 *				git remote add <repo_name>  https://github.com/user/RepositoryName
 *			
 *			B) Pushing Repo:
 *				git push <repo_name> <branch_name>
 *				^ On Password you must provide your OAuth Token instead of your regular git password.
 *				Note for future-self: 1 - GPG Key is used for local/remote signing authentication; when 
 *						      you do any local commits it'll sign them using your GPG Key 
 *						      as authentication.
 *
 *						      2 - OAuth Token is used for remote control of the GITHUB;
 *						      example: when doing GIT PUSH or GIT PULL an Auth token will 
 *						      usually be required.
 *
 *						      VERY IMPORTANT: Always use your OAuth Key as password when doing 
 *						      git push! ex.(non_valid): ghp_abasiuuqhd195349uasd8312950
 *
 *
 * 			C) Pulling Repo:
 * 				git pull <repo_name> <branch_name>
 * 				^ On Password you must provide your OAuth Token, and if set you must also insert your GPG Signing Key password.
 *
 *			D) Cloning Repos Using your OAuth Token:(Not Recommended, it also pulls on your password and adds it on .git)
 *				git clone https://scuzzlebuzzle:<MYTOKEN>@github.com/scuzzlebuzzle/ol3-1.git --branch=gh-pages gh-pages
 *				^ WARNING: Tokens have read/write access and should be treated like passwords. If you enter your token into the clone URL when cloning or 
				adding a remote, Git writes it to your .git/config file in plain text, which is a security risk.

 *
 *
 *			Git Push? - can't remember why this is here anylonger -:
 *				git remote set-url origin https://scuzzlebuzzle:<MYTOKEN>@github.com/scuzzlebuzzle/ol3-1.git
				
 *
 * 			READ MORE ON TOPIC: 
* 				GIT GPG - SIGNING GITHUB COMMIT LOCALLY AND REMOTELY | VERY IMPORTANT(2023)

--------------------
--------------------
*	GIT GPG - SIGNING GITHUB COMMIT LOCALLY AND REMOTELY | VERY IMPORTANT(2023):
*		source: https://git-scm.com/book/pt-br/v2/Fundamentos-de-Git-Criando-Tags
* 		source: https://medium.com/@ryanmillerc/use-gpg-signing-keys-with-git-on-windows-10-github-4acbced49f68
*
*				0) Enable local GPG Signing after either cloning a remote repo or creating one locally:
					$ git config --local commit.gpgsign true

				1) Fetch an existing GPG Key you have, copy the long numbers using: 
					1.1 - List existing GPG keys:
						# gpg --list-secret-keys --keyid-format LONG

						[OUTPUT EXAMPLE]
							sec   rsa3072/4E4C868AD7F7DBD6 2023-07-02 [SC]
							      F3B31E9A4F4F072D4025EA0E4E4C868AD7F7DBD6
							uid  		[ultimate] My Very First Name
									(Secret key for github) <email@hotmail.com>
							ssb   rsa3072/735B12E4D9BB293C 2023-07-02 [E]
						[/OUTPUT]

						Desired Result for the FULL Key should be:
							F3B31E9A4F4F072D4025EA0E4E4C868AD7F7DBD6

						Partial Key is:
							4E4C868AD7F7DBD6

					1.2 - (Optional) - if you have no secret gpg keys, try generating one and go back to step #1.1:
						# gpg --full-gen-key


*				2) $ git config --local --replace-all user.signingkey "[GPG_KEY_VALUE]"; 
*				^ REMEMBER TO ERASE [ AND ]

					2.1)(Optional) You can easily DISABLE THIS with:
						$ git config --local commit.gpgsign false

					2.2) IMPORTANT NOTE: 
						2.2.1) If you created the GPG Key with root,
						you'll need to add this to your /etc/zsh/zshenv file:
							[file: /etc/zsh/zshenv]
								GPG_TTY=$(tty)
							[/file]

						also make sure that root is using zsh shell as default shell.

						2.2.2) (Optional to 2.2.1 - better) or execute this as root before doing commits as root:
							# export GPG_TTY=$(tty)
							^ You need to stay logged as root and execute any commits as root.

						2.2.3) (Optional) If you still meet problems, you'll likely have to add
						your project directory as safe dir:
							# git config --global --add safe.directory '/home/user/my_git_repository'
								or
							# cd <local_git_repository>;
							# git config --global --add safe.directory $PWD
*
*				4) Insert the key in your "GITHUB Personal Settings"
*				VERY IMPORTANT: DO NOT DELETE ANY PREVIOUS GPG SIGNING KEYS ON THE GITHUB PAGE,
*				SINCE THEY'RE STILL USED TO AUTHENTICATE PREVIOUS COMMITS.
*
*					4.1 - Export gpg key's armor:
*						# gpg --armor --export "[GPG_KEY_VALUE]"; 
*						^ ERASE [ AND ] CHARACTERS
*
*					4.2 - COPY AND PASTE THE ENTIRE OUTPUT including the dashed(---) characters into your
*					"GITHUB Personal Settings"!
*
*						4.2.1 - Output armored key to a file:
*							# gpg --armor --export > /tmp/armor.file
*
*						4.2.3 - Easily Copy and Paste using NVIM Text Editor:
*							$ nvim /tmp/armor.file
*
*				3) done! just git commit -a -m "commit name"
*				
*				4(Optional) - Read more on GITHUB OAUTH AUTHENTICATION:
*					read topic: GIT OAUTH TOKEN | VERY IMPORTANT 2023

GIT TROUBLESHOOTING 
------
CLEANING COMMIT HISTORY FROM MAIN BRANCH:
	Easiest way, but shouldn't be done on a public branch:
		1 - The Method in here deletes the main branch and re-creates it as an orphan branch, cleaning all of the commit's history:
			1.1 - (Optional) If you want to clone the main branch into another branch for safety:
				$ git checkout main
				$ git checkout -b <branch_name>

			1.2 - Create a new orphaned branch so that we can completely delete main branch.
				We only create this here so that we can delete the 'main' branch:
					$ git checkout --orphan -b <new_branch_name>
			
			1.3 - Delete the old main branch:
				$ git branch -df main

			1.4 - Create a new orphan 'main' branch:
				$ git checkout --orphan main
		
			1.5 - Make your first ever commit into the new branch
				$ git commit -a -m "Adding files to staging area"

			1.6 - Finally push the changes made
				$ git push --force origin main

			1.7 - Erasing the orphan branch created at #1.2, since there's no need for it:
				$ git branch -df <branch_name_from_step_1.2>

			1.8 - Set new upstream branch
				Even under the same name, git will complain about new upstream branch not being set.
					$ git push --set-upstream origin main 

			1.9 - (Optional) If you really don't want to keep the history around you can completely 
			throw away the branch created on step #1.1f:
				$ git branch -df <branch_name_from_step_1>

		
			CLEANING REFERENCE LOGS
				1 - (Optional) reference logs are still kept even after deleting a branch
				you can check using the following command:
				Note: Orphaned reference logs aren't kept on remote side at github.
					$ git log --reflog
						or
					$ git log --reflog --graph --decorate --all --oneline

				2 - (Optional) Create a branch to protect the creation orphaned commit
					If you want to protect any of the orphaned reflog, you can use this here:
					2.1 - First, checkout to the commit you want to save using this:
						$ git checkout <commit_id_you_want_to_preserve> -b <branch_name>

				3 - (Optional) - Use the garbage collection for removing everything that has no reference:
					$ git gc --prune=now --aggressive

				4 - Clearing the reflog:
					Note: Orphaned reference logs aren't kept on remote side at github, this command is only here
				to clean it on the local repository:
				 
					$ git reflog expire --expire-unreachable=now --all
						or
					$ git reflog expire --expire=90.days.ago --expire-unreachable=now --all


------
THE FOLLOWING ERROR CAN BE EASILY FIXED: 
	Problem: "error: RPC failed; curl 92 HTTP/2 stream 0 was not closed cleanly: PROTOCOL_ERROR (err 1)"
	Solution: 
		$ git config --local http.version HTTP/1.1

 

--- GIT END TUTORIAL ---
------------------------------
================================================================================
---TUTORIAL AWESOME-WM----
	type '$man awesome' for more commands.

system+s						#Shows all available keys
							^ VERY USEFUL: When a(any) program has hotkeys it'll display those on the screen too!
							^ Only requirement is that you need to be using window of the given program.
system+ctrl+shift[number]				#Clones window to another workspace. 
system+r						#Runs prompt
system+p						#Opens dmenu
system+w						#Opens Main Awesomewm Menu
system+[number]						#Moves to a different workspace
system+ctrl+[number]					#Loads given workspace on the current workspace temporarily
system+esc						#Moves to previous workspace
system+u						#Moves to first urgent window
							^ Useful when a program demands attention in a given workspace.
system+shift+[number]					#Moves window to a different workspace
system+shift+{<j> | <k>}				#When pressing 'k' Moves current window "up" in the window stack.
							^ When pressing 'j' Moves current window "down" in the window stack.
							^ same as with vi/vim style keys.

system+t						#Sets client/window on top of all other windows in a given workspace.
							^ Useful if you want a screen clock to be displayed from a different workspace.?

system+n						#Minimizes current window.
system+ctrl+n						#Restores minimized window.
system+m						#Maximizes current window.
system+f						#Sets client/window to fullscreen.
< clicking on windows name above title >		#Maximizes/Minimizes/Switches a/to given window.
							^ When minimizing, another program/client/window will automatically take place.

mouse_button1						#Move  Window/Client on a given workspace
							^ Note: Maximized applications can't be moved around.
mouse_button2						#Resize Window/Client on a given workspace
							^ Note: Maximized Applications can't be resized.

system+shift+c						#Kills Current Window.
							^ Can't kill frozen applications.
xkill							#Kills a frozen application.




system+shift+q						#Kills Awesomewm;

---TUTORIAL DWM / I3-WM 2020-T ---

	
Old Ones:
alt+shift+c  = system + shift + q          		   kill window
shift + alt + [number] = system + shift + [number]         move window to another tag/workspace
alt + shift + q = system + shift + e       		   kill system - note.: DWM doesn't have confirmation 
					   		   dialogue for killing the system
alt + j or alt + k  = system + arrow keys                  move between windows
alt + p = system + d                                       opens dmenu for running program 
alt + enter (dwm only) 				   	   tile windows between master and slave				   
system + shift + arrows	(i3-wm)				   tile windows back and forth
system + h  and system + v(i3-wm)			   adds windows horizontally and vertically
alt + shift + enter(dwm only)				   opens new terminal
alt + h  and alt + l					   resizes window

alt + t   - switches current tag to tiling mode []=
alt + f   -  ""  floating mode
alt + m   -  ""  monocle mode good for keeping window maximized ><>

^notice the symbol  []= between the tag numbers and the title bar

-----------------
I3-WM EXCLUSIVE:
To switch users, just press ctrl+alt+f1

-----------------
Quick Notes DWM:
You can right-click a tag to bring other different windows to the same tag you're currently working with,
without changing their actual current place.

FLOATING:
	Floating layout will be familiar to Windows users. Use [Alt]+[right mouse button] to resize the 
floating window and [Alt]+[left mouse button] to move it around. This can also be used to raise a floating window.

There are several ways to get specific windows into being managed as a floating window, despite the tiled layout 
being activated. First there is [Alt]+[Shift]+[space], which will toggle floating mode on the active window. 
Then there is the possible to simply resize the window using [Alt]+[right mouse button]. The last method is 
to [Alt]+[middle mouse button] onto the window, to toggle it in being floating. These floating windows then 
can be used to force certain window sizes, when some application requires this for aesthetics or simply to being usable.

If you want to set some type of window to be always floating, look at the config.def.h and the rules array, 
where the last but one element defines this behaviour.


*I Decided not to use DWM, because the 'alt' key gets in the way of several other applications.
*When switching between tags/workspace, DWM doesn't clear the screen buffer,
*because of that you can see the previous workspace in the current workspace if there's enough space 
*on the screen to show it up.
--- END tutorial DWM / I3-WM
------------------------------
================================================================================
USEFUL LINKS:
https://ufile.io		#UPLOAD FILES WITH DATE/TIME EXPIRATION

------------------------------
================================================================================
THE DEVIL:
This is how a TRUE linux user removes his folders and files:
rm -r [path]
		WARNING: NEVER <TAB> <ENTER> AT THE SAME TIME WHEN USING THIS COMMAND
		ALWAYS CHECK IT TWICE, THRICE OR FOUR TIMES BEFORE PRESSING ENTER!!!!!!!!!!!!!!!
			ex.: rm -r /
			^ this will delete your entire system if done on SUDO or SU access
			and is harmful to other system folders/files like: 
				/home/
				/usr/bin/
				/etc/

YOU'VE BEEN WARNED!!!

------------------------------
================================================================================
THE DEVIL2:
	umounting /dev/sdx instead of /mnt/sdx will likely damage your filesystem,
this can happen post-arch installation, where you'll be required to umount the filesystems
right after leaving the arch-chroot environment and before rebooting the system.
But this can also happen in your daily linux life

YOU'VE BEEN WARNED
---------------------
================================================================================
SOME DIRECTORIES BACK FROM WHEN I WAS LEARNING:

all user programs config files are usually here:
	~/.[name of program]/

	Also notice that ~/  == /home/user/    
	^ It's the home user space, and the '.' character indicates it's a hidden directory or file

directories:
/etc/X11/xorg.conf - configurar nvidia para reiniciar com a opção: Option "Coolbits" "28"
/etc/X11/xorg.conf.d/ - diretorio para os scripts de inicialização xorg - as vezes é necessário criar 20-nvidia aqui
~/.config/autostart/ - diretorio dos scripts de inicialização do gnome3
~/.config/i3/config - arquivo de configuração de inicialização do I3wm
/bin/ - Installed programs
/usr/share/[application name] - configuration files for installed games
/usr/share/games/[application name] - configuration files for installed games
~/.config/[application name] - configuration files for installed applications
~/.[application name] - configuration files for given application
/run/media/[username] - space for mounting HDs, CDs, ISO, USB Disks and so on


directories2:
~/.config/[program name]  - default configuration files for programs are located here

SYSTEM LOGS:
/var/log/syslog or /var/log/messages: Shows general messages and info regarding the system. Basically a data log of all activity throughout the global system. Know that everything that happens on Redhat-based systems, like CentOS or Rhel, will go in messages. Whereas for Ubuntu and other Debian systems, they go in Syslog.
/var/log/auth.log or /var/log/secure: Keep authentication logs for both successful or failed logins, and authentication processes. Storage depends on system type. For Debian/Ubuntu, look in /var/log/auth.log. For Redhat/CentrOS, go to /var/log/secure.
/var/log/boot.log: start-up messages and boot info.
/var/log/maillog or var/log/mail.log: is for mail server logs, handy for postfix, smtpd, or email-related services info running on your server.
/var/log/kern: keeps in Kernel logs and warning info. Also useful to fix problems with custom kernels.
/var/log/dmesg: a repository for device driver messages. Use dmesg to see messages in this file.
/var/log/faillog: records info on failed logins. Hence, handy for examining potential security breaches like login credential hacks and brute-force attacks.
/var/log/cron: keeps a record of Crond-related messages (cron jobs). Like when the cron daemon started a job.
/var/log/daemon.log: keeps track of running background services but doesn’t represent them graphically.
/var/log/btmp: keeps a note of all failed login attempts.
/var/log/utmp: current login state by user.
/var/log/wtmp: record of each login/logout.
/var/log/lastlog: holds every user’s last login. A binary file you can read via lastlog command.
/var/log/yum.log: holds data on any package installations that used the yum command. So you can check if all went well.
/var/log/httpd/: a directory containing error_log and access_log files of the Apache httpd daemon. Every error that httpd comes across is kept in the error_log file. Think of memory problems and other system-related errors. access_log logs all requests which come in via HTTP.
/var/log/mysqld.log or /var/log/mysql.log : MySQL log file that records every  debug, failure and success message, including starting, stopping and restarting of MySQL daemon mysqld. The system decides on the directory. RedHat, CentOS, Fedora, and other RedHat-based systems use /var/log/mariadb/mariadb.log. However, Debian/Ubuntu use /var/log/mysql/error.log directory.
/var/log/pureftp.log: monitors for FTP connections using the pureftp process. Find data on every connection, FTP login, and authentication failure here.
/var/log/spooler: Usually contains nothing, except rare messages from USENET.
/var/log/xferlog: keeps FTP file transfer sessions. Includes info like file names and user-initiated FTP transfers.
------------------------------
================================================================================
STARTING DHCP SERVICE FOR FETCHING IP CONFIGURATION FROM ROUTER:
start dhcpcd service:

	$ dhcpcd
	    or
	$ systemctl start dhcpcd

------------------------------
================================================================================
everytime nvidia or system gets updated:
# mkinitcpio -P
# mkinitcpio -g IMAGE   #????
# mkinitcpio -p linux   #????

sound in i3:
export $(dbus-launch)
pulseaudio

move or rename a folder:
mv foldername newname

remove a folder:
rmdir foldername

Things yet to fix:
Hello,

I just found a solution.
My system uses ASUS motherboard and on their 
support page, they say to disable USB 
emulation if the keyboard does not work in 
GRUB.

I checked BIOS on my machine and found "USB 
Keyboard and Mouse Simulator" enabled. I 
disabled it and now keyboard works perfectly 
in GRUB menu.

Also, try disabling PS/2 emulation

Mount Tutorial:
df -h
mkdir /run/media/<username>/[mount folder]
mount /dev/sdaX /run/media/<username>/[mount folder]

pacman database lock solve:
# rm /var/lib/pacman/db.lck

----------
XORG/X11 STARTUP ORDER:

Start X11 Server: 
xinit
startx
systemctl start gdm

---------------
That did not resolve the issue. But it did get me to look in what seems to be the right place, i.e. 
the BIOS setup. I had originally setup ubuntu using a usb stick, and had therefore set the boot 
sequence to first look for a usb drive. I just undid this, and it seems like it did the trick, i.e. 
usb ports are now recognized. 

Similarly to the discussion on the comments in the other answer, the answer for me was in the BIOS. 
While legacy USB support had no impact, however the following configuration setting worked for me:

Boot Maintenance Manager
Advanced Boot Options
USB Boot Priority: <DISABLED> 
(Note there was no connected bootable USB device.)

---------------------
DIFFERENCE BETWEEN API AND FRAMEWORK

	A framework is a group of classes, interfaces and other pre-compiled code upon which or by 
the use of which applications can be built.

	The API is the public face of a framework. A well designed framework only exposes those classes, 
interfaces, etc that are needed to use the framework. Code that supports the operation of the framework 
but that is not necessary to users of the framework is kept internal to the framework's assemblies/dlls.
This keeps the public face of the framework small and encourages a "pit of success," or the quality of 
a framework which makes it simple to do the right thing.

https://stackoverflow.com/questions/4430329/difference-between-api-and-framework
-------------
GPU ISSUE IN LINUX - OLD TOPIC: use #nvidia-xconfig

[quote=TheOvermageOfCinbri;2790495976027222376]I need a solution to this as well.

Most games work fine on Windows and Ubuntu 20.04 both (I have a dual-boot system). But for some games, like GTA V, Destiny 2, and Warframe, I need the MSI Afterburner running to keep the games from crashing. I would like to run this on Linux as well.

I'm not sure if ambient temperature is a factor, but I notice that keeping my card at 35C-40C allows me to play those games. It COULD be a problem with the thermal paste, but I'm not sure, since other graphics heavy games seem to run okay.

Currently I let Afterburner keep the fan at 50%-90% throughout a gaming session which keeps it nice and cool.

I'm still not sure how to control fan speed in Linux. The X11 settings thing didn't really work for me. Would anyone recommend futzing with Afterburner + WINE?

EDIT: I have a NVIDIA 980Ti AMP Edition [/quote]

In archlinux you can modify the /etc/X11/xorg.conf
And Add: Option "Coolbits" "4" 

Like this,
	[code]
		Section "Device"
			Identifier "Device0"
			Driver "nvidia"
			VendorName "NVIDIA Corporation"
			BoardName "GeForce GTX 570"
			Option "NoLogo" "True"
			Option "Coolbits" "28"
		EndSection
	[/code]

Note that you need proprietary drivers installed + Xorg.
As far as I can tell, Unbuntu doesn't allow Proprietary Drivers; So you're stuck to Generic Open-Source drivers. The problem with generic drivers on linux is that neither Nvidia(nor Intel) did love sharing the Blueprints for their proprietary drivers to the Open-Source Community.

So Nvidia users will get stuck with generic drivers that under performs their GPU and also doesn't allow full-control  of it.

Alternatively, you could try this: 
http://www.upubuntu.com/2015/05/how-to-controladjust-gpu-fan-speed-for.html

But all of the listed methods will require Proprietary Drivers. So all of them will fail. Installing Prioprietary Drivers require Kernel Update + Other modifications on the Unbuntu Debian system. The Nvidia Kernel Update didn't work for me(back at the time) because it only supported newest GPUs and mine was old - So not all Nvidia GPUs are supported by the kernel neither. 

The methods for unbuntu users are there, most of them will fail. CPU Intel users might find the same problem under non-proprietary operational systems if they ever need things like virtualization.

I've no idea if they made it easier/better at this point in time. Back In the days I was forced to move on to Archlinux instead. ːshellshockː

I'm now using Win7 for most of my games, because fighting games doesnj't work on linux, not the newer ones.
---------
================================================================================
REMOVING READ ONLY MODE FROM HARD DISK || 2024
HOW TO SOLVE HD SSD USB DISK PARTITION ISSUES ON BASH 2020.2-T:
WARNING:

Creates a New Partition:
mkfs.ntfs - makes a new partition
mkfs.<tab_key><tab_key> - shows list of available partitions

install ntfs-3g to read AND write to ntfs-3g disks, otherwise only read is possible.


GRUB HINT!:
1 - Install microcode and insert it on /boot/grub/grub.cfg, makes system more fast!


/dev/sdxY /mnt/data vfat defaults 1,0  #Alternate Command for Mounting Unmount that gives r/w access

e2fsck /dev/sde #CHECKS DISK!

fsck   /dev/sde #FIXES DISK
		 #CAN also be used to unblock USB Disks from reading/write.
		 #USE with care, can erase disk files/partitions
		 #Backup your files first

fsck -p /dev/sde #same as above, runs the non-interactive mode
		 #use with care

hdparm -r0 /dev/sdX     #Removes Write/Read Protection from DISK - SOLVED!
NOTES On Solving this Problem:

1 - In order Solve this problem, I had to "chmod g=rwx,o=rwx,u=rwx /dev/sdX"
on the blocked /dev/sdX disk, and also "chmod g=rwx,o=rwx,u=rwx /dev/sdxY"
on the blocked partition.

2 - Then e2fsck disk told me it was write protected
3 - use fsck -p /dev/sde1
3.1 - using fsck didn't fix any issues, because protection was either encrypted or device was failing 
4 - only downloading and using hdparm truly did fix the issue
------
fdisk  #fdisk
sfdisk #no idea - same as above?
cfdisk #cli interface fdisk
fsck   #does work with mounted disks
e2fsck #does the same, but doesn't quite work with mounted disks
================================================================================
NOTES FOR FUTURE 2020.2-T:
1 - Downloading microcodes made the system faster. Needs to be setup on /boot/grub/grub.cfg for each Operational System's archlinux entry

2 - If a given system gives access to multi-tools, you can probably check them on bash by typing:
<system_name>-<tab_key><tab_key>
ex.: gnome-<tab_key   # will list all available gnome applications including disk utilities

3 - Disable power saving settings
4 - Set powermize mode in nvidia-settings to: Maximum Performance
Adaptive mode can cause crash if you're using 2D/3D Applications.
----------
Windows Steam Save Data:
/run/media/<username>/SAMSUNG/Users/srmf32/AppData/Local/GBVS/Saved/SaveGames/51159618
----------------------------------------
UART Communication Protocol:
https://www.youtube.com/watch?v=IyGwvGzrqp8
----------------------------------------
Steam:
~/.local/share   #Stores all application settings
/run/media/<username>/22046B21046AF767/LinuxSteamLibrary/steamapps/compatdata/  #Steam save folder
~/.steam/steam/steamapps/compatdata/   #Old Steam save folder


proton-tkg:
~/.steam/root/compatibilitytools.d/
------------------------------
What's /dev/shm ?
https://www.linuxforfreshers.com/2018/10/what-is-devshm-in-linux.html

Directory Structure:
https://www.howtogeek.com/117435/htg-explains-the-linux-directory-structure-explained/
================================================================================
================================================================================
================================================================================
================================================================================
================================================================================
2021.T - LINKS:
LEARNING CHROOT:
[PT-BR]
https://bbs.archlinux.org/viewtopic.php?id=252051
https://www.vivaolinux.com.br/topico/Linux-Mint/Problemas-no-GRUB-2

https://www.ubuntudicas.com.br/2012/11/boot-repair-repare-seu-boot-rapidamente/
https://www.vivaolinux.com.br/dica/Como-recuperar-o-GRUB-em-3-comandos-Esse-funciona/


Chroot serve para fazer root em uma outra partição/disco/instalação linux
a partir de um Live CD ou de um Sistema de Recuperação linux.

Uma vez dado chroot em um sistema: chroot /mnt/media/sdc3/
é possivel executar comandos que sejam executados no caminho root("/") daquele sistema,
tornando-se possível instalar e configurar grub e outros aplicativos a como se estivesse executando aquele próprio sistema.


ex.: caso você queira instalar e configurar ele em outra partição/disco/instalação, é necessário utilizar o chroot:
chroot /mnt/sdaY/
grub-install /dev/sda
grub-mkconfig -o /boot/grub/
grub-update
exit


O programa em questão porém deverá pré-existir, caso contrário será necessário montar os binários dependentes utilizando:
mount -o bind /bin /mnt/sdaY/

Ex.:
mount -o bind -t proc /proc /mnt/sdaY/proc
mount -o bind /sys /mnt/sdaY/sys
mount -o bind /dev /mnt/sdaY/dev
mount -o bind /dev/pts /mnt/sdaY/dev/pts

Obs.: Esta montagem deverá ser feita antes do chroot, com o disco/instalação/partição desejada já montada!

Ex. Final:
mount -t ext4 /dev/sdaY /mnt/sdaY
mount -o bind -t proc /proc /mnt/sdaY/proc
mount -o bind /sys /mnt/sdaY/sys
mount -o bind /dev /mnt/sdaY/dev
mount -o bind /dev/pts /mnt/sdaY/dev/pts
chroot /mnt/sdaY/
grub-install /dev/sda
grub-mkconfig -o /boot/grub/
grub-update
exit

EXISTE PORÉM UMA SOLUÇÃO SIMPLES:
# mount /dev/sdaY /mnt/sdaY
# grub-install --root-directory=/mnt/sdaY /dev/sda

lembrando que /dev/sda é o disco inteiro(contendo todas as partições), não apenas a partição
sdaY é a partição propriamente dita.

isso irá consertar qualquer problema relacionado a boot em quaisquer uma das partições linux

--------------------------
CUSTOMIZED KERNEL MICROCODE + INITRAMFS:
I'm using intel microcode image on grub.cfg: initrd /boot/intel-ucode.img /boot/initramfs-linux.img

FORCING FSCK TO BE RUN ON DISKS AT BOOT
You can still use: fsck.mode=force on the line that contains '/boot/vmlinux-linux-zen' right after 'psi=1'
this will force all pre-boot mounting disks and partitions to be checked for inconsistencies and/or errors by fsck during initramfs load.

furtherover, tune2fs can also be used to set the ERROR BIT on 'ext4' partitions to ensure that partition is checked during boot.
-------
Em última instância, utilizar o Boot-Repair:

Utilizando boot-repair:
# add-apt-repository ppa:yannubuntu/boot-repair
# apt-get update
# apt-get install boot-repair
--------
Sempre manter uma cópia de um Windows OS e Unbuntu em pen-drives bootaveis!
================================================================================
================================================================================
================================================================================
================================================================================
================================================================================
https://unix.stackexchange.com/questions/320103/whats-the-difference-between-creating-mdadm-array-using-partitions-or-the-whole

DO NOT USE THIS:
mdadm --create /dev/md0 --level=6 --raid-devices=4 /dev/sda1 /dev/sdb1 /dev/sdc1 /dev/sdd1

DO NOT USE YET:
mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdc7 /dev/sda4
-----------------------
====DICTIONARY/Dictionary====
--------2020-T LINUX TOOLS/COMMANDS/PROGRAMS/LINUX UTILS:---------------
--------INDEX-----------------------------------------------
note.: not in order, use search function to find it:

	env, dd, 
	pstree, lslogins, userbdctl, atop, pgrep,
	uname, ipmaddr, ifcfg(old), ifconfig, su, sudo,
	history, hdparm, vi, visudo, nvim, gdb, tbclock 
	printenv, watch, bash, source, zsh, echo, gcc, g++, ***dd, kill, tune2fs, print, curl,
	sysctl, lsusb, lspci, wc, chsh, tldr, df, du, install, diff, cp, lsmod, cat, zcat, zgrep,
	modinfo*, mkinitcpio, getfacl, secret-tool, makepkg, smartctl, dconf, loginctl, gpg,
	openssl, man, md5, sha1sum, date, cal, man, grep, sync, ls, zramctl, cd, free, htop,
	dmidecode, slabtop, lsbk, modprobe, 7z, nproc, lsattr, chattr,
	whereis, users, w, who, whoami,	lastlog, lastb, last, awk***, find, 
	passwd, useradd, userdel, adduser, deluser, nmap, journalctl, jobs, ps,
	mkdir, mount, mknod, rmod, insmod,
	disown(very important), 
	tail, truncate, lsinitcpio, grub-mkconfig, whereis
	lshw, dmidecode, hwinfo, hardinfo.
	lshw -class disk,
	xargs, strip, strings, readelf,
	lastlog, lastb, lslogins*, w, getent, faillock* - VERY IMPORTANT
	findmnt*, vegeta**, netstat*, telnet*, pkill*, lsof*, rsync*, vim***, convert*
	fuser***, mplayer***, strings*, ss*, apropos*, fsck***
	lynis*, nikto*, nmap*, figlet*, mke2fs, fsck.ext4, wipefs
	losetup*, efibootmgr, e4defrag*, e2fsck*, e2freefrag, *e2mmpstatus
	find, newgrp***, pkill***, netactview, *chvt, *fc-list, *fc-cache
	***man, *lsinitcpio, ***lscpu, *pidof, *kill, *killall, *jobs, *fg, *bg, *ps, *disown
	*lsns, *fallocate, *blockdev, *!halt, man, *ncdu.
	*aplay, *arecord, *stat, *file, fscrypt, ghex,
	debugfs, tree, dircolors, broot, pmap,  
	diff, cmp, comm, join, uniq, *xdelta3, *parallel, args, xargs, awk,
	*rclone, split, dar, tar, par2 (par2cmdline).

	look later:
		dircolors for setting colors to 'ls'

	stdin on linux:
		tcsetattr, termios, stty

	---new ones:
		pinky, pmap*, arp*, dig*, bmon*, install*, patch*, pushd, dirs, popd.

caption:
	*: new/important
	*!: new/uninmportant
	**: Only available through external installation
	***: old tool, new command

--------LINUX TOOLS:-----------------------
2020.1-T 2020.2-T 2020.-T 2020.T:
CPU KERNEL GRUB FLAGS AND INFO 2020.T AND OTHER COMMANDS:
IT'S IMPORTANT TO OWN MULTIPLE KERNELS, BECAUSE WHEN ONE KERNEL FAILS,
YOU CAN STILL BOOT FROM ANOTHER KERNEL.
-------------------------------
USEFUL: https://wiki.archlinux.org/title/Core_utilities#Essentials
USEFUL: https://www.gnu.org/proprietary/proprietary.html
USEFUL: https://linuxcommandlibrary.com/basic/oneliners#1463
-------------------------------
-------------------------------
ABOUT MAN PAGES
	Read man's manual page, it's very important!
In it you can find 

$ man 7 <command>				#Open page 7(if existant) for <command>

$ man --whatis <command> 			#List all available sections for a given <command>
$ man --path  				#Display the path searched for manpages
					^ VERY IMPORTANT: Useful for knowing which path does a linux 
					^ distribution uses when searching for man pages if you ever make your own manual for your 
					^ software.

$ man --where <command>  			#Display the location of a manpage rather than the manpage itself.

$ man --apropos "search_string"  		#Search for manpages containing a search string.
					^ VERY IMPORTANT: very useful if you don't know the manual name you're looking for.
					^ ex.: man --apropos "xorg.conf"
---
-------------------------------
Very Important tools:
$ pstree					#Lists process tree
$ ps -el					#Report snapshot of current running processes

---
-------------------------------
# pidof firefox | awk '{system("cat /proc/"$0"/status")}'	#For some reason, this should work, but it doesn't

# pmap -x $(pidof <program_name>)		#Returns memory usage in KB, on the first column of final row) 
						for a given program name.
						ex.: pmap -x $(pidof firefox)
						#^ Memory use of a program / Program memory usage
# memstat					#Memory use, not good, doesn't even order by size(Bad)
# free -h					#Total Free memory left and usage

broot -w					#Tree-like File Explorer
						^ Some commands:
							:img
							:close_preview
							:sort_by
							:filesystem
							:size

tree --du -CshafQ | less			#List directory tree style with colors and human readable data size
tree --du -CshLafQ 2 | less			#Same as above, uses '2' as directory depth 
tree --sort=size --du -CshLafQ  | less 		#Sorts by Size
tree --sort=ctime --du -CshLafQ  | less 		#Sorts by Creation Time
tree --sort=mtime --du -CshLafQ  | less 		#Sorts by Modification Time

ls -la --color | less -R		#List directory with 'ls' and pass colors to 'less'

---
-------------------------------
ghex						#Binary Editor
						^ VERY IMPORTANT: Don't use ghex on very large files, it'll crash/freeze the system
dumpe2fs					#Dumps filesystem features
hexdump						#Dumps hexadecimal data
debugfs -w /dev/sdX -R 'link <16> myfile'	#Recovers file with inode 16 with new name 'myfile'
						^ Only works on EXT2/3/4 filesystem without DISCARD or TRIM option.
file -i <filename>				#Gives info on filetype
stat <file/dir>					#Gives full stats on file/dir. access, modification and birth dates,
						and also inode number, permission-set for user, group, others as well
						as ownership of the given file/directory.
---
-------------------------------
diff -rdq <dir1> <dir2>			#Does a recursive comparison of all files and sub-directories, only reporting
					on file differences. VERY USEFUL.
-------------------------------
KILLING A FROZEN APPLICATION
KILLING APPLICATIONS / EXTERMINATING AN APPLICATION PROCESS

$ xkill					#Helps to kill a frozen application, requires the user to click the frozen application.
# killall -SIGKILL -I <process_name>	#Kills all processes
					note: optional -I enables case-insensitive
	or				
# killall -s KILL -r <regex_string>	#Kills all processes meeting the regex criteria
	or
# killall -s KILL -n<tab-key>		#Kills process by PID, 
					note: on zsh shell pressing <tab-key> allows to list and select all running processes on zsh environment.
					
$ pidof <process_name> 			#Retrieves the PID(Process ID) of a given executing application by it's name
					^ Useful for killing apps manually
# kill -15 <pid>				#SIGTERMs an application by it's PID
# kill -9 <pid>				#SIGKILLs an application by it's PID
# killall <process_name>			#SIGTERMs all processes related to process name

Fore more info about signals, type: $man 7 signal
---
-------------------------------
# lsinitcpio /boot/<initramfs-kernel_name>.img 		#Lists all kernel hooks & modules in the given image
---
-------------------------------
$ fc-list				#Lists fonts available in the system for applications using font-config
$ fc-cache -r				#Clears system font cache
-------------------------------
$ chvt					#Changes VT/Virtual Terminal
					^ Pressing ctrl+fN usually has the same effect					
-------------------------------
FIREWALLING:
$ netactview				#GUI Application that displays all STABILISHED connections,
					^ TIME_WAIT means that a given application has been closed, however
					the ports are still being listened for data. | VERY USEFUL
# gufw					#GUI Firewall Application for blocking ip/ports/protocol by program name | VERY USEFUL

# firewalld
# firewall-config
# iptables
# nmap
-------------------------------
# pkill -9 -u <username>		#Kills all created processes for a given user | Kill User Session
					^Useful if your tty gets stuck/frozen, you don't need to reboot the entire system.
-------------------------------
newgrp					#Changes user primary group
-------------------------------
find . -type f | file -f -		#Finds and lists all filetypes in the current folder, even if their contents are bit corrupted
					^ "file" command tries to determine the file type, while "find" returns a list of all filenames in a given directory, very useful for lost+find directories.

-------------------------------
UTILS:
$ man hier				#Shows every single directory hierarchy
$ grep ^Dirty /proc/meminfo		#Find out how much data is waiting to be written to disk
$ mkdir /home/foo/doc/bar && cd $_	#Make directory and CD Into it from a single command
$ man -t man | ps2pdf - filename.pdf  	#Creates a pdf version of a man page | Creates PDF Files from a program output 
$ oowriter -pt pdf your_word_file.doc	#Create PDF From DOC
$ while inotifywait -e modify /tmp/myfile; do firefox; done	#Runs a command when a file is changed | VERY IMPORTANT

$ rm -f ./ !(survivior.txt) 		#WARNING: Removes all files in the current directory except survivior.txt
					^ Don't use it!
$ gs -q -sPAPERSIZE=letter -dNOPAUSE -dBATCH -sDEVICE=pdfwrite -sOutputFile=out.pdf `ls *.pdf`	#Merge PDF Files

$ cat /dev/urandom | hexdump -C | grep "ca fe"		#Pretend being busy in the office!

$ gs -q -dNOPAUSE -dBATCH -sDEVICE=pdfwrite -sOutputFile=OUTPUT.pdf -c .setpdfwrite -f INPUT.pdf	 #Removes Security Limitations from PDF File

$ apropos network | less			#Lists all manual pages that contains the word 'Network'
					^ Searches for all network manual pages
					alternative: $pacman -sS network

$ curl ifconfig.me			#Asks ifconfig.me server to print your external IP Address

$ for i in *.html ; do mv $i ${i%.html}.htm ; done					#Changes all .html files to .htm
$ wget -r -l1 --no-parent -nH -nd -P/tmp -A".gif,.jpg" http://example.com/images		#Download all images from a site
$ wget -r -np <URL>									#Download all folders and files
											^ -r ensures recursive
											^ -np ensures not to follow parent folders recursively
											^ -nH removes hostname parent from the save path
$ diff <(cd dir1 && find | sort) <(cd dir2 && find | sort)				#Comapre two directory trees
$ wall <<< "Broadcast This"								#Broadcasts message to all logged users
	or 
$ wall "Broadcast this"

$ figlet TEST		#Figlets string
			 alternative: figlet <<< "TEST"
$ rev			#Reverts string when type afterwards

NEWER:
$ dd if=/dev/null of=sparsefile bs=1G count=1						#Creates a sparse file that doesn't takes up disk space.
$ dd if=/dev/random of=nonsparsefile bs=1G count=1					#Creates a non-sparse file

$ dd if=/bin/rmdir of=rmdir conv=sparse,fsync						#Clones a binary file

OLD:
$ dd if=/dev/zero of=testfile.seek seek=5242879 bs=1 count=1 				#Creates a 5MB blank file via sinkhole
$ dd if=/dev/zero of=raw_file.raw bs=10G count=2					#Creates a zeroed 20GB file
											^ INTERESTING: bs=10G actually
											specifies how much RAM should be used
											in the copy process.
-------------------------------
# fsck.ext4 -cDfty -C 0 /dev/sdxx			#Run a ext4 file system check and badblocks scan with progress info - NOT TESTED
------------------------------
strace -ff -e trace=write -e write=1,2 -p SOME_PID	#Intercept stdin/stdout of another process
-------------------------------
readom dev=/dev/scd0 f=/path/to/image.iso		#Create CD/DVD Iso image from disk
-------------------------------
# dd if=/dev/mem | cat | strings	#This command will display all the string (plain text) values in ram
dd if=/dev/zero of=testfile.seek seek=5242879 bs=1 count=1 	#Creates a 5MB empty file via sinkhole
-------------------------------
mplayer -ao pcm -vo null -vc dummy -dumpaudio -dumpfile <output-file> <input-file>	#RIP Audio from a Video File
-------------------------------
rsync -az /home/user/test user@sshServer:/tmp/			#Moves a lot of files over SSH
-------------------------------
vim -x <FILENAME>			#Adds password protection to a file beind edited on VIM
-------------------------------
ifconfig | convert label:@- ip.png	#Save command output to image
-------------------------------
cat /proc/cmdline			#Prints kernel options used for the current boot
					^ same as reading /boot/grub/grub.cfg for the kernel image

ls -laR /etc/ | grep -i -C 15 ".pacnew" 	#Prints +15 lines of contexts for each .pacnew occurrence found
						^ -i ensures case-insensitivity.
						^ -C 15 prints 15 lines of context.

----------
SOME FIND COMMANDS
# file /proc/					#'File' tool can be used to determine filetypes
						^ Swiss kinfe when used in conjunction with awk and find

# find ./ -size +10M				#Finds files with more than 10M,
# find ./ -size -10M				#Finds files with less than 10M.

# find ./ -size -10M -size +5M			#Finds files with more than 5M and less than 10M

# find /etc/ -name '*.pacnew'			#Does the same as above, but prints full path.
						^ Recursive fullpath searching

# find ./ -iname '*steam*'			#Case insensitive search

# find ./ -type <d | f | l | c | b>		#Searches for directory/file/symlink/character/block-devices.
						^ d - directory, f - files, l - symlink, c - character, b - block
						^ check man-page for more.

	# find ./ -maxdepth 1 -type l		#Same as above, but adds a directory maximum depth of 1,
						^ since 'find' works recursively.
	# find ./ -maxdepth 1 -type l,d		#Same as above, looks for symlinks and directories

# find ./<filename> -xtype l			#Same as -type, but follows broken symbolink links 

# find ~/ -daystart -mtime -1			#Finds all files modified today on the user home directory.
						^ By default, -mtime looks for files that have not been modified in less
						than n*24 hours. -daystart modifies this behavior and sets it to 
						the beginning of the day.
						^ Can be used for 'honey pot'

	# find ~/ -daystart -mtime -3		#Same as above for 3 days ago
	# find ~/ -daystart -atime -3		#Same, but for access time instead
						^ Only supported on filesystems with support to access timestamps
						on the underlying filesystem.

# find ~/ -daystart -mtime -1 -exec tar -cvf archive.tar {} \+	#Same as above, but STORES ALL MODIFIED FILES on an archive
								^ Not useful but still leaving it here.
MORE FIND COMMANDS:
# find ./ -size +20M | awk '{system("ls -lah \""$0"\" ")}'	#Lists all files with more than 20M in size using'ls'
								for displaying user/group owners, permissions and 
								modification time.

# find . -type f -exec file -N -i -- {} + | grep video		#Finds all video filetypes
# find . -type f -exec file -N -i -- {} + | grep gif				#Finds all animated gifs

# find . -type f -exec file -N -i -- {} + | sed -n 's!: video/[^:]*$!!p'	#Finds all video filetypes, but returns
										filename only.

# find . -type f | grep "\.gif"							#Finds all animated gifs, 
										(statical search) returning filename only.
		or									
# find . -type f -name "*.gif"						 
	or
# find . -type f -iname "*.gif"							#Case-insensitive search

# find . -type f -exec file -N -i -- {} + | grep "\.gif" | sed -n 's!: image/[^:]*$!!p' 	#Finds all animated gifs, but returns
										    		filename only
	---------------------
--Using 'find' on text/binary files using find, awk and grep:
	Finding all 'password' entries in the text files on the current dir(recursive for all dirs):
		# find . -type f -exec file -N -i -- {} + | sed -n 's!: text/[^:]*$!!p' | awk '{system("grep --color -Hi \"password\" -- \""$0"\"")}'

	Finding all 'http' references in all text files in a given directory(recursive for all dirs):
		# find . -type f -exec file -N -i -- {} + | sed -n 's!: text/[^:]*$!!p' | awk '{system("grep --color -Hi \"http\" -- \""$0"\"")}'

	Finding all 'www' references in all application files in a given directory(recursive for all dirs):
		# find . -type f -exec file -N -i -- {} + | sed -n 's!: application/[^:]*$!!p' | awk '{system("grep --color -Hi \"www\" -- \""$0"\"")}'

---Using find on text/binary files using find, awk and ugrep ( NEWER ) :
	# find . -type f | awk '{system("ugrep -C 3 -P \"pass|wd|word\" -- \""$0"\"")}'
	^ returns 'pass', 'password', 'passwd', but could also return "wd" and "word" alone which is not intended.
	^ BAD EXAMPLE, leaving here for reference.

	# find . -type f | awk '{system("grep -C 3 -P \"\\bpass\\b|pass([word].*)\" -- \""$0"\"")}'
	^ actual command is: grep -C 3 -P "\bpass\b|pass([word].*)" -- <filename>
	^ will find exact 'pass' word, but not 'pass:', will also find all pass + [word] combinations,
	^ may however return 'passo', which isn't intended.
	^ SLIGHTLY BETTER, BUT STILL BAD EXAMPLE. Leaving for reference

	# find . -type f | awk '{system("grep -C 3 -P \"pass(?=(:|word|wd|d))|\\bpass\\b\" -- \""$0"\"")}'
	^ actual command is: grep -P "pass(?=(:|word|wd|d))|\bpass\b"
				or
			     ugrep -P "pass(?=(:|word|wd|d))|\bpass\b"
	^ will find 'pass' followed by ':' or 'word' or 'wd' or 'd' by using lookeahead assertion
	^ also finds the exact 'pass' word by using \bpass\b
	^ CORRECT FORM, GOOD EXAMPLE.



	
-------------------------------
USING READELF:
readelf -wi a.out			#This requires the binary to be compiled with -g option for debugging:
					^ Read more on: LINUX EXECUTABLE BINARY FILE / ELF

-------------------------------
NETWORK / NETWORK SECURITY COMMANDS
# lynis audit system					#Runs a security audit of the system
# lynis audit dockerfile [path/to/dockerfile]		#Scans docker file

$ perl nikto.pl -h [192.168.0.1] [-p [port]]		#Perform a basic Nikto scan against a target host
$ perl nikto.pl -h [192.168.0.1] -p [80,88,443]		#Same as above, but for all ports listed
$ perl nikto.pl -h [https://192.168.0.1:443/]		#Same as above, scans http address isntead

# dhcpcd							#systemctl start dhcpcd
# firewalld						#systemctl start firewalld
# dhclient -r						#Fetches new IP Address

$ watch -n 1 "netstat -tpanl | grep ESTABLISHED"		#Monitor TCP Opened connections
$ ss -p							#Lists all processes connected to a socket

$ sudo arp-scan -I eth0 192.168.1.0/24			#Find all Active IPs in a subnet(Requires Installation)
$ lsof -Pan -i tcp -i udp				#Lists all listening ports together with the PID of the associated process | VERY IMPORTANT

# nmap -T0 -D [decoy1_ipaddress,decoy2_ipaddress,...,decoyN_ipaddress] [address_or_addresses]		#Stealhty Decoy Scans IP Address(es)
# nmap -sT <ip_address>
# nmap --script vuln <ip_address>
-------------------------------

TELNET / SSH SERVERS:
telnet telehack.com			#Telehack.com offers several online terminal games and applications
						After connecting to it, type for the games: zrun 
telnet bofh.jeffballard.us 666		#Find the best excuses
telnet bbs.archaicbinary.net 		#Archaic Binary is a (mostly) command line interpreted bulletin board system providing 
					^ a legacy interface for some useful APIs, Utilities, Files, Message Bases (FTN Style) 
					^ and access to ANSI/Text Door Games; http://archaicbinary.net/
ssh bbs.archaicbinary.net
putty bbs.archaicbinary.net

*note: ssh is safer
-------------------------------
SYSTEM PROCESSES COMMANDS:
# lsof +D <dirname>			#Lists all process using files in a given directory
# lsof -P -i -n				#Lists all processes that are using internet connection at the moment
# lsof | egrep "deleted|COMMAND" | less  #Lists all recently deleted file, per running process - VERY USEFUL
# lsof +L1				#Lists all processes that are still holding down, using deleted files,
					^ the files still in here results in disk space not bein freed.
# lsof -p <process_ID>			#Lists process using files by process id
# lsof -u <username>			#Lists process using files by username
# lsof -i				#Lists process using files by TCP/UDP port
# lsof -i :<port_number>			#Lists process using files by port_number
# lsof -i UDP:<port_number>		#Lists process using files by port_number and UDP only.
# lsof -c <process_name>			#Lists proccess using files by process name.
					ex.: when running '$ping google.com' on another terminal
					type: $lsof -c ping
# lsof -t				#Lists proccess using files by PID Only

$ pstree				#Lists process in tree format
# fuser -k filename			#Kills process that is locking a file
# pkill -x <process_name>		#Kills process by name
					^ ex.: pkill -x firefox
$ htop					#Lists all processes and resources in use
$ nvtop					#Lists all nvidia GPU Resources in use
# powertop				#Lists processes, resources, power consumtion
$ ps aux --sort=%mem,%cpu			#Sort al processes by memory and CPU usage
$ ps awwfux | less -S			#Shows processes in tree format

$ diff <(lsof -p 1234) <(sleep 10; lsof -p 1234) 	#Eavesdrop on your system for process ID 1234
							^ Not Really Tested
-------------------------------
CHANGING USER PASSWORD:
# usermod <username> -p <new_password>	#Changes the "username" password to "new_password" - exposes password
					^ useful when forgetting the current user password.
					^ requires administrative rights

# usermod <username> -s <shell>			#Changes user's shell
# usermod <username> -m <home_dir>		#Moves user current home directory to <home_dir>, attempts to change permissions when necessary.
						^ fails when directory <home_dir> doesn't exist.

# usermod <username> -d <home_dir> 		#Creates a new home directory for the given user

# usermod <username> --add-subuids FIRST-LAST	#Add user sub-uids to the given user, this allows the user to set extra permissions to files/directory using the new subuuid,
	or -v <subuid>				^ and use files that belong to the given subuuid.

# usermod <username> --del-subuids FIRT-LAST	#Remove sub-uid
	or -V <subuid>

# usermod <username> --add-subgids FIRST-LAST	#Adds sub-group ID
	or -w <subguid_n>

# usermod <username> --del-subgids FIRST-LAST 	#Deletes sub-group ID
	or -W <subguid_n>

# passwd					#Changes password for the given user - masks password
						^ requires knowing the current user password
						VERY IMPORTANT: "usermod -p" requires password to be cryptographed by crypt,
						the only way to modify a user password without remembering the current password is 
						to delete the current pass first using "passwd" as root by invoking 
						"passwd -d <username>", once that's done you need to login as that user
						using "sudo -iu <username>" and inserting new password with "passwd"
-------------------------------
LOGIN LOCK:
TOO MANY ATTEMPTS AT USER LOGIN:
TOO MANY FAILED LOGIN ATTEMPTS:

$ faillock --user <username>		#Checks failed loging password attempts
# faillock --user	<username> --reset	#Resets the fail-lock for a given user that has missed too many logins

note: 
	1 - /var/run/faillock should contain a file with the locked username
	2 - rm /var/run/faillock/<username>  to remove the fail-lock.
-------------------------------
Finding out GCC Version:
strings -a <binary/library> |grep "GCC: (" 	#This is how you can find out which GCC Version was used to compile a given binary file

-------------------------------
strip --strip-all -R .note -R .comment <binary>			#Strips all comments from a compiled binary file
strip --strip-unneeded -R .note -R .comment <library>		#same as above, both should be used together to help reduce binary size
-------------------------------
tldr <bash_command>		#Pulls Linux Command tldr manual from tldr git repository.
				 ^ Requires tldr installed.

-------------------------------
ls -la ~/*.txt | xargs cat 				#Display all .txt files located in ~/ directory
cat ~/list_of_files | xargs sudo rm -rf 		#Deletes all files displayed by cat for the given file
find / -name "*.log" -mtime +3 -print | xargs rm -rf 	#Deletes files older than 3 days using find command

find /u01 -name "*aud" -print | xargs rm -rf		#FOR LINUX - Deletes all files that finish in "aud"
							^ Inside the current directory - WARNING: BECAREFUL
-------------------------------
HARDWARE INFO / SYSTEM INFO COMMANDS:

	LISTING HARD DRIVES PER ATA NAMES
		ls -l /sys/class/block/sd*		#Lists hard disks per ata names.
							ex: ata0, ata1, ata2...
							^ This is needed when checking journalctl for disk errors
							^ or 'dmesg'.

hardinfo				#Displays all hardware information
# dmidecode -t 16			#Displays/Prints RAM Meta-data (Maximum Ram Supported, RAM frequency, etc)
# dmidecode -t 17 | awk -F":" '/Speed/ { print $2 }'	#Prints RAM Frequency
$ cat /proc/meminfo			#Virtual Memory Information
$ openssl speed md5			#Hosts CPU Performance
# lshw -class cpu			#Shows CPU Clock
# lshw -short -C memory			#Shows memory clock, size and type along with CPU Cache.
# lshw -class disk			#Shows Disk info
# lscpu					#Displays lists of welll known CPU Parameters
# lscpu --all --extended		#Shows CPU Frequencies/Frequency
# sensors				#Lists all CPU Core Temperatures
# dmidecode --type processor		#Displays all CPU Characteristics
# hwinfo --cpu				#Same as above
# dmidecode --type memory		#Displays all MEMORY Characteristics
# dmidecode -t bios
$ findmnt				#Lists all mounted devices
$ cat /proc/mounts			#Lists all mounted devices along with it's respective mount options/mount settings
$ cat /cat/mtab				#Same as above
$ cat /etc/issue			#Displays installed distro
$ uname -r				#Displays kernel being used
$ lspci -k				#Lists all hardware devices and their kernel modules/drivers
$ lsusb					#Lists all USB Devices
$ lsblk					#Shows all Connected HDs / Hard Drives / SSDs / Storage Media
$ lsblk -f				#^ Same as above more details
# blkid					#Lists all mounted connected HDs / Hard Drives / SSDs / Storage Media
# nvtop					#Lists GPU running processes for nvidia
$ cpu-x					#Port of an Old Windows program that displays CPU/GPU/Motherboard
# vmstat				#Reports virtual memory statistics, reports information about processes, 
					^ memory, paging, block IO, traps, disks and cpu activity.
# vmstat --stats			#Report/summary statistics
# vmstat -m				#???
# cpupower frequency-info		#Gives information about CPU
$ sudo tune2fs -l $(df -h / |(read; awk '{print $1; exit}')) | grep -i created 	#Shows how old your filesystem is;
										^ Show how old your linux OS installtion is.


# nmcli nm wifi on			#Starts wifi service
# lshw					#Lists all hardware devices

-------------------------------
HARD DISK COMMANDS:

# tune2fs -E force_fsck	/dev/sdX	#Forces check disk / fsck upon boot
					^ this is very useful when you need to check root disk for errors or changing features
					when the drive needs to be umounted.

# e4defrag -c <file>		#Online defragmenter for ext4 filesystem
				^ -c gets current fragmentation count and 
				     the ideal fragmentation count 
				<file> can be a device, mounted disk, file or directory
# e4defrag /dev/sdX		#Defrags disk

# dumpe2fs -h /dev/sdX		#Lists Data about EXT4 Filesystem, like Filesystem Features and Journaling Features and so on.
				^ Lists filesystem features.

# tune2fs -O <feature> /dev/sdX	#Enables a given EXT4 feature(s) on a given EXT4 Partition/Disk
				^ Requires umounting and using fsck.ext4 -Dfy after features have been enabled/disabled.
				^ some features can be enabled/disabled when mounting instead.

# e2fsck 			#Checks and Tests a linux ext4 filesystem 

# fsck.ext4 -Dfy		#Enhances and fixes any disk errors, requires disk to be umounted
				^ This needs to be used whenever a new feature is enabled/disabled using tune2fs

# mkfs.ext4 -m <reserved_block_space>		#Specify the percentage of the file system blocks reserved for 
						^ the super-user.  This avoids fragmentation,  and  allows  root-owned  
						^ daemons, such as syslogd(8), to continue to function correctly after 
						^ non-privileged processes are prevented from writ‐ ing to the file system.
						^ The default percentage is 5%.
						^ VERY IMPORTANT: FORMATS DISK!

# tune2fs -E force_fsck		#Forces fsck at next mount
# mke2fs -n /dev/sdc		#Shows all superblocks on a given disk
# tune2fs -O sparse_super	#Limit the number of backup superblocks to save space on large file systems
# dumpe2fs /dev/sdc		#Lists all EXT4 filesystem features
# fsck.ext4 -E discard		#Discards free blocks and unused inode blocks, useful for SSD(Solid State Devices) Disks
				^DO NOT USE THIS, instead use #tune2fs -E discard, read "SETTING UP TRIM / DISCARD" 
				^on topic 2.5.1 - VERY IMPORTANT
# wipefs -b			#Backups disk signature file to $HOME/wipefs-<devname>-<offset>.bak
				note.: read man page for examples on how to use dd to restore a signature file
# wipefs /dev/sda*		#Prints information about sda and all partitions on sda

# mount /dev/sdd5				#Mounts sdd5 according to /etc/fstab settings, fails mounting 
						if it doesn't exist

# mount --mkdir -t ext4 /dev/sdd5 /mnt/sdd5	#Mounts sdd5 to /mnt/sdd5 without following /etc/fstab settings, 
						this can be checked with: #findmnt
						^ mount command above allows mounting devices that doesn't exist in /etc/fstab
						^ --mkdir and -t not mandatory. 
						^ -t only mandatory when device has an unknown linux partition.
						^ --mkdir only mandatory when directory in /mnt/ doesn't exist yet.

# findmnt					#Lists all mounted disks/partitions on the systems along 
						it's options and attributes.

VERY IMPORTANT:
	fsck.ext4 -o and tune2fs -E may appear to enable similar features, but they're very different.
fsck.ext4 -o usually sets up options for the disk to execute when fsck is called, like the one in "SETTING UP TRIM / DISCARD"
on topic 2.5.1.
-------------------------------
EFIBOOTMGR: EFI BOOT MANAGER / UEFI

  - List the current settings then bootnums with their name:
    efibootmgr

  - List the filepaths:
    efibootmgr -v

  - Add UEFI Shell v2 as a boot option:
    sudo efibootmgr -c -d /dev/sda1 -l \EFI\tools\Shell.efi -L "UEFI Shell"

  - Change the current boot order:
    sudo efibootmgr -o 0002,0008,0001,0005

  - Delete a boot option:
    sudo efibootmgr -b 0008 --delete-bootnum

-------------------------------
whereis <application_name>	#Locates all folder/directories for a given application
				ex.: whereis firefox
-------------------------------
$ chsh -l 				#Lists available shells in the system
$ chsh -s <shell>			#Changes default user shell
$ usermod <username> -s <shell>		#Changes shell for a given user
-------------------------------
$ wc <filename>			#Counts number of lines, words, bytes in a given file.
-------------------------------
Disk Usage Tool:
	Checks how much disk space a directory is using.
-------------
# du -h --apparent-size file.img				#Apparent size of a file on disk
							^ Apparent size may differ from actual file size on the disk.
							^ This will usually display a bigger file when the given
							^ file/data is sparse.
-------------
# du -h <dir>						#Displays disk space of a given directory, listing all files
# du -sh <dir>						#Displays disk space of a given directory without listing it's files

# du -h --max-depth=10 ~/ | sort -hr | head -20   #Lists 20 biggest directories in '~' directory
# du -h --max-depth=10 ~/.config/ | sort -hr | head -20 
-------------------------------
# du -h --max-depth=1 / | sort -h 2> /dev/null	# Lists total size for each directory in a given folder | sorts them by size
							^ OBS.: 2> /dev/null redirects error messages to nowhere land

$ du -h --max-depth=1 2> /dev/null | sort -hr | tail -n +2 | head	# Same as above
									^ head only prints the biggest directories
									^ tail -n eliminates the . and ../ directory
									^ sorts by size and reverts the order before printing
-------------------------------
$ diff -q -r <FILE/DIRECTORY> <FILE_2/DIRECTORY_2>	#Compares both structures for differences, returns nothing if they're completely equal
--------------------------------
$ install				#Copy files and set attributes
-------------------------------
$ cp <file_in_another_path> $PWD  #Copies file from another directory into current one.
-------------------------------
KERNEL TOOLS
$ lsmod | grep 'modulename'	#Lists which modules have been loaded into the kernel at boot
				ex.: lsmod | grep 'nvidia'

$ modinfo <kernel_module>	#Lists information on hook/modules

$ mkinitcpio -M			#Lists all Kernel modules available on your system

$ lsmod				#Lists which Kernel Modules are in use by which modules
$ lsmod | grep <module_name>	#Same as above, but lists only the desirable kernel module

$ lspci -k			#Shows which Kernel Modules are being used by each system devices.
$ lsusb				#Lists USB Devices
$ lshw				#Displays hardware specifications
$ lsmem				#Exhibits memory specs
$ lscpu				#Lists cpu specifications

# modprobe <module_name>	#Activates kernel module(Temporary)
				^ If you want to make changes permanent, create a file with .conf prefix on /etc/modules-load.d/
# modprobe -r <module_name>	#Removes Kernel Module(Temporary)
				^ If you want to blocklist a module and not disable it entirely,
				create a file with .conf prefix on /etc/modprobe.d

$ cat /proc/cpuinfo | grep -E "vmx|svm" --color  		#Shows if you have a vmx or svm feature enabled in kernel using cat

$ zcat /proc/config.gz | grep <KERNEL_FEATURE_NAME>
		or
$ zgrep <KERNEL_FEATURE> /proc/config.gz
-------------------------------
GREP:
cat /proc/cpuinfo | grep -E "vmx|svm" --color  #Uses grep to search and color found occurrances
-------------------------------
loginctl			#Shows which users are logged in into the system
-------------------------------
dconf				#Very important - Describe Later
dconf-editor
-------------------------------
lsusb -v 			#Shows all USB Specification Variables, Including if USB is 2.0 or 3.0
lspci -k			#Shows hardware devices and it's kernel module driver
-------------------------------
$ makepkg -si			#Installs package into the system(-i) && installs it's dependencies(-s)
-------------------------------
# smartctl -a -i /dev/sdd	#Shows all of sdd's Disk Hardware Information, Including Disk Speed in RPM
				 ^ Also shows test information!
# smartctl -t short /dev/sdd	#Tests disk! Valid Optons: long(2 hours), short(2 minutes), conveyance, force, vendor
				 ^ type in #smartctl /dev/sdd -t for more info
# smartctl -X			#Aborts test
-------------------------------
# hdparm -I /dev/sdX | grep Rotation | grep --only-matching --extended-regexp '[0-9]+'		#Disk RPM
-------------------------------
# dumpe2fs /dev/sda2		  #Shows information on the given EXT Linux Partition
# tune2fs /dev/sda | LABEL | UUID  #Allows system administratorto adjust various tunable filesystem parameters on Linux.

# btrfs filesystem show		#Shows all mounted BTRFS filesystems
# btrfstune			#Used to enable, disable, or set various filesystem parameters.
				Filesystem must be unmounted! Common usecase is  to enable features that were not 
				enabled at mkfs time. Make sure you have kernel support for the features by 
				checking man page(man page: man btrfstune).

-------------------------------
$ getfacl -sR /bin/		#Returns facl for all files in the /bin/ folder, skipping the files that have
				default settings! 

-------------------------------
secret-tool			#Store and retrieve passwords
				 ^ Read man page.
-------------------------------
ENCRYPTING FILES WITH GPG / OPENPGP:
---HINT1: Read "MORE ABOUT ENCRYPTION" section
---HINT2: Check commands marked as VERY IMPORTANT
---HINT3: DO NOT USE STANDARD RSA, IT'S PRETTY MUCH DEAD THESE DAYS!
		^ Check it with gpg --gpgconf-list
		 Also ed25519, not ecDSA

---sources: 
	https://www.howtogeek.com/427982/how-to-encrypt-and-decrypt-files-with-gpg-on-linux/ - STUDY LATER
	http://pgp.mit.edu - POPULAR FREE KEYSERVER

---ENCRYPTING / DECRYPTING FILES:
# gpg --sign --encrypt <filename>		#Prompts the user to pick a GPG user ID before encrypting the message
						^ ASKS FOR PASSWORD.
# gpg --encrypt <filename>			#Prompts user to pick GPG User ID, doesn't prompts for password.

# gpg --store <filename>			#Encrypts file without picking a User Key, CAN BE DECRYPTED WITHOUT
						^ A KEY! - good for masking files inside a crypted file

# gpg -d <filename>				#Decrypts given filename, WRITES DECRYPTED MESSAGE TO STDOUT

# gpg -d <filename> > new_file.txt 		#Decrypts <filename> into new_file.txt


----ENCRYPTING FILES:
# gpg --out <new_filename> --sign --encrypt <filename>      #Encrypts <filename> into <new_filename>
								^ Uses Known KEY and Password FOR SIGNING.

# gpg --out <new_filename> --encrypt <filename>	       #Encrypts <filename>
								^ Uses known KEY, NO PASSWORD.

# gpg --out <new_filename> --store <filename>	       #Encrypts <filename> using standard gpg encryption
								^ Can be decrypted without a key or signature.
								^ UNSAFE, ONLY GOOD FOR MASKING FILES - VERY IMPORTANT

VERY IMPORTANT: It's possible to encrypt files with BOTH the SECRET AND PUBLIC KEYS!
^ SIGNED FILES can only be DECRYPTED USING SECRET KEYS + PASSWORD! UNSIGNED FILES WILL ALSO REQUIRE SECRET KEY, 
BUT NO PASSWORDS!  IMPORTING SECRET KEYS WILL REQUIRE THE PASSPHRASE/PASSWORD!

----MORE ON ENCRYPTION - DEFINITE ENCRYPTING:
# gpg -R <userkey> --out <encrypted_filename> --sign --encrypt -c <filename>
	^ By using SUDO, it's possible to add an extra passphrase for File Decryption
	this makes it possible to decrypt the file without a signature! - VERY IMPORTANT - DO NOT USE -c!

# gpg -R <userkey> --out <encrypted_filename> -s -c -e <filename>
	^ Signs, Sets an EXTRA Passphrase, Encrypts it!
	^ Better solution to the above method.

# gpg --out <encrypted_filename> -e <filename.txt>
	^ Short version for: #gpg -R <userkey> --out <encrypt_filename> -s -e <filename>

# gpg --out <encrypted_filename> -c <filename.txt>
	^ Encrypts filename without using a <user_key>, just a passphrase(custom password).

VERY VERY VERY IMPORTANT: GPG keeps a session open of it's own when files gets decrypted with either a given User-Key
or Passphrase it'll store password and/or key used for it,
so that the user don't have to type it again for the remainder of the session!  It's advised to kill any GPG processes
once you're done with it by using sudo htop!

----EXTRA COMMANDS FOR ENCRYPTING/DECRYPTING:
-3A) -u 0x12345678 -sb file 					#make a detached signature with the key 0x12345678

-2A) -sb <file>							#Make a detached signature

-1A) --clear-sign <file>					#Makes a cleartext signature to a given file,
								no encryption.

0A) --gpgconf-list						#Similar to --list-config but in general only 
								internally used by gpgconf tool.

1A) --output <filename>						#Write output to file instead of standard STDOUT.
	-o	

A) --recipient <user_key>					#Uses a given user_key for encrypting/decrypting.
	-r

B) --recipient-file <user_key_filename> 			#User a User_Key File instead for encrypting/decrypting.
	-f

C) --hidden-recipient <user_key_name>				#Same as A) but hides Key ID from the receiver.
	-R

D) --hidden-recipient-file <user_key_filename>			#Same as B) but hides Key ID from the receiver.
	-F
E) --try-secret-key <user_key>					#For hidden recipients GPG needs to know the key to use.

F) --try-all-secrets						#Tries all secret keys stored to find the right 
								decryptiong key.

G) --armor							#Creates human readable ASCII armored output.
      -a							^ VERY IMPORTANT: useful for exporting keys
								to real life papers!

H) --key-origin							#Tracks the origin of a key.

I) --import-options keep-ownertrust				#Keeps owner trusted of a given imported key

J) -t, --textmode, --no-textmode				#Defaults to on. Useful when communicating between
								two platforms that have different line ending(\n)
								conventions. --no-textmode disables it.

K) --disable-signer-uid
              #By default the user ID of the signing key is embedded in the data signature.  As of now this  is  only
              done if the signing key has been specified with local-user using a mail address, or with sender.  This
              information can be helpful for verifier to locate the key; see option --auto-key-retrieve.
	      ^ VERY IMPORTANT

L) --include-key-block
              #This option is used to embed the actual signing key into  a  data  signature.   The  embedded  key  is
              stripped down to a single user id and includes only the signing subkey used to create the signature as
              well as as valid encryption subkeys.  All other info is removed from the key to keep it and  thus  the
              signature small.  This option is the OpenPGP counterpart to the gpgsm option --include-certs.

M) --cipher-algo name
              #Use  name  as  cipher  algorithm. Running the program with the command --version yields a list of sup‐
              ported algorithms. If this is not used the cipher algorithm is selected from  the  preferences  stored
              with  the  key. In general, you do not want to use this option as it allows you to violate the OpenPGP
              standard.  --personal-cipher-preferences is the safe way to accomplish the same thing.

N) --digest-algo name
              #Use name as the message digest algorithm. Running the program with the command --version yields a list
              of  supported  algorithms.  In general, you do not want to use this option as it allows you to violate
              the OpenPGP standard. --personal-digest-preferences is the safe way to accomplish the same thing.

O) --compress-algo name
              #Use compression algorithm name. "zlib" is RFC-1950 ZLIB compression. "zip" is RFC-1951 ZIP compression
              which  is used by PGP.  "bzip2" is a more modern compression scheme that can compress some things bet‐
              ter than zip or zlib, but at the cost of more memory used during compression and  decompression.  "un‐
              compressed" or "none" disables compression. If this option is not used, the default behavior is to ex‐
              amine the recipient key preferences to see which algorithms the recipient supports. If all else fails,
              ZIP is used for maximum compatibility.


P) --cert-digest-algo name
              #Use name as the message digest algorithm used when signing a key. Running the program with the command
              --version yields a list of supported algorithms. Be aware that if you choose an algorithm  that  GnuPG
              supports  but  other  OpenPGP  implementations do not, then some users will not be able to use the key
              signatures you make, or quite possibly your entire key.

Q) --throw-keyids
   --no-throw-keyids
              #Do not put the recipient key IDs into encrypted messages. This helps to hide the receivers of the mes‐
              sage  and  is  a  limited countermeasure against traffic analysis. ([Using a little social engineering
              anyone who is able to decrypt the message can check whether one of the other recipients is the one  he
              suspects.])   On the receiving side, it may slow down the decryption process because all available se‐
              cret keys must be tried.  --no-throw-keyids disables this option. This option is essentially the  same
              as using --hidden-recipient for all recipients.


---------
---------
---------
----DECRYPTING FILES:
# gpg --out <new_filename> -d <filename>			#Decrypts <filename> into <new_filename>
								^ If file was SIGNED, it'll ask for a KEY.
								^ If file was UNSIGNED, it'll decrypt without a key.

VERY IMPORTANT: Signature is always shown when decrypting files regardless if it's been signed or not!
							
----EXPORTING KEYS:
# gpg -o <public_key.txt> --export --armor <key|username>		#Exports Public Key as ASCII armored text
# gpg -o <secret_key.txt> --export-secret_keys <key|username>   	#Exports Secret Key as armored ASCII text
# gpg --out <new_key> --export <key | username>				#Exports Public Key or USERNAME into <new_key>
# gpg --out <new_skey> --export-secret_keys <key | username>		#Exports Secret Key, SAME AS ABOVE

---MANAGING KEYS:
gpg --quick-generate-key user-id [algo [usage [expire]]]
gpg --quick-gen-key
		#This  is  a simple command to generate a standard key with one user id.  In contrast
              	to --generate-key the key is generated directly without the need to answer  a  bunch
              	of  prompts.  Unless the option --yes is given, the key creation will be canceled if
              	the given user id already exists in the keyring.


gpg --generate-key
gpg --gen-key
      #Generate  a new key pair using the current default parameters.  This is the standard
      command to create a new key.  In addition to the key  a  revocation  certificate  is
      created  and  stored in the ‘openpgp-revocs.d’ directory below the GnuPG home direc‐
      tory.

gpg --full-generate-key
gpg --full-gen-key
	#Generates new key pair with dialogs for all options.
	^ Extended version o --gen-key
	^ Secret keys can be automatically generated by using sudo command:
		#gpg --full-generate-key

gpg --edit-key
	#Present a menu which enables you to do most of the key management related tasks!

gpg --quick-set-expire <fpr(key) | expire>
	#Sets a expiration date on the given key. Set 0 as value for no expiration!

gpg --list-keys
gpg -k
gpg --list-public-keys
	#List the specified keys.  If no keys are specified, then all keys from the configured public  keyrings
        are listed.

gpg --list-secret-keys
gpg -K
	#Lists Secret Keys

gpg --fingerprint
	#Lists all keys along with their fingerprint

gpg --check-signatures
gpg --check-signs
	#Same as above: --list-keys! but also checks and verify the key signatures
	but the revokation status aren't shown!

gpg --locate-keys < keyname | username >
	#Locates key(s) by user or key names.

gpg --delete-keys < keyname | username >
gpg --delete-keys --yes
	#Deletes all keys owned by the user

gpg --delete-secret-keys <keyname | username>

gpg --delete-secret-and-public-key <keyname | username> 
	#Deletes both public and secret keys!!! - VERY GOOD - VERY IMPORTANT COMMAND

gpg --out <new_key> --export <username | keyname>
	#Export/Save key as a file - VERY GOOD - VERY IMPORTANT COMMAND
		example:
			#gpg --out /my_key_file --export fscrypt_key

gpg --out <new_key> --export-secret-keys
gpg --out <new_key> --export-secret-subkeys
              Same  as --export, but exports the secret keys instead.  The exported keys are written to STDOUT or to
              the file given with option --output.  This command is often used along with the option --armor to  al‐
              low  for  easy  printing of the key for paper backup; however the external tool paperkey does a better
              job of creating backups on paper.  Note that exporting a secret key can be a security risk if the  ex‐
              ported keys are sent over an insecure channel.

              The  second  form of the command has the special property to render the secret part of the primary key
              useless; this is a GNU extension to OpenPGP and other implementations can not be expected to  success‐
              fully import such a key.  Its intended use is in generating a full key with an additional signing sub‐
              key on a dedicated machine.  This command then exports the key without the primary key to the main ma‐
              chine.

              GnuPG may ask you to enter the passphrase for the key.  This is required, because the internal protec‐
              tion method of the secret key is different from the one specified by the OpenPGP protocol. - 
	      ^ VERY IMPORTANT

gpg --import <filename>
	#Imports the given key to the local keyring - VERY GOOD - VERY IMPORTANT COMMAND

gpg --sign-key <keyname>
	#Signs a public key with your secret key, 
	^ Short version of subcommand "lsign" from --edit-key

gpg --lsign-key <keyname>
	#Signs a public key with your secret key, but marks it as non-exportable.
	^ Short version of subcommand "lsign" from --edit-key

gpg --quick-sign-key <fpr(fingerprint)> [key_usernames]
gpg --quick-lsign-key <fpr(fingerprint)> [key_usernames]
	#Signs all given key names with the given fingerprint!
	^ WARNING: If no key_usernames are given, ALL EXISTING KEYS ARE GOING TO BE SIGNED!

gpg --quick-add-uuid <user_id> <new_user_id>
	#Adds new user id to an exisiting key.
	^ SIMILAR to --edit-key adduid

gpg --quick-revoke-uid <user_id> <user_id_to_revoke>
	#Revokes a user ID on an existing key.

gpg --quick-revoke-sig <fpr> <signing-fpr> [names]
	#Revokes the key signatures made by <signing_fpr> from the key specified by <fpr>.
	If [names] is given, only the signatures on the matching keys of usernames will be affected.

gpg --quick-set-primary-uid <user_id> <primary_user_id>
	#Sets or updates the primary user ID Flag on an existing key.

gpg --change-passphrase <user_id>
gpg --passwd <user_id>
	#Change the passphrase of the secret key belonging o the certificate specified as <user_id>.
	^ Read more on man pages! this is a short command for the subcommand passwd of the edit key menu!!!
	^ VERY GOOD - VERY IMPORTANT COMMAND


-------------------------------
ENCRYPTING FILES WITH OPENSSL:
1 - openssl
2 - openssl pkcs12, pkcs12
3 - openssl enc, enc

Please, read more on openssl-enc
---
Cryptographing Text:
	$ echo "plaintext" | openssl enc -e -bf -pbkdf2 -k sneedlog -base64

	Output will be:
		U2FsdGVkX1+8AqvUOAyzoOEz05TxpOYP6rLLKsbTUiM=

Decrypting text:
	$ echo "U2FsdGVkX1+8AqvUOAyzoOEz05TxpOYP6rLLKsbTUiM=" | openssl enc -d -bf -pbkdf2 -k sneedlog -base64
---
Cryptographing File:
	$ openssl enc -e -bf -pbkdf2 -k sneedlog -base64 -in <filename> -out <encrypted_filename>


Decrypting File:
	$ openssl enc -d -bf -pbkdf2 -k sneedlog -base64 -in <encrypted_filename> -out <decrypted_filename>

---Example:
	DSA and Elgamal, 3072 bits long keysize, no expiration, a***as3*3*64
-------------------------------
Compute and Check SHA message digest of each file:

sha1sum <filename>
sha224sum <filename>
sha256sum <filename>
sha384sum <filename>
sha512sum <filename>

Examples.:
sha1sum <filename1> <filename2>				#Calculate SHA1 checksums for multiple files
sha1sum <filename1> <filename2> > filename.sha1		#Ouputs SHA1 calculations from files 1 and 2 into filename.sha
sha1sum --check filename.sha1				#Read a file of SHA1 sums and verify all files have matching
							 ^ checksums.
sha1sum --check --quiet filename.sha1			#Only show a message for files for which verification fails.
-------------------------------
Calculates MD5 cryptographic checksums:

md5 <filename>
md5 <filename1> <filename2>
md5 -q <filename>
md5 -s <text>
-------------------------------
man man				#opens a manual for the linux manual program
man -a intro			#opens the introductions to linux	
man <package_name>		#opens manual for the given pacage(if it exists)

date				#executes the date calendar
cal				#executes the calendar package on the terminal
cal -y				#shows the entire year

tldr cal			#shows more info about calendar binary

VERY IMPORTANT: download tldr package for extra 101 documentation.
-------------------------------
CHECKING TEMPERATURE
Commandline Programs:
	sensors
	hddtemp
	
GUI Programs:
	psensors

-------------------------------
zramctl				#used for managing zram devices, useful as swap partition

CREATING ZRAM DEVICE

Note.: Either follow Alternative A or Alternative B, do all Mandatory steps.
Changing ZRAM Settings(Alternative A - Not a Permanent change):
	1)(OPTIONAL) # swapoff /dev/zram0
	2)(OPTIONAL) # zramctl --reset /dev/zram0
	3) # zramctl --find --size 6000M
	4) # mkswap /dev/zram0
	5) # swapon /dev/zram0

Turning OFF Zram device:
	1) # swapoff /dev/zram0
	2) # zramctl --reset /dev/zram0

Creating a ZRAM device:(Alternative A - Not a permanent change)
	1) # zramctl --find --size 6000M /dev/zram0
	2) # mkswap /dev/zram0
	3) # swapon /dev/zram0

Enabling ZRAM Kernel Module(Mandatory For Alternative A & B):
	1) $ echo "zram" | sudo tee /etc/modules-load.d/zram.conf
	2) $ echo "options zram num_devices=1" | sudo tee /etc/modprobe.d/zram.conf

Creating /etc/fstab Entry(Mandatory - Not Mandatory if Alternative B!):
	1) $ echo "/dev/zram0 none swap defaults 0 0" | sudo tee -a /etc/fstab

Creating Rule on /etc/udev/rules.d/ (Alternative B - Mandatory for doing permanent changes):
	1) $ echo "KERNEL==\"zram0\", ATTR{disksize}=\"4000M\", ATTR{comp_algorithm}="lz4", RUN=\"/usr/bin/mkswap /dev/zram0\", TAG+=\"systemd\"" | sudo tee /etc/udev/rules.d/99-zram.rules
		^ Observation: DO NOT Mistake "Kernel==" WITH "Kernel=", those are different and "kernel=" won't work.
		^ Observation2: all values for all attributes are surrounded by " " double-quotes!
		^ Observation3: Each attribute is comma separated!
		^ Note: Filename has .rules extension, otherwise udev won't read/execute it!
		 
		 NEW 2023:
			ACTION=="add", KERNEL=="zram0", ATTR{comp_algorithm}="lz4", ATTR{disksize}="4G", RUN="/usr/bin/mkswap -U clear /dev/%k", TAG+="systemd"
		 NEW 2025:
			ACTION=="add", KERNEL=="zram0", ATTR{comp_algorithm}="lz4", ATTR{disksize}="6G", RUN="/usr/bin/mkswap -U clear /dev/%k", TAG+="systemd"

(Mandatory) Read Topic:
	THE THREE ZRAM FILES

Restarting UDEV RULES on Archlinux without rebooting(Alternative B):
	#udevadm control --reload-rules && udevadm trigger
		or
	sudo udevadm control --reload-rules && sudo udevadm trigger
	^ Obs.: Won't work if it's the first time you need to reboot first.

TESTING UDEV RULES BEFORE APPLYING THEM:
	udevadm test /sys/class/backlight/acpi_video0/
		or
	udevadm test /dev/zram0
		or
	udevadm test $(udevadm info --query=/dev/zram0 --name=zram0) 2>&1 	
	^ ABOVE HAS NOT BEEN TESTED

		or
	udevadm test /etc/udev/rules.d/99-zram.rules
	(This one doesn't really work. Really don't use it)


Notes.: Do not use zswap if you want zram enabled as zram will never be used in this case,
The linux kernel uses zswap enabled by default, you can check it with:
	cat /sys/modules/zswap/parameters/enabled

To disable zswap permanently: 
	add zswap.enabled=0 to your kernel parameters in your active /boot/grub/grub.cfg file!

"Zswap is a kernel feature that provides a compressed RAM cache for swap pages. Pages which would otherwise be swapped out to disk are instead compressed and stored into a memory pool in RAM. Once the pool is full or the RAM is exhausted, the least recently used (LRU) page is decompressed and written to disk, as if it had not been intercepted. After the page has been decompressed into the swap cache, the compressed version in the pool can be freed." source: https://wiki.archlinux.org/title/Zswap

The difference compared to ZRAM is that zswap works in-conjunction WITH a swap device while zram is a swap device in RAM that does not require a backing swap device. 

IMPROVING PERFORMANCE: 
	https://wiki.archlinux.org/title/Improving_performance#zram_or_zswap


THE THREE ZRAM FILES
	There are three known zram files that need to be created in order to persist zram intialization throughout system reboots.

	Talk about all 3 .conf files require to create  zram device on section 'CREATING ZRAM DEVICE' :
			/etc/modules-load.d/zram.conf
			/etc/modprobe.d/zram.conf
			/etc/udev/rules.d/99-zram.rules

	ZRAM MODULES-LOAD.D FILE
		This file will load the zram kernel module.

		[file: /etc/modules-load.d/zram.conf]
			zram
		[/file]

	ZRAM MODPROBE FILE
		Configures kernel modules.

		[file: /etc/modprobe.d/zram.conf ]
			options zram num_devices=1
		[/file]
	

	ZRAM UDEV FILE
		Allows the initialization of the zram device, setting up zram size, compression algorithm and other desired settings.

		[file: /etc/udev/rules.d/99-zram.rules]
			ACTION=="add", KERNEL=="zram0", ATTR{comp_algorithm}="lz4", ATTR{disksize}="6G", RUN="/usr/bin/mkswap -U clear /dev/%k", TAG+="systemd"
		[/file]

------------------------------
LS && Less:
ls /{bin,home}/				#Displays bin/ and home/ at the same time
ls -lahSR ~/Downloads/ | less -S	#Pipes LS Command to less 
					^ ls -S guarantees output will be ordered by size
					^ ls -h displays human readable output
					^ ls -a shows all columns available
					^ ls -l outputs data as list
					^ ls -R recurses over sub-directories
					^ less -S Causes  lines longer than the screen width to be chopped (truncated) rather than wrapped.  
					That is, the portion of a long line that does not fit in the screen width is not displayed until you press RIGHT-ARROW.  
					The de‐ fault is to wrap long lines; that is, display the remainder on the next line.

-------------------------------
TEE:
echo Y | sudo tee /sys/module/zswap/parameters/enabled			#Tee reads an input and writes it back to the output(Overwrites Files) - VERY IMPORTANT
									it's very useful for concatenating/creating files residing in different user space
									as a substitute to the >(overwrite) or >>(concatenate) operators,
									since they only work when the user has the set of permissions on the file, never as sudo.

echo "Append_Text" | sudo tee -a ~anotheruser/Desktop/file.txt		#Appends text to another user file
echo "Hello World" | tee test_file.out > /dev/null			#Silently uses tee
-------------------------------
sudo udevadm info /dev/zram0	#shows all available parameters for creating an udev rule in /etc/udev/rules.d/
-------------------------------
CLEANING CACHE

	OLD WAY OF CLEANING CACHE
		$sudo sync; sudo echo 1 | tee > /proc/sys/vm/drop_caches      #CLEARS PageCache Only.
		$sudo sync; sudo echo 2 | tee > /proc/sys/vm/drop_caches      #CLEARS Dentries and Inodes Only.
		$sudo sync; sudo echo 3 | tee > /proc/sys/vm/drop_caches      #CLEARS the page Cache, Dentries and Inodes.
                                                     	      
		note:
			The 'sync' command will flush data back into disk 
			when they're still pending to be written on the disk, therefore FREEING THEM FROM MEMORY.

	NEW WAY OF CLEANING CACHE

		1. To free pagecache, dentries and inodes, use the below command (NOT RECOMMENDED):
			# sync; sysctl -w vm.drop_caches=3
			^ Not recommended, as it may crash/freeze/glitch running programs.

		2. To free dentries and inodes only, use the below command (NOT RECOMMENDED):
			# sync; sysctl -w vm.drop_caches=2 
			^ Not recommended, as it may crash/freeze/glitch running programs.

		3. To free the pagecache only, use the below command:
			# sync; sysctl -w vm.drop_caches=1

VERY IMPORTANT!
-------------------------------
Grep:
ls -aR /proc/ | grep 'cpu'	#Searches entire directory for CPU
-------------------------------
SLABTOP:
sudo slabtop			#Shows how much kernel is using the memory - Very Important
-------------------------------
free -m				#Shows how much memory you have available/use in the system
htop				#Htop is very useful to show processes by tree structures
-------------------------------
find /sys/ -name "control" -exec grep -H on {} \		#Finds all control related variables in /sys/
find / -name "*.log" -mtime +3 -print | xargs rm -rf 		#Deletes .log files older than 3 days using find command
								 ^ at the current directory.
find /u01 -name "*aud" -print | xargs rm -rf			#FOR LINUX - Deletes all files that finish in "aud"
-------------------------------
dmidecode			#Dmidecode, 
sudo dmidecode --type memory	#Fetches memory type
-------------------------------
$ df -a				#Lists all filesystems in use by the system
$ df -ha				#same as above, but enumerates disk size in human readable format
$ du -hs <directory>		#Tells the size of a given directory
# ncdu <directory>		#Lists all files and directories by size
				^ AUR Package, needs to be installed
# broot <directory>		#Same as ncdu, a bit faster
$ lsblk				#Lists all mounted devices on the system
# blkid				#Lists available Block Devices by UUID
				^ Shows both mounted and umounted devices, including umounted devices not listed
				in /etc/fstab

-------------------------------
KERNEL OPTIONS - NEW:
	These options should be appended at the same line as the kernel declaration,
since they are a kernel option.

$vt_handoff 5			#Boots into Display Manager 
$vt_handoff 3			#Boot into multi-user, command line
$vt_handoff 1			#Boots int single-user, command line
					^ At any given point you can start Display Manager again with:
						$sudo init 5
					vt.handoff (vt = virtualterminal) is a kernel boot parameter unique to Ubuntu

zswap.enabled=0			#Disables ZSWAP! Enabled effective use and creation of ZRAM.
mitigations=off			#Disable CPU's security patches
acpi=off			#Disables acpi, only required on laptops
pti=off  or nopti          	#risky, disables Meltdown Security 
					^ Already turned off by mitigations=off
random.trust_cpu=on	   	#ALLOWS INTEL's RNDNG
					^ Risky on Intel Ivy Bridge architecture and a few others
random.trust_bootloader=on 	#Same as random.trust_cpu, but for bootloader instead.

rdrand=force			#Useless - Override the decision by the kernel to hide the
                                ^ advertisement of RDRAND support (this affects
                                ^ certain AMD processors because of buggy BIOS
                                ^ support, specifically around the suspend/resume
                                ^ path).

nordnrand			#DEPRECATED IN FAVOR OF random.trust_cpu
				^ The generic random.trust_cpu=0 support for indicating you don't 
				^ trust your CPU's RNG was added to the kernel back in 2018. 
				^ Source: https://www.phoronix.com/news/Linux-Drop-No-RdRand - Date: 2022 - Jul - 10

pcie_bus_perf 			#Maximizes pcie bus perf when possible
psi=1				#?
rw				#?
quiet				#?
intel_iommu=on			#Enables iommu on intel CPUs when available
nvidia-drm.modeset=1		#Enables drm modeset module
usbcore.blinkenlights=1		#?

source: https://docs.kernel.org/admin-guide/kernel-parameters.html
------
------
Module parameters can be specified in three known ways: 
	1 - via the kernel options on /boot/grub/grub.cfg,

	2 - via modprobe, ex.: 
		$modprobe usbcore blinkenlights=1

	3 - via creating a file with .conf prefix on /etc/modprobe.d/, ex:
		3.1 - vfio.conf
		3.2 - blacklist-modules.conf
		3.3 - blacklist-nouveau.conf
		3.4 - firewalld-sysctls.conf
		3.5 - v4l2loopback.conf
		3.6 - vfio.conf
		3.7 - vmware-fuse.conf
		3.8 - zram.conf
		3.9 - nvidia.conf

Kernel Modules can be loaded using:
	1 - /etc/mkinitcpio.conf
		^ require "#mkinitcpio -P" to be used and system reboot soon afterwards
	2 - /etc/modules-load.d/

Listing Kernel Modules Parameters Manually:
	lsmod
		or
	grep -R . /sys/module/zswap/parameters
		or
	cat /proc/driver/nvidia/params
		or
	cat /proc/driver/nvidia/gpus/0000:01:00.0/registry
		or
	cat /proc/driver/nvidia/registry

------
/ETC/MODPROBE.D/
Note for below: Identation not required in the file(/etc/modprobe.d/): 
Note2: These are just module attribute options, the modules themselves should be loaded on mkinitcpio.conf or modules-load.d as described above.
Note3: use modprobe -r <module_name> to unload modules after changing them, then use modprobe <module_name> again to enable it.
if the given module is in use, you'll have to restart!
------------
------------
------------
------------

SETTING UP 'HIGH PERFORMANCE' OPTION FOR OPENGL
	This option makes system a lot more smoother for NVIDIA GPUS, including when playing non OpenGL games:
		nvidia-settings > OpenGL Settings > Image Settings > High Performance

------------
------------
------------
------------

NVIDIA PERFORMANCE TRICK:
[/etc/modprobe.d/nvidia.conf] - VERY IMPORTANT: reduces keyboard input delay from 50~120ms to fixed 36ms
	options nvidia-drm modeset=1 fbdev=1
	options nvidia NVreg_UsePageAttributeTable=1 NVreg_UpdateMemoryTypes=0 NVreg_EnableStreamMemOPs=1 NVreg_EnableGpuFirmware=1
[/end-of-file]
	VERY IMPORTANT: 
	^ options nvidia-drm modeset=1 fbdev=1 for some wayland compositors to work
	^ use fbdev option when needed for wayland -  Newer Drivers automate this
	^ Starting from nvidia-utils 560.35.03-5, DRM defaults to enabled.
	^ use the following command to make sure  DRM is Enabled:
	^ 	run0 cat /sys/module/nvidia_drm/parameters/modeset
	^ use the following command to make sure fbdev is Enabled:
	^	run0 cat /sys/module/nvidia_drm/parameters/fbdev
	^^ source: https://github.com/ventureoo/nvidia-tweaks

	^^VERY IMPORTANT: It's required to rebuild initramfs for all kernels and reboot! 
		#mkinitcpio -p

		^Once you restart the system make sure there are no error messages related to modprobe or kmod in your journal:
			#journalctl -b-0 | less

		^ if they exist, make sure to re-install kmod first, then finally rebuild initramfs following the 
		^ previous step.
			#pacman -S --overwrite '*' kmod

	^^ note: if you type modinfo nvidia you'll see a list of kernel parameters that can be used here
	NVReg_UsePageAttributeTable=1 can only be used if your System Supports PAT(Page Attribute Table),
	type: "$cat /sys/kernel/debug/x86/pat_memtype_list" to check if your system supports PAT, a long list of numered addresses should appear
	if it supports PAT. If your system does not support PAT, enabling this option for your GPU shall result in slow downs and unstability.
		^ source: https://www.kernel.org/doc/html/latest/x86/pat.html
	when enabling this, make sure "NVreg_UpdateMemoryTypes" isn't set to 2, since it disables PAT.
		^ source: https://wiki.gentoo.org/wiki/NVIDIA/nvidia-drivers#Kernel_module_parameters

	once PAT is enabled and system is rebooted, "$cat /proc/driver/nvidia/params" should have "UsePageAttributeTable: 1".

	^^NVreg_EnableGpuFirmware=1 enables Nvidia GSP feature only exist in Turing Cards(Nvidia GT16 and beyond).
	  ^To make sure it's enabled, Type in: $nvidia-smi -q | grep -i "gsp"
	  ^Once it's enabled, Output should be: "GSP Firmware Version                  : 530.41.03"
	  ^ Only enable this featurue for drivers beyond 530 version, older versions may breakup your system
	  ^ Source : https://github.com/ventureoo/nvidia-tweaks
	  ^ https://download.nvidia.com/XFree86/Linux-x86_64/530.41.03/README/gsp.html


	Other sources:
		Keyboard Input Delay Test: https://joltfly.com/keyboard-scan-rate-test/

[blacklist-nouveau.conf]
	blacklist nouveau
[/end-of-file]

[firewalld-sysctls.conf]
	install nf_conntrack /usr/bin/modprobe --ignore-install nf_conntrack $CMDLINE_OPTS && /sbin/sysctl --quiet --pattern 'net[.]netfilter[.]nf_conntrack.*' --system
[/end-of-file]

[v4l2loopback.conf]
	options v4l2loopback nr_devices=2 exclusive_caps=1,1,1,1,1,1,1,1 video_nr=0,1 card_label=v4l2lo0,v4l2lo1
[/end-of-file]


[vfio.conf]
	options vfio-pci ids=10de:11bf,10de:1c82,10de:0fb9
[/end-of-file]

[zram.conf]
	options zram num_devices=1
[/end-of-file]
------
------
/ETC/MODULES-LOAD.D/

[v4l2loopback.conf]
	v4l2loopback
[/end-of-file]

[zram.conf]
	zram
[/end-of-file]

[nvidia-dkms.conf]
	nvidia
	nvidia_modeset
	nvidia_drm
	nvidia_uvm
[/end-of-file]
------
------
/ETC/MKINITCPIO.CONF

	MODULES=(vfio vfio_iommu_type1 vfio_pci vfio_virqfd virtio-gpu nvidia nvidia_modeset nvidia_drm nvidia_uvm wd719x zram v4l2loopback)
------
------
Hyphens (dashes) and underscores are equivalent in parameter names, so:

log_buf_len=1M print-fatal-signals=1 IS THE SAME AS log-buf-len=1M print_fatal_signals=1

Double-quotes can be used to protect spaces in values, ex.: param="spaces in here"
----
----
KERNEL COMPILE OPTIONS:
You can find current kernel compile options in: /proc/config.gz     - VERY IMPORTANT
That's only available if CONFIG_IKCONFIG_PROC was set when the kernel was built. 
This is a default behavior in archlinux, but not in RHEL or Unbuntu

SOURCE OF KERNEL INFO: https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html
----------------------------------------
ZIP, .7Z / 7Z, ZCAT, ZCMP:

zcat filename				#Display contents of compressed files
zgrep <text> <filename>			#Searchs for text in the given compressed filename
zcat <filename>	| grep <text>		# ^ Same as above

zcmp					#Compares 2 different compressed files

7z a <zip_File> <source_files>		#Compacts source_file into zip_file
7z e <zip_file>				#Uncompress zip_file into current folder
---------------------------------------
LESS:  -- VERY IMPORTANT
lsbk -O | less -S			#Displays output data in a format the user can slowly read in a terminal in a vi style
					^ Very useful for reading very large outputs on a terminal.
						For example: journalctl or dmesg

----------------------------------------
SYSCTL CAN SHOW/MODIFY KERNEL SETTINGS AT RUNTIME:

sysctl -n kernel.hostname			#Prints content of the given variable
sysctl -w kernel.domainname="example.com"	#Makes temporary change to the given variable in the commandline
sysctl -a | grep 'memory'			#Shows all memory related variables 
sysctl kernel.random				#Shows kernelrandom variable content
						^ 2023 NOTE: read 'KERNEL NO LONGER SHOWS AVAILABLE ENTROPY' topic
sysctl -p [FILE]				#Loads in sysctl settings from the file specified, or the default /etc/sysctl.conf.
						Regular expressions can also be used, ex.:
							sysctl -p /etc/sysctl.conf

						this will permanently load any changes on boot time from the given specified file

defaul directory: /etc/sysctl.conf
^ settings can be set permanently here

-------------------------------
printenv 			#Prints all Environment Variables
-------------------------------
mount: 

# systemctl daemon-reload					#Reloads /etc/fstab in case it has been changed
# mount -a							#Mounts/Remounts all disks/drives 
								^ in /etc/fstab without restarting the computer
	$mount -av						#Same as above, using verbose mode.
# mount /path/to/file.iso /mnt/cdrom -oloop			#Mounts an iso image 
# mount /mnt/<device_label/device-id>				#Mounts device if it exists in /etc/fstab
# mount --mkdir -o loop,offset=537919488 ./archlinux_vm.raw /mnt/archlinux_v		#Mounts a block-device from a .raw image file by creating a loop device(/dev/loop1)
											^ From an offset on the given block device(fdisk -l ./archlinux_vm.raw)
											^ offset=Sector_Size*Device_Start_Sector
											^ loop devices are block devices that maps it's data blocks into a regular file into a
											^ filesystem or another block device
			VERY IMPORTANT: do not allow 'journaling filesystems' on loop devices, it will break write order.
#losetup -a					#Lists all loop-devices
#losetup /dev/loop2 ./android_vm.raw		#Atach a file to a loop device | Create a loop device.
#losetup -D					#Detach all loop devices
#losetup -d /dev/loop2				#Detach a loop device
#fdisk -l /dev/loop2				#Lists all partitions in a given loop block device after it has been attached to a given file
#fsck.ext4 -Dfy	/dev/loop2p2			#Runs filesystem checkdisk on an ext4 partition located in the /dev/loop2 device after it has been attached
						^ in this case, it's partition number 2. ###SPECIAL STEP

Mount a directory to another directory:
	mount --bind path/to/old_dir path/to/new_dir
	use 'lsblk' to list all bind mounts on the system.

CREATING RAW IMAGE:
	qemu-img create -f raw ./archlinux-install.raw 5G

	^VERY IMPORTANT: This is 200x times faster than using dd to create an empty zeroed file!!!

RESIZING EXISTING RAW IMAGES | RESIZING LOOP DEVICES:
	Very Important:
		Some filesystems can not be resized without it's Kernel Drivers installed on linux.
		Sometimes it may not be possible to resize given partition on Linux due to lack 
		of filesystem kernel module/driver!
		^ This isn't related to cqow or raw images.
		^ MAKE SURE TO MODPROBE THE DESIRABLE FILESYSTEM FIRST AND CHECK WITH LSMOD AFTERWARDS!
		^ It's advisable to use the virtual machine to allocate the desirable free space, so you 
		don't have to deal with this
		^ Optionally, if gparted doesn't work, try using cfdisk inside your current system, not the VM.

	0 - VERY IMPORTANT: Unattach & Umount all loop devices
		#losetup -D

	1 - Resize Image File: 
		#qemu-img resize <image_file> +2G
		^ VERY IMPORTANT: Read IMPORTANT NOTE2 about Shrinking DISK Image.

	2 - Attach image file to loop device
		#losetup /dev/loop2 <image_file>
		^ Very important: if you have more than one partition, make sure to select the proper partition here
		^ Check ###SPECIAL STEP

	3 - Fix filesystem using gparted
		#gparted /dev/loop2

		3.1 - Check Filesystem and fix errors:
			3.1.1 - Right Click > Check

	4 - (Optional - If gparted doesn't work at first)
		4.1 - Type:
			#fsck.ext4 -Dfy /dev/loop2

		4.2 - Repeat step #3

IMPORTANT NOTE: Step #2 has not been tested with multiple partitions
	
IMPORTANT NOTE2: Refering to manpage, Before using this command TO SHRINK A DISK IMAGE, you MUST use file system and partitioning tools inside the VM to reduce allocated file systems and partition sizes accordingly. Failure to do so will result in data loss! 

READ MORE ON: 
	CREATING AND MOUNTING LOOP DEVICES
-------------------------------
KERNEL HOOK MODULES:
modprobe		#adds or removes modules from the kernel
modinfo [kernel_hook]   #Shows info on kernel_hook	

mkinitcpio -H [hook_name] 	#it's supposed to the the same as modinfo, but doesn't works???
-------------------------------
SYSTEM BOOT ORDER:

uefi->grub->initrd->systemd
-------------------------------
watch -n1 nvidia-smi		#runs nvidia-smi every 1 second
				works with other programs that update data overtime
				useful for creating scripts that are able to check previous and current state of text-outputs.

watch -n -n1 sensors		#??? No idea

watch -n 10 df -h		#Watches df -h using a 10secs interval
-------------------------------
BASH - GNU BOURN-AGAIN SHELL:
Shell Introduction:
	The shell is an interface for you to interact with your operating system (OS). 
	As you type commands into the shell, it's responsible for interpreting those commands 
	and making the magic happen. Operations like copying files, piping, listing files are all within a shell's remit.

$ cat /etc/shells	#Prints all shells available on your system, 
			These will usually be available from /bin/
			

$ bash			#Starts a bash shell session, reloads all bash scripts(.sh) without rebooting
			including ~/.bashrc, refreshing all LOCAL and GLOBAL ENVIRONMENT VARIABLES.
			
$ source <shell_script>	#not exactly a shell command
			This command takes the contents of the specified file or resource and 
			passes it to the Tcl interpreter as a text script.	
			In other words, it reloads the shell script for the current shell session.
			This is usefull when changing/adding shell scripts, or modifying ENVIRONMENT VARIABLES 
			for the current shell session.
	
$ bash -c "command"	#Executes command

$ bash file.sh		#Runs command(s) from a file
  or
$ sh file.sh

$ bash -x file.sh		#Runs and logs every command executed to the terminal
  or
$ sh file.sh

$ bash -e file.sh		#Runs all commands until an error occurs
  or
$ sh file.sh

$ bash -s			#Runs commands from stdin
  or
$ sh file.sh


Each Operating System has it's own Shell System;
Windows has MSDOS and Windows Power Shell;
Linux Default Shell Application is either Bash or C-Type;
Others can be installed alongside it, like: 
	1 - ZSH (Z Shell)
	2 - Fish
	3 - BASH (Bourn-Again Shell) 
		BASH includes many prominent features from the Korn Shell (ksh) 
		as well as the C-Shell (csh).

		Features:
		One Dimensional Array: 1-D Arrays in BASH allows easy referencing of data. The manipulation of 
		the lists of the data also becomes possible.

		IMPORTANT: Environment Security: BASH offers you an extraordinary feature of ‘Restricted Mode’. 
		A BASH starting with the name rbash allows the shell to function in the restricted mode.
	4 - KSH (KornShell)
	5 - TCSH
	6 - C-Type Shells:
		6.1 - TCSH (TENEX/TOPS C)
		6.2 - CSH (C Shell)

echo $SHELL		#Prints your current default shell
-------------------------------
LESS Binary:
the "less" binary can pipeline text-output from another progam and
print them by asking the user to press spacebar to continue.
less only prints data until it fits the entire screen by
checking font-size and screen-width it can calculates how many letters
can fit on your screen and prevents it from leaking out of the terminal-buffer.

examples:
$ printenv | less		#Prints all environment variables

ls -laR / | less		#Prints all directories 

-------------------------------
--OPERATORS:
!! OPERATOR:
Very Important command, allows you to repeat the last given command,
ex.:
pacman -Syu xorg-devel
!! i3-wm
!! nvidia-dkms

for more info type: tldr !!


! OPERATOR:
When using a very large compiling command like the one below
	g++ -std=c++17 MemRam.cpp -lstdc++fs

It is possible to re-use the same statement by typing: 
	!g++

Same for the gcc compiler:
	!gcc

| PIPELINE OPERATOR:
fetches the output-text of a given program and pipes it to another program
that is able to read the passed data as an argument.

This is also the reason why you're taught about int main(int argsc, char *argsv[]);
on school, your program can receive arguments from another program and therefore
execute or request tasks from/to other programs by making/receiving system calls from/to them.

IMPORTANT: It's important that the program on the receiving end of the pipe is able to work on the given args,
or else piping will likely fail.
-------------------------------
edit  #An alias to a `run-mailcap`'s action edit.  
 	Originally `run-mailcap` is used to process/edit mime-type/file.


Edit action can be used to view any file on default mailcap explorer:
ex.:	edit <filename>
-------------------------------
sudo --edit <filename> 			#edit files as sudo in your default text editor
					 ex.: sudo --edit /etc/fstab
sudo -iu <username>			#logs in as another user in the system 

sudo su					#use sudo to log as root -- #DEPRECATED - Use sudo -iu root instead 
					because su will not change your $HOME and a few other user ENVIRONMENT VARIABLES
					will remain as the ones in the non-sudo user, this can lead to bad configuration set
					by other programs when installing packages, automated/temporary files creation for that user.
-------------------------------
CPU TOOLS:
nproc				#Checks how many cores are available on the current system,
				doesn't show many are being used.

lscpu				#Lists how many CPUs are available, and how many are being used

ls /sys/devices/system/cpu/	#Lists all cpu cores AVAILABLE

Disabling CPU Cores at runtime:
	echo 0 > /sys/devices/system/cpu/cpu3/online

	If you want to switch all CPUs off except cpu0:
		[code]
			for x in /sys/devices/system/cpu/cpu[1-9]*/online; do
			  echo 0 >"$x"
			done
		[/code]
^ not tested, don't use this in the current machine, leave it for a virtual machine. - WARNING
Warning: It is not possible to disable CPU0 on Linux systems i.e do not try to take cpu0 offline. Some architectures may have 
some special dependency on a certain CPU. For e.g in IA64 platforms we have ability to sent platform interrupts to the OS. a.k.a 
Corrected Platform Error Interrupts (CPEI). In current ACPI specifications, we didn’t have a way to change the target CPU. Hence if 
the current ACPI version doesn’t support such re-direction, we disable that CPU by making it not-removable. In such cases you will 
also notice that the online file is missing under cpu0.

---------more on CPU Cores:

Turns of CPU 3:
echo 0 > /sys/devices/system/cpu/cpu3/online

echo 1 > /sys/devices/system/cpu/cpu3/online

Lists Online CPUs:
	/sys/devices/system/cpu/online

Lists Offline CPUs:
	/sys/devices/system/cpu/offline

----------Information on CPUs:
When typing lscpu you can see total of cores, cpus, threads, etc.

Total CPUs = Cores_Per_Socket * Sockets * Threads_Per_Core 
-------------------------------
SYMLINKS(SOFTLINKS) LN SYNTAX:

The man pages are obnoxious regarding the creation of symlinks:
	ln -s <TARGET> <LINK_NAME> 

TARGET is the actual current directory that the symlink file will point to, 
LINK_NAME is the symlink's filename, since this is actually a file it can be moved elsewhere using mv or copied using cp.
because of that there can't exist any file/directory with the same name as the LINK_NAME,
backup or removal will be required.

example:
	ln -s /tmp/mydir/ram/<program_name> <program_name>
	 ^ Creates a symlink file called <program_name>




Making Hard Links from Directories:
The command cp -al is especially useful in this regard. It makes a complete copy of a directory structure, where all the files are represented by hard links to the original files. You can then proceed to update files in the structure (after creating actual copies of only these files), and only the files that you update will take up additional space. This is especially useful when maintaining multigenerational backups.
-------------------------------
$ curl -LOR http://path/to/file			#Downloads a single URL file into the current path
-------------------------------
print -p ~/mydir/mysubdir/mysubsubdir/ok/	#Creates parent and listed subdirectories 
-------------------------------
$ systemctl status <service_name> 	#Checks status of service
$ systemctl status --all		#Lists detailed status on all system services
$ systemctl --all			#Lists dead, inactive, active system services
$ systemctl list-unit-files
-------------------------------
# dmesg					#Checks kernel message
# dmesg --level err
-------------------------------
# mkinitcpio -P				#Creates all presets on /etc/mkinitcpio.d/
# mkinitcpio -p <kernel_name>		#Creates based on the given kernel name
-------------------------------
DD COMMAND:
	# dd if=input_file/directory of=output_file/directory

# dd if=file.iso of=/dev/usb_drive status=progress		#Makes bootable USB Drive from an isohybrid file
								AND shows the progress
# dd if=/dev/sdx3 of=~/my_backup.img status=progress		#Creates an image from /dev/sdx3
								AND shows progress
# kill -USR1 $(pgrep ^dd)						#Check progress of an ongoing dd operation
								(run this command from another shell)

# tune2fs -U time						#Sets a new UUID to the device, needs to be used 
								after dd if=<block_device2> of=<block_device2> 



***When cloning or backing up operational system, you need to set
the output_file/directory to a different partition/disk than the one
used as input_file!!!! - WARNING

***REWRITE THIS LATER - TODO - REASON: IT WORKS WELL ON A PEN-DRIVE
***REWRITE THIS LATER - CLONE SYSTEM TO PEN DRIVE OR ISO, MAKE MODIFICATIONS USING A VIRTUAL MACHINE SO IT CAN RUN ON OTHER SYSTEMS.
***IF YOU PLAN ON MAKING AN INSTALLATION DISK:
***ALWAYS BACKUP /HOME/ DIRECTORY TO A DIFFERENT PARTITION
***REMOVE ALL USERS WITH USERDEL FROM THE SYSTEM(EXCEPT ROOT)
***REMOVE USER GROUPS 
***REMOVE /HOME/ SUB-DIRECTORIES, LEAVE /HOME INTACT
***REMOVE HOME USERS FROM /etc/sudoers
***INVALID SYMLINKS CAN BE LATER REMOVED FROM /BIN/
*** VIDEO DRIVERS WILL HAVE TO BE INSTALLED BY HAND IN CASE SYSTEMS ARE DIFFERENT
*** MAKE FALLBACK GRUB ENTRIES AND NOMODESET KERNEL SETTINGS JUST IN CASE ON /boot/grub/grub.cfg
*** AFTER BACKUP RESTAURATION, RE-INSTALL GRUB USING ARCH-CHROOT
*** OPEN AND EDIT GRUB.CFG EDIT IT AS NEEDED
*** MKINITRAMFS IF SYSTEMS ARE DIFFERENT
*** EASY ALTERNATIVE METHOD: get all user installed packages and do a clean install on another system: 
	1 - sudo pacman -Qe >> userpackages; 
	2 - copy the file into the installation .iso | bring it on a pen-drive after installation
	3 - sudo pacman -Syu $(cat userpackages)
*** THE METHOD LISTED ABOVE MIGHT NOT WORK ON A DIFFERENT SYSTEM, BECAUSE DRIVERS INSTALLED BY THE USER
WILL BE RE-INSTALLED ON THE DIFFERENT SYSTEM. Causing it not to boot!
*** THIS STILL A GOOD WAY TO BACKUP YOUR ARCH SYSTEM TO BE USED IN THE SAME MACHINE, ON DIFFERENT PARTITIONS/DISKS

Clone Disks:
# dd if=/dev/sdX of=/dev/sdY conv=noerror,sync
-------------------
Clone Partitions:
# dd if=/dev/sdaY of=/dev/sdbY conv=noerror,sync
-------------------
Backup Entire Hard-Disk To Disk Image:
	# dd if=/dev/sdX of=/unknown/var/tmp/sdX_disk.img

Restore a Hard-Disk with Disk Image:
	# dd if=sdX_disk.img of=/unknown/dev/sdY

------------------
Backup your Linux System:
	# dd if=/dev/sdX7 of=/unknown/sdY1_disk.img
	
Restore your Linux System to Another Hard Drive:
	# dd if=~/sdc7_disk.img of=/mnt/sdc8/

In this specific case, you'll need to arch-chroot in the system and install grub,
then manually edit /boot/grub.cfg as needed
and maybe mkinitcpio -p just in case initramfs aren't there.
-------------------------------
lists all system services that are currently active
systemctl status
-------------------------------
Test Write/Read Speed:

Test Read Speeds:
# hdparm -tT /dev/sda  		#Tests Disk Speed!

Test at least 3~5 times!
source: https://www.shellhacks.com/disk-speed-test-read-write-hdd-ssd-perfomance-linux/
	https://www.lostsaloon.com/technology/how-to-test-the-read-and-write-speeds-of-a-hard-disk-in-linux/

Write Speeds:
sync; dd if=/dev/zero of=/tmp/sample.bin bs=8k count=128k; sync;
		or
sync; dd if=/dev/zero of=/tmp/tempfile bs=1M count=1024; sync

obs1: remember to erase the files with "sudo rm /tmp/tempfile" or "sudo rm /tmp/sample.bin"
obs2: if you want to test another Hard-Drive, you should use the same if(input-file) as the of(output-file) for testing!
	Example: 1 - first create the file for testing purposes with:
				dd if=/dev/zero of=/mnt/sdc7/tempfile bs=1M count=1024;
		 2 - then test by copy & pasting using the same source path into the same hard drive on a diff. file:
		 		sync; dd if=/mnt/sdc7/tempfile of=d/mnt/sdc7/2nd_tempfile/ sync
		 3 - Remember to erase the files you just created!
-------------------------------
Creating Alias on Local Bash:
	echo 'alias ytmp3="youtube-dl --extract-audio --audio-format mp3"' >>  ~/.bashrc
-------------------------------
CD: - CHANGE DIRECTORY | CP: COPY | CHMOD
$ cd $OLDPWD			#Changes directory to the previous directory CD was used
					example: cd $OLDPWD/another.directory/
					$PWD contains the current path, can be used with echo $PWD!
$ cd ~				#Changes directory to user home space
$ cd ~username			#Changes directory to the given username home path 

$ cd ../../			#Changes Directory to that of the parent inode of the current parent inode,
				VERY IMPORANT: Very Useful when you want to preserve $OLDPWD for changing back into that directory
				And also to edit config files from parent inodes.
			
$ cp ~/username/Music/ ./		#Changes directory Copies all files from /home/username/Music to current Inode/Directory 
$ chmod o-rwx ../			#Changes directory Changes (o)thers permissions from Parent Inode/directory
-------------------------------
Acessing user home without typing full path /home/username/:

~username/Documents is better than /home/username/Documents

obs.: this is really usefull when you want to access a shared group folder.
-------------------------------
history			#shows bash / console / terminal history
-------------------------------
Easy method to create and append contents to text files:

$ echo "Text" > New_File.txt
$ echo "More_Text" >> Old_File.txt

The difference between '>' and '>>' is simple,
'>' truncates the file before writting to it.
'>>' concatenates contents to file if it already exists.
---------------------------
Obs: Smartctl requires smartmontools package!

smartctl can be used to check Hard Disk Hardware Failures.
Although, your hard drive must support it.

smartctl -i /dev/sdx			#Checks if HD Supports SMART features AND checks if it's enabled

smartctl -s on -d ata /dev/sdx		#Enables SMART support in case its not enabled 
smartctl -d ata -H /dev/sdx		#Run overall-health self-assessment test
	or
smartctl -d auto -H /dev/sdx

smartctl -a -i /dev/sdx				#Displays full info on hard disk smart data
smartctl -a -i /dev/sdx > ./disklog_sdx 	#Outputs smart data info into given file
smartctl -A /dev/sdx				#Displays short info on hard disk smart data

smartctl -t short /dev/sdx			#Does small smart test on the disk
smartctl -t long /dev/sdx			#Does a long test on disk

-H attribute: If  the  device  reports failing health status, this means either that the device has already failed, or that it is predicting its own failure within the next 24 hours.  If this hap‐
              pens, use the '-a' option to get more information, and get your data off the disk and to someplace safe as soon as you can.
---------------------------
Commands:
whereis			#Gives location of installed program / package

users			#Lists which users are connected to the system
w			#Lists how many users are using the system and what they're executing
who			#Lists which users have connected to the system and when
whoami			#Gives effective information on the current user
lastlog			#Shows information relative to last logins done into the system
lastb			#Shows failed login attempts
last			#Shows all information related to logins and logouts
---------------------------
find -name myfile 1> /dev/null		#Finds which errors were generated by that command
---------------------------
ex.:
$ ls --file-type *.jpg | awk '{system("file=\""$0"\"; echo rm ./${file%%.*}.png")}'
^ #Removes all files under the same name with a different file type extension. Assumes they're the same files.
^ remove the echo command
										

$ ls | awk '{system("mv \"${PWD}/"$0"\" ../")}'					#Moves all files/dirs in the current directory
										into another dir.; In this case, another-dir
										here is the current parent-directory.

$ ls /any_dir | awk '{system("stat \"${PWD}/"$0"\"")}'				#Stats all given files in a given directory,
										with current dir. to make sure they don't
										exist in the current dir.

cd "~/Games/gog-galaxy/drive_c/Program Files (x86)/GOG Galaxy/Games/Diablo"; ls | grep -i ".mpq" | awk '{system("sudo ln -sf \"${PWD}/"$0"\" /usr/share/diasurgical/devilutionx/"$0)}'
^ #Creates symlink for all .mpq files in the current bash directory into /usr/share/diasurgical/devilutionx/, \" around ${PWD} exists to ensure white-spaces is interpreted as a full-path instead of it being ignored and interpreted as multiple paths. note that $0 is an argument that comes from the pipeline 'ls' and 'grep', because of this $0 MUST NOT be surrounded by double-quotes " and ". This commandline is meant for Diablo 1 devilux port of the game. The above command must be run for both standard and hellfire directories of the game. beware of the cd command when going into hellfire dir.

$ awk '/start_pattern/,/stop_pattern/' file.txt			#Shows a block of text using awk
$ echo one two three four five | awk '{print $3, $4}'     	#prints 3rd and 4th arguments
$ awk '/^st/' /etc/group  					#prints all groups that start a line with 'st'
$ awk -F: '{print $1}' /etc/group | head -4			#prints all groups
$ awk -F: '$3 > 1000' /etc/group				#prints all groups with ID greater than 1000
$ awk '! /:$/' /etc/group					#shows which processes are being used by those groups

USAGE: awk 'PATTERN {ACTION}' <INPUT_FILE>
AWK is useful for textual data manipulation!

More examples:
$ awk '{print NF}' <input_file> 				#counts and prints number of fields for each line
$ awk '{print NF, $NF}' <input_file>				#counts and prints number of fields, and last fiel contents
								#Fields = Columns
$ awk '{print NR}' <input_file>					#counts and prints number of line for each line								

awk can also do math operations:
$ awk '{print $1, $2 ^ $3}' <input_file>			#prints argument 1 then prints argument 2 to the power of argument 3

awk also works with strings:
$ awk '{print "the total pay for ", $1, " is ", $2 * $3}' <input_file>      #String as argument


$ awk also works with printf:
$ awk '{printf("%f made %.2f dollars", $1, $2*$3)}' <input_file>			#uses printf formatting style
$ awk '{printf("%-8s made $%6.2f\n", $1, $2*$3)}' <input_file>				#idents columns - VERY USEFUL
$ awk '{printf("%-8s made $%6.2f\n", $1, $2*$3)}' <input_file> | sort			#Alphabetical Order
$ awk '{printf("%-8s made $%6.2f\n", $1, $2*$3)}' <input_file> | sort -r		#Reverse Alphabetical Order
$ awk '{printf("%-8s made $%6.2f\n", $1, $2*$3)}' <input_file> | sort r | grep "K"	#Returning all values that have "K"

$ awk '$2 * $3 > 50 {printf("%s made more than 50 dollars\n", $1)} <input_file>		#Prints the name of all people who make more than 50 dollars
$ awk '$1 == "Beth" | $1 =="Mark" {printf("%s made more than 50 dollars\n", $1)}	<input_file> 


$ awk 'BEGIN { print "Begin" } {print $0} END {print "End Of Program"}' <input_file>		#Because it's a programming language, it allows modularity & structurability
												#each { } allows for a different {PATTERN {ACTION}} structure! - SP
											
Awk can also create variables:										
$ awk '{emp = emp + 1} END {printf("found %d many employees\n", emp)}' <input_file>		#creates a global variable										
												#increments the global variable for each line existing in the file
												#prints total ammount of employees by the END OF THE PROGRAM

Awk can also run scripts:
$ awk -f path15k.awk ls_usr_bin.txt								#Awk Script

-----script example: path15k.awk-------------------------
func my_round(n){
	n = n + 0.5
	n = int(n)
	return n
}
/^w/ && $2>9000 {print $1,$2/1024,my_round($2/1024)"K"}
-----end script:-----------------------------
-----script example 2: path2.awk------------
func printlist(n){
	for(i=1;i<=n;i++){ 
		printf("%d ", i);
	}
	printf("\n");
}

{printlist($1)}
-------end script-----------------------------
^ this last script will get 'n' as argument and print 1 to n 
from each line feed in the document passed as parameter: awk -f path2.awk document.txt
if the lines do not contain a number, the script will crash!
that's because the script expects a number to be passed as argument 'n' in {printlist($1)}

$ awk '/^w/ && $2>9000 {print $1,$2/1024, int($2/1024)"K"}' ls_usr_bin.txt			#Another Script

$ Other Examples:
awk '/path/ && $2 >15000 {print $1,$2/1024"K"}' ls_usr_bin.txt

MORE AWK==============================:
Run BASH/CONSOLE/TERMINAL command IN awk:
	1 - awk '{ system("openssl s_client -connect host:port -cipher " $1) }' ciphers.txt
	
Awk can take the following options:

-F fs     To specify a file separator.
-f file     To specify a file that contains awk script.
	ex.: awk -f myfile /etc/passwd		#Copies content of /etc/passwd into myfile
		or
	     awk -f 'BEGIN {print "THE FILE CONTENTS:"} {print $0} END {print "FILE FOOTER" }' /etc/passwd
	obs.: only prints header once!

-v var=value     To declare a variable.

With awk, you can process text files. Awk assigns some variables for each data field found:

$0 for the whole line.
$1 for the first field.
$2 for the second field.
$n for the nth field.

NF counts how many fields exists
$NF prints contents of last field

FIELDWIDTHS     Specifies the field width.
	Sometimes, the fields are distributed without a fixed separator. In these cases, FIELDWIDTHS variable solves the problem.
	ex.: awk 'BEGIN{FIELDWIDTHS="3 4 3"} {print $1,$2,$3}' <input_file>

RS     Specifies the record separator. #Check what's RS Ascii Character

FS     Specifies the field separator. #Check Later what's FS Ascii Character

OFS  Specifies the Output separator.

ORS  Specifies the Output separator.

MORE VARIABLES:
There are some other variables that help you to get more information:

ARGC     Retrieves the number of passed parameters.
ARGV     Retrieves the command line parameters.
	ex.: awk 'BEGIN{print ARGC,ARGV[1]}' myfile
	result.: 2 myfile

ENVIRON     Array of the shell environment variables and corresponding values.
	ex.: awk ' BEGIN{ print ENVIRON["PATH"] }'
	result.: prints the environment variable:
			/home/example/bin:/home/example/.local/bin:/usr/local/sbin:/usr/local/bin:usr/sbin:/usr/bin:/bin:/usr/games:
			/usr/local/games

FILENAME    The file name that is processed by awk.

NF     Fields count of the line being processed.

NR    Retrieves total count of processed records.

FNR     The record which is processed.

IGNORECASE     To ignore the character case.

Other BASH VARIABLES can be used like this:
	echo | awk -v home=$HOME '{print "My home is " home}'

The PRINTF COMMAND in awk allows you to print formatted output using format specifiers.

The format specifiers are written like this:

%[modifier]control-letter

This list shows the format specifiers you can use with printf:

c              Prints numeric output as a string.

d             Prints an integer value.

e             Prints scientific numbers.

f               Prints float values.

o             Prints an octal value.

s            Prints a text string.

ex.: awk 'BEGIN{ x = 100*100; printf "The result is: %e\n", x}'
================================================================================
MORE EXAMPLES:
echo "Hello Tom" | awk '{$2="Adam"; print $0}'        #Changes Tom to Adam and prints it

awk 'BEGIN{FS=","}{print $1,"FNR="FNR}' myfile myfile  #awk options can also be given/input this way
							#"FNR UNIQUELY IDENTIFIES A ROW GIVEN THE SAME FILE IS PROCESSED TWICE
							
awk 'BEGIN{FS=","}{print $1,"FNR="FNR, "NR="NR}' myfile myfile  #"NR DOESN'T UNIQUELY IDENTIFY A ROW, IT WILL COUNT THE TOTAL OF LINES INSTEAD

MORE SCRIPTS:
USER DEFINED FUNCTIONS:
---script1--------
awk 'function myfunct(){ printf "The user %s has home path at %s\n", $1, $6} BEGIN{FS=":"} { myfunct()}' /etc/passwd
---end of script1--------
---script2-------
awk '{ total=0;  for(var=1; var<5; var++){ total+=$var; } avg = total / 3; print "Average:",avg}' <input_file>
---end of script2--------
---script 3---- IF CONDITIONAL STATEMENTS----
awk '{if($1>30){x=$1*3; print x;} }' <input_file>
---end of script 3----
----
-----
lslocks						#Lists processes that have file locks - 2024
================================================================================
---------------------------
MANAGING USERS
useradd/adduser and userdel:

# useradd test -m 				#Creates a user called 'test' with no group, along with it's home directory
						^ use the following to create pass for the given user:
							1) su root
							2) passwd <username>
						^ use 'id <username>' to confirm user has been created.

# useradd test -m -ou 0				#Creates a user 'test' that has the same power as 'root'
						^ by using the same UID as root but owning a different home directory.
						^ if you do this, you'll need to remove user by force:
							1) # userdel -fr test
						^ VERY IMPORTANT: 
							remember not to leave a passwordless root account around!!!

# useradd guest -m -p123456789 -g games  	#Creates a user guest, 
						^ -m creates it's home directory, and add Primary Group called games.
						^ as of 2022, -p only accepts encrypted passwords and shouldn't be used here.
						^ IF you did create user with -p option, you may need to
						^ Follow this topic for further instructions: 
							RESETTING USER PASSWORD || PASSWORD RECOVERY

# useradd -p<password> -U <username_and_groupname_are_same> -m    #Creates a new user which belongs to a group 
								 ^ of it's same name, -m creates it's userspace 
								 ^ directory inside /home/ 

# userdel -r <username>					       	 # Deletes user along with it's home directory
								 ^ you can use 'id <username>' to confirm user doesn't
								 ^ exist anymore.

# userdel <username>					       	 # Deletes user without deleting home directory

----
usermod:

# usermod -aG <groupname> <username>		#Appends/Adds new group to the group list of a given user
# usermod -rG <groupname> <username>		#Removes group from the group list of a given user

# usermod -s /usr/bin/zsh <username>		#Changes the user's login shell
						^ this is the same as invoking 'chsh' on the terminal
						^ difference is 'usermod' always require root permission.

# usermod -ou <id> <username>			#Allows username to have a non-unique ID/UID
						^ this allows different users to get access to the same files/data
						yet each of them owning different user home directories.
						^ VERY IMPORTANT: It's possible to share a single steam library
						for multiple different users on the system given symlinks 
						have been created for it.

# usermod -u <UID> <username>			#Adds new UID to user

----
groupadd / addgroup:

# groupadd <groupname>				#Creates regular group
# groupadd -r <groupname>				#Creates system group

# groupadd -p123456 <groupname>			#Creates group and add password to it
# groupadd -p123456 <groupname>			#Creates group and add password to it

# groupadd -U <username> <groupname>		#Adds userlist to the new group

----
Shadow Group Management tool:

# gpasswd -a <username> <group_name> 		#Adds user to group located under /etc/gshadow
						^ use 'getent gpasswd' to get a list of groups
---
Summary:
	A) Use 'userdbctl' for listing all users on the system in formatted text
	^ similar to doing: $ cat /etc/passwd
	 
	B) Use 'getent group' for listing all groups on the system
	^ this is the same as doing: $ cat /etc/group

	C) Use 'id <username>' for listing all groups a specific user belongs to

	D) Use 'lslogins' to list a complete log of system logins

	E) Use 'loginctl' for listing complete information on system users

	F) Use 'gpasswd' for adding user into a group belonging to /etc/gshadow
----
Login as another user:
	$ sudo -iu <username>					#Logins as another user on the system on the terminal
								^ No user password is requested
								^ The User trying to login as someone else
								^ must be included in the /etc/sudo file

	$ su - <username>					#Logins as another user
								^ Requires knowing the user password
								^ Does NOT requires being included on /etc/sudo

Login as another group:
	$ newgrp <groupname>					#Logins to a new group on the terminal

	$ sg - <groupname>					#Logins as another group
								 ^ same as above
	
	$ sg - <groupname> -c <program>				#Executes given program as another group in the system

---
userdbctl:							#Displays all users on the system
# userdbctl

---
groups:
#groups								#Lists all group the current user belongs to

---
id:
# id								#Prints real and effective user id and group id
# id numex							#Prints user id and group id from user called 'numex'

---
getent:								#Getent display entries from Name Service Switch libraries
# getent shadow							#Displays the contents of the /etc/shadow file
								^ /etc/shadow contains user password details
								^ it can only be read by root.
# getent gshadow							#Same as shadow, but for groups
								^ /etc/gshadow contains groups password details.

# getent group							#Lists all groups in the system
								^ Displays contents of the /etc/group file
								^ contains only group details, no password data.
								^ linux refers to /etc/gshadow for 
								^ group details like passwords.

# getent passwd							#Displays contents of the /etc/passwd file
								^ it contains all existing users on the system
								^ contains only user details, no password data.
								^ file can be read by anyone on the system.
								^ linux usually refers to /etc/shadow for user password.


---
using grep to list users/groups:

# cat /etc/group | grep -P -i "\brichard\b"			#Lists all groups that user named 'richard' belongs to.
# cat /etc/group | grep -P -i "\bmlocate\b"			#Listing all users that belong to a group named 'mlocate'
-----
change user password:
$ passwd [user_name]
---------------------------
# visudo        #Edits sudo options. ex.: Defaults env_reset,timestamp_timeout=20
---------------------------
Checking for Vulnerabilities in the Network:
nmap -sT <ip_address>
nmap --script vuln <ip_address>

---------------------------
ifcfg / ip addr / ifconfig / hostname -i   #finds local IP Address
$ curl ifconfig.me	#shows Internet IP Address - Requires connection!

$ ip addr show eth0       #shows local and internet IP address
	or
$ ip addr enp3s0
	or
$ ip addr <network_device>

hint.: hostname -i finds IP of hostname on the network,
therefore it'll always result in 127.0.0.1 (loopback address)
---------------------------
# journalctl -f    #Show only the most recent journal entries, and continuously print new entries AS THEY ARE APPENDED to the journal.

---
CLEANING JOURNALCTL
	Cleaning the journalctl helps keeping traces of what the user does on the system!
	So it's kind of a security concern.

	A) Rotate current journal from /run/ into /var/log
		# journalctl --rotate
	
	B) Clean all journals
		# journalctl --vacuum-files=1

	C)(Optional) Verify that you have no journal files
		# journalctl -b-0
		^ checking current journal should retrieve only a few lines on screen.
			or
		# journalctl --list-boots
		^ only one file should be listed here
			or
		# journalctl --disk-usage
		^ disk usage should be mimimal

-----
Just listing a few commands here:
journalctl -u					#Shows messages for a specified systemd service unit
journalctl --unit=<UNIT|PATTERN>		^ Same as above. ex.: journalctl -u sddm.service

journalctl --user-unit=<Username>		^ Same as above, but for users in the system. - ?????

journalctl --list-boots				#Shows a tabular list of boot numbers, along with their IDs
						^ and Timestamps. VERY IMPORTANT

journalctl -m					#Shows entries interleaved from all available journals.
journalctl --merge				^ Same as above.

journalctl -p					#Filter output by message priorities or priority ranges.
journalctl --priority				^ Same as above.

journalctl -g					#Filter output to entries where the MESSAGE= field matches the
						specified regular expression.
journalctl --grep=				^ Same as above.

journalctl --facility=				#Filter output by syslog faciulity. --facility=help
						will show a list of known facility names. - VERY IMPORTANT
						VERY IMPORTANT: Enables reading a huge ammount of different logs
						that would otherwise make the system hang/crash because of size.
----
New ones:
journalctl -x					#Gives explanation on each journaling entry, also explains how to
						 ^ solve them in case of error.
journalctl --catalog				^ Same as above.					


journalctl -r					#Reverses output; Newest entries are shown first.
journalctl --reverse				^ Same as above.

journalctl -o json				#Outputs journal as a json data
journalctl -o json-pretty			#Same as above, makes it human readable
journalctl -o short-ful				#shows timestamps in full format by including weekday, 
						year and timezone.

journalctl -n					#Shows the most recent journal events
journalctl --lines=10				^ Same as above.

	------related---

journalctl -b <ID | +- offset>			#Shows message from a specific boot . This will add a match for 
						"_BOOT_ID=". Either a journal ID or an Offset can be used.
						a negative offset will add boot logs starting from the last journal,
						ex.: -b-1 will add boot 0(last boot) and boot 1(previous from last)
						to the journal output.
journalctl --boot=<ID | +- offset>		^Same as above.

journalctl -k					#Shows only kernel messages. This implies -b and adds the match
						 ^ "_TRANSPORT=kedrnel". - VERY IMPORTANT
journalctl --dmesg				^Same as above - VERY IMPORTANT
	-----related---

journalctl -t					#Shows message for the specified syslog identifier.
journalctl --identifier=SYSLOG_IDENTIFIER	#Same as above, this parameter can be specified multiple times

------
Advanced Commands:
journalctl -D					#Takes directory path as argument. Journalctl will operate on the
journalctl --directory 				specified journal directory DIR instead of the default runtime and 
						system journal entries

journalctl --file=<GLOB>			#Takes a file glob as an argument. If specified, journalctl will
						operate on the specified journal files matching <GLOB> instead of
						the default runtime and system journal paths. Can be specified
						multiple times.

journalctl --root=<ROOT>			#Takes directory path as an argument. Journalctl will operate on 
						journal directories and catalog file hierarchy undernjeath the
						specified directory instead of the root directory.
						eg.: --update-catalog will create those files.

journalctl --image=<IMAGE>			#Takes a path to a disk image file or block device node. Journalctl
						will operate on the file system in the indicated disk image.
						This is similar to --root but operates on file systems stored in 
						disk images or block devices, thus providing an easy way to extract
						log data from disk images. Requires file systems to be under a 
						GPT partition table, or just a readable filesystem like EXT4.

journalctl --header				#Shows header information of all journal files

journalctl --list-boots				#Shows a tabular list of boot numbers, along with their IDs

		-------related---
journalctl --sync				#Asks the journal daemon to write all yet unwritten journal data to
						the backing file system and syncrhonize all journals. This guarantees
						all journaling data are storing on disk before actually accessing it.

journalctl --rotate				#Journal file rotation has the effect that all current active journal
						files are marked as archived and renamed. New empty files are then
						create in their place as current one.
						Can be combined with: --vacuum-size=, --vacuum-time=, --vacuum-file=
		------related---
logrotate -f /etc/logrotate.conf
							

journalctl --flush				#Asks the journal daemon to flush any log data stored in 
						/run/log/journal/ into /var/log/journal/, if persistent storage is
						enabled.

journalctl --relinquish-var			#Asks the journal daemon for the reverse operation to --flush.
			
----
Old Ones(Still Work):
journalctl --vacuum-files=1			#Retain only 1 file
journalctl --vacuum-time=2d			#Retain only the past two days, Cleans/erase the rest
journalctl --vacuum-size=500M			#Retain only the past 500 MB, Cleans/erase the rest

journalctl -b-1					#Shows log from previous & current boot
journalctl -k -b-1				#Shows kernel logs from previous & current boot


journalctl -b --unit=sddm.service		#Shows log for service sddm


journalctl -b-1 >> journaling.log   	#Fetches journaling errors from at least 1 day ago to current moment
					Some errors might require restarting the system in order for them to be logged in the journaling, such as boot errors;
cat journaling.log | grep -iF "erro" >> journalingerrors.log
cat journalingerrors.log | grep -iF "audio"
---------------------------
systemctl start <service_name>
systemctl stop <service_name>

systemctl enable <service_name>  	#Starts given service on system boot
systemctl disable <service_name>	#Disable given service on system boot

Services are usually stored on /etc/systemd/system/
they usually have this scope:

HINT: remove [code] and [/code] lines!


[code filename="service_name.service" hints="you can create a symlink to /etc/systemd/system/"]
[Unit]
Description=Some Really Important Service

[Service]
Type=simple
WorkingDirectory=/root
ExecStart=/root/my_program.sh

[Install]
WantedBy=multi-user.target

[/code]

Hint2.: WantedBy=multi-user.target  # allows service to start at boot
Hint3.: ExecStart can execute more than one program as in:
	ExecStart=/root/bash /root/my_program.sh

Other Service options:
[Service]
Type=simple
User=root
Group=root
Environment=SOMEVAR=someval
WorkingDirectory=/root
ExecStart=/root/my_program.sh
TimeoutSec=30s
Restart=always
RestartSec=15s
---------------------------
EXTENDED ATTRIBUTES:
chattr		#sets extended attributes
ex.: 
	chattr +s <filename> 		#Sets file for permanent deletion
					Deleting a file with 's' attribute will now also zeroe the blocks they were stored on.
					By default, when you delete a file you're simplying removing the reference, 
					this attribute changes this behavior.

lsattr		#lists extended attributes
---------------------------
Bonus: Exit Terminal but leave all processes running
disown -a && exit            #Allows terminal to be closed, but leaves all it's running processes open.

---------------------------
kill		#Signal sender, can put processes to restart/stop/sleep/etc.. through signaling
---------------------------
ps -eLf	      	#-L Shows how many threads are being used by the given proccesses running in the operating system.
		^NWLP - Column for number of threads, PID - Column for the proccess ID.

ps -f         	#Full format listing, can be used in combination with other options, can set number of columns, etc.
		also shows which jobs are running, sleeping

ps	      	#Shows current processes in the shell
ps f 		#Same as above - tree format
ps ax		# ???


jobs		#shows current running/stopped/sleeping processes - #SHOWS JOBS TABLE
		
---------------------------
ps aux	      #Shows snapshot of current processes

ex 1.:
       To see every process running as root (real & effective ID) in user format:
          ps -U root -u root u
---------------------------
Quickly create folders:
	mkdir -p folder/{sub1,sub2}/{sub1,sub2,sub3}
	
ex.: mkdir -p folder/{0..100}      	#creates 100 folders	

$mkdir -p a/long/directory/path		#Creates all directories when they don't even exist
$mkdir -p work/{d1,d2}/{src,bin,bak}	#Quickly Creates Directory trees
---------------------------
Reads from Standard Input and Write to Standard Output or File:

	tee -a file.txt

ex.: ls -la | tee -a file.txt    #This is a really method

ex.: ls -la >> file.txt		 #Newer method
---------------------------
INTERCEPT STDOUT AND LOG TO FILE
cat file.txt | tee -a file.log | cat > /dev/null     #copies content of stdout into file.log without showing it on screen, 'cus cat > /dev/null clears the screen

ex.: cat ls | cat > /dev/null
---------------------------
Fixes a Really Long Command that You Messed UP:
fc			   #Edits last given command on system's default text editor, usually vi editor
---------------------------
history                    #Shows current and previously used commands

hint.: if you add a space(white space) before the command, it will not appear in the history
---------------------------
MOUNTING A TMPFS FILESYSTEMS ON FSTAB
MOUNTING DISK/FILE/DIRECTORY IN MEMORY
mkdir -p /mnt/ram
mount -t tmpfs /mnt/ram -o size=8192M


---CORRECT FORM:
0 - Create directory on:
	sudo mkdir -p /tmp/mydir/ram/
1 - Add on /etc/fstab:
	tmpfs /tmp/mydir/ram tmpfs defaults,noatime,nosuid,nodev,mode=0777,size=8000M 0 0 
2 - Mount With:
	sudo mount -t tmpfs /tmp/mydir/ram -o size=8000M
3 - Copy Contents:
	sudo cp /bin/myapp	/tmp/mydir/ram/

Beware, the content in /tmp/mydir/ram/ will be ERASED after reboot! AND ALSO ON MOUNTING!!!!
you can umount the drive if you want to, or erase the files that you've copied without unmounting the drive
or even make a backup!
---
#Allocates RAM Space for a mounted drive, the mounted drive will work as a super read/write disk
#Once the drive is umounted the data disappears

-t option is:
           /sbin/mount.suffix spec dir [-sfnv] [-N namespace] [-o options] [-t type.subtype]

       where the suffix is the filesystem type and the -sfnvoN options have the same meaning as the normal mount options.  The
       -t option is used for filesystems with subtypes support (for example /sbin/mount.fuse -t fuse.sshfs).

so mount -t tmpfs speicifies a type tmpfs for mounting the files located /mnt/ram as memory space.
files will disappear after it's unmounted OR MOUNTEDJ! BEWARE!!! 
---------------------------
mknod rickroll c 508 0      #Make block or special device characters
---------------------------
insmod <module_name>      #Simple program to Insert module into kernel
rmod <module_name>	  #Remove module from the kernel
----------------------------
tail -f /var/log/syslog   #keep reading files even on updates/modification
----------------------------
Data Truncation:

ex.0-1 - TRUNCATE:
	truncate -s 0 myfile.txt   #works with wildcards, all other commands DON'T

ex.2 - /DEV/NULL:
cp /dev/null myfile.txt  #creates a new file
	or
cat /dev/null > myfile.txt

ex.1-2 - I/O REDIRECTION: 
 > file.wmv    #Truncantes an existing files | re-writes as a 0 bytes data

ex.3 - I/O REDIRECTION:
:> myfile.txt

ex.4 - I/O REDIRECTION:
true > myfile.txt

ex.5 - I/O REDIRECTION:
echo > myfile.txt
----------------------------
NoClobber:
echo "Test File Output" |> file.log    #This is called noclobber, if file exists it will not overwrite it's data
----------------------------
systemctl status
----------------------------
watch -n -n1 nvidia-smi
----------------------------
chattr 
----------------------------
ps -e
----------------------------
lscpu
----------------------------
lspci 
----------------------------
lsinitcpio /boot/initramfs.img 		#Shows all kernel modules

lsmod
---------------------------
lslogins
-----------------------------
lsblk
-----------
ifcfg
----
sudo -iu <username>
----
Shows Kernel Version:
uname -r    
----
#grub-mkconfig -o /boot/grub/grub.cfg		#Generates a new grub configuration file
						^ AVOID THIS AT ALL COSTS IF YOU HAVE AN ALREADY WORKING LINUX!
						^ Use this instead: #grub-mkconfig -o /boot/grub/grub.cfg-newfile
						^ read more on 'CUSTOMIZING GRUB THEME'
----
Checking if a CPU Feature called PCID is available on the linux kernel:
cat /proc/cpuinfo | grep pcid

The feature is only available in 64-bit x86 and the kernel needs to be built with it enabled. We recommend checking your environment to ensure that it’s enabled, you can do this by running cat /proc/cpuinfo | grep pcid.
-----
Look for Mitigations, you can disable them for performance:
https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html
-----
2020.1-T:
Kernel Command Line options:
pcie_bus_perf 
acpi=off
pti=off  or nopti          #risky, disables Meltdown Security 
				^ Already turned off by mitigations=off
mitigations=off		   #VERY risky!
random.trust_cpu=on	   #ALLOWS INTEL's RNDNG

pcie_bus_perf acpi=off mitigations=off

================================================================================
src:
https://linuxreviews.org/HOWTO_make_Linux_run_blazing_fast_(again)_on_Intel_CPUs
---------
SHOWS WHICH VULNERABILITIES ARE ACTIVATED OR DISABLED:
grep -H '' /sys/devices/system/cpu/vulnerabilities/*

You can also get such a list with awk:

awk 'FNR==1{print "==>"FILENAME"\n"}1' /sys/devices/system/cpu/vulnerabilities/*

It can also be done by creating a function that prints a filename followed by its contents and running it:

header-cat () { local f; for f in "$@" ; do printf '=== %s ===\n' "$f"; cat "$f"; done; }

header-cat /sys/devices/system/cpu/vulnerabilities/*

You can get a similar output from the less and possibly least elegant:

for f in /sys/devices/system/cpu/vulnerabilities/*;do echo $f;cat $f;done

The files in /sys/devices/system/cpu/vulnerabilities/ will either contain Mitigation (with a comment about how that is done), Vulnerable (if mitigations are disabled) or Not affected.
=======================================
Investigate Later:
 -> Error during AUR search: Get "https://aur.archlinux.org/rpc.php?arg=nvidia&by=name-desc&type=search&v=5": x509: certificate has expired or is not yet valid: current time 2020
-09-20T17:29:43-03:00 is before 2020-11-16T11:55:43Z

https://bbs.archlinux.org/viewtopic.php?id=253224
https://bbs.archlinux.org/viewtopic.php?id=258550

========================================
Things to pay attention Later/TAG/KEY:
Cabo de 12v está frouxo na placa mãe,
apertar de tempo em tempo.

o ATX está OK!
=======================================
Arch Install 2020:
https://www.youtube.com/watch?v=PQgyW10xD8s
-------------------------
ARCH LINUX POST-INSTALL INTRODUCTION 2020.1-T:
After you install archlinux, you'll start into terminal mode,
by installing a login manager you can access your login screen with all Desktop Environments available as option for booting.

There are several Display Managers available for login, 2 are installed on my system: GDM and SDDM
GDM allows me to select which X11 Desktop Environment I want to boot, but doesn't allows me to select Wayland desktops.
SDDM allows me to boot into Wayland desktops.

if you want to start GDM, type into terminal: sudo systemctl start gdm
if you want to start Wayland, type: sudo systemctl start sddm

Both wayland and x11 are Display Servers.

for starting network servives type: 
sudo dhcpcd


on systemctl you can also stop and restart services and check status on other services 
and applications running the system, for example try: systemctl status

if you've just installed arch, you can see that archlinux is minimal; that means it only boots
the necessary parts, requiring the user to start the rest of the system.

if you want you can create a script that will auto-boot into your preferred Display Manager of choice,
and eventually even start applications for you in your favorite Desktop Environment.

For the sake of being simple: KISS. :)
the more scripts you make, the more hasle you'll have to deal with when things go terrible wrong.

as you boot into your system of choice, you can press ctrl+alt+f2 or ctrl+alt+f3
as you can see you can open & run many user-sessions at once,
some applications however may only allow the execution of a single copy of itself for a single unique user.

that's because linux is a multi-user system, many users can use applications at the same time,
in fact, you could even start/boot a Desktop Environment from another host computer if you wanted to.

dbus-run-session application & will allow you to execute more than 1 instance of a single program for more than 1 user for programs that wouldn't allow that to happen.
dbus-run session application & will also permit you to keep using the terminal-session as the program executes in the background. that is given you're executing a Desktop Environment application, if it's a CLI application it'll have it's own flown on the terminal.
------
As of 2020.2 Gnome runs on 3 modes:
1 - Gnome(X11)
2 - Gnome(Wayland)
3 - Gnome

The third option runs wayland on a X Compatible mode, using xwayland-server.

Full Wayland  

Either that or it's running EGLStream wayland(OLD INFO)

	NOTE ON OLD INFO:
		as of 2024.1, Nvidia has grown support for GBM and Xwayland will be dropping support for Nvidia's EGLStream.
		https://www.phoronix.com/news/XWayland-Drops-EGLStream
--------
====================ABOUT KERNEL & VIDEO CARDS================================================
The newest kernels have moved the video mode setting into the kernel. So all the programming of the hardware specific clock rates and registers on the video card happen in the kernel rather than in the X driver when the X server starts.. This makes it possible to have high resolution nice looking splash (boot) screens and flicker free transitions from boot splash to login screen. Unfortunately, on some cards this doesnt work properly and you end up with a black screen. Adding the nomodeset parameter instructs the kernel to not load video drivers and use BIOS modes instead until X is loaded.

----
libva-
----
pacman -Qqo '/usr/lib/perl5/5.28'   #Lists of affected packages
------
VIM HINTS 2021.1-T
Always remember: ESC, CTRL+[, CTRL+\ CTRL+N
3 shortcut keys exist in VIM for going into normal-mode, because terminals usually reservate one or the other.

Smart ways to use VIM:

1 - BUFFERS + WINDOWS:
use :sb to create a window using the same buffer/file,
this allows you to edit 2 different parts of a given file at the same time, without relying on bookmarks to go back and forth and/or search functions.

Working on 2 different parts of a same file can be useful when you're working on 2 different spaces at the same given time, for example:
Editing INDEX section of a given Project WHILE creating/editing another session of the same project.

This can be useful when studying as well, you can have a window open for inserting important notes while a 2nd window remains open for reading/studying contents ON THE SAME GIVEN DOCUMENT/FILE.
You can also work on 2 different files using 2 windows, very useful when coding or doing 2 different job tasks at the same time and you want to save yourself from switching tabs/moving back and forth between files.

A modification in a given window will reflect changes on all other windows associated with that buffer. :)

2 - REGISTERS:
You can use local/global registers from A to Z(globals) and a to z(locals) to save pretty much anything.
While globals can be used to save data to save/read everything in a given vim session, locals be used for saving/reading everything in a given vim session for each buffer that has been opened.
Each buffer can have up 32 local register that can and will be associated for each file/buffer you have open. 

Sessions can be later saved/loaded with those same local registers saved for each specific buffer.

3 - BOOKMARKS:
There are also local and global bookmarks on vim, it'll save a line reference insid for the given buffer/file that can be either accessed globally or locally at the document.


4 - VIM SESSIONS:
Sessions can be used to define which and how buffers, windows, tabs are open. 
This can help a lot hint #1.

A Saved session will also store which line you were before the session was closed.
This can be used to save a reference of where you were working before the session was closed(remember to save the session as needed!)
-----------
-----------
2020-T:
COMPILING C, C++ CODE ON LINUX:
COMPILING C++, FROM G++ COMPILER MAN PAGES:
g++ command line arguments:
       	-ansi
           In C mode, this is equivalent to -std=c90. In C++ mode, it is equivalent to -std=c++98.
	   
	-std=
		c++17
			The 2017 ISO C++ standard plus amendments.
		gnu++17
			GNU dialect of -std=c++17.

	example: g++ -std=gnu++17 file.cc

For C++ Code, filenames must be:
       file.cc
       file.cp
       file.cxx
       file.cpp
       file.CPP
       file.c++
       file.C

For C++ Header Files:
       file.hh
       file.H
       file.hp
       file.hxx
       file.hpp
       file.HPP
       file.h++
       file.tcc
           C++ header file to be turned into a precompiled header or Ada spec.
	

Compiling with option set:
       -x <language>
           Specify explicitly the language for the following input files (rather than letting the compiler choose a default based on the
           file name suffix).  This option applies to all following input files until the next -x option.  Possible values for language
           are:

                   c  c-header  cpp-output
                   c++  c++-header  c++-cpp-output
                   objective-c  objective-c-header  objective-c-cpp-output
                   objective-c++ objective-c++-header objective-c++-cpp-output
                   assembler  assembler-with-cpp
                   ada
                   d
                   f77  f77-cpp-input f95  f95-cpp-input
                   go
                   brig
		  

VERY IMPORTANT:	assembler-with-cpp must be passed as argument to -x if you want to compiled Assembly + C++
VERY IMPORTANT: only use file.h and file.c extension types if you're coding in raw C Language.
		Remember to use G++ instead of gcc to compile C++ Code
-----------------------------
Online Compiler:
https://www.onlinegdb.com
-----------------------------
Compiling C++ Code in Linux:
1 - Use g++ instead of gcc
	ex.: g++ main.cpp


VERY IMPORTANT:
When using a very large compiling command like the one below
	g++ -std=c++17 MemRam.cpp -lstdc++fs

It is possible to re-use the same statement by typing: 
	!g++

Same for the gcc compiler:
	!gcc

-----------------------------
Compiling C Code in Linux:
1 - Either save code as .c or .cpp extensions
2 - gcc filename.c
3 - Execute program: ./filename.out

Steps of GCC Compilation:
1 - The Preprocessor
	ex.: gcc -E main.c
		will halt the compilation process as soon as the PREPROCESSOR FILE is created as 
	a main.i file.

The preprocessor has several roles:
it gets rid of all the comments in the source file(s)
it includes the code of the header file(s), which is a file with extension .h which contains C function declarations and macro definitions
it replaces all of the macros (fragments of code which have been given a name) by their values

2 - The Compiler
	The compiler will take the PREPROCESSED FILE and generate IR code (Intermediate Representation), so this will produce a “.s” file. 
That being said, other compilers might produce ASSEMBLY CODE at this step of compilation.

In order to halt the compilation process after this step is finished: 
	ex.:  gcc -S main.c 	#this will produce a .s file

this file is the assembly code.


3 - The Assembler
	The assembler takes the IR code and transforms it into OBJECT CODE, that is code in machine language (i.e. binary). 
This will produce a file ending in “.o”.

In order to halt the compilation process after this step:
	ex.: gcc -c main.c

This code isn't human readable.


4 - The Linker
The linker creates the final executable, in binary, and can play two roles:
linking all the source files together, that is all the other object codes in the project. For example, if I want to compile main.c with another file called secondary.c and make them into one single program, this is the step where the object code of secondary.c (that is secondary.o) will be linked to the main.c object code (main.o).

linking function calls with their definitions. The linker knows where to look for the function definitions in the static libraries or dynamic libraries. Static libraries are “the result of the linker making copy of all used library functions to the executable file”, according to geeksforgeeks.org, and dynamic libraries “don’t require the code to be copied, it is done by just placing the name of the library in the binary file”. Note that gcc uses by default dynamic libraries. In our example this is when the linker will find the definition of our “puts” function, and link it.

By default, after this fourth and last step, that is when you type the whole “gcc main.c” command without any options, the compiler will create an executable program called a.out, that we can run by typing “./a.out” in the command line.

We can also choose to create an executable program with the name we want, by adding the “-o” option to the gcc command, placed after the name of the file or files we are compiling, and pressing enter:

ex.: 
	gcc main.c -o executablebinaryname.out
		or
	gcc main.c

the generated code can be executed with: ./main.out   or   ./executablebinaryname
----------------------------------------
RUNNING A C++ PROGRAM USING GDB:
0 - Compile the program: $g++ -g main.cpp
1 - Execute GDB with the program: $gdb a.out
2 - Inside gdb's shell:
	run --entropy		#is the same as sudo ./a.out --entropy
		or
	run <function_name>
	run <commandline call>

3 - If an error occurs, you can see GDB's backtrace using the bt command.
bt will show all of the program backtrace from start to end of the program.




commands:
print info.st_gid
================================================================================
POWER SUPPLY / STABILIZER:
That's because power supply can & will fail as they get old & older. It might start with just 1 game as stated by OP and then evolve into USB Ports disconnecting, Computer not starting, etc etc. If you're using an old Stabilizer, it could cause these same issues and/or start damaging the power supply if the proper wattage isn't supplied - lower wattages can also cause permanent damage to your motherboard from functioning properly.

I'm graduating on Information System this year, trust me.
The best thing you can do is buy a new Power Supply and/or Stabilizer as soon as these problem start appearing to avoid damaging other parts of the system. If the problem persists, there's a chance it has already affected the motherboard or gpu. CPUs/Memories are rarely affected because they're well protected from these kinds of issue in modern computing, but other parts of the motherboard might be exposed depending on the manufacturer, same for GPU.

There are reasons people who work with computer need to invest in good Power Supplies and Stabilizers. That's just one of the reasons.

FURTHEROVER: https://outervision.com/power-supply-calculator
------------
------------
SOMETHING ABOUT POWER STABILIZERS
It's always important to check how much wattage is being provided by your Power Stabilizer and how much your Power Supply requires 
in order to function optimally, using a Power Stabilizer that supplies wattage below the requirements of your PC & Power Supply will
likely make your Computer and Operating System run slow.

So if you ever need a power stabilizer, make sure that it supplies enough wattage for your PC to work properly.
------------
------------
SOMETHING ABOUT LOW WATTAGE AND POWER SUPPLY

Source: The Below is from an Unknown, it's probably somewhere on the internet.

"HIGHER VOLTAGE: It’s bad for the laptop and might kill it instantly. Sometimes even just a tiny bit of extra voltage can kill a laptop - I once destroyed an HP laptop by using a charger that was 0.5v above the rated voltage. I think the laptop was designed to take 18.5v and I gave it 19v. The plug size and polarity were the same, and I’ve always heard (including from my EE101 professor) that with electronics anything within 20% of the spec is good enough. For whatever reason, just an extra half a volt caused one of the components to pop and fry and release the smoke on that old circa 2006 HP notebook.

LOWER VOLTAGE: It’s still bad for the laptop, but probably won’t kill it immediately. The laptop might just not charge, or it might charge more slowly. It might also become unstable and crash when you put it under a heavy computing load (launch a game, do 3d stuff, edit a video, etc) due to maybe not having as much wattage available to it.  The laptop could also become damaged over time, as whenever electronics are running at a lower-than-spec’d voltage, they need to draw more amps in order to get the same number of watts. This means that while the laptop may work on a lower voltage, some of the components are experience a higher-than-intended amount of current and thus may fail prematurely.

Regardless of whether you’re using higher or lower rating, bigger differences in voltage will always be more dangerous. You might be able to run a 20v laptop on a 19v charger for years, but running that same 20v laptop on a 12v charger might kill it in a few days. You could also get unlucky, as I did, with a laptop that somehow dies instantly with a charger only 0.5v higher than it was meant for."

================================================================================
================================================================================
================================================================================
UNDERSTANDING RPCS3 | PS3 EMULATOR

	FIRMWARE
		There's 2 firmware options:

		1 - (NOT USING) Modified Firmware:
			HFW_4.91.1_PS3UPDAT.PUP
		2 - Default Firmware 4.90:
			PS3UPDAT2020.PUP
================================================================================
================================================================================
================================================================================
UNDERSTANDING PCSX2-QT
	USING VKBSALT | VKBASALT ISSUE
		If you're running vkabsalt on pcsx2-qt, some games might break during video playback.
	the workaround is to run pcsx2-qt without vkbasalt just to skip the crashing/freezing part, then save state, close and reload again 
	using vkbasalt.

		PERMANENT SOLUTION: 
			The vkbasalt crash happens because of X11, but here's a way to solve it.

			1 - Just for pcsx2-qt, create a launcher like this:
				[file: /bin/pcsx2-qt-vulkan]
					#!/bin/sh
					ENABLE_VKBASALT=1 /usr/bin/pcsx2-qt
				[/file]

				1.1 - then given it execute permission: 
					#chmod o+x /usr/bin/pcsx2-qt-vulkan

			2 - Then export this variable and run weston:
				export I_WANT_A_BROKEN_WAYLAND_UI=YES;
				weston;

				2.1 - Finally run weston-terminal from within weston:
					$weston-terminal
					^ make sure to have a desktop icon for this if you don't have one.

			2-1(Alternate Option) - You can run weston and then export the variable inside a weston terminal:
				export I_WANT_A_BROKEN_WAYLAND_UI=YES;

				^ finally follow step 2.1 from here

			3 - Run pcsx2-qt-vulkan using weston instead of  X11.
				3.1 - if you want, you can make a all-in-one script for this
				3.1-2 (Alternate Option) - Have an icon in weston for auto-starting pcsx2-vulkan-qt with all the given options.


			It's advisable to have these environment variables set:
				Read more on "/etc/environment"

	GAME INPUTS
		Note, Each game can have their own controller profile assigned to them.
	Just right click and go to properties.

	MEMORY CARD
		To avoid running out of Memory Card space due to the default 8MB being used, 
	create instead a Folder Type memory card which is supported by PCSX2, then use it:
		Settings->Memory Card->Create

	You can also easily convert known used memory cards to folder type at the 'convert' button.
================================================================================
================================================================================
================================================================================
================================================================================
UNDERSTANDING PROTON
SOME LINKS:
	https://github.com/Open-Wine-Components/umu-launcher	#As of 2024, Steam uses this so that steam players aren't limited to 
								Steam's game Library or forced to add game to steam's game library.
								One doesn't even needs to own steam.

Non-Related:
	Lutris:
		Folders:
			~/Games/

	Steam:
		Folders:
			~/.steam/steam/steamapps/common/		#Steam Games
			~/.steam/steam/steamapps/compatdata/		#Steam Games Save Data/Configuration/Wine-Prefixes
			
	Vrchat Cache Data:
		~/.steam/steam/steamapps/compatdata/438100/pfx/drive_c/users/steamuser/AppData/LocalLow/VRChat/VRChat/Cache-WindowsPlayer
		^ #This directory contains downloaded worlds and avatars.
---	
PROTON COMMANDLINE ARGUMENTS | VKBASALT | GAMESCOPE | IMPROVING GAME EXPERIENCE | PERFORMANCE:

	source: https://developer.valvesoftware.com/wiki/Command_line_argument#Command-Line_Parameters

%command% -vulkan		#Forces games to run as vulkan

	UPSCALING STEAM INTERFACE:
		/usr/bin/steam -forcedesktopscaling=1.5 %U

	2024 PROTON ARGUMENTS:
		ENABLE_VKBASALT=1 WINE_FULLSCREEN_FSR_STRENGTH=5 PROTON_ENABLE_NVAPI=1 %command%

	2024.2 PROTON ARGUMENTS:
		WINE_FULLSCREEN_FSR=1 WINE_FULLSCREEN_FSR_STRENGTH=5 
		PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope --force-grab-cursor -w 1280 -h 800 -W 1280 -H 1024 -S stretch -f --force-windows-fullscreen -r 60 -- %command%

		^ VERY IMPORTANT: 'gamescope' may incur a few issues when playing a few games, it'll impact performance and degrade graphics
		in some cases due using '-S stretch'. Some game might see it as a 'cheat engine' application of the sorts and 
		will allow it's game freeze/crash the game upon launch.

		^ It's a better option to reduce/lower the general Resolution, by creating a new one by reading the following topic:
			SETTING UP STRETCHED RESOLUTION FOR NVIDIA WITHOUT USING GAMESCOPE

		^ VERY IMPORTANT: gamescope needs to know the internal resolution used by a game, before either upscaling it by using
		-W and -H, set internal resolutions using -w and -h respectively. In older games, not setting this up isn't an issue,
		but on newer games there's always quality loss. It's important to note that on linux -w and -h must be your actual 
		desktop resolution, -W and -H is the resolution you want to upscale to.
		
		^ Some games might run slow on higher resolution under gamescope, to circumvent this it's possible to lower the resolution and then
		upscale it to higher; if the issue persists, just use a lower resolution, example:
			-w 1280 -h 800 -W 1280 -H 800.

		^ On lutris, you can enable gamescope by going to settings -> system option -> advanced settings, then add this on custom settings:
			-S stretch -f --force-windows-fullscreen


		^ gamescope uses 1024x720 when not specifying -W or -H, which is the default resolution for steamdeck, so always use -W and -H.
		^ Gamescope allows OPENGL Games to work atop VKBASALT by using instead DXVK
		However, in-game OpenGL Graphical options won't be applied.

		^ -f forces fullscreen
		If you don't want screen to stretch, you can use:
			-W 1280 -H 1024 -S integer

		^ It's possible to turn off wide-screen for a game by using '-S stretch' instead of '-S integer'
		^ type gamescope --help for more options
		^ use -r 60 to cap a vsynced game to 60FPS
		^ can be used for starting games and programs alike:
			ENABLE_VKBASALT=1 gamescope -W 1280 -H 1024 --force-windows-fullscreen -f -S stretch xnviewmp
			^ Moving window to a different workspace sometimes crashes the program.

		^ Gamescope allows you to run games that would otherwise not work with VKBASALT:
			ENABLE_VKBASALT=1 gamescope -w 1240 -h 1024 -W 1240 -H 1024 --force-grab-cursor -S stretch -f --force-windows-fullscreen -- /bin/snes9x-gtk	<PATH_TO_ROM> 

---
Proton:
	Most of these games run well on Proton-GE-7.14 - VERY IMPORTANT
avoid using future versions of Proton-GE-7.14, use Wine-ge instead, since it low performs on low-end computers.

	Most of games run well under Proton-GE-7.36

	Folders:
		/usr/share/steam/compatibility.d/		#This folder only exists after installing proton-custom-ge from AUR - VERY IMPORTANT
		/$HOME/.local/share/Steam/compatibility.d/???

	Small Note About proton 2022.3:
		Proton-GE relies on a kernel feature called futex, because of that
		it won't work on Standard Kernel Linux and other linux kernels like Hardened and LTS.
		Proton-GE works on linux-zen, because linux-zen has futex feature.


RUNNING GAMES ON NON-VULKAN GPU:
	Use the following argument when starting games through steam:
		PROTON_USE_WINED3D=1 %command%

	This has to be added individually for each game in the game settings.

COMPILING PROTON-GE:
	It's possible to compile not only proton-ge but also every single proton dependency and install them on your system.
This will make games that rely on proton much faster.

You can do this installation through AUR repository, through the non-binary package.

STEAM GAMES NOT WORKING FOR UNKNOWN REASON (very old):
	Compile proton-ge through AUR Repository with the following option:
		yay -Sy --overwrite '*' proton-ge-custom

this will overwrite any dependency files that might already exist on your sytem and that could be broken/corrupt/missing.
if compiling from source-code is not possible, then install the binary version and find all dependencies by typing: 
	$pacman -Qi proton-ge-custom

finally install all dependencies by using:
	#pacman -Sy --overwrite '*' <dependency_list>

HINTS AND TIPS FOR STEAM:
	Disable GPU Hardware Acceleration for Steam Client.
Remember, steam switches this option to ON from time to time between updates or when clearing catches and clearing web browser data.

SOLVING AUDIO RUNNING UNDERRUN OCCURRED ISSUE | CRACKLING SOUNDS & LOW FPS ON WINE-GE:
	Edit the following fine and change the following variable to the following attribute:

		[file: /etc/pulse/daemon.conf]
			default-fragments = 5
			default-fragment-size-msec = 2
		[/end-of-file]
	
	The crackling may still happen at low-fps conditions, but it'll be mitigated and not cause slowdowns anylonger.
	Default values are:
		; default-fragments = 4
		; default-fragment-size-msec = 25

AUDIO VOLUME NOT SET CORRECTLY 
	Both Alsa and Pulseaudio can set volume settings of their own for each device.
Make sure that audio is set correctly on both 'alsamixer' and 'pavucontrol' to your desires

STEAM OPTIONS:
	PROTON_NO_ESYNC=1	
	PROTON_USE_WINED3D=1 %command%

GAMES NOT RUNNING FAST (Very Old):
	Some games run faster with WINED3D instead of Vulkan, this is rare but if this is an issue
	use the "PROTON_USE_WINED3D=1 %command%" as argument to the game, remove double-quotes it's not necessary.

SOME GAMES HAVE ISSUES RUNNING IN FULL SCREEN / GAME BLACK SCREEN ISSUE ( Very Old )
	1 - To solve this, run the game first with:
		PROTON_USE_WINED3D=1 %command%
	2 - Try running the game

	Note.: If this doesn't solve the problem, try erasing the game prefix under:
		~/.steam/steam/steamapps/compatdata
	
	Note2.: Use the game's ID(APPID) to find the proper prefix and delete it,
	it can be found on: game settings > update.

WHAT HAPPENS TO CORRUPT SAVE DATA / CORRUPT PREFIX
	Whenever proton detects a corrupt wine prefix, it backup all the prefix by renaming the "pfx" directory
to a ".bak" extension, save-games can be recovered from the .bak directory by copying the save-data back to the new 'pfx' directory.

The wine/proton prefix for each game can be found under: 
	~/.steam/steam/steamapps/compatdata/<GAME_ID>

Just make sure to look at file-modification date before copying it to the newly created 'pfx' folder, 
because this could destroy your steam-cloud save-data permanently DEPENDING ON THE GAME,
so always make sure it's the saved-data date you want.

Once you've copied, make sure to test everything before you delete it.
Not deleting the .bak folder may cause confusion later, so it's advised to remove it.

---
FORCE CHANGING RESOLUTION FOR GAMES THAT DON'T SUPPORT IT:
	Add the following line as argument for launching the game on steam:
		-resx=1440 -resy=900
			or
		STENCIL x=1440 y=900
			or 
		   x=1440 y=900
		   	or
		-w 1440 -h 900

---
FORCING WINDOWED MODE FOR GAMES THAT DISPLAY BLACK SCREEN:
	Add the following line as argument:
		-windowed
---
How to Make Sonic CD Work:
protontricks 200940 d3dcompiler_43 d3dx9_43

Megaman X8 Demake (Non-Steam Game):
	LD_PRELOAD="libpthread.so.0 libGL.so.1" ENABLE_VKBASALT=1 gamescope -S stretch -f --force-windows-fullscreen --force-grab-cursor -w 1280 -h 800 -W 1280 -H 800 -- wine  ./Mega\ Man\ X8\ 16-bit\ 1.0.0.7.exe

Resident Evil Remaster:
	PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope  --force-grab-cursor -W 1280 -H 1024 -S stretch -f --force-windows-fullscreen -r 60 -- %command%

Resident Evil on GOG Launcher;
	 use the dgvoodoo2 override option in your lutris starter.

VRCHAT:
	WINE_FULLSCREEN_FSR=1 WINE_FULLSCREEN_FSR_STRENGTH=5 PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope --backend wayland --force-grab-cursor -w 1280 -h 1024 -W 1280 -H 1024 -S stretch -f -F nis --force-windows-fullscreen %command%

PHASMOPHOBIA:
	PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 SteamDeck=1 %command%
	^ -w and -h represents actual screen  resolution that works full-screen, doesn't matters your actual desktop res.
	^ -W and -H helps upscaling the graphics a bit.
	^ Remember to change in-game resolution to 1280 x 800
	^ SteamDeck=1 enables to hear other players in lobby.
	^NOTE: you may have to disable and enable the microphone device in-game between one game and another to make your mic work again.

	Note: Recommended topic if you're having issue when talking to players:
		MICROPHONE GARBLED AUDIO INPUT
		MICROPHONE UNMUTED BUT STILL GIVES NO AUDIO
		MICROPHONE INPUT/SOURCE AUDIO MUTE OR CRACKLING ON FEW APPLICATIONS
	
		NOTE: At current state, phasmos only accept S16 audio depth.

	NOTE: Install 'vosk-api' and 'python-vosk' for Voice Recognition to work.
		# pacman -S vosk-api python-vosk

DRAGON BALL SPARKING ZERO:
	PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope --force-grab-cursor -f -S stretch -w 1280 -h 800 -W 1280 -H 800 -- %command%

DNF DUEL:
	PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope --force-grab-cursor -f --force-windows-fullscreen -S stretch  -w 1280 -h 720 -W 1280 -H 720 -- %command%

Mighty Gunvolt Burst:
	PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope --force-grab-cursor -f --force-windows-fullscreen -S stretch  -w 1280 -h 768 -W 1280 -H 768 -- %command%

Granblue Fantasy Versus Rising | GBVSR | GBFVSR:
	PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope --force-grab-cursor -f -S stretch -w 1280 -h 800 -W 1280 -H 800 -- %command% -fileopenlog
	^ Using a lower resolution to get the game running in fullscreen instead of widescreen, then upscaling with -W and -H back to a higher resolution.
	^ Upscaling to larger makes the game a bit laggy.
	^ Using this width and height allows the game to run full-screen.

	AppData > GBVSR > Saved > Config > WindowsNoEditor > Open & Edit "Engine" file

	Add the following lines into the file and save:

	When streaming,  if you run into audio stutters:
					$ pw-metadata -n settings 0 clock.force-rate 48000 && pw-metadata -n settings 0 clock.force-quantum 500
					^ Microphone audio output may display audio cracklings and stutter with higher quantum values
					VERY IMPORTANT: This changes both audio input & audio output rate and quantum and may break games/apps that rely
					on Microphone Input!!!

	[file: Engine.ini]
		[ConsoleVariables]
		r.ScreenPercentage=200
	[/file]

	
	If you want mods, You need to copy .pak and .sig files to this location, create ~mods folder if it doesn't exist:

	~/.local/share/Steam/steamapps/common/Granblue Fantasy Versus Rising/RED/Content/Paks/~mods

	You'll need to start  the game with this command on steam:
		%command% -fileopenlog

	B) (Optional) If you need to do a Data Save Transfer, you'll have to Install and run GBVS1, then copy the Save Data located under...
		~/.steam/steam/steamapps/compatdata/1090630/pfx/drive_c/users/steamuser/AppData/Local/GBVS

		...into this directory: 
			~/.steam/steam/steamapps/compatdata/2157560/pfx/drive_c/users/steamuser/AppData/Local/GBVS
	
	Finally, launch and transfer save data.


	Mods Source:
		https://gamebanana.com/mods/314540	#100% Save Data; Author: UltIMa647 

Counter Strike Source:
	Option 1: __GL_THREADED_OPTIMIZATION=1 %comand%
	^ THIS ENABLES MULTICORE RENDERING IN-GAME

	Option 2:
	PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope --force-grab-cursor -W 1280 -H 1024 -S stretch -r 60 -- %command%
	^ Remember that, in-game OpenGL settings won't be applied using this,  including anti-alias and any other graphical improvement.
	^ gamescope uses 1024x720 when not specifying -W or -H, which is the default resolution for steamdeck, so always use -W and -H.

REMEMBER TO DISABLE VSYNC IN-GAME
	

Left 4 Dead 2:
	Works with proton experimental without using this:
	%command% -vulkan -resX=1280 -resY=1024 +precache_all_survivors 1
		or
	ENABLE_VKBASALT=1 %command% -vulkan -resX=1280 -resY=1024 +precache_all_survivors 1

Vrchat:
	Proton 7.0-6
	^ Using default standard proton(the most updated version) works with EAC for vrchat.
	^ Avoid using TKG or GE versions of proton.

Doom Eternal:
	Use this as argument for launching the game, or else the game will freeze:
		%command% +com_skipIntroVideo 1 +com_skipKeyPressOnLoadScreens 1

Doom 3 BFG Edition:
	Proton 5.13-6
	^ As of 2023(April 05) game doesn't work in newer versions of proton.
	^ Avoid using steam overlay and pressing F12 screenshot key, since this will disable achievements in-game.

Need for Speed Payback:
	proton ge 7.14, use PROTON_NO_ESYNC=1 to launch the game, due to EA Launcher.

Swat 2:
	protontricks 607120 mfc42

	use proton ge 7.14

Sleeping Dogs:

	use Proton 7.0rc2-GE-1 (1639951196) 
		or
	use proton 6.3-8

	protontricks 307690 vd=1280x720

Sonic Origins:
	use GE-Proton9-12

	PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope --force-grab-cursor -w 1280 -h 800 -W 1280 -H 800 -S stretch -f --force-windows-fullscreen -- %command%


Sonic Mania Fix(proton-tkg 6.1):
	protontricks 584400 dxvk

Samurai Shodown(REBOOT):
	use Proton 7.1-GE-2

	

Megaman X Legacy Collection 1:
	1 - Trucante all *.wmv files:
		ex.: truncate -s 0 *.wmv
	2 - use Proton 6.3-8


	note: It's possible to make it work with newer versions of proton:
		Run it as WINED3D instead of Vulkan:
			PROTON_USE_WINED3D=1 %command%

Megaman X Legacy Collection 2:
	1 - Trucante all *.wmv files:
		ex.: truncate -s 0 *.wmv
	2 - use Proton 6.3-8

	note: It's possible to make it work with newer versions of proton:
		Run it as WINED3D instead of Vulkan:
			PROTON_USE_WINED3D=1 %command%

Megaman Zero/ZX Collection:
	use Proton 7.0rc2-GE-1 (1639951196) 
		or
	use Proton 7.0rc3-GE-1 (1639951196) 

	note: Works perfectly in newer versions of proton.

The Legend of Heroes: Trails in the Sky
	Install 'amstream', 'lavfilter' and 'quartz' to make video work:
		protontricks 251150 amstream lavfilters quartz  

	NOTE: Running Config_DX9.exe or Config.exe will be required using regular 'wine' or 'wine-ge', on a small test steam proton didn't save the
	settings; Click 'Save Config' and then Click 'OK', DO NOT LAUNCH THE GAME YET as settings have yet not been applied until you click 'OK' after
	saving it.

	you can edit further configurations manually if you want(optional):
	"~/.local/share/Steam/steamapps/common/Trails in the Sky FC/SAVE/config.ini" manually first and add your resolution:
		example:
			WidthDX9=1280
			HeightDX9=768
			WindowMode=0
			KeepAspect=1
			Camera45Deg=0
			UseMipmap=1
	
	Mod:
		Download and install the linux version, then point installation to 'ed6_win_DX9.exe':
			https://github.com/chyyran/skyinstaller

Garou Mark of the Wolves:
	use Proton 5.0-10

	Notes: This workaround is necessary because the game doesn't work when starting in full-screen when using newer proton versions.
	Do not run the game at full-screen / Do not set full-screen on settings mode, set the game to full screen using a hotkey instead.

Megaman Legacy Collection 2:
	use Proton 7.0rc2-GE-1 (1639951196) 
		or
	use Proton 7.0rc3-GE-1 (1639951196) 
		or
	use Proton 6.3-3

Mighty Gunvolt Burst:
	use Proton-6.9-GE-1

Mighty Morphin Power Rangers - Battle for The Grid:
	use Proton 7.0rc2-GE-1 (1639951196) 
		or
	use proton 6.3

	protontricks 1110100 wmp11

Azure Striker - Gunvolt 2 - :
	use Proton-tkg 6.8.r0.g0f00e37c.release

The King Of Fighters XII Steam:
	use Proton 5.0-10
		or
	use Proton 7-36

The King Of Fighters 2002um Steam:
	use Proton 5.13-6
		or
	Proton-GE 7-36

Tekken 7:
	use Proton-6.16-GE-1

Soul Calibur VI:
	use Proton-6.14-GE-2

Sonic Generations:
	use Proton-7.14-GE

---About Protontricks:
Some games might require things to be installed,
use protontricks to install things like dxvk, d3dcompiler_43, d3dx9_43, mf, wmp11 and so on.

You can check what programs might be needed to install when running steam through a terminal,
and checking it's output for errors like "Missing d3dcompiler_43" or "dxvk not installed".

---Steam Proton Directories:

/usr/share/steam/compatibilitytools.d/
~/.steam/steam/compatibilitytools.d/

---HINTS:
	Disable steamwebhelper.exe if possible; steamwebbrowser will be disabled,
but it'll mitigate the FPS Drops in games like Granblue Fantasy Versus.

Also, do not enable web-browser GPU Hardware Accelerated within steam.
It causes steamwebhelper to use even more CPU Resources.

Steam on Linux doesn't require that option,
because wine automatically uses GPU Hardware Acceleration for it.
on windows, use this:

"C:\Program Files (x86)\Steam\Steam.exe" -console -no-browser
or
"C:\Program Files (x86)\Steam\Steam.exe" -no-browser +open steam://open/minigameslist
or
"C:\Program Files (x86)\Steam\Steam.exe" vrdisable -no-browser +open steam://open/minigameslist

On Linux:
steam vrdisable -no-browsr +open steam://opem/minigameslist

--Other hints: Game Slowdowns
Some games may slow down in newer version of proton, due to Xaudio2
use proton 5.13-6 instead.

---Finding Saved Games:
You need to find the app id, by right clicking the game and going to settings then
finally go to update tab.

With APP ID in hands, you'll go to "Local Files -> Browse" then navigate 2 times backwards,
you'll find and double-click a folder called "compatdata", then navigate to the APP ID folder.

The Drive_C folder will contain all your saved_game data:

hint.: Each steam game library will have their own compatdata folder, so that means if you install 
the game on different steam libraries(possibly on different hard-drives), you'll end up with different
save-games for games that don't have Cloud Save feature.

Dark Souls III (AppID: 374320):
	drive_c/users/steamuser/Application Data/

Granblue Versus Fantasy(AppID: 1090630):
	drive_c/users/steamuser/AppData/Local/

--- Fixing Broken Games / Deleting Game Prefixes:
Each game prefix is located the same way as in "Finding Saved Games".
Once you find the prefix through the APP ID, you should first locate and backup
any saved-game data, then finally remove the entire prefix, then play the game through steam interface
using the proton build of your choice!

Using old proton-builds without cleaning/removing prefixes WILL cause issues.

--- Missing Game's DLC:
Clear Steam Download Cache

--------------------------
================================================================================
TUTORIAL INSTALLING ARCHLINUX ON USB DISK / EFI INSTALLATION / UEFI INSTALLATION:
1. Boot from Archlinux Installation Media
2. Use 'fdisk /dev/sdX' as root to partition your disk:
	$ sudo fdisk /dev/sdX
	^ if '#' shows up on your console/terminal instead of '$', that means you're already logged as root and 
	^ that you shouldn't worry about using sudo.
	^ /dev/sdX should be one of your hard disks! use 'lsblk', 'df -ha' or 'blkid' as root to identify it

	2.0 - Create either GPT or MBR Partition Table.
		Very Important Note: MBR Partition Tables have restriction to how much disk space can be managed by the operating system.
		So always choose GPT.

	2.1 -(Mandatory If Using GPT) Create an EFI Primary Partition: The EFI Partition has to be the FIRST PARTITION on the disk.
		Very Important Note: The more operating systems you plan on installing, the larger the EFI Partition has to be.
		Furtherover, multiple kernels, bootloaders, firmware updates are demanding more and more EFI free space. 

		To be safe, it's important to create an EFI Partition big enough, currently as of 09/01/2025 this date, 1GB~2GB should be enough.
		if you plan on installing different bootloaders, use 4GB.

	2.2 - Create Linux Partition: The Linux Partition can be the SECOND OR Nth PARTITION on the disk.
		The linux partition will hold all the operating system programs, tools, configuration files and  - when desired - the user home directory
		which will contain all of the user files, pictures, documents, games, musics and so on. 
		
		It should usually be the size of the entire disk. If you're using an SSD Disk, don't use the entire disk and 
		reserve some free unpartitioned space to be used by the SSD Firmware for preserving the SSD life.

	2.3 - Write contents to table for saving modifications.
	2.4 - Exit

3(OPTIONAL) Use either 'gparted', 'parted' or 'fdisk' for resizing EFI Partition to 300MB(1024*300) if you need it
	^ This step for resizing is not needed, make sure to use +/- when specifying Disk Size when first using fdisk when you're still on STEP 2.

	IMPORTANT NOTE: 'gparted' is a graphical tool and will require a full graphicall environment to run.

	3.1(OPTIONAL) open fdisk and re-adjust partition size using +/-:
		example:  +600GB
		^ use '-' for decreasing partition size.

4. Format 'EFI Partition' as VFAT32(Formally fat32 for mkfs):
	# mkfs.fat -F32 /dev/[efi_partition_disk]/

5. Format 'Linux Partition' as EXT4:
	# mkfs.ext4 /dev/[linux_partition_disk]/

6. Mount Installation device in /mnt
	ex.: # mount /dev/sdx2 /mnt

7. Create EFI directory for mounting the EFI partition later:
	# mkdir /mnt/efi

8. Mount EFI partition device on /mnt/efi:
	ex.: # mount /dev/sdx1 /mnt/efi/

	^ --alternative to step 8 and 9: 
		# mount --mkdir /dev/sdx1 /mnt/efi/

9. Install Linux:
	ex.: #pacstrap -K /mnt base linux linux-firmware
	^ -K Avoids using the same pacman keyring and mirrorlist as the host's linux distribution
	^ hint.: To install other packages or package groups, append the names to the pacstrap command above 
	^ (space separated) or use pacman while chrooted into the new system.

10. Generate FStab File: 
	The fstab files holds which hard-disks and devices are mounted at system boot,
	genfstab grabs all current mounted disks and devices and creates an fstab file.
	If a device/disk isn't desirable at boot time, you can still remove it from the file manually
	just by editing the file.

	For now, just generate the fstab like this:

		ex.: # genfstab -U /mnt >> /mnt/etc/fstab
		tip.: use -U to define devices by UUID,
		Always type -U for UUID identification, not -L for labels.

11. Chroot into the new system:
	Arch-chroot allows you to login into the newly installed system from another linux distro installation,
	for doing modifications on it's configurations:

		ex.: # arch-chroot /mnt 

12. Set the time zone:
	ex.: # ln -sf /usr/share/zoneinfo/Brazil/East /etc/localtime

13. Run hwclock to generate /etc/adjtime
	ex.: # hwclock --systohc

14. Create the Initramfs (Initial Ramdisk Filesystem):
	This will build the initramfs for all current installed kernels.
	ex.: # mkinitcpio -P

	Tip.: Before creating the initial RAM disk above, in /etc/mkinitcpio.conf move the block and keyboard hooks before the autodetect hook 
	in order to boot your OS from any machine. - ADVICE: This tip can be done later after you have installed and tested the system.

15. Change your root/admin password:
	# passwd

16. Download and Install 'GRUB' package by using 'Pacman':
	'Pacman' is the default package manager for archlinux, 'grub' is the bootloader that will allow you to boot into it.
	# pacman -Syu grub
	^ You can configure other distros to boot by editing '/boot/grub/grub.cfg' at some other point. - Not Described Here in these steps

17.1 Setup grub as your system's bootloader on your current Hard Disk:
	# grub-install --target=i386-pc /dev/sdX
	^ ex.: where /dev/sdX IS THE DISK (not a partition) where GRUB is to be installed. 
	^ Step 17.1 usually fails if Partition Table is GPT and not MBR, if this is the case follow 17.2 steps instead!!! - VERY IMPORTANT
	^ use #fdisk /dev/sdX and press 'p', either 'gpt' or 'mbr' should be mentioned in "Disklabel type"

18 (MANDATORY ONLY FOR GPT PARTITION TABLES) If you're going to use a GPT Partition Table, you'll need to install EFI Boot Manager:
	MBR Partition Tables have limitation regarding maximum manageable disk size, more modern installations will always use GPT Partition Tables
	So doing this will likely be a more mandatory step for everyone:
		
		ex.: # pacman -Syu efibootmgr

19. Follow the below steps for setting up GRUB:

	0 - Make sure BIOS SETTINGS has UEFI or UEFI AND LEGACY enabled, and ALSO MAKE SURE AHCI is enabled, otherwise booting from EFI Partition
	won't be possible! - VERY IMPORTANT

	1 - Mount the EFI system partition and in the remainder of this section, substitute esp with its mount point.

	2 - Choose a bootloader identifier, here named GRUB. A directory of that name will be created in esp/EFI/ to store the EFI binary 
	and this is the name that will appear in the UEFI boot menu to identify the GRUB boot entry.

	2.1 - Verify that you have a full EFI Environment available for install:
		$ls -la /sys/firmware/efi

		the output should look like this:
		[OUTPUT]
			drwxr-xr-x  5 root root    0 Feb 23 13:27 .
			drwxr-xr-x  6 root root    0 Feb 23 13:27 ..
			-r--r--r--  1 root root 4096 Feb 24 02:25 config_table
			drwxr-xr-x  2 root root    0 Feb 23 13:27 efivars
			-r--r--r--  1 root root 4096 Feb 24 02:25 fw_platform_size
			-r--r--r--  1 root root 4096 Feb 24 02:25 fw_vendor
			drwxr-xr-x  2 root root    0 Feb 24 02:25 mok-variables
			-r--r--r--  1 root root 4096 Feb 24 02:25 runtime
			drwxr-xr-x 16 root root    0 Feb 24 02:25 runtime-map
			-r--------  1 root root 4096 Feb 23 14:07 systab
		[/OUTPUT]
		^ Furtherover, /sys/firmware/efi/efivars/ whould be populated with pseudo-files.

	2.2 - Use efibootmgr to make sure you have EFI Available as well

	2.3 - If EFI isn't available for either 2.1 step or 2.2, make sure to reboot, go to BIOS Settings and set UEFI and restart your computer!

	3 - Execute the following command to install the GRUB EFI application grubx64.efi to esp/EFI/GRUB/ and install its modules to /boot/grub/x86_64-efi/.

	4 - #grub-install --target=x86_64-efi --efi-directory=esp --bootloader-id=GRUB

	ex.: #grub-install --target=x86_64-efi --efi-directory=/boot/efi/ --bootloader-id=GRUB
		^ This is going to create a new EFI Directory inside /boot/efi/, but don't worry!
		^ VERY IMPORTANT: /boot/efi/ is the EFI Partition Mountpoint not a directory.
		^ So make sure you CREATE an EFI Partition first.

	18. #grub-mkconfig -o /boot/grub/grub.cfg
		^ VERY IMPORTANT: If you already have a grub.cfg file, better generate a new file with a different name:

		18.1 - Generate new grub file with different name:
			#grub-mkconfig -o /boot/grub/grub.cfg-new

		18.2 - Make a backup of the old file, since it's already working:
			#cp /boot/grub/grub.cfg /boot/grub/grub.cfg-backup

		18.3 - Sort out the differences between old and new file:
			#nvim -d /boot/grub/grub.cfg /boot/grub/grub.cfg-new
		
		18.4 - ADVICE: make sure you have a installation media storage nearby in case things go wrong,
		       you may have to restore the backup file after doing an arch-chroot if things don't quite work!

	19. #pacman -Syu dhcpcd

	20. exit chroot environment
		ex.: exit

	21. umount all devices(mandatory on archroot environment) BEFORE REBOOTING
		ex.: umount /mnt/efi     #Umounts efi partition
		     umount /mnt/	 #Umounts system

	22. restart / reboot
		ex.: reboot

	23. once the system is booted and running, install video graphic drivers.

	24. once the system is booted, install display manager(gdm or sddm), display server(xorg), and install a dekstop environment(gnome, xfce, i3-wm, etc).

	25. After everything is installed and working make sure to use efibootmgr to change boot order:
		ex.: Change the current boot order:
			#efibootmgr -o 0002,0008,0001,0005

	26. If this is a clean install, make sure to install thermald and iptables: 
		read 'INSTALL THERMALD' topic!

---------------
-----------ALTERNATE CONFIGURATIONS(between 17-19):--------

RECOMMENDED ARCHLINUX PACKAGES:
	pacman -Syu neovim man-db man-pages texinfo dhcpcd firewalld clamav nano tldr linux-utils systemd-ui systemdgenie coreutils base linux-zen linux-zen-headers linux-zen-docs xterm terminology linux-lts linux-lts-headers linux-lts-docs linux-hardened linux-hardened-headers linux-hardened-docs sddm awesomewm i3 nvidia-dkms nvidia-settings nvidia-utils git edk2-ovmf qemu-full sudo v4l2loopback xorg-xinit

start systemd-ui by typing 'systemadm'

xorg-xinit packages allows both xinit and startx to be used. This is important for a few old desktop environments and window managers.

----------------
INSTALLING XORG:
	If you want, you can skip the step by installing sddm package using pacman instead.
it'll install all the required dependencies for booting the sddm display manager(2023). Read 'INSTALLING AND CUSTOMIZING SDDM'
	#pacman -Syu xf86-video-vesa xorg xorg-server xorg-apps xorg-drivers xorg

2023 note: not all packages in xorg-apps xorg-drivers and xorg are needed
^ It's possible to remove unnecessary packages with either pacman -Rnu or pacman -Rnu
this will remove all related orphaned packages. xf86-video-vesa can be removed as well if not being used.

Note.: Arch supplies default configuration files in /usr/share/X11/xorg.conf.d/, and no extra configuration is necessary for most setups. - DO NOT TOUCH THIS ONE - Requires Checking this information somewhere


USING .CONF FILES
The /etc/X11/xorg.conf.d/ directory stores host-specific configuration. You are free to add configuration files there, but they must have a .conf suffix: the files are read in ASCII order, and by convention their names start with XX- (two digits and a hyphen, so that for example 10 is read before 20). These files are parsed by the X server upon startup and are treated like part of the traditional xorg.conf configuration file. Note that on conflicting configuration, the file read last will be processed. For that reason the most generic configuration files should be ordered first by name. The configuration entries in the xorg.conf file are processed at the end.

For option examples to set, see also the Fedora wiki.

Using xorg.conf
Xorg can also be configured via /etc/X11/xorg.conf or /etc/xorg.conf. You can also generate a skeleton for xorg.conf with:
# Xorg :0 -configure

^This should create a xorg.conf.new file in /root/ that you can copy over to /etc/X11/xorg.conf.
^ VERY IMPORTANT - 2023 note: Avoid using this, the newly alocated file in either /root/ or ~/ will prevent Display Managers and Window Managers from starting if not removed from /root/ or ~/, specially if you're using proprietary drivers, the -configure option will usually create a misconfigured xorg.conf file.

GENERATING NVIDIA XORG.CONF FILE(2023):
	An nvidia xorg.conf file can be generated by #nvidia-xconfig, it will use the current xorg.conf as source for outputting a new nvidia xorg.conf file, so make sure you haven't created a misconfigured file with Xorg :0 -configure, if you did so delete both /usr/share/X11/xorg.conf (if it exists) and also /etc/X11/xorg.conf files. nvidia-xconfig will then be able to generate a new xorg.conf from scratch.

make sure to have all drivers installed, nvidia-utils, nvidia-dkms, and finally that nvidia-xconfig is installed!

---------------
------------------------------------
MY CUSTOM SUDOERS CONFIG:
add this line to /etc/sudoers:
Defaults        env_reset,timestamp_timeout=200000000000

this will prevent pacman and yay from timing out when installing packages when asking for sudo passwords
---------------
------------------------------
MY CUSTOM ZFS MODULE:
/etc/modules-load.d/zfs.conf:
	[CONTENT]
		zfs
	[/CONTENT]

description: CONTENT TAG not included, loads zfs dkms module into the kernel at boot,
requires zfs-dkms from AUR.
================================================================================
USER AND GROUPS CREATION AND OWNERSHIP:
in here we'll see each of the commands below:
chmod,chown,useradd,usergroup, id, userdel, groupdel, sudo.

===========ID:
id [username] 							#Shows user id, groups the user belongs to, and groups ID.
id <user_name>							#Lists all groups and ids that belong to that user

===========SU AND SUDO:
su <username>							#SU allows to login as another user
sudo <command>							#Sudo allows to run a given command as root for the requesting user, all uses are logged.
sudo su <username>						#Runs su command as a sudo,
								 this can help when a given a user can't login as another user into the system, this will then allow them
								 to login as another user as a sudo/root user.
sudo -iu							#allows a sudoer to login as another user in the system

Protip.: if no username is given it'll login as root.
	sudo can also keep track of which user was running the system as a sudo user and what they were doing in the system.

Sudoers file:
	/etc/sudoers						#Can be used to add/remove users from SUDO List
=============GROUP CREATION:
Creating Groups:
groupadd <groupname> -r						#Creates a system group with a home directory
groupadd <groupname>						#Creates a group | DEFAULT OPTION FOR CREATING GROUPS


Protip.: If you want you can create a home directory for a existing group:
	sudo mkdir /home/<groupname1>/
	sudo chown root:<groupname1> /home/<groupname1>/
^this way only root and group can access it's contents

Group Account Information: 
	/etc/group
Secure Group Account Information: 
	/etc/gshadow
Shadow password suite configuration: 
	/etc/login.defs

============USER CREATION:
Creating Non-System Users:
useradd <username> -m -p <password> -g <group_name> 		#creates a user guest, -m creates it's home directory, and add Primary Group called games.
useradd -p <password> -U <username_and_groupname_are_same> -m   #creates a new user which belongs to a group of it's same name, -m creates it's userspace directory inside /home/ 

Creating System Users:
useradd <username> -r -m -p <password> -g <group_name>		#-r allows for the creation of System User, System users are allowed to execute programs.
								^ automatically sets a System UID for the given username.

Creating Secret Users:
useradd <username> -p -g <groupname>				#Creates a Secret User, #Users without a home-directory can not execute programs!
useradd <username> -p <password> -U <user_group_name>		#Same as above, but creates a group whose name is the username.

Creating User with Shell Name:
useradd <username> -s <shellname> 				#The name of the users login shell

User Account Information:
	/etc/password
Secure user account information:
	/etc/shadow
Group account information:
	/etc/group
Secure group account information:
	/etc/gshadow
Default values for account creation:
	/etc/default/useradd
Directory containing default files:
	/etc/skel
Per user subordinate group IDs:
	/etc/subgid
Per user subordinate user IDs:
	/etc/subuid
Shadow Password suite configuration:
	/etc/login.defs
===============USERMOD:
usermod -u <UID> <username>		#Changes user ID associated with a username
usermod -d <NEW_HOME_PATH> <username> 	#Change user home DIRECTORY
usermod -l <NEW_LOGIN> <username>	#Changes username to new username, nothing else is changed, user home directory and mail spool should be renamed manually


RESETTING USER PASSWORD || PASSWORD RECOVERY
usermod -p <NEW_PASSWORD> <username>    #Changes password for a given user
						alternative: passwd <username>
					VERY IMPORTANT(old info): "usermod -p" requires password to be cryptographed by crypt,
					the only way to modify a user password without remembering the current password is 
					to delete the current pass first using "passwd" as root by invoking 
					"passwd -d <username>", once that's done you need to login as that user
					using "sudo -iu <username>" and inserting new password with "passwd".
					but in order to use sudo, you need to add yourself to the sudo list first!

					^ 2024 note/method: there's no need for sudo, you can just login as 'root' 
					and do the same job, deleting password isn't required neither anylonger:
							1) su root
							2) passwd <username>

					^ if this new method doesn't work, try the old one.

usermod -md <NEW_HOME_PATH> <username>	#Move contents of user home space and also changes user homepath to NEW_HOME_PATH
usermod -s <NEW_SHELL_NAME> <username>	#Changes the name of the user's new login shell
usermod -a -G <GROUPNAME> <username>	#Appends/Adds group as subgroup to the user groups list
					  ^ GROUP must exist beforehand
usermod -a <GROUPNAME> <username>	#Adds/Sets main group, only one can exist for each user
					  ^ GROUP must exist beforehand


VERY IMPORTANT:
SOMETIMES IT'S PREFERRABLE TO CHECK THINGS ON /etc/passwd AND /etc/group WHEN ADDING OR MODIFYING EXISTING GROUPS or USER GROUPS.
===============USER REMOVAL/DELETION:
userdel <username> 			#Removes username but keeps all it's home directory content
userdel <username> -r			#Removes user and all it's data in home directory, including it's home directory as well.
						^ Other files owned by the users will need to be deleted manually.
===============GROUP REMOVAL/DELETION:
groupdel <group>			#Removes group

===============GROUP IDENTIFICATION:
Id can be used to identify which groups a user belongs to:

id <user_name>						#Lists all groups and ids that belong to that user
===============CHANGE OWNERSHIP:
chown <username> <directory_name> 			#Changes directory_name ownership to username
chown :<groupname> <direcory_name>			#Only changes the group ownership of the given directory

chown <username>:<groupname> <directory>		#Changes directory ownership to both username and groupname
chown -R <user>:<group> <directory_name>		#Changes directory and all files directory ownership to user and group
chown -hR <user>:<group> <directory>			#Changes directory ownership, files ownership to user and group without affecting files being pointed by symlinks in other directories.

===============CHANGE GROUP OWNERSHIP:
chgrp <groupname> <diretocry/file>

chgrp -R <groupname> <diretocry/file>
chgrp -hR <groupname> <diretocry/file>

protip.: this command can be replaced with chown :<groupname>
===============GETENT
# getent group			#Lists all groups
# getent group richard 		#Lists all users that participate from richard group
# getent passwd			#Lists all users in the system, including system users
# getent passwd richard 	#Lists all groups that richard participates

getent can be used to manually track issues in the passwd and group files!

VERY IMPORTANT:
SOMETIMES IT'S PREFERRABLE TO CHECK THINGS ON /etc/passwd AND /etc/group WHEN ADDING OR MODIFYING EXISTING GROUPS or USER GROUPS.
===============MODERATION CHANGE:
chmod is used to give rights/permissions as write,read,execute to files and directories:

chmod [OPTION] <MODE> <DIRECTORY/FILE>			   #Changes the given Directory/File to permissions set in MODE following OPTION as rules

chmod -R u=rwx,g=rwx,o=rwx,a=rwx <directory/file>          #Changes permissions to Directory and all files contained in it to read,write,execute modes for user/groups/others
								^u = current user/owner; g=all users contained in the file's group; o=users NOT CONTAINED in the file's group; a=all users.
								the difference between 'o' and 'a' is that 'o' allows to set specific moderation rights to people OUTSIDE the file's group.
								'a' allows to set the same mode for all three of them: user(u), group(g), others(o). 
									for ex.:
										chmod -R a=rwx ~grouphome ========= chmod -R u=+rwx,g=+rwx,o=+rwx


chmod o=---		      #Sets others to no permissions!

chmod u=rw,g=r,o=r test.txt   #Sets user, group and other users permission for text.txt
			      #Symbolic mode for defining permissions

chmod 644 test.txt            #Same as above - Octal Notation Method
				      #R(EAD) has the value of 4
				      #W(RITE) has the value of 2
				      #(E)X(ECUTE) has the value of 1
				      #NO PERMISSION has the value of 0

---UGOA = user/group/others/all
chmod u+rw test.txt	      #Appends new permissions to pre-existing ones
chmod g+r test.txt	      #Does the same as "chmod 644 test.txt" Above - Note.: Appends read access to current user group
chmod o-r test.txt	      #ALSO APPENDS, in this case it removes the Read moderation from a given file to any user outside groups and owner(u)

chmod a+w test.txt	      #Appends write permission to ALL: users groups and others

Alternatives:
	1 - chmod g= ~/Documents  ====  chmod g-rwx ~/Documents
	  ^ Both commands will set Null Moderation to the current group of a given file/directory. They're both the same
	
	2 - chmod go= ~/Documents ==== chmod go+rwx ~/Documents
	  ^ Same description as above

	3 - chmod -R a+rX ./data/
	  ^ the X prevents files from receiving x-execution-bit, so only directory structure gets the 'x', because of the -R recursive option
	  without this you'd have to go through 2 commands: chmod -R a+r ./data;  AND  chmod a+x ./data
	
	4 - chmod g=u ./data
	 ^ Copies permission to group from user on ./data folder
	 so if current permission is: rwx------
	 it'll turn to : rwxrwx---
	 after the given command is executed


VERY IMPORTANT NOTE: users who participate in groups can't remove someone else's file without +wx permission set to the group access!
		     if this happens at some point, beware of users who might delete, rename or move it away, in this case always add the sticky bit!

=====SETUID, SETGUID, STICKY - NEEDS REVISION - TODO LATER

STICKY = The sticky has one main purpose: blocking people from renaming, moving or deleting a file when they have +w and +x Permission.
When the Sticky is set on a executable binary, it allows people to execute the program as the root itself.

CHALLENGE 1 = Let's say I want specific people inside a group to be able to write, but never read any of the documents they share IN THAT FOLDER,
the only people who are allowed to read are the ones OUTSIDE of that group, yet these will not be given rights to write/modify these 
documents without making a full copy and modifying it themselves.

chmod g=w,o=r ~grouphome

CHALLENGE 2 = The owner of an estabilishment - who doesn't manage the system - needs all of his employees to share files with him, but he doesn't wants
any of them to be able to read or copy those documents for themselves because the document's informations can compromise the company's work, save for the people who work
at the Human Resource Administration, who will be able to read but not delete or write into it.

mkdir ~sharedfolder
chown <ownername>:<HRgroupname> ~sharedfolder
chmod u=rwx,g=r,o=w ~sharedfolder

===============ABOUT UID/GUID:
The UID(User ID) uniquely identifies each user in the system, it's value can never be negative; 0 to 99 are usually Kernel Reserved, 0 being root; 100 to 999 are System Reserved IDs; 1000 to 29999 are Group Reserved IDs(GUIDs). It's configuration can be found in: /etc/adduser.conf

ex.: 
FIRST_SYSTEM_UID=100
LAST_SYSTEM_UID=999

FIRST_UID=1000
LAST_UID=29999

Using UID 0 isn't the same as being root. Should be used with caution.

====================FINAL CONSIDERATIONS: 
about file/directory ownership, if a given directory is given g-x moderation for a CERTAIN GROUP, 
that means all files inside that directory won't be deleted by any of it's group members IF THEY'RE MEANT FOR THAT SAME CERTAIN GROUP, EVEN if the files comes set with g+x,
execution will also be blocked because the directory permissions has a higher priority. This is true for read,write modes as well.

HOWEVER, if the group that owns certain directory moderations is different from the file moderation, then the directory permission won't have a higher priority - NEEDS TO BE CHECKED
Maybe a priority by GROUP ID - NEEDS TO BE CHECKED

====================DIFFERENCES ON USERADD AND ADDUSER:
Manpage for adduser says:
	adduser and addgroup add users and groups to the system according to command line options and configuration information in /etc/adduser.conf. They are friendlier front ends to the low level tools like useradd, groupadd and usermod programs, by default choosing Debian policy conformant UID and GID values, creating a home directory with skeletal configuration, running a custom script, and other features. adduser and addgroup can be run in one of five modes:

Manpage for useradd says:
	useradd is a low level utility for adding users. On Debian, administrators should usually use adduser(8) instead.

THINGS TO DO LATER:
	Check what useradd -r actually does. - Answer: useradd -r creates a UID number of a System User for the given username
	What happens when a home/user space isn't created with -m. - Answer: ??

==================================================END==================================================
SUDO AND SU DIFFERENCES:

SU runs a command with substitute user and group ID in the current path. Should be run with --login option instead of default -, to avoid mixed environments(READ MAN PAGE)
SUDO allows a permitted user to execute a command as the superuser or antoher user. SUDO -i allows to run an environment similar to the one a user would receive at login.
SUDO -u <username> <command_name> allows to run the command as user other than the default target user(usually root).


Unlike SU, each user can have it's own configuration inside /etc/sudoers file,
not only that you can also set which commands they're allowed to run as sudo.

So for example, a guest user could be allowed to use pacman, but not su.

When you execute commands as SUDO, they don't run as root, they actually run as the user who have made them the first call. You can check that with History Command.

FURTHEROVER:
‘su‘ forces you to share your root password to other users whereas ‘sudo‘ makes it possible to execute system commands without root password. ‘sudo‘ lets you use your own password to execute system commands i.e., delegates system responsibility without root password.

SUDO SYNTAX: /etc/sudoers
The Syntax of configured ‘sudo‘ line is:
	User_name Machine_name=(Effective_user) command

The above Syntax can be divided into four parts:

1 - User_name: This is the name of ‘sudo‘ user.
2 - Machine_name: This is the host name, in which ‘sudo‘ command is valid. Useful when you have lots of host machines.
3 - (Effective_user): The ‘Effective user’ that are allowed to execute the commands. This column lets you allows users to execute System Commands.
4 - Command: command or a set of commands which user may run.

SUDO HINTS:
Some of the Situations, and their corresponding ‘sudo‘ line:

Q1. You have a user mark which is a Database Administrator. You are supposed to provide him all the access on Database Server (beta.database_server.com) only, and not on any host.

For the above situation the ‘sudo‘ line can be written as:
	mark beta.database_server.com=(ALL) ALL

Q2. You have a user ‘tom‘ which is supposed to execute system command as user other than root on the same Database Server, above Explained.

For the above situation the ‘sudo‘ line can be written as:
	mark beta.database_server.com=(tom) ALL

Q3. You have a sudo user ‘cat‘ which is supposed to run command ‘dog‘ only.

To implement the above situation, we can write ‘sudo’ as:
	mark beta.database_server.com=(cat) dog

Q4. What if the user needs to be granted several commands?

If the number of commands, user is supposed to run is under 10, we can place all the commands alongside, with white space in between them, as shown below:
	mark beta.database_server.com=(cat) /usr/bin/command1 /usr/sbin/command2 /usr/sbin/command3 ...

If this list of command varies to the range, where it is literally not possible to type each command manually we need to use aliases. Aliases! Yeah the Linux utility where a long-lengthy command or a list of command can be referred as a small and easy keyword.

A few alias Examples, which can be used in place of entry in ‘sudo‘ configuration file.
	User_Alias ADMINS=tom,jerry,adam
	user_Alias WEBMASTER=henry,mark
	WEBMASTERS WEBSERVERS=(www) APACHE
	Cmnd_Alias PROC=/bin/kill,/bin/killall, /usr/bin/top

It is possible to specify a System Groups, in place of users, that belongs to that group just suffixing ‘%’ as below:
	%apacheadmin WEBSERVERS=(www) APACHE

Q5. How about executing a ‘sudo‘ command without entering password?

We can execute a ‘sudo‘ command without entering password by using ‘NOPASSWD‘ flag.
	adam ALL=(ALL) NOPASSWD: PROCS

Here the user ‘adam‘ can execute all the commands aliased under “PROCS”, without entering password.

“sudo” provides you a robust and safe environment with loads of flexibility as compared to ‘su‘. Moreover “sudo” configuration is easy. Some Linux distributions have “sudo” enabled by default while most of the distros of today needs you to enable it as a Security Measure.

To add an user (bob) to sudo just run the below command as root.
	adduser bob sudo

That’s all for now. I’ll be here again with another Interesting article. Till then stay tuned and connected to Tecmint. Don’t forget to provide us with your valuable feedback in our comment section.
===============CUSTOM SUDO:
===============CUSTOM SUDOERS:
CUstom modification in: /etc/sudoers

Defaults log_host, log_year, logfile="/etc/sudologs/sudo.log"
Defaults log_input, log_output  #The default I/O log directory is /var/log/sudo-io
Defaults        env_reset,timestamp_timeout=200000000000 #this will prevent pacman and yay from timing out when installing packages when asking for sudo passwords
================================================================================
TURNING OFF / DISABLE SPECIFIC LINUX BINARY && COMMANDS:

chmod o-x /bin/[command_name]	#Removes other user access from executing command

although, this won't work as expected in all cases,
the proper way to block some specific user and groups from certain access modifications is to user setfacl as follows:

setfacl -m u:teamuser:--- <directorypath>
setfacl -m g:teamuser:--- <directorypath>

**It's not needed to use the recursive flag -R, 
**It's required to setfacl for both user and it's main group for blocking them from acessing specific resources(test later),
probably because o(thers) being set as r-x will still guarantee them to have access as "others". - NOT NEEDED
EXPLANATION: it's not needed because the user settings has higher priority than the group. 
That means if the user is negated access, he can't access it with his/her own group permissions.

BACKUP FILE PERMISSIONS BEFORE CHANGING THEM:
IMPORTANT:
It's important to backup file permissions before changing them:
	getfacl -R Concrete5 > permissions.acl

This will recursively (that's what the -R does) back up all of the files in the Concrete5 folder and store them in a file called permissions.acl.

RESTORE FILE PERMISSIONS
To restore your permissions, use this command:
	setfacl --restore=permissions.acl

This will reset all of the permissions back to where they were when you backed them up.
---------------------------------------------------------------
CHATTR/LSATTR:
By default when we create a file or directory, it does not has any extended attributes other than "e" which means extent format i.e. these files support extended attributes
ex.: -------------e-- /tmp/deepak

IMPORTANT: By default, extended attributes are not preserved by cp, rsync, and other similar programs, see #Preserving extended attributes.

CHATTR ATTRIBUTES:
a: Append text to a file
	Can't overwrite | Restrict Access and allows only append mode

i: Makes a file immutable
	Can't be deleted or changed in any way.

c: compressed
	A file with the c attribute set is automatically compressed on the disk by the kernel.
	A read from this file returns uncompressed data.
	A write to this file compresses data before storing them on the disk.

d: no dump
e: extent format

j: data journalling
	A file with the j attribute has all of its data written to the ext3 journal before being written to the file itself, 
	if the filesystem is mounted with the "data=ordered" or "data=writeback" options.  When the filesystem is mounted with 
	the "data=journal" option all file data is already journaled, so this attribute has no effect.

s: secure deletion
	When a file with the s attribute set is deleted, its blocks are zeroed and written back to the disk.

t: no tail-merging

u: undeletable
	When a file with the u attribute set is deleted, its contents are saved.
	This allows the user to ask for its undeletion.

A: no atime updates
C: no copy on write
D: synchronous directory updates
S: synchronous updates
T: top of directory hierarchy

I: Indexed 
	The I attribute is used by the htree program code to indicate that a directory is being indexed using hashed trees.

Source: https://en.wikipedia.org/wiki/Chattr

EXAMPLES:
chattr +a /tmp/deepak/secret_file
chattr +i /tmp/deepak/secret_file
chattr +ai /tmp/deepak/secret_file

USER EXTENDED ATTRIBUTES:
Extended Attributes can also be used to store arbitrary information about a file:
setfattr -n user.checksum -v "3baf9ebce4c664ca8d9e5f6314fb47fb" foo.txt

To retrieve all information set on a file:
getfattr -d foo.txt

PRESERVING EXTENDED ATTRIBUTES
Command		Required flag
cp		--preserve=mode,ownership,timestamps,xattr
mv		preserves by default1
tar		--xattrs for creation and extraction
bsdtar		-p for extraction
rsync		--xattrs

mv silently discards extended attributes when the target file system does not support them.
To preserve extended attributes with text editors you need to configure them to truncate files on saving instead of using rename.

WHY ADDING 777 PERMISION IS BAD:

1 - YOU'VE JUST LET EVERYONE READ/MODIFY EVERY FILE ON YOUR SYSTEM.
Kiss password security goodbye (anyone can read the shadow file and crack your passwords, but why bother? Just CHANGE the password! It's much easier!).
Kiss security for your binaries goodbye (someone can just write a new login program that lets them in every time).
Kiss your files goodbye: One user misdirects rm -r / and it's all over. The OS was told to let them do whatever they wanted!

2 - YOU'VE PISSED OFF EVERY PROGRAM THAT CHECKS PERMISSIONS ON FILES BEFORE STARTING. - VERY IMPORTANT SPSPSP
sudo, sendmail, and a host of others simply will not start any more. They will examine key file permissions, see they're not what they're supposed to be, and kick back an error message.
Similarly ssh will break horribly (key files must have specific permissions, otherwise they're "insecure" and by default SSH will refuse to use them.)

3 - YOU'VE WIPED OUT THE SETUID / SETGID BITS ON THE PROGRAMS THAT HAD THEM.
The mode 777 is actually 0777. Among the things in that leading digit are the setuid and setgid bits.
Most programs which are setuid/setgid have that bit set because they must run with certain privileges. They're broken now.

4 - YOU'VE BROKEN /TMP AND /VAR/TMP 
the other thing in that leading octal digit that got zero'd is the sticky bit -- that which protects files in /tmp (and /var/tmp) from being deleted by people who don't own them.
There are (unfortunately) plenty of badly-behaved scripts out there that "clean up" by doing an rm -r /tmp/*, and without the sticky bit set on /tmp you can kiss all the files in that directory goodbye.
Having scratch files disappear can really upset some badly-written programs...

5 - YOU'VE CAUSED HAVOC IN /DEV /PROC AND SIMILAR FILESYSTEMS
This is more of an issue on older Unix systems where /dev is a real filesystem, and the stuff it contains are special files created with mknod, as the permissions change will be preserved across reboots, but on any system having your device permissions changing can cause substantial problems, from the obvious security risks (everyone can read every TTY) to the less-obvious potential causes of a kernel panic.

6 - SOCKETS AND PIPES MAY BREAK OR HAVE OTHER PROBLEMS 
sockets and pipes may break entirely, or be exposed to malicious injection as a result of being made world-writeable.

7 - YOU'VE MADE EVERY FILE ON YOUR SYSTEM EXECUTABLE
A lot of people have . in their PATH environment variable (you shouldn't!) - This could cause an unpleasant surprise as now anyone can drop a file conveniently named like a command (say make or ls, and have a shot at getting you to run their malicious code.

8 - ON SOME SYSTEMS CHMOD WILL RESET ACCESS CONTROL LISTS (ACLS)
This means you may wind up having to re-create all your ACLs in addition to fixing permissions everywhere (and is an actual example of the command being destructive).
================================================================================
HISTORY COMMAND:
Descrição
Este aplicativo mostra os últimos comandos utilizados pelo usuário.

Exemplo
O comando

history 5

mostra os últimos 5 comandos digitados pelo usuários. Abaixo, é apresentada uma possível saída. Note que o sistema tem armazenado 508 comandos do usuário.

504 ls
505 top
506 vi teste
507 cd
508 history 5


É possível limpar o histórico com o comando “history -c”.
No shell bash, o arquivo que armazena os comandos mostrados pelo history é o .bash_history.
A distribuição Ubuntu define os seguintes padrões no arquivo .bashrc para o comando history:

Ignora linhas duplicadas (comandos repetidos)
HISTCONTROL=ignoreboth

Os novos comandos digitados são adicionados no final do arquivo usado pelo history.
shopt -s histappend

O termo “shopt -s” indica que a opção histappend deve ser habilitada no shell.
Define o número de comandos que devem ser lembrados. O padrão é 1000.
HISTSIZE=1000
Define o tamanho máximo de linhas do arquivo usado pelo history. O padrão é 2000 linhas.
HISTFILESIZE=2000

Note que nas definições acima foram usadas três variáveis de ambiente: HISTCONTROL, HISTSIZE e HISTFILESIZE.

===============================
=====================================
==============================TROUBLESHOOTING==============================
SOFTWARE/PROGRAMS/APPLICATIONS NOT RUNNNING FOR MULTI USERS / MULTIUSERS ON THE SYSTEM:
	steam won't run for more than 1 user by default, because it won't be able to access the $HOME folder for the first time it's open,
this problem can also affect other programs, so you'll need to run them as: [program_name] --filesystem=$HOME
									ex.: steam --filesystem=$HOME

This will allow /bin/steam to run into your HOME FOLDER for the first time.
then wait until steam gets updated/installed accordingly. - it may take a while -

Once the settings are made, you'll likely won't need to run the the program with --filesystem attribute again.

Observation: it's well noted that programs installed under the current local logged user(even as sudo) will not run under this problem,
it's very likely that this happens because setup installations are already configured to run with --filesystem $HOME.
--------------------
Programs Randomly Crashing:
	Some programs like video/image editors kdenlive can randomly crash on runtime,
that will happen if their current local working folder on the user home path don't give them access for read/write/execute.
so use chmod ug+rwx as needed, if that still doesn't work, try giving it o+rwx.

--------------------
-----------
---------------
HOW TO CREATE A SHORTCUT:
ln -s					#Creates a soft link Shortcut
ln -s <path/directory> <symlink_name>	#Creates a symlink file with the name of <symlink_nam,e> that points to <path/directory>.
					 ^ this file can then be copied or moved using either cp or mv.

ln -s / -t ~/Desktop/root_dir

--------------
Gnome Desktop Location:
Might be a bit obscure to find location for each desktop, but they'll be usually located under HOME PATH of each user:
~/.gnome/  # WRONG

---------------------
Gnome Logs:
A Few of the System's logs can be found using: gnome-log
sudo logs must be enabled in sudo configuration files.
---------------------
Gnome Extensions:
gnome-extension

	Extension - Applications Menu
		Description - Add category based menu for applications (Start Menu from Windows)

	Extension - Auto Move Windows
		Description - Move Applications to Specific Workspace

	Extension - Places Status Indicator
		Description - Add a menu for quickly navigating places in the system.

	Extension - Window List
		Description - Displays a window list at the bottom of the screen (Taskbar from Windows)

	Extension - User Themes
		Description - Enables User Themes

	Extension - Horizontal Workspace
		Description - Horizontal Workspace

Gnome Settings: Privacy, File History, File Cache, Background / Wallpaper / Notifications / Sharing / Network / Removable Media Otpions...
gnome-settings

Gnome Disks: All disks available for mounting / unmounting
gnome-disks

Gnome Network:
gnome-network

Gnome Tweaks:
gnome-tweaks

Gnome Vinagre:
gnome-vinagre			#Vinagre is a remote desktop viewer for GNOME.
==================================================----------------
Arch Linux Installed Programs / Binary Location / Directory:
/usr/bin/
/bin/

Most binaries have root:root access and u=rwx,g=rw,o=rw.
They ALWAYS execute as the USER asking to run it.


====================
====================
ABOUT COBOL:
ex of cobol line: cust-personal-details   RENAMES cust-name THRU cust-dob.

Cobol means common business-oriented language, it's a procedural, imperative, object-oriented programming language designed for business use. 
It's meant for people that work on BI(Business Intelligence) on data and reports analysis.


HISTORY OF COBOL IN COMPUTER SCIENCE:
The COBOL community has always been isolated from the computer science community. No academic computer scientists participated in the design of COBOL: all of those on the committee came from commerce or government. Computer scientists at the time were more interested in fields like numerical analysis, physics and system programming than the commercial file-processing problems which COBOL development tackled.

Jean Sammet attributed COBOL's unpopularity to an initial "snob reaction" due to its inelegance, the lack of influential computer scientists participating in the design process and a disdain for business data processing. The COBOL specification used a unique "notation", or metalanguage, to define its syntax rather than the new Backus–Naur form which the committee did not know of. This resulted in "severe" criticism.

---------------------
======================================================
ABOUT FILESYSTEM:
ABOUT BTRFS:
BTRFS is the only filesystem that can check on actual Hard Disk Hardware Failures - NEEDS CHECK

	One of the reasons why they like Btrfs for the data storage use case is because the data CRCs(cyclic redundancy checks) and the metadata CRCs give us the ability to detect problems in the hardware such as silent data corruption ...   - Chris Mason (Facebook)

Snapcshot feature in BTRFS is way faster in BTRFS system.

it also features CoW(Copy on Write):
	Copy-on-write (COW), sometimes referred to as implicit sharing or shadowing, is a resource-management technique used in computer programming to efficiently implement a "duplicate" or "copy" operation on modifiable resources. If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred until the first write. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations.


COW CAN ALSO BE used as the underlying mechanism for snapshots, such as those provided by logical volume management, file systems such as Btrfs and ZFS,[9] and database servers such as Microsoft SQL Server. Typically, the snapshots store only the modified data, and are stored close to the main array, so they are only a weak form of incremental backup and cannot substitute for a full backup.[10] Some systems also use a COW technique to avoid the fuzzy backups, otherwise incurred when any file in the set of files being backed up changes during that backup.

That's the reason why snapshots on BTRFS are so much better, faster and secure.

DOWNSIDES TO COW:

	1 - Fragmentation is inevitable
	2 - Trees can become unbalanced
	3 - Manual Rebalance
	4 - Virtual Machines
		chattr +C disables CoW on new Directories
	5 - "Device Full"
		Meta-data takes space, unbalanced-trees
	6 - df -h
		isn't exactly honest, btrfs filesystem df /mount/point	

-------STORAGE LEVELS:
Single: 1 Copy of data.
Duplicate: 2 Copies of data using the same Drive
Raid_1: 2 Copies of data(different Drives)(For Backup)
Raid_0: 1 Copy of Data Across different Drives(For Performance)
Raid_10: 2 copies of data across drives in small stripes.
Raid_5: 2 chunks of data, 1 chunk of parity
Raid_6: 2 chunks of data, 2 chunks of parity

===============ABOUT PORTLAND PROJECT - IMPORTANT
	The Portland Project is an initiative by freedesktop.org aiming at easing the portability of application software between desktop environments and kernels by designing cross-platform APIs and offering implementations thereof as libraries to independent software vendors (ISVs).

The project was taken to establish a GREATER FOOTHOLD of Linux and other Unix-like operating systems in THE DESKTOP MARKET. It aims at resolving a number of key factors that are believed to reduce the adoption rate of Linux distributions as operating system of choice for desktop computers at home or in the office.

While the TANGO DESKTOP PROJECT was started to give users a more UNIFIED GRAPHICAL EXPERIENCE(In order words, tango project was used to give different programs a look and feel that matches other icons and applications that follow the Tango Guideline, the aim of Tango Project was to create a standard style that makes applications look appropriate running on operating systems common at the time), the Portland Project is intended to ease the porting of desktop applications to Linux for independent software vendors (ISVs). 

Tango's Initial release - 2005; 16 years ago
Tango's Final release - v0.8.90 / February 25, 2009; 11 years ago

In 2006, the project released Portland 1.0 (xdg-utils; "Cross Desktop Group Utilities"), a set of common interfaces for desktop environments. A key part of the interface is a common MIME type database for icons and programs associated with file types.

--------- ABOUT XDG MIME APPLICATIONS - IMPORTANT
The XDG Shared MIME-info Database specification facilitates a shared MIME database ACROSS DESKTOP ENVIRONMENTS and allows applications to EASILY REGISTER new MIME types system-wide.

The database is built from the XML files installed by 3rd party packages/applications in /usr/share/mime/packages/ using the tools from shared-mime-info.

HOWEVER, The files in /usr/share/mime/ should not be directly edited, but it is possible to maintain a separate database on a per-user basis in the ~/.LOCAL/SHARE/MIME/ tree.

"URI scheme handling [..] are handled through applications handling the x-scheme-handler/foo MIME type, where foo is the URI scheme in question."
	ex.: x-scheme-handler/x-[applicationname]

*URI = Unified Resource Identifier

This example defines a new MIME type application/x-foobar and assigns it to any file with a name ending in .foo. Simply create the following file:

Path: ~/.local/share/mime/packages/application-x-foobar.xml
[CODE]
<?xml version="1.0" encoding="UTF-8"?>
<mime-info xmlns="http://www.freedesktop.org/standards/shared-mime-info">
    <mime-type type="application/x-foobar">
        <comment>foo file</comment>
        <icon name="application-x-foobar"/>
        <glob-deleteall/>
        <glob pattern="*.foo"/>
    </mime-type>
</mime-info>
[/CODE]
^ Update of the mime database is required: update-mime-database ~/.local/share/mime 
And of course this will not have any effect if no desktop entries are associated with the MIME type. You may need to create new desktop entries or modify mimeapps.list.

That means that every mime created must be associated with a desktop entry OR modify the mimeapps.list

---------MIMEAPPS.LIST
The XDG standard is the most common for configuring desktop environments. Default applications for each MIME type are stored in mimeapps.list files, which can be stored in several locations. They are searched in the following order, with earlier associations taking precedence over later ones:

--Mime Specification documents(man):
Users, system administrators, application vendors and distributions can change associations between applications and mimetypes by writing into a file called mimeapps.list.

The lookup order for this file is as follows:

$XDG_CONFIG_HOME/$desktop-mimeapps.list				user overrides, desktop-specific (for advanced users)
$XDG_CONFIG_HOME/mimeapps.list					user overrides (recommended location for user configuration GUIs)
$XDG_CONFIG_DIRS/$desktop-mimeapps.list				sysadmin and ISV overrides, desktop-specific
$XDG_CONFIG_DIRS/mimeapps.list					sysadmin and ISV overrides
$XDG_DATA_HOME/applications/$desktop-mimeapps.list		for completeness, deprecated, desktop-specific
$XDG_DATA_HOME/applications/mimeapps.list			for compatibility, deprecated
$XDG_DATA_DIRS/applications/$desktop-mimeapps.list		distribution-provided defaults, desktop-specific
$XDG_DATA_DIRS/applications/mimeapps.list			distribution-provided defaults

In this table, $desktop is one of the names of the current desktop, lowercase (for instance, kde, gnome, xfce, etc.)
This is determined from taking the ascii-lowercase form of a component the environment variable $XDG_CURRENT_DESKTOP, which is a colon-separated list of names that the current desktop is known as.
The $desktop variable should be each of these values in turn.

All of the above files are referred to as "mimeapps.list" in the rest of this specification, for simplicity.

Note that the desktop-specific files can only be used for specifying the default application for a given type. It is not possible to add or remove associations from these files.
The algorithms for determining the list of all applications associated with a mime type and for determining the default application are (almost) completely unrelated, and so they are presented separately.


------------MY description OLD: 
PATH						DESCRIPTION
/etc/mime.types					list of known mime types	#Exists
~/.config/mimeapps.list				user overrides                	#EXISTS 
/etc/xdg/mimeapps.list				system-wide overrides		##Doesn't exist in arch
~/.local/share/applications/mimeapps.list	(deprecated) user overrides     #DEPRECATED -  EXISTS
/usr/local/share/applications/mimeapps.list	distribution-provided defaults  ##WORKS
/usr/share/applications/mimeapps.list		distribution-provided defaults	##DOESN EXIST IN ARCH???
/usr/share/applications/mimeinfo.cache						##WORKS

/etc/xdg/autostart/				system-wide autostart xdg     	##WORKS

VERY IMPORTANT:
	Additionally, it is possible to DEFINE DESKTOP ENVIRONMENT-SPECIFIC default applications in a file named desktop-mimeapps.list 
where desktop is the name of the desktop environment (from the XDG_CURRENT_DESKTOP environment variable). For example:
/etc/xdg/xfce-mimeapps.list defines system-wide default application overrides for Xfce. These desktop-specific overrides take precedence 
over the corresponding non-desktop-specific file.  For example, /etc/xdg/xfce-mimeapps.list takes precedence over /etc/xdg/mimeapps.list 
but is still overridden by ~/.config/mimeapps.list.

Although, it's more practical to use XDG-UTILS than looking on your own for these files,
since they could be located/named differently for each different distro.

ex.: --xdg-settings is meant to be used inside a Desktop Environment only: 
	$xdg-settings get default-web-browser						#Returns default web-browser for local user

	#sudo xdg-settings get default-web-browser

	$xdg-settings set default-web-browser opera.desktop				#Sets Opera as default web-browser for local user


	$xdg-settings --list								#Returns a list of known mime-types for the user
	#sudo xdg-settings --list							#Returns a list of known mime-types for root user

	Note: xdg-settings set options idn't work for me, the others below did
		xdg-settings works on xfce!!?!?! while xdg-mime doesnt!

    --xdg-mime is only meant to be used inside a desktop environment:
    	$xdg-mime query default inode/directory					#Returns default inode/directory

	$xdg-mime default pcmanfm.desktop inode/directory			#Sets the default inode/directory for the current desktop

	$xdg-mime set opera web/browser			

Both xdg-mime and xdg-settings requires application to be installed INSIDE the desktop-menu installed - CHECK THIS, xdg-mime does!

   --xdg-desktop-menu can be used to install .desktop files that were created by software developers/users to be installed on the 
   current user desktop. For this, a .desktop file must first be created beforehand by using the following syntax.
           shinythings-webmirror.desktop:

             [Desktop Entry]
             Encoding=UTF-8
             Type=Application

             Exec=webmirror
             Icon=webmirror

             Name=WebMirror
             Name[nl]=WebSpiegel

             Categories=Network;WebDevelopment;

       Now the xdg-desktop-menu tool can be used to add the shinythings-webmirror.desktop file to the desktop application menu: 
           $xdg-desktop-menu install ./shinythings-webmirror.desktop

   --xdg-desktop-icon can also be used to install a .desktop file along other files like README, ICON, and so on.
   whereas xdg-desktop-menu only installs the .desktop file.

	xdg-desktop-icon install [--novendor] FILE		#FILE can be a .Desktop or any other filetype like icons, etc
								#--novendor ensures that the vendor attribute is not a requirement
								when installing .desktop files, since vendor is a default attribute

       	xdg-desktop-icon uninstall FILE				#UNINSTALL the given file


   --xdg-user-dir

Type in $XDG and <tab_key> <tab_key> to checkout all ENVIRONMENT VARIABLES related to XDG
Echo them as needed.
^---------SYMLINK - NOT NEEDED TO USE:
tip: Although deprecated, several applications still read/write to ~/.local/share/applications/mimeapps.list. To simplify maintenance, simply symlink it to ~/.config/mimeapps.list:
$ ln -s ~/.config/mimeapps.list ~/.local/share/applications/mimeapps.list

---------ABOUT XDG-UTILS - IMPORTANT
XDG-UTILS provides the official utilities for managing XDG MIME Applications.

xdg-desktop-menu - Install desktop menu items
xdg-desktop-icon - copies desktop entries to the user's desktop
xdg-email - Compose a new email in the user's preferred email client, potentially with subject and other info filled in
xdg-icon-resource - Install icon resources
xdg-mime - Query and install MIME types and associations
xdg-open - Open a file or URI in the user's preferred application
xdg-screensaver - Enable, disable, or suspend the screensaver
xdg-settings - Get or set the default web browser and URL handlers

---------XDG EXAMPLES:
locate -i program.desktop				#Locates where program.desktop is
xdg-mime query default inode/directory			#Fetches default application for exploring partition directories/inodes
xdg-mime default krusader.desktop inode/directory       #Sets krusader as default inode/directory application
xdg-mime query filetype photo.jpeg			#Determines a file's MIME TYPE 
xdg-mime query default image/jpeg			#Queries default application for oppening for a MIME TYPE
xdg-mime default feh.desktop image/jpeg			#Changes the default application for a MIME TYPE

Debugs Default Application for a given MIME TYPE:
env XDG_UTILS_DEBUG_LEVEL=10  xdg-mime query default text/html
Checking /home/you/.config/mimeapps.list
Checking /home/you/.local/share/applications/defaults.list and /home/you/.local/share/applications/mimeinfo.cache
Checking /usr/local/share/applications/defaults.list and /usr/local/share/applications/mimeinfo.cache
Checking /usr/share/applications/defaults.list and /usr/share/applications/mimeinfo.cache
qutebrowser.desktop

---xdg-open
xdg-open is a resource opener that implements XDG MIME Applications and is used by many programs, see xdg-open(1) for the usage.

xdg-open is desktop-environment-independent in the sense that it attempts to use each environment's native default application tool.

If no desktop environment is detected, MIME type detection falls back to using file which—ironically—does not implement the XDG standard. If you want xdg-open to use XDG MIME Applications without a desktop environment, you will need to install perl-file-mimeinfo or switch to one of the resource openers that support XDG MIME Applications.

---xdg-settings
See xdg-settings(1).

Shortcut to open all web MIME types with a single application:
xdg-settings set default-web-browser firefox.desktop

Shortcut for setting the default application for a URL scheme: 
xdg-settings set default-url-scheme-handler irc xchat.desktop

===============DICTIONARY==============
The Common Vulnerabilities and Exposures (CVE) system provides a reference-method for publicly known information-security vulnerabilities and exposures. The National Cybersecurity FFRDC, operated by The MITRE Corporation, maintains the system, with funding from the National Cyber Security Division of the United States Department of Homeland Security. The system was officially launched for the public in September 1999.

source: https://en.wikipedia.org/wiki/Common_Vulnerabilities_and_Exposures

Known CVEs: 
CVE-2016-5195 - https://en.wikipedia.org/wiki/Dirty_COW - The vulnerability has the Common Vulnerabilities and Exposures designation CVE-2016-5195
CVE-2021-3156  - Heap-Based Buffer OVerflow in SUDO(Baron Samedit)

===============================
ORLOV BLOCK ALLOCATOR
https://en.wikipedia.org/wiki/Orlov_block_allocator

===============
CAPABILITY-BASED SECURITY SYSTEMS
Capability-based security is a concept in the design of secure computing systems, one of the existing security models. A capability (known in some systems as a key) is a communicable, unforgeable token of authority. It refers to a value that references an object along with an associated set of access rights. A user program on a capability-based operating system must use a capability to access an object. Capability-based security refers to the principle of designing user programs such that they directly share capabilities with each other according to the principle of least privilege, and to the operating system infrastructure necessary to make such transactions efficient and secure. Capability-based security is to be contrasted with an approach that uses hierarchical protection domains.
==============
Honey Pot, System Detection Prevention
==============
https://en.wikipedia.org/wiki/Linux_distribution

Android	Android-IA | Android-x86 | EMUI | Fire OS | GrapheneOS | ColorOS | LineageOS (/e/CyanogenMod) | MIUI | One UI | Remix OS | Replicant | Resurrection | Remix OS | OmniROM
Arch	ArchBang | ArchLabs| Artix | BlackArch | Chakra | EndeavourOS | Hyperbola GNU | Manjaro | Parabola GNU
Debian	antiXAstraBharat Operating System SolutionsdeepinDevuanEndless OSgNewSenseHandyLinuxKaliKnoppixMX LinuxParrot OSRaspberry Pi OSSlaxSparkyLinuxSolydXKSteamOSTailsUOS
Ubuntu	Official: KubuntuLubuntuUbuntu BudgieUbuntu KylinUbuntu MATEUbuntu StudioXubuntu
Other: AsturixBodhi Linuxelementary OSEmmabuntüsKDE neonLinux LiteLinux MintLXLE LinuxPeppermint OSPinguy OSPop! OSTrisquelUbuntu UnityUrukZorin OS
Fedora	BLAG Linux and GNUKorora
Red Hat	CentOSClearOSEulerOSLinpus LinuxOracle LinuxQubes OSRocks Cluster DistributionRocky LinuxScientific Linux
Gentoo	Calculate LinuxChromium OS Chrome OSSabayon LinuxNova OS
Slackware	Austrumi LinuxDNALinuxKongoniNimbleXPlatypuxPorteusSalix OSTopologiLinuxVectorLinuxZenwalk
Mandriva	ALT LinuxMageiaOpenMandriva LxPCLinuxOSROSA Linux
Other	4MLinuxAlpineCRUXFrugalwareGeckoLinuxGoboLinuxGuix SDKaOSKwortLunarNixOSopenSUSE SUSE Linux Enterprise ServerPuppySource MageSolusVoidUruk GNU/Linux
=====================================
MY SETFACL INSTRUCTIONS:

sudo setfacl -m g:teamuser:--- /mnt/ &&
sudo setfacl -m u:teamuser:--- /boot/ &&
sudo setfacl -m u:teamuser:--- /usr/bin/su &&
sudo setfacl -m u:teamuser:--- /usr/bin/sudo &&
sudo setfacl -m u:teamuser:--- /usr/bin/sudoedit &&
sudo setfacl -m u:teamuser:--- /home/<username> &&
sudo setfacl -m u:teamuser:--- /usr/bin/rm &&
sudo setfacl -m u:teamuser:--- /usr/bin/rmdir &&
sudo setfacl -m u:teamuser:--- /usr/bin/rmid &&
sudo setfacl -m u:teamuser:--- /usr/bin/rmiregistry &&
sudo setfacl -m u:teamuser:--- /usr/bin/rmmod &&
sudo setfacl -m u:teamuser:r-x /usr/bin &&
sudo setfacl -m u:teamuser:r-x /usr/lib &&
sudo setfacl -m u:teamuser:--- /bin/chmod &&
sudo setfacl -m u:teamuser:--- /bin/chown &&
sudo setfacl -m u:teamuser:--- /bin/setfacl &&
sudo setfacl -m u:teamuser:--- /bin/getfacl &&
sudo setfacl -m u:teamuser:--- /bin/chattr &&
sudo setfacl -m u:teamuser:--- /usr/bin/testdisk &&
sudo setfacl -m u:teamuser:--- /usr/bin/mkfs &&
sudo setfacl -m u:teamuser:--- /usr/bin/parted &&
sudo setfacl -m u:teamuser:--- /usr/bin/gparted
===================================================================
ABOUT DIRECTORIES
root directory has two important symlinks that points to /usr/,
/bin, /lib, /lib64, /sbin, they all have rwx in my current folder set to ugoa

bin and sbin points to the same folder in /usr/bin/
lib and lib64 also points to the same folder in /usr/bin/

TODONEXT: How to copy symlinks and change permissions for symlinks
	  Setting keyboard; Keyboard settings is probably generated for each user.
	  Fix SETUID, SETGUID, STICKY Explanation
-----------
About User Home Directory:
This would be similar to Raid0 / Raid-0, without actual Raid
In linux you can create Symbolic Links(Symlinks) to Other Hard-Drives in the system!
This will increase system performance since HDDs are very limited regarding to Write/Read output,
This will allow 2 Hard Drives to work at the same time.

ln -s <TARGET> <LINK_NAME>

*TARGET must exist.
LINK_NAME is the name of the symlink file to be created. This file will point to TARGET.

example:
	mv ~/MyGames/SomeGame /mnt/Another_Disk/
	ln -s /mnt/Another_Disk/SomeGame/ ~/MyGames/SomeGame

this creates a soft symbolic link named SomeGame in ~/MyGames/ .


Observation: It's possible to mount --bind instead of doing symlink,
the main difference is when using the chroot environment.
if you do, it's possible to list system bind mounts using 'lsblk'.

VERY IMPORTANT: Bind mounts also have speed limitation, which will impact performance.

Other than that, Symlinks will do the same work, are more flexible,
they won' leave you with duplicated empty directories.

---------------MAKING STEAM WORK WITH TMPFS FOR GAMES - VERY OLD - :
STEAM GAMES ON RAM:
	VERY IMPORTANT: This is a very old trick that I personally used very long ago,
	today there's a software called 'Zicroram' that works as Ramdisk tool which is of recommended usage:
		source: https://github.com/srmfx/Zicroram
	
	This just an hypothetical example:

	1 - Follow the instructions for creating a temporary memory access for an application.
	2 - Go to steam game folder, rename the folder of the game to GAMENAME-BACKUP:
		mv /path\ to\ steam\ games/GAMENAME /path\ to\ steam\ games/GAMENAME-BACKUP/
	3 - Type in to create a symlink on Steam Game Folder:
		ln -s /tmp/mydir/ram/path\ to\ GAMENAMEFOLDER /path\ to\ steam\ games/GAMENAMEFOLDER/
	4 - Open the game on steam
	5 - When you're finished, don't forget to delete the soft link:
		rm /path\ to\ steam/GAMENAME
	6 - And, Rename the BACKUP FOLDER BACK to it's original name:
		mv ./GAMENAME-BACKUP ./GAMENAMEFOLDER/
	7 - umount /path.to.tmpfs/



SYMLINKS(SOFTLINKS) LN SYNTAX:

The man pages are obnoxious regarding the creation of symlinks:
	ln -s <TARGET> <LINK_NAME> 

TARGET is the actual current directory that the symlink file will point to, 
LINK_NAME is the path and name where the symlink will be created. 

example:
	ln -s /tmp/mydir/ram/kdenlive /bin/kdenlive
-----------------
DIFFERENCES BETWEEN SOFTLINKS/SYMLINKS, HARDLINKS AND REFLINKS:

1 - A soft link does not contain the data in the target file.
2 - A soft link points to another entry somewhere in the file system.
3 - A soft link has the ability to link to directories, or to files on remote computers networked through NFS.
4 - Deleting a target file for a symbolic link makes that link useless.
5 - A hard link preserves the contents of the file.
6 - A hard link cannot be created for directories, and they cannot cross filesystem boundaries or span across partitions.
7 - In a hardlink you can use any of the hardlink names created to execute a program or script in the same manner as the original name given.

8 - A Reference Link (REFLINK) can only be created on filesystems with actual support for such.
9 - A Reflink has to be created with tools like 'cp --reflink=always <file_A> <file_B>'
10 - A Reflink is pretty much like a Hard Link with one main difference:
	When a Hardlink is created, a new file will be created pointing to the same inode structure,
	if either the new file(or the old) is modified, both will point to the same modified data structure.

	On Reflinks the same happens, However, modifications will be retained only by the file that has been modified.
	In practice, the data blocks are referenced to each other instead of just the inode structure,
	modification/update to data blocks will change/remove the referenced data-blocks,
	and incoming/inserted new data blocks won't be referenced.

	This helps saving disk space and allows "same data" modifications..

	Trivia:
		On Windows, hard links are created as Reflinks by default.
		 -- (Needs checking, i've only seen this on older windows) -- 
		On linux, hard links do not default to being created as reflink.
	
	READ MORE ON:
		PROPERLY USING 'CP' FOR COPYING FILES

REFLINK Support as of 2024:
	Support for reflinks is indicated using the 'remap_file_range' operation, 
	which is currently (6.7) supported by bcachefs, Btrfs, CIFS, NFS 4.2, OCFS2, overlayfs, and XFS.
	^ source: https://unix.stackexchange.com/questions/631237/in-linux-which-filesystems-support-reflinks

IN ESSENCE:
“Underneath the file system files are represented by inodes; 
Normally, A file in the file system is basically a link to an inode;  
A hard link then just creates another file with a link to the same underlying inode;

When you delete a file it removes one link to the underlying inode.
The inode is only deleted (or deletable/over-writable) when all links to the inode have been deleted.

A symbolic link(soft link) is a link to another name in the file system.

Once a hard link has been made, the link is to the inode, 
therefore deleting renaming or moving the original file will not affect the hard-link as it links to the underlying inode. 

Any changes to the data on the inode is reflected in all files that refer to that inode.

Note: 
	1 - Hard links are only valid within the same File System. 
	2 - Symbolic links can span file systems as they are simply the reference to another file 
	    that actually points to an inode.
		^ Because of that, when chrooting into a directory that contains a Symbolic Link, the symbolic link
		will loose meaning, because chrooting changes the / root path and therefore References lose their meaning.

" - Source: https://blog.usejournal.com/what-is-the-difference-between-a-hard-link-and-a-symbolic-link-8c0493041b62

Personal Note:
	for example.: When chrooting into /mnt/sdc2, a symbolic link that refer to /mnt/sdc1 will cease to exist,
	because the reference to /mnt/sdc1 has nevedr existed in the new chrooted environment at: /mnt/sdc2.

	A Hard-Link in the other hand points to the actual Inode and therefore will not be lost when chrooting,
	however a hard-link can't point to an inode outside the filesystem it's contained, nor across partitions or
	different filesystems.
-----------
ABOUT INODES:
Actual files point to Inodes.
Once a file is deleted, a reference to the Inode is lost.

Allowing the actual Inode to be replaced/overwritten on the disk.
Erasing an actual inode content will require bytes to be overwritten on top of it.

chattr allows files to have their inode contents to be overwritten as soon as they're deleted with 0 bytes.
ex.: chattr +s <file>

-----------
ABOUT BASHRC

Each user has a bashrc file that can be used to create alias, initialize programs, etc:
~/.bashrc

-----------
Gnome uses this directory to autostart applications within it:
~/.config/autostart/[filename].desktop

all files in there have .desktop suffix
[CODE]
Type=Application
Exec=nvidia-settings -a "[gpu:0]/GPUFanControlState=1" -a "[fan:0]/GPUTargetFanSpeed=97"
X-GNOME-Autostart-enabled=true
Name=nvidia-fan-speed
[/CODE]
-----------
XDG AUTOSTART PROGRAMS:
/etc/xdg/autostart/
---------------
TEST: --- FREERDP REMOVAL---
gnome-boxes
gnome-remote-desktop
hydra
vinagre 
-----------------
--------------
SOURCE / LINKS / WEBSITES:
https://www.mankier.com
https://man.archlinux.org
----------------
---------------
Python:
https://www.tutorialspoint.com/python/python_files_io.htm
-------------
------------
WHEN ERRORS OCCURRS:
dmesg >> dmesg.log
journalctl -b-0 >> journaling.log
	or
journalctl -b-1 >> journaling.log  	#if it's from 1 previous boot
cp /var/log/Xorg.* ~/

----------------
FIX FOR GDM NOT STARTING:
after starting gdm with #systemctl start gdm:
1 - ctrl+alt+f2
2 - systemctl restart gdm && exit

another solution:
go to /etc/gdm/custom.conf and Uncomment it:
	[daemon]
	# Uncomment the line below to force the login screen to use Xorg
	#WaylandEnable=false

GDM tries using wayland by default and fallsback to Xorg when possible.
-----------------
DRACUT IS AN ALTERNATIVE FOR MKINITCPIO:
177.527] (**) OutputClass "nvidia" ModulePath extended to "/usr/lib/nvidia/xorg,/usr/lib/xorg/modules,/usr/lib/xorg/modules"
---------------
XORG PATHS / NVIDIA PATHS:
/usr/lib/xorg/
/usr/lib/nvidia/
[   177.527] (**) OutputClass "nvidia" ModulePath extended to "/usr/lib/nvidia/xorg,/usr/lib/xorg/modules,/usr/lib/xorg/modules"
--------------
KERNEL MODULES:
all kernel modules/hooks are installed from the initramfs in /boot/.
once they're installed they're located in /usr/lib/modules/

there are also other modules, please use lsinitcpio [initramfs_name].
ex.: lsinitcpio /boot/vmlinuz-linux-mainline.img
====================================
WHEN INSTALLING PACMAN -Syu nvidia-dkms:
----

(2/2) Install DKMS modules
==> dkms install --no-depmod -m nvidia -v 460.56 -k 5.11.2-arch1-1

Good news! Module version 460.56 for nvidia-drm.ko.xz
exactly matches what is already found in kernel 5.11.2-arch1-1.
DKMS will not replace this module.
---

nvidia-dkms automatically install any of it's hooks to the KERNEL IN USE!
=====================
CUSTOM ARCH FILES:
/etc/mkinitcpio.conf | /etc/mkinitcpio.conf-mybackup  		#Modified MODULES to use nvidia, this fixes wrong resolution issues on some desktop environments - REQUIRES NVIDIA HOOKS
/etc/X11/xorg.conf | /etc/X11/xorg.conf-mybackup
/etc/pacman.conf	#Modified mirrors to use testing and multilib  
/usr/lib/systemd/system/gdm.service   #Can be found using systemctl edit gdm, edited ExecStartPre=/bin/sleep 10 just for test
					^ Should use  Editing /etc/systemd/system/gdm.service.d/override.conf instead
					to avoid pacman -Syu re-writting the custom file gdm.service
					TODO: remove the customized option from it
==========================
ADDING 3rd PARTY CUSTOMIZED OR STOCK KERNELS PART 1

In Order to install customized kernels in your system and boot from them,
you'll need to either install one of the official supported kernels,
or you'll need to compile any of the non-supported 3rd party customized kernels.

On both cases, if you're using a HARDWARE ACCELERATED GRAPHIC CARDS GPU,
you'll likely meet with the issue of it not booting into your Display Manager or any other Graphical User Interface that relies
on either wayland, xorg, or xwayland. In order to fix this issue, you'll have to install your GPU Kernel Hook/Modules into the 
kernels you have or else boot failure will persist.


In order to download or compile the kernel, you'll have to download them from either ARCH Repository or AUR Repository.
Extra ARCH Package Repository Kernels can be downloaded from the testing repository, which you'll have to enable in /etc/pacman.conf 
by uncommenting it's mirror repository line.

Non-Supported Kernels can be automatically downloaded & Compiled from the AUR Repository using the yay aur package manager(as of 2020).
Compilation will be automatically handled by the PKGBUILD bash script created by the package maintainer on the AUR Repo.

A Third Option Involves downloading & building the kernel on your own
------------------------------
================================================================================
NVIDA KERNEL HOOKS / INSTALLING CUSTOMIZED & STOCK KERNELS PART 2

The path: /lib/modules/5.11.2-arch1-1/extramodules/
has nvidia, nvidia-modeset, nvidia-drm, nvidia-uvm KERNEL HOOK/MODULES.

note that even if the files have slash character: '-'
you should add them as underline '_' character instead on the mkinitcpio.conf files as follows:
	MODULES=(nvidia, nvidia_modeset, nvidia_drm, nvidia_uvm)

These are the nvidia Official Nvidia Hooks as of 2020:
nvidia, nvidia_modeset, nvidia_uvm and nvidia_drm

You'll need to install the nvidia-dkms system, do as follows: pacman -Syu nvidia-dkms

After installing the nvidia-dkms package, You're going to need to add the hooks on /etc/mkinitcpio.conf on the MODULES=() line.
After doing that then finally type mkinitcpio -P to generate the All Kernel Presets located on: /etc/mkinitcpio.d/ 
according to the /etc/mkinitcpio.conf configuration file as previously stated.

Any Kernel(s) that fails to generate any of those modules will likely show failure error during the mkinitcpio -P process,
And therefore will not load the modules on boot.

Note.: You NEED TO reinstall the kernels in order for this to work properly, because the PKGBUILD knows how to to use nvidia-dkms
to install all nvidia kernel-modules to be processed for the mkinitcpio program as described before.

	so do as follows:
		#pacman -Syu linux linux-headers linux-zen linux-zen-headers

--LINUX-ZEN
Linux-Zen is an optinal official supported kernel for the arch-linux, it's aimed for gaming performance.

----CHECKING WHETHER MODULES HAVE BEEN INSTALLED AFTER REBOOT
Type: modinfo <hook_name>

ex.: modinfo nvidia_modeset
     modinfo nvidia_drm
     modinfo nvidia_uvm

When installing 3rd party custom kernels(USING PACMAN), the DKMS will automatically run and add the HOOKS added ONLY WHEN the linux-headers of the given 3rd party custom kernel is installed! Remember it uses the settings in /etc/mkinitcpio.conf to do that

---NOTE:
When you installed linux-zen-headers did the DKMS run and generate nvidia module for the zen kernel?
---- 

VERY IMPORTANT: Always install kernel and headers and also manual when possible. ex.: pacman -Syu linux-zen linux-zen-headers
Without the linux headers it won't be possible for DKMS to link the HOOK modules with the kernel.

==========================
DOWNLOADS PKGBUILD FROM ABS OR AUR REPOSITORY:  #ABS REPO ONLY AVAILABLE ON ARCHLINUX
yay -G <package_name>

The Arch build system is a ports-like system for building and packaging software from source code. While pacman is the specialized Arch tool for binary package management (including packages built with the ABS), ABS is a collection of tools for compiling source into installable .pkg.tar.zst packages.

Ports is a system used by *BSD to automate the process of building software from source code. The system uses a port to download, unpack, patch, compile, and install the given software. A port is merely a small directory on the user's computer, named after the corresponding software to be installed, that contains a few files with the instructions for building and installing the software from source. This makes installing software as simple as typing make or make install clean within the port's directory.

ABS is a similar concept. A part of ABS is a SVN repository and an equivalent Git repository. The repository contains a directory corresponding to each package available in Arch Linux. The directories of the repository contain a PKGBUILD file (and sometimes other files), and do not contain the software source nor binary. By issuing makepkg inside a directory, the software sources are downloaded, the software is compiled, and then packaged within the build directory. Then you can use pacman to install the package.
======================================
WHAT'S PKGBUILD:
This article discusses variables definable by the maintainer in a PKGBUILD. For information on the PKGBUILD functions and creating packages in general, refer to Creating packages. Also read PKGBUILD(5).

A PKGBUILD is a shell script containing the build information required by Arch Linux packages.

Packages in Arch Linux are built using the makepkg utility. When makepkg is run, it searches for a PKGBUILD file in the current directory and follows the instructions therein to either compile or otherwise acquire the files to build a package archive (pkgname.pkg.tar.zst). The resulting package contains binary files and installation instructions, readily installable with pacman.

Mandatory variables are pkgname, pkgver, pkgrel, and arch. license is not strictly necessary to build a package, but is recommended for any PKGBUILD shared with others, as makepkg will produce a warning if not present.

It is a common practice to define the variables in the PKGBUILD in same order as given here. However, this is not mandatory, as long as correct Bash syntax is used.
========================================
PROGRAMMING NOTES(THIS SHOULD BE COPIED TO ANOTHER FILE):
https://www.cplusplus.com/reference/cstdlib/malloc/
size_t is CPU Architecture dependent, should only be used with casts as in:
	ex.: buffer = (char*) malloc (i+1);

because malloc returns a size_t type.

HOWEVER:
https://en.cppreference.com/w/cpp/types/size_t

size_t can store the maximum size of a theoretically possible object of any type (including array).

size_t is commonly used for array indexing and loop counting. Programs that use other types, such as unsigned int, for array indexing may fail on, e.g. 64-bit systems when the index exceeds UINT_MAX or if it relies on 32-bit modular arithmetic.
========================================
GROUPS, USER, OTHER - DIRECTORY AND FILE PERMISSIONS EXPLAINED; CHMOD/CHOWN

If a given file has "-rwxr-xr-x root root" access, they can still be executed by a non-authorized user,
but the program will run as the user and not the owner of the file(root), and the user will inherit only the permissions in others=r-x.

Therefore the program will not have the same privileges of the root user, if run by another user; If this program needs to execute root tasks, it'll likely fail when run normally without sudo.

However, the access list above might not be desirable, because unauthorized users could still delete the file with "rm", and/or copy them somewhere else. IN ORDER to protect this file AND still give users access privileges one could do this: 
	1 - chmod o=---,g=r-x; 
	2 - chown :games /usr/bin/binaryfile1

	now the permission will look like this "-rwxr-x--- root games"!

Now only the users INSIDE the Games' Group will be able to execute the file and run commands against it. No one else will.
BUT The program will now run with UID(user id) but will use Privileges of the Game's Group INSTEAD of the User System's Privileges, 
thus/therefore it will now be able to write on disk, because there is no g+w...
IN THIS CASE there won't be any Risks, because even IF THE User has somewhat privileges into the system, his GROUP ID wouldn't
(if properly configured) In fact you can even block the Group ID from executing specific tasks in the system just to be sure, 
like this(continuing from the steps before):

	1 - setfacl -m g:games:--- /usr/bin/* 
	2 - setfacl -m g:games:r-x /usr/bin/binaryfile1

Now, all the users who belong to games will not be able to run any other binary in the system. This is a safer way
to execute any unwanted program as your user.

If you want to make an exception to some users in the Games' Group, you should add them to a 2nd group and set the following permission:
(also continuing the same steps from before):

	1 - groupadd new_groupname				#Creates a group called groupname
	2 - usermod username -g new_groupname			#Adds user into groupname
	3 - setfacl -m g:new_groupname:r-x /usr/bin/*		#

Groups can be seen as cards, you can distribute cards to all or just some users in the system,
whomever get the cards have to strictly follow the rules set by it.

-
Primary and Secondary Group
	1 - A user’s primary group is defined in the /etc/passswd file.
	2 - A user’s secondary groups are defined in the /etc/group file.
	3 - The primary group is important because files created by this user will inherit that group affiliation.
	4 - The primary group can temporarily changed by running: newgrp groupname
	    ^where groupname is one of the user’s secondary groups. The user can return to their original group by typing exit.

CHECK LATER:
1 - What if groups conflict with one another permission settings;
2 - What's the difference in adding a group as main?
3 - Read freebsd handbook - Done
4 - What's EUID, FSUID, GUID, UID
khttps://docs.freebsd.org/en/books/handbook/

========================================
DEFAULT DIRECTORIES AND DEFAULT FILES:
~/.cache/yay/						#Yay Package Cache
/var/cache/pacman/pkg/					#Pacman Package Cache
/etc/rc.local						#Runtime Configuration File
/etc/X11/xinit/xinitrc					#Xinit's runtime configuration file
/usr/share/sddm/scripts/Xsetup				#sddm's Xsetup file / This is run as root
/usr/share/sddm/scripts/Xsession			#sddm's Xsession file / This is run as the user
/etc/zsh/zprofile					#ZSH's Initialization Shell script / Runs only one time per user
/opt/							#Some Program Executable Data go here; a designated location for installing optional 
							or add-on software packages that are not part of the core operating system. 

/usr/share/						#Default global configuration folder used by program, all global settings go here
/usr/share/icons/					#Global Icon Directory
~/.local/share/						#Program data, cache are stored here.
~/.config/						#Default local config folder used by programs

/etc/make.conf						#Fedora's default make.conf file
/etc/makepkg.conf					#Archlinux's default make.conf file for building programs
							^ One can set CFLAGS, CXXFLAGS here
/etc/environment					#Default file for Environment Variables
							^ Used by PAM, requires login to reload the file.

/etc/default/						#Default?
/etc/default/grub/					#Useful for setting up Grub themes
								^ https://github.com/gustawho/grub2-theme-breeze
/etc/grub.d/						#Grub modules/services/scripts
/boot/							#kernel and initramfs
/boot/grub/						#Grub boot files
/etc/skel/						#Skeletone files that are created whenever a new user is added into the system
/etc/profile/						#???
/etc/initcpio/						#??? Rarely used
/etc/mkinitcpio.d/					#Preset configuration files for each kernel modules installed in the system
/etc/X11/						#X11 configuration files
/etc/xinetd.d/						#??

/etc/securetty						#List of available terminals for Root login
							^ Makes it possible to limit from which terminals root can log in into the system
/etc/security/						#Group of files related to system security
/etc/security/faillock.conf				#Configures how many times a login password is allowed to fail 
							^ | login fail | login lock | sudo lock

/usr/share/kbd/keymaps/					#Available keymaps on the system that can be set as keyboard layout
/usr/share/X11/locale/
/usr/share/xdg-desktop-portal/portals/			#gnome-shell.portal kde.portal gtk.portal etc
/etc/default/grub					#default grub configuration options
								grub_timeout=0
								grub_terminal_input="console"
/usr/lib/systemd/scripts/				#Systemd script directory

/etc/zsh/zprofile					#Zprofile
							[file: /etc/zsh/zprofile ]
								emulate sh -c 'source /etc/profile'
								alsactl --file /etc/alsa/asound.state restore
								(easyeffects --gapplication-service > /dev/null 2>&1 &)
								echo -e "\n\033[1;34mWELCOME TO ARCHLINUX\n"
								calendar_output=$(cal -m); echo -e "\033[1;36m"$calendar_output
								echo "\n"
								tclock; echo "\n"
							[/file]

/etc/environment					#File for setting up global environment variables
							^ Used by pam_env module, this is shell agnostic.

							[file: /etc/environment]
								#
								# This file is parsed by pam_env module
								#
								# Syntax: simple "KEY=VAL" pairs on separate lines
								#

								##The Environment vars below are meant for NVIDIA GPUs Only
								##Erase this below if you don't own an NVIDIA GPU
								GBM_BACKEND=nvidia-drm
								__GLX_VENDOR_LIBRARY_NAME=nvidia
								#__GL_SHARPEN_ENABLE=1  #Default is 0, this is known to cause problems with vlc, glxinfo and nvidia-settings as well
								__GL_SHARPEN_VALUE=100 #Default is 50, 100 is maximum; only works with __GL_SHARPEN_ENABLE=1
								__GL_SHARPEN_IGNORE_FILM_GRAIN=17 #Default is 17, 100 is maximum; only works with __GL_SHARPEN_ENABLE=1
								__GL_THREADED_OPTIMIZATION=1
								LIBVA_DRIVER_NAME=nvidia #Supposedly Fixes some issue with VLC and Nvidia-Settings - AVOID USING THIS, NOT NEEDED - USING CAUSES NO PROBLEMS THO

								##VERY IMPORTANT: below are my personal compiler flags for C, C++ and RUST based on my ivybridge processor
								##Either erase or modify the below env. vars if you do not own an ivy bridge processor.
								#CFLAGS="-mcpu=ivybridge -march=ivybridge -mtune=ivybridge -fomit-frame-pointer -O3 -pipe"
								#CXXFLAGS="-mcpu=ivybridge -march=ivybridge -mtune=ivybridge -fomit-frame-pointer -O3 -pipe"
								#RUSTFLAGS="-C opt-level=3 -C target-cpu=ivybridge"
								#
								PULSE_SERVER=tcp:127.0.0.1:4713
							[/file]

/etc/pipewire/client.conf				[file: /etc/pipewire/client.conf]
								# Client config file for PipeWire version "1.2.3" #
								#
								# Copy and edit this file in /etc/pipewire for system-wide changes
								# or in ~/.config/pipewire for local changes.
								#
								# It is also possible to place a file with an updated section in
								# /etc/pipewire/client.conf.d/ for system-wide changes or in
								# ~/.config/pipewire/client.conf.d/ for local changes.
								#

								context.properties = {
								    ## Configure properties in the system.
								    #mem.warn-mlock  = false
								    #mem.allow-mlock = true
								    #mem.mlock-all   = false
								    log.level        = 0

								    #default.clock.quantum-limit = 8192
								}

								context.spa-libs = {
								    #<factory-name regex> = <library-name>
								    #
								    # Used to find spa factory names. It maps an spa factory name
								    # regular expression to a library name that should contain
								    # that factory.
								    #
								    audio.convert.* = audioconvert/libspa-audioconvert
								    support.*       = support/libspa-support
								}

								context.modules = [
								    #{ name = <module-name>
								    #    ( args  = { <key> = <value> ... } )
								    #    ( flags = [ ( ifexists ) ( nofail ) ] )
								    #    ( condition = [ { <key> = <value> ... } ... ] )
								    #}
								    #
								    # Loads a module with the given parameters.
								    # If ifexists is given, the module is ignored when it is not found.
								    # If nofail is given, module initialization failures are ignored.
								    #

								    # The native communication protocol.
								    { name = libpipewire-module-protocol-native }

								    # Allows creating nodes that run in the context of the
								    # client. Is used by all clients that want to provide
								    # data to PipeWire.
								    { name = libpipewire-module-client-node }

								    # Allows creating devices that run in the context of the
								    # client. Is used by the session manager.
								    { name = libpipewire-module-client-device }

								    # Makes a factory for wrapping nodes in an adapter with a
								    # converter and resampler.
								    { name = libpipewire-module-adapter }

								    # Allows applications to create metadata objects. It creates
								    # a factory for Metadata objects.
								    { name = libpipewire-module-metadata }

								    # Provides factories to make session manager objects.
								    { name = libpipewire-module-session-manager }
								]

								filter.properties = {
								    #node.latency = 1024/48000
								}

								stream.properties = {
								    #node.latency          = 1024/48000
								    #node.autoconnect      = true
								    #resample.quality      = 10
								    #channelmix.normalize  = false
								    #channelmix.mix-lfe    = true
								    #channelmix.upmix      = true
								    #channelmix.upmix-method = psd  # none, simple
								    #channelmix.lfe-cutoff = 150
								    #channelmix.fc-cutoff  = 12000
								    #channelmix.rear-delay = 12.0
								    #channelmix.stereo-widen = 0.0
								    #channelmix.hilbert-taps = 0
								    #dither.noise = 0
								}

							[/file]

/etc/firejail/steam.profile				#Steam's firejail custom profile
							[file: /etc/firejail/steam.profile]
								# Firejail profile for steam
								# Description: Valve's Steam digital software delivery system
								# This file is overwritten after every install/update
								# Persistent local customizations
								include steam.local
								include wine.local

								# Persistent global definitions
								include globals.local

								#CUSTOM - ADDED BY ME - allows gamescope to run games
								#include wine.profile
								noblacklist /tmp/.wine-*
								noblacklist ${HOME}/.cache/wine
								noblacklist ${HOME}/.cache/winetricks
								noblacklist ${HOME}/.Steam
								noblacklist ${HOME}/.local/share/Steam
								noblacklist ${HOME}/.local/share/steam
								noblacklist ${HOME}/.steam
								noblacklist ${HOME}/.wine
								noblacklist ${HOME}/.nvidia-settings-rc

								#CUSTOM - ADDED BY ME - Nvidia whitelist/noblacklist
								noblacklist /sys/module
								whitelist /sys/module/nvidia*
								read-only /sys/module/nvidia*

								noblacklist ${HOME}/.config/Epic
								noblacklist ${HOME}/.config/Loop_Hero
								noblacklist ${HOME}/.config/MangoHud
								noblacklist ${HOME}/.config/ModTheSpire
								noblacklist ${HOME}/.config/RogueLegacy
								noblacklist ${HOME}/.config/RogueLegacyStorageContainer
								noblacklist ${HOME}/.killingfloor
								noblacklist ${HOME}/.klei
								noblacklist ${HOME}/.local/share/3909/PapersPlease
								noblacklist ${HOME}/.local/share/aspyr-media
								noblacklist ${HOME}/.local/share/bohemiainteractive
								noblacklist ${HOME}/.local/share/cdprojektred
								noblacklist ${HOME}/.local/share/Colossal Order
								noblacklist ${HOME}/.local/share/Dredmor
								noblacklist ${HOME}/.local/share/FasterThanLight
								noblacklist ${HOME}/.local/share/feral-interactive
								noblacklist ${HOME}/.local/share/HotlineMiami
								noblacklist ${HOME}/.local/share/IntoTheBreach
								noblacklist ${HOME}/.local/share/Paradox Interactive
								noblacklist ${HOME}/.local/share/PillarsOfEternity
								noblacklist ${HOME}/.local/share/RogueLegacy
								noblacklist ${HOME}/.local/share/RogueLegacyStorageContainer
								noblacklist ${HOME}/.local/share/Steam
								noblacklist ${HOME}/.local/share/SteamWorldDig
								noblacklist ${HOME}/.local/share/SteamWorld Dig 2
								noblacklist ${HOME}/.local/share/SuperHexagon
								noblacklist ${HOME}/.local/share/Terraria
								noblacklist ${HOME}/.local/share/vpltd
								noblacklist ${HOME}/.local/share/vulkan
								noblacklist ${HOME}/.mbwarband
								noblacklist ${HOME}/.paradoxinteractive
								noblacklist ${HOME}/.paradoxlauncher
								noblacklist ${HOME}/.prey
								noblacklist ${HOME}/.steam
								noblacklist ${HOME}/.steampath
								noblacklist ${HOME}/.steampid
								# needed for STEAM_RUNTIME_PREFER_HOST_LIBRARIES=1 to work
								noblacklist /sbin
								noblacklist /usr/sbin

								# Allow java (blacklisted by disable-devel.inc)
								include allow-java.inc

								# Allow python (blacklisted by disable-interpreters.inc)
								include allow-python2.inc
								include allow-python3.inc

								include disable-common.inc
								include disable-devel.inc
								include disable-interpreters.inc
								include disable-programs.inc

								mkdir ${HOME}/.config/Epic
								mkdir ${HOME}/.config/Loop_Hero
								mkdir ${HOME}/.config/MangoHud
								mkdir ${HOME}/.config/ModTheSpire
								mkdir ${HOME}/.config/RogueLegacy
								mkdir ${HOME}/.config/unity3d
								mkdir ${HOME}/.killingfloor
								mkdir ${HOME}/.klei
								mkdir ${HOME}/.local/share/3909/PapersPlease
								mkdir ${HOME}/.local/share/aspyr-media
								mkdir ${HOME}/.local/share/bohemiainteractive
								mkdir ${HOME}/.local/share/cdprojektred
								mkdir ${HOME}/.local/share/Colossal Order
								mkdir ${HOME}/.local/share/Dredmor
								mkdir ${HOME}/.local/share/FasterThanLight
								mkdir ${HOME}/.local/share/feral-interactive
								mkdir ${HOME}/.local/share/HotlineMiami
								mkdir ${HOME}/.local/share/IntoTheBreach
								mkdir ${HOME}/.local/share/Paradox Interactive
								mkdir ${HOME}/.local/share/PillarsOfEternity
								mkdir ${HOME}/.local/share/RogueLegacy
								mkdir ${HOME}/.local/share/Steam
								mkdir ${HOME}/.local/share/SteamWorldDig
								mkdir ${HOME}/.local/share/SteamWorld Dig 2
								mkdir ${HOME}/.local/share/SuperHexagon
								mkdir ${HOME}/.local/share/Terraria
								mkdir ${HOME}/.local/share/vpltd
								mkdir ${HOME}/.local/share/vulkan
								mkdir ${HOME}/.mbwarband
								mkdir ${HOME}/.paradoxinteractive
								mkdir ${HOME}/.paradoxlauncher
								mkdir ${HOME}/.prey
								mkdir ${HOME}/.steam
								mkfile ${HOME}/.steampath
								mkfile ${HOME}/.steampid
								whitelist ${HOME}/.config/Epic
								whitelist ${HOME}/.config/Loop_Hero
								whitelist ${HOME}/.config/MangoHud
								whitelist ${HOME}/.config/ModTheSpire
								whitelist ${HOME}/.config/RogueLegacy
								whitelist ${HOME}/.config/RogueLegacyStorageContainer
								whitelist ${HOME}/.config/unity3d
								whitelist ${HOME}/.killingfloor
								whitelist ${HOME}/.klei
								whitelist ${HOME}/.local/share/3909/PapersPlease
								whitelist ${HOME}/.local/share/aspyr-media
								whitelist ${HOME}/.local/share/bohemiainteractive
								whitelist ${HOME}/.local/share/cdprojektred
								whitelist ${HOME}/.local/share/Colossal Order
								whitelist ${HOME}/.local/share/Dredmor
								whitelist ${HOME}/.local/share/FasterThanLight
								whitelist ${HOME}/.local/share/feral-interactive
								whitelist ${HOME}/.local/share/HotlineMiami
								whitelist ${HOME}/.local/share/IntoTheBreach
								whitelist ${HOME}/.local/share/Paradox Interactive
								whitelist ${HOME}/.local/share/PillarsOfEternity
								whitelist ${HOME}/.local/share/RogueLegacy
								whitelist ${HOME}/.local/share/RogueLegacyStorageContainer
								whitelist ${HOME}/.local/share/Steam
								whitelist ${HOME}/.local/share/SteamWorldDig
								whitelist ${HOME}/.local/share/SteamWorld Dig 2
								whitelist ${HOME}/.local/share/SuperHexagon
								whitelist ${HOME}/.local/share/Terraria
								whitelist ${HOME}/.local/share/vpltd
								whitelist ${HOME}/.local/share/vulkan
								whitelist ${HOME}/.mbwarband
								whitelist ${HOME}/.paradoxinteractive
								whitelist ${HOME}/.paradoxlauncher
								whitelist ${HOME}/.prey
								whitelist ${HOME}/.steam
								whitelist ${HOME}/.steampath
								whitelist ${HOME}/.steampid
								include whitelist-common.inc
								include whitelist-var-common.inc

								# NOTE: The following were intentionally left out as they are alternative
								# (i.e.: unnecessary and/or legacy) paths whose existence may potentially
								# clobber other paths (see #4225).  If you use any, either add the entry to
								# steam.local or move the contents to a path listed above (or open an issue if
								# it's missing above).
								#mkdir ${HOME}/.config/RogueLegacyStorageContainer
								#mkdir ${HOME}/.local/share/RogueLegacyStorageContainer

								caps.drop all
								##CUSTOM / Keeping sys_nice, using sys_nice REQUIRE commenting/disabling 'caps.drop all', but works the same and will only exempt 'sys_nice'.
								##Currently DISABLED, because i haven't got any good performance outside firejail
								#caps.keep sys_nice

								#ipc-namespace
								netfilter
								nodvd
								nogroups

								##CUSTOM / Using nonewprivs forces sys_nice to be disabled
								nonewprivs

								##CUSTOM / Added comment by me - allows /tmp/.X11-unix/ to be read-write same for other root owned files under /tmp/,
								##Otherwise, gamescope will not work:
								#noroot

								notv
								nou2f
								# For VR support add 'ignore novideo' to your steam.local.
								novideo
								protocol unix,inet,inet6,netlink
								# seccomp sometimes causes issues (see #2951, #3267).
								# Add 'ignore seccomp' to your steam.local if you experience this.
								# mount, name_to_handle_at, pivot_root and umount2 are used by Proton >= 5.13
								# (see #4366).
								seccomp !chroot,!mount,!name_to_handle_at,!pivot_root,!ptrace,!umount2

								# process_vm_readv is used by GE-Proton7-18 (see #5185).
								seccomp.32 !process_vm_readv

								# tracelog breaks integrated browser
								#tracelog

								# private-bin is disabled while in testing, but is known to work with multiple games.
								# Add the next line to your steam.local to enable private-bin.
								#private-bin awk,basename,bash,bsdtar,bzip2,cat,chmod,cksum,cmp,comm,compress,cp,curl,cut,date,dbus-launch,dbus-send,desktop-file-edit,desktop-file-install,desktop-file-validate,dirname,echo,env,expr,file,find,getopt,grep,gtar,gzip,head,hostname,id,lbzip2,ldconfig,ldd,ln,ls,lsb_release,lsof,lspci,lz4,lzip,lzma,lzop,md5sum,mkdir,mktemp,mv,netstat,ps,pulseaudio,python*,readlink,realpath,rm,sed,sh,sha1sum,sha256sum,sha512sum,sleep,sort,steam,steamdeps,steam-native,steam-runtime,sum,tail,tar,tclsh,test,touch,tr,umask,uname,update-desktop-database,wc,wget,wget2,which,whoami,xterm,xz,zenity
								# Extra programs are available which might be needed for select games.
								# Add the next line to your steam.local to enable support for these programs.
								#private-bin java,java-config,mono
								# To view screenshots add the next line to your steam.local.
								#private-bin eog,eom,gthumb,pix,viewnior,xviewer

								private-dev
								# private-etc breaks a small selection of games on some systems. Add 'ignore private-etc'
								# to your steam.local to support those.
								private-etc alsa,alternatives,asound.conf,bumblebee,ca-certificates,crypto-policies,dbus-1,drirc,fonts,group,gtk-2.0,gtk-3.0,host.conf,hostname,hosts,ld.so.cache,ld.so.conf,ld.so.conf.d,ld.so.preload,localtime,lsb-release,machine-id,mime.types,nvidia,os-release,passwd,pki,pulse,resolv.conf,services,ssl,vulkan

								##CUSTOM / DEFAULT OPTION REMOVED, due to gamescope not working under it and working very slow under bind mount:
								#private-tmp

								#CUSTOMIZED BY ME:
								ignore private-tmp

								#CUSTOM / NOT NEEDED:
								#ignore private-etc

								#CUSTOM / NO NEED TO USE:
								#noblacklist  /usr/share/wireplumber
								#noblacklist  /usr/share/wireplumber/*
								#noblacklist /usr/share/pipewire
								#noblacklist /usr/share/pipewire/*
								#noblacklist ~/.local/state/wireplumber
								#noblacklist ~/.local/state/wireplumber/*
								#noblacklist ~/asound.state
								#whitelist ~/.local/state/wireplumber

								#CUSTOM / NOT IN USE
								#ignore private-etc

								#dbus-user none
								#dbus-system none

								read-only ${HOME}/.config/MangoHud
								#restrict-namespaces
							[/file]

/etc/pipewire/pipewire.conf				[file: /etc/pipewire/pipewire.conf]
								# Daemon config file for PipeWire version "1.2.3" #
								#
								# Copy and edit this file in /etc/pipewire for system-wide changes
								# or in ~/.config/pipewire for local changes.
								#
								# It is also possible to place a file with an updated section in
								# /etc/pipewire/pipewire.conf.d/ for system-wide changes or in
								# ~/.config/pipewire/pipewire.conf.d/ for local changes.
								#
								# TAGS:
								# {Experimenting}

								context.properties = {
								    ## Configure properties in the system.
								    #library.name.system                   = support/libspa-support
								    #context.data-loop.library.name.system = support/libspa-support
								    #support.dbus                          = true
								    #version < 3 clients can't handle more than 16 link.max-buffers:
								    #link.max-buffers                      = 64 
								    #mem.warn-mlock                        = false
								    #mem.allow-mlock                       = true
								    #mem.mlock-all                         = false
								    #clock.power-of-two-quantum            = true
								    #log.level                             = 2
								    #cpu.zero.denormals                    = false

								    #loop.rt-prio = -1            # -1 = use module-rt prio, 0 disable rt
								    #loop.class = data.rt
								    #thread.affinity = [ 0 1 ]    # optional array of CPUs
								    #context.num-data-loops = 1   # -1 = num-cpus, 0 = no data loops
								    #
								    #context.data-loops = [
								    #    {   loop.rt-prio = -1
								    #        loop.class = [ data.rt audio.rt ]
								    #        #library.name.system = support/libspa-support
								    #        thread.name = data-loop.0
								    #        #thread.affinity = [ 0 1 ]    # optional array of CPUs
								    #    }
								    #]

								    core.daemon = true              # listening for socket connections
								    core.name   = pipewire-0        # core name and socket name

								    ## Properties for the DSP configuration.
								    #default.clock.rate          = 48000
								    #default.clock.allowed-rates = [ 48000 ]
								    #default.clock.quantum       = 1024
								    #Experimenting here, Default values:
								    #Default Values:
								    #default.clock.min-quantum   = 32
								    #default.clock.max-quantum   = 2048
								    #	Note: This is a replacement for "pw-metadata -n settings 0 clock.force-quantum 4000" for fixing audio stuttering
								    #Experimentation:
								    #	default.clock.min-quantum   = 400
								    #	default.clock.max-quantum   = 4000
								    #default.clock.quantum-limit = 8192
								    #default.clock.quantum-floor = 4
								    #default.video.width         = 640
								    #default.video.height        = 480
								    #default.video.rate.num      = 25
								    #default.video.rate.denom    = 1
								    #
								    #settings.check-quantum      = false
								    #settings.check-rate         = false

								    # keys checked below to disable module loading
								    module.x11.bell = true
								    # enables autoloading of access module, when disabled an alternative
								    # access module needs to be loaded.
								    module.access = true
								    # enables autoloading of module-jackdbus-detect
								    module.jackdbus-detect = true
								}

								context.properties.rules = [
								    {   matches = [ { cpu.vm.name = !null } ]
									actions = {
									    update-props = {
										# These overrides are only applied when running in a vm.
										#Experimenting here - Original value was:
										default.clock.min-quantum = 1024
										#default.clock.max-quantum was never set
										#Experimentation:
										#	default.clock.min-quantum = 400
										#	default.clock.max-quantum = 4000
									    }
									}
								    }
								]

								context.spa-libs = {
								    #<factory-name regex> = <library-name>
								    #
								    # Used to find spa factory names. It maps an spa factory name
								    # regular expression to a library name that should contain
								    # that factory.
								    #
								    audio.convert.* = audioconvert/libspa-audioconvert
								    avb.*           = avb/libspa-avb
								    api.alsa.*      = alsa/libspa-alsa
								    api.v4l2.*      = v4l2/libspa-v4l2
								    api.libcamera.* = libcamera/libspa-libcamera
								    api.bluez5.*    = bluez5/libspa-bluez5
								    api.vulkan.*    = vulkan/libspa-vulkan
								    api.jack.*      = jack/libspa-jack
								    support.*       = support/libspa-support
								    video.convert.* = videoconvert/libspa-videoconvert
								    #videotestsrc   = videotestsrc/libspa-videotestsrc
								    #audiotestsrc   = audiotestsrc/libspa-audiotestsrc
								}

								context.modules = [
								    #{ name = <module-name>
								    #    ( args  = { <key> = <value> ... } )
								    #    ( flags = [ ( ifexists ) ( nofail ) ] )
								    #    ( condition = [ { <key> = <value> ... } ... ] )
								    #}
								    #
								    # Loads a module with the given parameters.
								    # If ifexists is given, the module is ignored when it is not found.
								    # If nofail is given, module initialization failures are ignored.
								    # If condition is given, the module is loaded only when the context
								    # properties all match the match rules.
								    #

								    # Uses realtime scheduling to boost the audio thread priorities. This uses
								    # RTKit if the user doesn't have permission to use regular realtime
								    # scheduling. You can also clamp utilisation values to improve scheduling
								    # on embedded and heterogeneous systems, e.g. Arm big.LITTLE devices.
								    { name = libpipewire-module-rt
									args = {
									    nice.level    = -11
									    rt.prio       = 88
									    #rt.time.soft = -1
									    #rt.time.hard = -1
									    #uclamp.min = 0
									    #uclamp.max = 1024
									}
									flags = [ ifexists nofail ]
								    }

								    # The native communication protocol.
								    { name = libpipewire-module-protocol-native
									args = {
									    # List of server Unix sockets, and optionally permissions
									    #sockets = [ { name = "pipewire-0" }, { name = "pipewire-0-manager" } ]
									}
								    }

								    # The profile module. Allows application to access profiler
								    # and performance data. It provides an interface that is used
								    # by pw-top and pw-profiler.
								    { name = libpipewire-module-profiler }

								    # Allows applications to create metadata objects. It creates
								    # a factory for Metadata objects.
								    { name = libpipewire-module-metadata }

								    # Creates a factory for making devices that run in the
								    # context of the PipeWire server.
								    { name = libpipewire-module-spa-device-factory }

								    # Creates a factory for making nodes that run in the
								    # context of the PipeWire server.
								    { name = libpipewire-module-spa-node-factory }

								    # Allows creating nodes that run in the context of the
								    # client. Is used by all clients that want to provide
								    # data to PipeWire.
								    { name = libpipewire-module-client-node }

								    # Allows creating devices that run in the context of the
								    # client. Is used by the session manager.
								    { name = libpipewire-module-client-device }

								    # The portal module monitors the PID of the portal process
								    # and tags connections with the same PID as portal
								    # connections.
								    { name = libpipewire-module-portal
									flags = [ ifexists nofail ]
								    }

								    # The access module can perform access checks and block
								    # new clients.
								    { name = libpipewire-module-access
									args = {
									    # Socket-specific access permissions
									    #access.socket = { pipewire-0 = "default", pipewire-0-manager = "unrestricted" }

									    # Deprecated legacy mode (not socket-based),
									    # for now enabled by default if access.socket is not specified
									    #access.legacy = true
									}
									condition = [ { module.access = true } ]
								    }

								    # Makes a factory for wrapping nodes in an adapter with a
								    # converter and resampler.
								    { name = libpipewire-module-adapter }

								    # Makes a factory for creating links between ports.
								    { name = libpipewire-module-link-factory }

								    # Provides factories to make session manager objects.
								    { name = libpipewire-module-session-manager }

								    # Use libcanberra to play X11 Bell
								    { name = libpipewire-module-x11-bell
									args = {
									    #sink.name = "@DEFAULT_SINK@"
									    #sample.name = "bell-window-system"
									    #x11.display = null
									    #x11.xauthority = null
									}
									flags = [ ifexists nofail ]
									condition = [ { module.x11.bell = true } ]
								    }
								    { name = libpipewire-module-jackdbus-detect
									args = {
									    #jack.library     = libjack.so.0
									    #jack.server      = null
									    #jack.client-name = PipeWire
									    #jack.connect     = true
									    #tunnel.mode      = duplex  # source|sink|duplex
									    source.props = {
										#audio.channels = 2
										#midi.ports = 1
										#audio.position = [ FL FR ]
										# extra sink properties
									    }
									    sink.props = {
										#audio.channels = 2
										#midi.ports = 1
										#audio.position = [ FL FR ]
										# extra sink properties
									    }
									}
									flags = [ ifexists nofail ]
									condition = [ { module.jackdbus-detect = true } ]
								    }
								]

								context.objects = [
								    #{ factory = <factory-name>
								    #    ( args  = { <key> = <value> ... } )
								    #    ( flags = [ ( nofail ) ] )
								    #    ( condition = [ { <key> = <value> ... } ... ] )
								    #}
								    #
								    # Creates an object from a PipeWire factory with the given parameters.
								    # If nofail is given, errors are ignored (and no object is created).
								    # If condition is given, the object is created only when the context properties
								    # all match the match rules.
								    #
								    #{ factory = spa-node-factory   args = { factory.name = videotestsrc node.name = videotestsrc node.description = videotestsrc "Spa:Pod:Object:Param:Props:patternType" = 1 } }
								    #{ factory = spa-device-factory args = { factory.name = api.jack.device foo=bar } flags = [ nofail ] }
								    #{ factory = spa-device-factory args = { factory.name = api.alsa.enum.udev } }
								    #{ factory = spa-node-factory   args = { factory.name = api.alsa.seq.bridge node.name = Internal-MIDI-Bridge } }
								    #{ factory = adapter            args = { factory.name = audiotestsrc node.name = my-test node.description = audiotestsrc } }
								    #{ factory = spa-node-factory   args = { factory.name = api.vulkan.compute.source node.name = my-compute-source } }

								    # A default dummy driver. This handles nodes marked with the "node.always-process"
								    # property when no other driver is currently active. JACK clients need this.
								    { factory = spa-node-factory
									args = {
									    factory.name    = support.node.driver
									    node.name       = Dummy-Driver
									    node.group      = pipewire.dummy
									    node.sync-group  = sync.dummy
									    priority.driver = 20000
									    #OLD VALUE WAS: priority.driver = 20000
									    #clock.id       = monotonic # realtime | tai | monotonic-raw | boottime
									    #clock.name     = "clock.system.monotonic"
									}
								    }
								    { factory = spa-node-factory
									args = {
									    factory.name    = support.node.driver
									    node.name       = Freewheel-Driver
									    priority.driver = 19000
									    #OLD VALUE WAS: priority.driver = 19000
									    node.group      = pipewire.freewheel
									    node.sync-group  = sync.dummy
									    node.freewheel  = true
									    #freewheel.wait = 10
									}
								    }

								    # This creates a new Source node. It will have input ports
								    # that you can link, to provide audio for this source.
								    #{ factory = adapter
								    #    args = {
								    #        factory.name     = support.null-audio-sink
								    #        node.name        = "my-mic"
								    #        node.description = "Microphone"
								    #        media.class      = "Audio/Source/Virtual"
								    #        audio.position   = "FL,FR"
								    #        monitor.passthrough = true
								    #    }
								    #}

								    # This creates a single PCM source device for the given
								    # alsa device path hw:0. You can change source to sink
								    # to make a sink in the same way.
								    #{ factory = adapter
								    #    args = {
								    #        factory.name           = api.alsa.pcm.source
								    #        node.name              = "alsa-source"
								    #        node.description       = "PCM Source"
								    #        media.class            = "Audio/Source"
								    #        api.alsa.path          = "hw:0"
								    #
								    #        DEFAULT VALUE IS -> api.alsa.period-size   = 1024
								    #        DEFAULT VALUE IS -> api.alsa.headroom      = 0
								    #        api.alsa.period-size   = 2048
								    #        api.alsa.headroom      = 22192
								    #
								    #        api.alsa.disable-mmap  = false
								    #        api.alsa.disable-batch = false
								    #        audio.format           = "S16LE"
								    #        audio.rate             = 48000
								    #        audio.channels         = 2
								    #        audio.position         = "FL,FR"
								    #    }
								    #}

								    # Use the metadata factory to create metadata and some default values.
								    #{ factory = metadata
								    #    args = {
								    #        metadata.name = my-metadata
								    #        metadata.values = [
								    #            { key = default.audio.sink   value = { name = somesink } }
								    #            { key = default.audio.source value = { name = somesource } }
								    #        ]
								    #    }
								    #}
								]

								context.exec = [
								    #{   path = <program-name>
								    #    ( args = "<arguments>" | [ <arg1> <arg2> ... ] )
								    #    ( condition = [ { <key> = <value> ... } ... ] )
								    #}
								    #
								    # Execute the given program with arguments.
								    # If condition is given, the program is executed only when the context
								    # properties all match the match rules.
								    #
								    # You can optionally start the session manager here,
								    # but it is better to start it as a systemd service.
								    # Run the session manager with -h for options.
								    #
								    #{ path = "/usr/bin/pipewire-media-session" args = ""
								    #  condition = [ { exec.session-manager = null } { exec.session-manager = true } ] }
								    #
								    # You can optionally start the pulseaudio-server here as well
								    # but it is better to start it as a systemd service.
								    # It can be interesting to start another daemon here that listens
								    # on another address with the -a option (eg. -a tcp:4713).
								    #
								    #{ path = "/usr/bin/pipewire" args = [ "-c" "pipewire-pulse.conf" ]
								    #  condition = [ { exec.pipewire-pulse = null } { exec.pipewire-pulse = true } ] }
								]

							[/file]

/etc/pipewire/pipewire-pulse.conf			[file: /etc/pipewire/pipewire-pulse.conf]
								# PulseAudio config file for PipeWire version "1.2.3" #
								#
								# Copy and edit this file in /etc/pipewire for system-wide changes
								# or in ~/.config/pipewire for local changes.
								#
								# It is also possible to place a file with an updated section in
								# /etc/pipewire/pipewire-pulse.conf.d/ for system-wide changes or in
								# ~/.config/pipewire/pipewire-pulse.conf.d/ for local changes.
								#

								context.properties = {
								    ## Configure properties in the system.
								    #mem.warn-mlock  = false
								    #mem.allow-mlock = true
								    #mem.mlock-all   = false
								    #log.level       = 2

								    #default.clock.quantum-limit = 8192
								}

								context.spa-libs = {
								    audio.convert.* = audioconvert/libspa-audioconvert
								    support.*       = support/libspa-support
								}

								context.modules = [
								    { name = libpipewire-module-rt
									args = {
									    nice.level   = -11
									    #rt.prio      = 83
									    #rt.time.soft = -1
									    #rt.time.hard = -1
									    #uclamp.min = 0
									    #uclamp.max = 1024
									}
									flags = [ ifexists nofail ]
								    }
								    { name = libpipewire-module-protocol-native }
								    { name = libpipewire-module-client-node }
								    { name = libpipewire-module-adapter }
								    { name = libpipewire-module-metadata }

								    { name = libpipewire-module-protocol-pulse
									args = {
									    # contents of pulse.properties can also be placed here
									    # to have config per server.
									}
								    }
								]

								# Extra scripts can be started here. Setup in default.pa can be moved in
								# a script or in pulse.cmd below
								context.exec = [
								    #{ path = "pactl"        args = "load-module module-always-sink" }
								    #{ path = "pactl"        args = "upload-sample my-sample.wav my-sample" }
								    #{ path = "/usr/bin/sh"  args = "~/.config/pipewire/default.pw" }
								]

								# Extra commands can be executed here.
								#   load-module : loads a module with args and flags
								#      args = "<module-name> <module-args>"
								#      ( flags = [ nofail ] )
								pulse.cmd = [
								    { cmd = "load-module" args = "module-always-sink" flags = [ ] }
								    { cmd = "load-module" args = "module-device-manager" flags = [ ] }
								    { cmd = "load-module" args = "module-device-restore" flags = [ ] }
								    { cmd = "load-module" args = "module-stream-restore" flags = [ ] }
								    #{ cmd = "load-module" args = "module-switch-on-connect" }
								    #{ cmd = "load-module" args = "module-gsettings" flags = [ nofail ] }
								]

								stream.properties = {
								    #node.latency          = 1024/48000
								    #node.autoconnect      = true
								    #resample.quality      = 10
								    #channelmix.normalize  = false
								    #channelmix.mix-lfe    = true
								    #channelmix.upmix      = true
								    #channelmix.upmix-method = psd  # none, simple
								    #channelmix.lfe-cutoff = 150
								    #channelmix.fc-cutoff  = 12000
								    #channelmix.rear-delay = 12.0
								    #channelmix.stereo-widen = 0.0
								    #channelmix.hilbert-taps = 0
								    #dither.noise = 0
								}

								pulse.properties = {
								    # the addresses this server listens on
								    server.address = [
									"unix:native"
									#"unix:/tmp/something"              # absolute paths may be used
									#"tcp:4713"                         # IPv4 and IPv6 on all addresses
									#"tcp:[::]:9999"                    # IPv6 on all addresses
									#"tcp:127.0.0.1:8888"               # IPv4 on a single address
									#
									{ address = "tcp:127.0.0.1:4713"    # address
									   max-clients = 130                 # maximum number of clients
									   listen-backlog = 130              # backlog in the server listen queue
									   client.access = "unrestricted"     # permissions for clients
									}
								    ]
								    #server.dbus-name       = "org.pulseaudio.Server"
								    #pulse.allow-module-loading = true
								    #pulse.min.req          = 128/48000     # 2.7ms
								    #pulse.default.req      = 960/48000     # 20 milliseconds
								    #pulse.min.frag         = 128/48000     # 2.7ms
								    #pulse.default.frag     = 96000/48000   # 2 seconds
								    #pulse.default.tlength  = 96000/48000   # 2 seconds
								    #pulse.min.quantum      = 128/48000     # 2.7ms
								    #pulse.idle.timeout     = 0             # don't pause after underruns
								    #pulse.default.format   = F32
								    #pulse.default.position = [ FL FR ]
								}

								pulse.properties.rules = [
								    {   matches = [ { cpu.vm.name = !null } ]
									actions = {
									    update-props = {
									    # These overrides are only applied when running in a vm.
										pulse.min.quantum = 1024/48000      # 22ms
									    }
									}
								    }
								]

								# client/stream specific properties
								pulse.rules = [
								    {
									matches = [
									    {
										# all keys must match the value. ! negates. ~ starts regex.
										#client.name                = "Firefox"
										#application.process.binary = "teams"
										#application.name           = "~speech-dispatcher.*"
									    }
									]
									actions = {
									    update-props = {
										#node.latency = 512/48000
									    }
									    # Possible quirks:"
									    #    force-s16-info                 forces sink and source info as S16 format
									    #    remove-capture-dont-move       removes the capture DONT_MOVE flag
									    #    block-source-volume            blocks updates to source volume
									    #    block-sink-volume              blocks updates to sink volume
									    #quirks = [ ]
									}
								    }
								    {
									# Snes9x suffers from audio stutter when frame time is set to 14ms
									matches = [ 
										{ application.process.binary = "snes9x-gamescope" } 
										{ application.process.binary = "snes9x-gtk" } 
										{ application.process.binary = "snes9x" } 
										{ application.process.binary = "alsa_playback.snes9x-gtk" } 
									]
									actions = {
									    update-props = {
										#pulse.min.quantum = 4000 #It works for other applications, but not for snes9x
										#pulse.max.quantum = 4000 #It works for other applications, but not for snes9x
										#pulse.force-quantum= 2000 #It works for other applications, but not for snes9x
										#clock.force-quantum = 2000 #not sure if this works
										pulse.min.req = 3048/48000 #works
										pulse.default.req = 3048/48000 #works
										pulse.min.quantum = 3048/48000 #works
									    }
									}
								    }
								    {
									# skype does not want to use devices that don't have an S16 sample format.
									matches = [
									     { application.process.binary = "teams" }
									     { application.process.binary = "teams-insiders" }
									     { application.process.binary = "skypeforlinux" }
									     { application.process.binary = "wine" }
									]
									actions = { quirks = [ force-s16-info ] }
								    }
								    {
									# firefox marks the capture streams as don't move and then they
									# can't be moved with pavucontrol or other tools.
									matches = [ { application.process.binary = "firefox" } ]
									actions = { quirks = [ remove-capture-dont-move ] }
								    }
								    {
									# speech dispatcher asks for too small latency and then underruns.
									matches = [ { application.name = "~speech-dispatcher.*" } ]
									actions = {
									    update-props = {
										pulse.min.req          = 512/48000      # 10.6ms
										pulse.min.quantum      = 512/48000      # 10.6ms
										pulse.idle.timeout     = 5              # pause after 5 seconds of underrun
									    }
									}
								    }
								    #{
								    #    matches = [ { application.process.binary = "Discord" } ]
								    #    actions = { quirks = [ block-source-volume ] }
								    #}
								]

							[/file]

~/.config/environment.d/environment.conf		#File for setting up user-local environment variables
							^ It's possible to have more than one .conf file.

/usr/lib/						#Contains all linux libraries
/usr/lib/modules/					#Linux Kernel modules/hooks used when creating initramfs
/etc/modules-load.d/					#Used for loading Linux Kernel modules/hooks at system boot.
								ex.: 	[file: /etc/modules-load.d/zram.conf]
										zram
									[/file]

									[file: /etc/modules-load.d/v4l2loopback.conf] - NOT USING IT AT THE MOMENT, MAYBE NOT NEEDED ANYLONGER, WORTH CHECKING
										v4l2loopback
									[/file]

								^ note: adding the module name in the /etc/mkinitcpio.conf file,
								makes this step unnecessary, since rebuilding a kernel's initramfs will add it to the list of active modules.

~/.config/mpv/mpv.conf					#Used for default mpv player settings
								Ex.:
								[file: ~/.config/mpv/mpv.conf ]
									volume=60
									#video-sync=display-resample
									#video-sync=display-desync
									#video-sync=display-resample
									video-sync=display-tempo
									interpolation=yes
									#tscale=linear
									tscale=oversample
									interpolation-preserve=no
									gpu-api=vulkan
									target-prim=s-gamut
									keepaspect=no
									autofit-larger=100%x100%
									autofit-smaller=100%x100%
									profile=gpu-hq
									glsl-shader="/usr/share/mpv/shaders/FSRCNNX_x2_16-0-4-1.glsl"
									loop=inf
									cache=yes
									cache-on-disk=no
								[/file]

/etc/modprobe.d/					#Used for probing existing Linux Kernel modules
								ex.:	[file: /etc/modprobe.d/nvidia.conf]
										options nvidia-drm modeset=1 fbdev=1
										options nvidia NVreg_UsePageAttributeTable=1 NVreg_UpdateMemoryTypes=0 NVreg_EnableStreamMemOPs=1 NVreg_EnableGpuFirmware=1
									[/file]
									VERY IMPORTANT: 
									^ options nvidia-drm modeset=1 fbdev=1 for some wayland compositors to work
									^ use fbdev option when needed for wayland -  Newer Drivers automate this
									^ Starting from nvidia-utils 560.35.03-5, DRM defaults to enabled.
									^ use the following command to make sure  DRM is Enabled:
									^ 	run0 cat /sys/module/nvidia_drm/parameters/modeset
									^ use the following command to make sure fbdev is Enabled:
									^	run0 cat /sys/module/nvidia_drm/parameters/fbdev

									[file: /etc/modprobe.d/blacklist-nouveau.conf]
										blacklist nouveau
									[/file]

									[file: /etc/modprobe.d/firewalld-sysctls.conf]

										install nf_conntrack /usr/bin/modprobe --ignore-install nf_conntrack $CMDLINE_OPTS && /sbin/sysctl --quiet --pattern 'net[.]netfilter[.]nf_conntrack.*' --system
									[/file; note: no idea what this does]

									[file: /etc/modprobe.d/v4l2loopback.conf] - OLD FILE
										options v4l2loopback nr_devices=2 exclusive_caps=1,1,1,1,1,1,1,1 video_nr=0,1 card_label=v4l2lo0,v4l2lo1
									[/file]

									[file: /etc/modprobe.d/v4l2loopback.conf] - NEWER FILE, NOT SURE IF WORKING:
										options v4l2loopback nr_devices exclusive_caps=1,1,1,1,1,1,1,1 video_nr=0,1 card_label=v4l2lo0,v4l2lo1
									[/file]

									[file: /etc/modprobe.d/vfio.conf] - NOT USING ANYLONGER, MIGHT BE USEFUL
										options vfio-pci ids=10de:11bf,10de:1c82,10de:0fb9
									[/file]

									[file: /etc/modprobe.d/zram.conf]
										options zram num_devices=1
									[/file]

									[file: /etc/modprobe.d/firewalld-sysctl.conf]
										install nf_conntrack /usr/bin/modprobe --ignore-install nf_conntrack $CMDLINE_OPTS && /sbin/sysctl --quiet --pattern 'net[.]netfilter[.]nf_conntrack.*' --system
									[/file]

/etc/X11/xorg.conf					#Main xorg configuration file
							VERY IMPORTANT: if you're both an nvidia owner and a xorg user, 
							You may have to use 'nvidia-xconfig' to generate a new xorg.conf file from time to time, 
							this is due to changes and likely updates in both xorg and nvidia configurations.
							you can always compare both files using nvim -d <file_1> <file_2> and apply changes
							wherever necessary to fit your old configuration needs.

							[file:  /etc/X11/xorg.conf]
								# nvidia-settings: X configuration file generated by nvidia-settings
								# nvidia-settings:  version 570.86.16

								Section "ServerLayout"
								    Identifier     "Layout0"
								    Screen      0  "Screen0" 0 0
								    InputDevice    "Keyboard0" "CoreKeyboard"
								    InputDevice    "Mouse0" "CorePointer"
								    Option         "Xinerama" "0"
								EndSection

								Section "Files"
								    ModulePath      "/usr/lib/nvidia/xorg"
								    ModulePath      "/usr/lib/xorg/modules"
								EndSection

								Section "Module"
								    Load           "modesetting"
								    Load           "glx"
								EndSection

								Section "InputDevice"

								    # generated from default
								    Identifier     "Mouse0"
								    Driver         "mouse"
								    Option         "Protocol" "auto"
								    Option         "Device" "/dev/psaux"
								    Option         "Emulate3Buttons" "no"
								    Option         "ZAxisMapping" "4 5"
								EndSection

								Section "InputDevice"

								    # generated from default
								    Identifier     "Keyboard0"
								    Driver         "kbd"
								EndSection

								Section "Monitor"

								    # Generated using https://tomverbeure.github.io/video_timings_calculator, Custom Modeline:
								    Identifier     "Monitor0"
								    VendorName     "Unknown"
								    #model name can be anything:
								    #ModelName     "DFP-1"
								    ModelName      "LG Electronics L1742"
								    HorizSync       30.0 - 71.0
								    VertRefresh     56.0 - 75.0
								    ##This enables the use of a custom resolution using the following  modeline:
								    #Default startup resolutions have to be set on the "Screen" section.
								    ModeLine       "1280x768_75n" 78.948 1280 1288 1320 1360 768 782 790 796 +hsync -vsync
								    #ModeLine       "1280x768_70n" 73.685 1280 1288 1320 1360 768 780 788 794 +hsync -vsync
								    Option         "DPMS"
								EndSection

								Section "Device"
								    Identifier     "Device0"
								    Driver         "nvidia"
								    VendorName     "NVIDIA Corporation"
								    BoardName      "NVIDIA GeForce GTX 1650"
								EndSection

								Section "Screen"
								    ##These has been added by nvidia:
								    # Removed Option "metamodes" "1280x768_70n +0+0"
								    # Removed Option "metamodes" "DFP-1: 1280x768_75n +0+0"
								    Identifier     "Screen0"
								    Device         "Device0"
								    Monitor        "Monitor0"
								    DefaultDepth    24
								    Option         "Coolbits" "28"
								    ##Don't use this option at all, always prefer "ModeValidation" "AllowNonEdidModes":
								    #Option         "UseEDID" "FALSE"
								    #https://us.download.nvidia.com/XFree86/Linux-x86_64/384.69/README/xconfigoptions.html
								    ##Below is Very Dangerous use with care:
								    #Option "IgnoreEDIDChecksum" "true"
								    Option         "ModeValidation" "AllowNonEdidModes"
								    Option         "Stereo" "0"
								    Option         "nvidiaXineramaInfoOrder" "HDMI-0"
								    ##Leaving here for reference, but don't use them, they didn't work:
								    #Option         "metamodes" "1280x768_73 +0+0"
								    #Option         "metamodes" "VGA-0: nvidia-auto-select +0+0 {ForceCompositionPipeline=On}, HDMI-0: nvidia-auto-select +0+0 {ForceCompositionPipeline=On}"
								    #Option 	    "metamodes" "DFP-1: 1280x1024"
								    #Option 	    "metamodes" "DFP-1: nvidia-auto-select"
								    ##The below metamode sets up a default resolution for when xorg starts, 
								    #this one is meant for an actual custom resolution on the 'Monitor Section'.
								    #example: Option         "metamodes" "DFP-1: 1280x768_75n +0+0"
								    #Option         "metamodes" "1280x768_75n +0+0; 1280x1024 +0+0; nvidia-auto-select;" - Doesn't work!, nvidia sets 2 resolution at the same time.
								    Option         "metamodes" "1280x768_75n +0+0"
								    Option         "SLI" "Off"
								    Option         "MultiGPU" "Off"
								    Option         "BaseMosaic" "off"
								    SubSection     "Display"
									Depth       24
								    EndSubSection
								EndSection

							[/file]

/etc/X11/xorg.conf.d/					#Folder Used by Xorg for storing and auto-loading configuration files
								ex.:
									[file: /etc/X11/xorg.conf.d/20-nvidia.conf]
										Section "Device"
											Identifier "Nvidia Card"
											Driver "nvidia"
											VendorName "NVIDIA Corporation"
											Option "NoLogo" "true"
											Option "Coolbits" "28"
										EndSection
									[/file]
									^ OR: sudo nvidia-xconfig --cool-bits=28


									[file: /etc/X11/xorg.conf.d/00-keyboard.conf]
										# Written by systemd-localed(8), read by systemd-localed and Xorg. It's
										# probably wise not to edit this file manually. Use localectl(1) to
										# instruct systemd-localed to update it.
										Section "InputClass"
											Identifier "system-keyboard"
											MatchIsKeyboard "on"
											Option "XkbLayout" "br"
											Option "XkbModel" "abnt2"
											Option "XkbVariant" "abnt2"
										EndSection
									[/file]

/boot/grub/grub.cfg					#Grub configuration file defines linux OSes to boot from hard disks
							COMPLETE FILE 2025(Check the incomplete section):
							[file: /boot/grub/grub.cfg]
								#
								# DO NOT EDIT THIS FILE
								#
								# It is automatically generated by grub-mkconfig using templates
								# from /etc/grub.d and settings from /etc/default/grub
								#

								### BEGIN /etc/grub.d/00_header ###
								insmod part_gpt
								insmod part_msdos
								if [ -s $prefix/grubenv ]; then
								  load_env
								fi
								if [ "${next_entry}" ] ; then
								   set default="${next_entry}"
								   set next_entry=
								   save_env next_entry
								   set boot_once=true
								else
								   set default="0"
								fi

								if [ x"${feature_menuentry_id}" = xy ]; then
								  menuentry_id_option="--id"
								else
								  menuentry_id_option=""
								fi

								export menuentry_id_option

								if [ "${prev_saved_entry}" ]; then
								  set saved_entry="${prev_saved_entry}"
								  save_env saved_entry
								  set prev_saved_entry=
								  save_env prev_saved_entry
								  set boot_once=true
								fi

								function savedefault {
								  if [ -z "${boot_once}" ]; then
								    saved_entry="${chosen}"
								    save_env saved_entry
								  fi
								}

								function load_video {
								  if [ x$feature_all_video_module = xy ]; then
								    insmod all_video
								  else
								    insmod efi_gop
								    insmod efi_uga
								    insmod ieee1275_fb
								    insmod vbe
								    insmod vga
								    insmod video_bochs
								    insmod video_cirrus
								  fi
								}

								if [ x$feature_default_font_path = xy ] ; then
								   font=unicode
								else
								insmod part_gpt
								insmod ext2
								set root='hd1,gpt2'
								if [ x$feature_platform_search_hint = xy ]; then
								  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
								else
								  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
								fi
								    font="/usr/share/grub/unicode.pf2"
								fi

								if loadfont $font ; then
								  set gfxmode=auto
								  load_video
								  insmod gfxterm
								  set locale_dir=$prefix/locale
								  set lang=en_US
								  insmod gettext
								fi
								terminal_input console
								terminal_output gfxterm
								insmod part_gpt
								insmod ext2
								set root='hd1,gpt2'
								if [ x$feature_platform_search_hint = xy ]; then
								  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
								else
								  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
								fi
								insmod gfxmenu
								loadfont ($root)/boot/grub/themes/Archlinux/Sans-10.pf2
								loadfont ($root)/boot/grub/themes/Archlinux/Sans-12.pf2
								loadfont ($root)/boot/grub/themes/Archlinux/Sans-14.pf2
								insmod png
								set theme=($root)/boot/grub/themes/Archlinux/theme.txt
								export theme
								if [ x$feature_timeout_style = xy ] ; then
								  set timeout_style=menu
								  set timeout=10000
								# Fallback normal timeout code in case the timeout_style feature is
								# unavailable.
								else
								  set timeout=10000
								fi
								### END /etc/grub.d/00_header ###

								### BEGIN /etc/grub.d/10_linux ###
								menuentry 'Arch Linux' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-simple-4f4ca4ad-777e-49ce-96f4-923d630cee48' {
									load_video
									set gfxpayload=keep
									insmod gzio
									insmod part_gpt
									insmod ext2
									set root='hd1,gpt2'
									if [ x$feature_platform_search_hint = xy ]; then
									  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
									else
									  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
									fi
									echo	'Loading Linux linux-zen ...'
									linux	/boot/vmlinuz-linux-zen root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw  loglevel=3 quiet
									echo	'Loading initial ramdisk ...'
									initrd	/boot/intel-ucode.img /boot/initramfs-linux-zen.img
								}

								menuentry 'Arch Linux (LTS on /dev/sdb2(SSD)) with intel-ucode.img From ArchLinux(Linux-LTS MITIGATIONS OFF(new)) GRUB' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'osprober-gnulinux-simple-555d2084-401a-4002-8217-61547c0eedab' {
									insmod part_msdos
									insmod ext2
									set root='hd1,msdos2'
									if [ x$feature_platform_search_hint = xy ]; then
									  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,msdos2 --hint-efi=hd1,msdos2 --hint-baremetal=ahci1,msdos2 4f4ca4ad-777e-49ce-96f4-923d630cee48
									else
									  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
									fi
									linux /boot/vmlinux-linux-lts root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw quiet mitigations=off zswap.enabled=0 intel_iommu=on psi=1 nvidia_drm.modeset=1 systemd.gpt_auto=0
									initrd /boot/intel-ucode.img /boot/initramfs-linux-lts.img
								}

								menuentry 'Arch Linux (ZEN on /dev/sdb2(SSD)) with intel-ucode.img From ArchLinux(Linux-ZEN MITIGATIONS OFF(new)) GRUB' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'osprober-gnulinux-simple-555d2084-401a-4002-8217-61547c0eedab' {
									insmod part_msdos
									insmod ext2
									set root='hd1,msdos2'
									if [ x$feature_platform_search_hint = xy ]; then
									  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,msdos2 --hint-efi=hd1,msdos2 --hint-baremetal=ahci1,msdos2 4f4ca4ad-777e-49ce-96f4-923d630cee48
									else
									  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
									fi
									linux /boot/vmlinux-linux-zen root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw quiet mitigations=off zswap.enabled=0 intel_iommu=on psi=1 nvidia_drm.modeset=1 systemd.gpt_auto=0
									initrd /boot/intel-ucode.img /boot/initramfs-linux-zen.img
								}

								menuentry 'Arch Linux (HARDENED on /dev/sdb2(SSD)) with intel-ucode.img From ArchLinux(Linux-HARDENED MITIGATIONS OFF(new)) GRUB' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'osprober-gnulinux-simple-555d2084-401a-4002-8217-61547c0eedab' {
									insmod part_msdos
									insmod ext2
									set root='hd1,msdos2'
									if [ x$feature_platform_search_hint = xy ]; then
									  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,msdos2 --hint-efi=hd1,msdos2 --hint-baremetal=ahci1,msdos2 4f4ca4ad-777e-49ce-96f4-923d630cee48
									else
									  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
									fi
									linux /boot/vmlinux-linux-hardened root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw quiet mitigations=off zswap.enabled=0 intel_iommu=on psi=1 systemd.gpt_auto=0
									initrd /boot/intel-ucode.img /boot/initramfs-linux-hardened.img
								}

								menuentry 'Arch Linux (on /dev/sdc7) with intel-ucode.img From ArchLinux(Linux-Zen MITIGATIONS OFF(new)) GRUB' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'osprober-gnulinux-simple-555d2084-401a-4002-8217-61547c0eedab' {
									insmod part_msdos
									insmod ext2
									set root='hd2,msdos7'
									if [ x$feature_platform_search_hint = xy ]; then
									  search --no-floppy --fs-uuid --set=root --hint-bios=hd2,msdos7 --hint-efi=hd2,msdos7 --hint-baremetal=ahci2,msdos7 555d2084-401a-4002-8217-61547c0eedab
									else
									  search --no-floppy --fs-uuid --set=root 555d2084-401a-4002-8217-61547c0eedab
									fi
									linux /boot/vmlinux-linux-zen root=UUID=555d2084-401a-4002-8217-61547c0eedab rw quiet acpi=off mitigations=off zswap.enabled=0 intel_iommu=on nvidia-drm.modeset=1 psi=1
									initrd /boot/intel-ucode.img /boot/initramfs-linux-zen.img
								}

								submenu 'Advanced options for Arch Linux' $menuentry_id_option 'gnulinux-advanced-4f4ca4ad-777e-49ce-96f4-923d630cee48' {
									menuentry 'Arch Linux, with Linux linux-zen' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-linux-zen-advanced-4f4ca4ad-777e-49ce-96f4-923d630cee48' {
										load_video
										set gfxpayload=keep
										insmod gzio
										insmod part_gpt
										insmod ext2
										set root='hd1,gpt2'
										if [ x$feature_platform_search_hint = xy ]; then
										  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
										else
										  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
										fi
										echo	'Loading Linux linux-zen ...'
										linux	/boot/vmlinuz-linux-zen root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw  loglevel=3 quiet
										echo	'Loading initial ramdisk ...'
										initrd	/boot/intel-ucode.img /boot/initramfs-linux-zen.img
									}
									menuentry 'Arch Linux, with Linux linux-zen (fallback initramfs)' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-linux-zen-fallback-4f4ca4ad-777e-49ce-96f4-923d630cee48' {
										load_video
										set gfxpayload=keep
										insmod gzio
										insmod part_gpt
										insmod ext2
										set root='hd1,gpt2'
										if [ x$feature_platform_search_hint = xy ]; then
										  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
										else
										  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
										fi
										echo	'Loading Linux linux-zen ...'
										linux	/boot/vmlinuz-linux-zen root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw  loglevel=3 quiet
										echo	'Loading initial ramdisk ...'
										initrd	/boot/intel-ucode.img /boot/initramfs-linux-zen-fallback.img
									}
									menuentry 'Arch Linux, with Linux linux-lts' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-linux-lts-advanced-4f4ca4ad-777e-49ce-96f4-923d630cee48' {
										load_video
										set gfxpayload=keep
										insmod gzio
										insmod part_gpt
										insmod ext2
										set root='hd1,gpt2'
										if [ x$feature_platform_search_hint = xy ]; then
										  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
										else
										  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
										fi
										echo	'Loading Linux linux-lts ...'
										linux	/boot/vmlinuz-linux-lts root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw  loglevel=3 quiet
										echo	'Loading initial ramdisk ...'
										initrd	/boot/intel-ucode.img /boot/initramfs-linux-lts.img
									}
									menuentry 'Arch Linux, with Linux linux-lts (fallback initramfs)' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-linux-lts-fallback-4f4ca4ad-777e-49ce-96f4-923d630cee48' {
										load_video
										set gfxpayload=keep
										insmod gzio
										insmod part_gpt
										insmod ext2
										set root='hd1,gpt2'
										if [ x$feature_platform_search_hint = xy ]; then
										  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
										else
										  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
										fi
										echo	'Loading Linux linux-lts ...'
										linux	/boot/vmlinuz-linux-lts root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw  loglevel=3 quiet
										echo	'Loading initial ramdisk ...'
										initrd	/boot/intel-ucode.img /boot/initramfs-linux-lts-fallback.img
									}
									menuentry 'Arch Linux, with Linux linux-hardened' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-linux-hardened-advanced-4f4ca4ad-777e-49ce-96f4-923d630cee48' {
										load_video
										set gfxpayload=keep
										insmod gzio
										insmod part_gpt
										insmod ext2
										set root='hd1,gpt2'
										if [ x$feature_platform_search_hint = xy ]; then
										  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
										else
										  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
										fi
										echo	'Loading Linux linux-hardened ...'
										linux	/boot/vmlinuz-linux-hardened root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw  loglevel=3 quiet
										echo	'Loading initial ramdisk ...'
										initrd	/boot/intel-ucode.img /boot/initramfs-linux-hardened.img
									}
									menuentry 'Arch Linux, with Linux linux-hardened (fallback initramfs)' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-linux-hardened-fallback-4f4ca4ad-777e-49ce-96f4-923d630cee48' {
										load_video
										set gfxpayload=keep
										insmod gzio
										insmod part_gpt
										insmod ext2
										set root='hd1,gpt2'
										if [ x$feature_platform_search_hint = xy ]; then
										  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
										else
										  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
										fi
										echo	'Loading Linux linux-hardened ...'
										linux	/boot/vmlinuz-linux-hardened root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw  loglevel=3 quiet
										echo	'Loading initial ramdisk ...'
										initrd	/boot/intel-ucode.img /boot/initramfs-linux-hardened-fallback.img
									}
									menuentry 'Arch Linux, with Linux linux' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-linux-advanced-4f4ca4ad-777e-49ce-96f4-923d630cee48' {
										load_video
										set gfxpayload=keep
										insmod gzio
										insmod part_gpt
										insmod ext2
										set root='hd1,gpt2'
										if [ x$feature_platform_search_hint = xy ]; then
										  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
										else
										  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
										fi
										echo	'Loading Linux linux ...'
										linux	/boot/vmlinuz-linux root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw  loglevel=3 quiet
										echo	'Loading initial ramdisk ...'
										initrd	/boot/intel-ucode.img /boot/initramfs-linux.img
									}
									menuentry 'Arch Linux, with Linux linux (fallback initramfs)' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-linux-fallback-4f4ca4ad-777e-49ce-96f4-923d630cee48' {
										load_video
										set gfxpayload=keep
										insmod gzio
										insmod part_gpt
										insmod ext2
										set root='hd1,gpt2'
										if [ x$feature_platform_search_hint = xy ]; then
										  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,gpt2 --hint-efi=hd1,gpt2 --hint-baremetal=ahci1,gpt2  4f4ca4ad-777e-49ce-96f4-923d630cee48
										else
										  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
										fi
										echo	'Loading Linux linux ...'
										linux	/boot/vmlinuz-linux root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw  loglevel=3 quiet
										echo	'Loading initial ramdisk ...'
										initrd	/boot/intel-ucode.img /boot/initramfs-linux-fallback.img
									}
								}

								### END /etc/grub.d/10_linux ###

								### BEGIN /etc/grub.d/15_ostree ###
								### END /etc/grub.d/15_ostree ###

								### BEGIN /etc/grub.d/20_linux_xen ###
								### END /etc/grub.d/20_linux_xen ###

								### BEGIN /etc/grub.d/25_bli ###
								if [ "$grub_platform" = "efi" ]; then
								  insmod bli
								fi
								### END /etc/grub.d/25_bli ###

								### BEGIN /etc/grub.d/30_os-prober ###
								### END /etc/grub.d/30_os-prober ###

								### BEGIN /etc/grub.d/30_uefi-firmware ###
								if [ "$grub_platform" = "efi" ]; then
									fwsetup --is-supported
									if [ "$?" = 0 ]; then
										menuentry 'UEFI Firmware Settings' $menuentry_id_option 'uefi-firmware' {
											fwsetup
										}
									fi
								fi
								### END /etc/grub.d/30_uefi-firmware ###

								### BEGIN /etc/grub.d/40_custom ###
								# This file provides an easy way to add custom menu entries.  Simply type the
								# menu entries you want to add after this comment.  Be careful not to change
								# the 'exec tail' line above.
								### END /etc/grub.d/40_custom ###

								### BEGIN /etc/grub.d/41_custom ###
								if [ -f  ${config_directory}/custom.cfg ]; then
								  source ${config_directory}/custom.cfg
								elif [ -z "${config_directory}" -a -f  $prefix/custom.cfg ]; then
								  source $prefix/custom.cfg
								fi
								### END /etc/grub.d/41_custom ###
							[/file]

							INCOMPLETE FILE:
							[file: /boot/grub/grub.cfg]
								menuentry 'Arch Linux (ZEN on /dev/sdb2(SSD)) with intel-ucode.img From ArchLinux(Linux-ZEN MITIGATIONS OFF(new)) GRUB' --class arch --class gnu-linux --class gnu --class os $menuentry_id_option 'osprober-gnulinux-simple-555d2084-401a-4002-8217-61547c0eedab' {
									load_video
									set gfxpayload=keep
									insmod part_msdos
									insmod ext2
									set root='hd1,msdos2'
									if [ x$feature_platform_search_hint = xy ]; then
									  search --no-floppy --fs-uuid --set=root --hint-bios=hd1,msdos2 --hint-efi=hd1,msdos2 --hint-baremetal=ahci1,msdos2 4f4ca4ad-777e-49ce-96f4-923d630cee48
									else
									  search --no-floppy --fs-uuid --set=root 4f4ca4ad-777e-49ce-96f4-923d630cee48
									fi
									linux /boot/vmlinux-linux-zen root=UUID=4f4ca4ad-777e-49ce-96f4-923d630cee48 rw quiet mitigations=off zswap.enabled=0 intel_iommu=on psi=1 nvidia_drm.modeset=1 systemd.gpt_auto=0
									initrd /boot/intel-ucode.img /boot/initramfs-linux-zen.img
								}
							[/file]

/etc/firewalld/firewalld.conf				# Firewalld Configuration File
							[file: /etc/firewalld/firewalld.conf]
								# firewalld config file

								# default zone
								# The default zone used if an empty zone string is used.
								# Default: public
								DefaultZone=public

								# Clean up on exit
								# If set to no or false the firewall configuration will not get cleaned up
								# on exit or stop of firewalld.
								# Default: yes
								CleanupOnExit=yes

								# Clean up kernel modules on exit
								# If set to yes or true the firewall related kernel modules will be
								# unloaded on exit or stop of firewalld. This might attempt to unload
								# modules not originally loaded by firewalld.
								# Default: no
								CleanupModulesOnExit=no

								# IPv6_rpfilter
								# Performs reverse path filtering (RPF) on IPv6 packets as per RFC 3704.
								# Possible values:
								#   - strict: Performs "strict" filtering as per RFC 3704. This check
								#             verifies that the in ingress interface is the same interface
								#             that would be used to send a packet reply to the source. That
								#             is, ingress == egress.      
								#   - loose: Performs "loose" filtering as per RFC 3704. This check only
								#            verifies that there is a route back to the source through any
								#            interface; even if it's not the same one on which the packet
								#            arrived.
								#   - strict-forward: This is almost identical to "strict", but does not perform
								#                     RPF for packets targeted to the host (INPUT).
								#   - loose-forward: This is almost identical to "loose", but does not perform
								#                    RPF for packets targeted to the host (INPUT).
								#   - no: RPF is completely disabled.
								#
								# The rp_filter for IPv4 is controlled using sysctl.
								# Note: This feature has a performance impact. See man page FIREWALLD.CONF(5)
								# for details.
								# Default: strict
								IPv6_rpfilter=strict

								# IndividualCalls
								# Do not use combined -restore calls, but individual calls. This increases the
								# time that is needed to apply changes and to start the daemon, but is good for
								# debugging.
								# Default: no
								IndividualCalls=no

								# LogDenied
								# Add logging rules right before reject and drop rules in the INPUT, FORWARD
								# and OUTPUT chains for the default rules and also final reject and drop rules
								# in zones. Possible values are: all, unicast, broadcast, multicast and off.
								# Default: off
								LogDenied=off

								# FirewallBackend
								# Selects the firewall backend implementation.
								# Choices are:
								#	- nftables (default)
								#	- iptables (iptables, ip6tables, ebtables and ipset)
								# Note: The iptables backend is deprecated. It will be removed in a future
								# release.
								FirewallBackend=nftables

								# FlushAllOnReload
								# Flush all runtime rules on a reload. In previous releases some runtime
								# configuration was retained during a reload, namely; interface to zone
								# assignment, and direct rules. This was confusing to users. To get the old
								# behavior set this to "no".
								# Default: yes
								FlushAllOnReload=yes

								# ReloadPolicy
								# Policy during reload. By default all traffic except for established
								# connections is dropped while the rules are updated. Set to "DROP", "REJECT"
								# or "ACCEPT". Alternatively, specify it per table, like
								# "OUTPUT:ACCEPT,INPUT:DROP,FORWARD:REJECT".
								# Default: ReloadPolicy=INPUT:DROP,FORWARD:DROP,OUTPUT:DROP
								ReloadPolicy=INPUT:DROP,FORWARD:DROP,OUTPUT:DROP

								# RFC3964_IPv4
								# As per RFC 3964, filter IPv6 traffic with 6to4 destination addresses that
								# correspond to IPv4 addresses that should not be routed over the public
								# internet.
								# Defaults to "yes".
								RFC3964_IPv4=yes

								# StrictForwardPorts
								# If set to yes, the generated destination NAT (DNAT) rules will NOT accept
								# traffic that was DNAT'd by other entities, e.g. docker. Firewalld will be
								# strict and not allow published container ports until they're explicitly
								# allowed via firewalld.
								# If set to no, then docker (and podman) integrates seamlessly with firewalld.
								# Published container ports are implicitly allowed.
								# Defaults to "no".
								StrictForwardPorts=no

								# NftablesFlowtable
								# This may improve forwarded traffic throughput by enabling nftables flowtable.
								# It is a software fastpath and avoids calling nftables rule evaluation for
								# data packets. This only works for TCP and UDP traffic.
								# The value is a space separated list of interfaces.
								# Example value "eth0 eth1".
								# Defaults to "off".
								NftablesFlowtable=enp3s0 eth0

								# NftablesCounters
								# If set to yes, add a counter to every nftables rule. This is useful for
								# debugging and comes with a small performance cost.
								# Defaults to "no".
								NftablesCounters=no

								# NftablesTableOwner
								# If set to yes, the generated nftables rule set will be owned exclusively by
								# firewalld. This prevents other entities from mistakenly (or maliciously)
								# modifying firewalld's rule set. If you intentionally modify firewalld's
								# rules, then you will have to set this to "no".
								# Defaults to "yes".
								NftablesTableOwner=yes
							[/file]

/etc/rc.conf						#Runtime Configuration files
/etc/pacman.conf					#Archlinux Package Manage configuration file
/etc/mkinitcpio.conf					#Configuration file used when generating initramfs using mkinitcpio
								ex.: mkinitcpio -P

/etc/passwd						#User and system users
								can be used to set default shell system for a given user

/etc/group						#group and system groups

~/.bashrc						#user default configuration for it's bash shell 
							^ can be used to set user ENVIRONMENT VARIABLES,
							and also to execute user bash scripts(.sh files) for this User's Shell.
							can also be set to load other shell environments, and keyboard settings, along other things
							and user preferences.

~/.bash_profile						#Loads/Sources on .bashrc only once during login on system boot
							#While .bashrc loads everytime a shell session is open for that given user.
							^ You can quick test modifications to ~/.bash_profile by using ctrl+alt+f3
							to initialize another virtual terminal in the system

~/.zshrc						#Same as ~/.bashrc
~/.zprofile						#Same as ~/.bash_profiles, should be used instead of ~/.zshenv

/etc/conf.d/rngd					#Rngd config file for rng-tools

/etc/udev/rules.d/					#Directory for UDEV rules configuration files
								ex.:	[file: /etc/udev/rules.d/99-zram.rules]
										KERNEL=="zram0", ATTR{priority}="100", ATTR{disksize}="4GB", RUN="/usr/bin/mkswap /dev/zram0", TAG+="systemd"
									[/file; file name numbering matters! this file may fail]
								^VERY IMPORTANT: Udev rules make system boot much slower!
								

VERY IMPORTANT: DO NOT SOURCE FROM ZPROFILE OR BASH_PROFILE!!!!!!!!!!!!!

/etc/pam.d/password-auth				#Login Authorization Settings
/etc/pam.d/system-auth					#Login Authorization Settings

/etc/profile.d/[filename].sh				#can be used to drop Global .SH Scripts that will be automatically run
							by bash.

~/.steam/root/compatibilitytools.d/			#default steam folder for proton, proton-ge and proton-tkg
/usr/share/steam/compatibilitytools.d/			^ same as above, add proton, proton-ge, proton-tkg here if you want more multi-users to access it.


/usr/bin/						#All your sistem and user binaries are located here
/bin/							#Symlink to /usr/bin for easier access

/var/log/Xorg.0.log					#Xorg.0.log
/dev/shm						#User Accessible Ram-Disk

---------
---------
---------
---------
---------
---------
more on zsh:
There are five startup files that zsh will read commands from:

$ZDOTDIR/.zshenv
$ZDOTDIR/.zprofile
$ZDOTDIR/.zshrc
$ZDOTDIR/.zlogin
$ZDOTDIR/.zlogout

If ZDOTDIR is not set, then the value of HOME is used; this is the usual case.

`.zshenv' is sourced on all invocations of the shell, unless the -f option is set. It should contain commands to set the command search path, plus other important environment variables. `.zshenv' should not contain commands that produce output or assume the shell is attached to a tty.

`.zshrc' is sourced in interactive shells. It should contain commands to set up aliases, functions, options, key bindings, etc.

`.zlogin' is sourced in login shells. It should contain commands that should be executed only in login shells. `.zlogout' is sourced when login shells exit. `.zprofile' is similar to `.zlogin', except that it is sourced before `.zshrc'. `.zprofile' is meant as an alternative to `.zlogin' for ksh fans; the two are not intended to be used together, although this could certainly be done if desired. `.zlogin' is not the place for alias definitions, options, environment variable settings, etc.; as a general rule, it should not change the shell environment at all. Rather, it should be used to set the terminal type and run a series of external commands (fortune, msgs, etc).

---------
----------------------------
========================================
WHAT'S VERBOSE MODE:
Verbose mode usually show extra information on the current task that is being executed,
if a console command has a verbose option, it'll likely show all the information of what it's doing 
step-by-step.

for example: cp -rv <directory_source> <directory_target>		#Will show all files as they are copied one-by-one
========================================
INTERACTIVE MODE:
Prompts the user for each task being executed by the given console-command.

cp -ri will likely ask if user wants to copy each of the files
========================================
---ABOUT APPIMAGE AND FUSE:
Download nvim.appimage
Run chmod u+x nvim.appimage && ./nvim.appimage

	IF YOUR SYSTEM DOES NOT HAVE FUSE YOU CAN EXTRACT THE APPIMAGE:
		./nvim.appimage --appimage-extract
		./squashfs-root/usr/bin/nvim
========================================
WHY ROADMAPS ARE IMPORTANT:
In Software development, making and looking at future roadmaps is important,
because it helps to create a system that will not be able to expand without a total complete re-engeneering.
========================================
TODO LATER: 
Add a Section for /ETC/FSTAB;
Change the default text-editor for the terminal;
Check if it's possible to create ENV PATH VARS as ARRAY;
Study more about Shells: ZSHELL GLOBBING IS VERY USEFUL
Add a section for PKGBUILD
---------------------------------------
MOZILLA FIREFOX TIPS

	USING AND MANAGING DIFFERENT PROFILES ON MOZILLA
		This might come in handy if you like using RamDisk:
			$ firefox -P

		Note: Profiles are located under  ~/.mozilla/ 

	MOZILLA FIREFOX HOTKEYS

	F7			#Enables Caret Mode: Allows cursor to be set on text when clicked | ENABLING CARET MODE IN FIREFOX
	alt+<numbers>		#Changes tab
	ctrl + mouse_click	#Opens link in new tab



---------------------------------------
BROWSER/OPERA HOTKEYS 2020:
ctrl+u			#Erases entire line
ctrl+f			#Search text in open document


alt+mouse_click		#Opens link in a new tab and changes current open page to it
ctrl+mouse_click	#Silently opens link in a new tab
shift+mouse_click   	#Opens link on an entirely new Window

VIVALDI HOTKEYS:
hold right_click + left_click 	#Move to next page in history
hold left_click + right_click   #Move to previous page in history
---------------------------------------
MEMORY RAM TMP ERASE LATER:
Applications on memory: kdenlive, nvim 
========================================
SYSTEM ENVIRONMENT VARIABLES:
SET AN ENVIRONMENT VARIABLE IN LINUX PERMANENTLY

If you wish a variable to persist after you close the shell session, you need to set it as an environmental 
variable permanently. You can choose between setting it for the current user or all users.

--------------------
PERMANENT USER ENVIRONMENT VARIABLES
In order to set environment variables, the following file must be edited: 
	/etc/environment

ex.:
	[file: /etc/environment]
		#
		# This file is parsed by pam_env module
		#
		# Syntax: simple "KEY=VAL" pairs on separate lines
		#

		##The Environment vars below are meant for NVIDIA GPUs Only
		##Erase this below if you don't own an NVIDIA GPU
		GBM_BACKEND=nvidia-drm
		__GLX_VENDOR_LIBRARY_NAME=nvidia
		#__GL_SHARPEN_ENABLE=1  #Default is 0, this is known to cause problems with vlc, glxinfo and nvidia-settings as well
		__GL_SHARPEN_VALUE=100 #Default is 50, 100 is maximum; only works with __GL_SHARPEN_ENABLE=1
		__GL_SHARPEN_IGNORE_FILM_GRAIN=17 #Default is 17, 100 is maximum; only works with __GL_SHARPEN_ENABLE=1
		__GL_THREADED_OPTIMIZATION=1
		LIBVA_DRIVER_NAME=nvidia #Supposedly Fixes some issue with VLC and Nvidia-Settings - AVOID USING THIS, NOT NEEDED - I'M CURRENTLY USING IT THO

		##VERY IMPORTANT: below are my personal compiler flags for C, C++ and RUST based on my ivybridge processor - Not Using Anylonger
		##Either erase or modify the below env. vars if you do not own an ivy bridge processor.
		#CFLAGS="-mcpu=ivybridge -march=ivybridge -mtune=ivybridge -fomit-frame-pointer -O3 -pipe"
		#CXXFLAGS="-mcpu=ivybridge -march=ivybridge -mtune=ivybridge -fomit-frame-pointer -O3 -pipe"
		#RUSTFLAGS="-C opt-level=3 -C target-cpu=ivybridge"
		PULSE_SERVER=tcp:127.0.0.1:4713	#Only use this if you're running a PULSE SERVER through Pipewire.
	[/file]
--------------------
SETTING UP SHELL ENVIRONMENT VARIABLES
	VERY IMPORTANT: Despite some people saying on the internet that this is a permanent way
to set environment variables, this isn't permanent and will make env. vars only work for console/terminal use.
actual permanent environment variables can be set on /etc/environment.

1. To set permanent environment variables for a single user, edit the .bashrc file:

	sudo nano ~/.bashrc

2. WRITE A LINE for each variable you wish to add in ~/.bashrc using the following syntax:

	export <VARIABLE_NAME>=<variable_value>

The changes will be applied after you restart the shell.

3.  If you want to apply the changes without rebooting, 
on your current working session use the source command:

	source ~/.bashrc

VERY IMPORTANT: If you want to make the VARIABLE Available to all users,
just create a file on /etc/profile.d/ with .sh extension:

	sudo nano /etc/profile.d/[filename].sh

BUT NOW The changes will only be applied on the next reboot.

-------------------------
Example of System Variables:
	export VIDEO="~/Videos/"
	export MYOWNVARIABLE="Value text" 
------
To use the Variables:
	cd $VIDEO
	echo $MYOWNVARIABLE

------
Unsetting Variables:
	unset $VIDEO
	unset $MYOWNVARIABLE


------------------------------
================================================================================
PRINTING ENVIRONMENT VARIABLES:
	$printenv | less

the "less" binary will pipeline printenv text-output and
print them by asking the user to press spacebar to continue.
less only prints data until it fits the entire screen, by
checking font-size and screen-width, it can calculate how letters
can fit on your screen.

$printenv | grep [VARIABLE_NAME]	$PRINTS VARIABLE CONTENT
	or
$echo [VARNAME]


------EXTRA:
The simplest way to set a variable using the command line is to type its name followed by a value:
	[VARIABLE_NAME]=[variable_value]

1. As an example, create a variable called EXAMPLE with a text value. If you type the command correctly, the shell does not provide any output.

2. The set | grep command confirms the creation of the variable. However, printenv does not return any output.

Setting a shell variable
This is because the variable created in this way is a shell variable.

3. Another way to confirm this is to type bash and start a child shell session. Using the echo command to search for the EXAMPLE variable now returns no output:
------------------------------
================================================================================
------------------------------
================================================================================
------------------------------
================================================================================
MORE ABOUT RAMDISK FILESYSTEMS: [VERY OLD]
Because the storage is in RAM, it is volatile memory, which means it will be lost in the event of power loss, whether intentional (computer reboot or shutdown) or accidental (power failure or system crash). This is, in general, a weakness (the data must periodically be backed up to a persistent-storage medium to avoid loss), but is sometimes desirable: for example, when working with a decrypted copy of an encrypted file.

Very Important: In many cases, the data stored on the RAM drive is CREATED FROM DATA PERMANENTLY STORED ELSEWHERE, for faster access, and is RE-CREATED on the RAM drive when the system reboots.

VERY IMPORTANT: volatility is an advantage if security requires sensitive data to not be stored permanently, and to prevent accumulation 
of obsolete temporary data. Data can be copied between conventional mass storage and a RAM drive to preserve it on power-down and load 
it on start-up.

--------
RAM DRIVES ON LINUX SYSTEMS:
	Modern Linux systems come pre-installed with a user-accessible ramdisk mounted at /dev/shm.
	2024 Update Note: Never use /dev/shm as Ramdisk, it's meant for something else.

If you need a ramdisk tool, use Zicroram, it's on github.
----------
RAPID DISK OPEN SOURCE PROJECT:
RapidDisk is a free and open source project containing a Linux kernel module and administration utility that functions similar to the 
Ramdiskadm on the Solaris (operating system). With the rxadm utility, the user is capable of dynamically attaching, removing, and 
resizing RAM disk volumes and treat them like any other block device.
----------
RAMDisk:
Free and open-source utility that allows using RAM as a folder.
----------
TMPFS and RAMFS:
Since I've previously stated in this document how to mount filesystem type of tmpfs, i will just be explainning about the differences
between RAMFS and TMPFS:

There are 2 differences between tmpfs and ramfs:
1) the mounted space of ramfs is theorically infinite, as ramfs will grow if needed, which can easily cause system lockup or 
crash for using up all available memory, or start heavy swapping to free up more memory for the ramfs. For this reason limiting 
the size of a ramfs area can be recommendable.

2) tmpfs is backed by the computer's swap space, so if it detects any swap filesystems on the disc, it'll use the Hard-Drive disk instead
of the swap space. - VERY IMPORTANT

There are also many "wrappers" for the RAM disks for Linux as Profile-sync-daemon (psd) and many others allowing users to 
utilize RAM disk for desktop application speedup moving intensive IO for caches into RAM.
------------
RAM DRIVE SOFTWARES ON WINDOWS: [VERY OLD]
RAMDISK.SYS SAMPLE DRIVER FOR WINDOWS 2000
	Microsoft Windows offers a 'demonstration' RAM disk for Windows 2000 as part of the Windows Driver Kit. 
Limited to using the same physical RAM as the operating system. It is available as free download with source code.[24]

RAMDISK SAMPLE FOR WINDOWS 7/8
	Microsoft provides source code for a RAM disk driver for Windows 7 and 8 [25]

WINDOWS NATIVE
	Windows also has a rough analog to tmpfs in the form of "temporary files". Files created with both FILE_ATTRIBUTE_TEMPORARY 
and FILE_FLAG_DELETE_ON_CLOSE are held in memory and only written to disk if the system experiences high memory pressure.
In this way they behave like tmpfs, except the files are written to the specified path during low memory situations, rather 
than to swap space. This technique is often used by servers along with TransmitFile to render content to a buffer before sending 
to the client. - VERY IMPORTANT

------------------------------
RAM DRIVE SOFTWARES ON DOS: [VERY OLD]
	FreeDOS includes SRDISK
	MS-DOS 3.2 includes RAMDRIVE.SYS
	PC DOS 3.0 includes VDISK.SYS
	DR-DOS included VDISK.SYS
	Multiuser DOS included an automatic RAM disk as drive M
------------------------------
RAM DRIVE SOFTWARE FEATURES: [VERY OLD]
Features that vary from one package to another:

1 - Some RAM drives automatically back up contents on normal mass storage on power-down, and load them when the computer is started. If this functionality is not provided, contents can always be preserved by start-up and close-down scripts, or manually if the operator remembers to do so.

2 - Some software allows several RAM drives to be created; other programs support only one.

3 - Some RAM drives when used with 32-bit operating systems (particularly 32-bit Microsoft Windows) on computers with IBM PC architecture allow memory above the 4 GB point in the memory map, if present, to be used; this memory is unmanaged and not normally accessible. Software using unmanaged memory can cause stability problems.

4 - Some RAM drives are able to use any 'unmanaged' or 'invisible' RAM below 4 GB in the memory map (known as the 3 GB barrier) i.e. RAM in the 'PCI hole'. Note: Do not assume that RAM drives supporting 'AWE' (or Address Windowing Extensions) memory above 4 GB will also support unmanaged PAE (or Physical Address Extension) memory below 4 GB—most don't.

------------------------------
MORE ABOUT /DEV/SHM: - VERY IMPORTANT
/dev/shm is nothing but implementation of traditional shared memory concept. It is an efficient means of passing data between programs. 
One program will create a memory portion, which other processes (if permitted) can access. This will result into speeding up things on Linux.

If you own virtual machines in your system, server oriented files/aplications, this is a great way to speed up your system. - VERY IMPORTANT

shm / shmfs is also known as tmpfs, which is a common name for a temporary file storage facility on many Unix-like operating systems. 
It is intended to appear as a mounted file system, but one which acts as a temporary virtual disk instead of a persistent storage device.

By typing df -h you can list all physical and virtual filesystems you have on your system:

FILESYSTEM		      SIZE  USED AVAIL USE% MOUNTED ON
/dev/mapper/wks01-root	      

			      444G   70G  351G  17% /
tmpfs			      3.9G     0  3.9G   0% /lib/init/rw
udev			      3.9G  332K  3.9G   1% /dev
tmpfs			      3.9G  168K  3.9G   1% /dev/shm
/dev/sda1		      228M   32M  184M  15% /boot
....

In this example, you will give you tmpfs instance on /disk2/tmpfs which can allocate 8GB RAM/SWAP in 5K inodes and 
it is only accessible by root:
	#mount -t tmpfs -o size=8G,nr_inodes=5k,mode=700 tmpfs /disk2/tmpfs

Where: 
	-o <opt1>, [opt2], [opt3]	#Allows you to pass various mount options

		size=8G			#Overrides the default maximum size of /dev/shm
		
		nr_inodes=5k		#https://mjmwired.net/kernel/Documentation/filesystems/tmpfs.txt
		
		mode=700		#sets access mode to root only			

More Information on: https://mjmwired.net/kernel/Documentation/filesystems/tmpfs.txt


in order for the mount type to work, you'll need to add it to the /etc/fstab file,
the reason is because your folder isn't actually a hardware device in the system like the ones in /sys/dev/,
because of this you'll need to manually add it as a device on the fstab file. so do as follows:

1 - Open /etc/fstab:
	/dev/shm        tmpfs   defaults,size=8G        0 0   #- CHECK NOT TESTED

2 - Type in on your Shell Session for the changes to take immediate system effects:
	#mount -o remount /dev/shm

----------------------
PAGE CACHE == DISK CACHE
https://en.wikipedia.org/wiki/Page_cache

In computing, a page cache, sometimes also called disk cache, is a transparent cache for the pages originating from a secondary storage device such as a hard disk drive (HDD) or a solid-state drive (SSD). The operating system keeps a page cache in otherwise unused portions of the main memory (RAM), resulting in quicker access to the contents of cached pages and overall performance improvements. A page cache is implemented in kernels with the paging memory management, and is mostly transparent to applications.

Usually, all physical memory not directly allocated to applications is used by the operating system for the page cache. Since the memory would otherwise be idle and is easily reclaimed when applications request it, there is generally no associated performance penalty and the operating system might even report such memory as "free" or "available".

-----------------------
MEMORY PAGING OR MEMORY MANAGEMENT [Very Old]
In computer operating systems, memory paging is a memory management scheme by which a computer stores and retrieves data from secondary storage for use in main memory. In this scheme, the operating system retrieves data from secondary storage in same-size blocks called pages. Paging is an important part of virtual memory implementations in modern operating systems, using secondary storage to let programs exceed the size of available physical memory.

For simplicity, main memory is called "RAM" (an acronym of "random-access memory") and secondary storage is called "disk" (a shorthand for "hard disk drive, drum memory or solid-state drive"), but the concepts do not depend on whether these terms apply literally to a specific computer system.
----------------------------------------
---------------------------
------------------------------
================================================================================
INODE:
An inode is a data structure in UNIX operating systems that contains important information pertaining to files within a file system. When a file system is created in UNIX, a set amount of inodes is created, as well. Usually, about 1 percent of the total file system disk space is allocated to the inode table.

Sometimes, people interchange the terms inode and inumber. The terms are similar and do correspond to each other, but they don’t refer to the same things. Inode refers to the data structure; the inumber is actually the identification number of the inode — hence the term inode number, or inumber. The inumber is only one important item of information for a file. Some of the other attributes in an inode are discussed in the next section.

The inode table contains a listing of all inode numbers for the respective file system. When users search for or access a file, the UNIX system searches through the inode table for the correct inode number. When the inode number is found, the command in question can access the inode and make the appropriate changes if applicable.

Take, for example, editing a file with vi. When you type vi <filename>, the inode number is found in the inode table, allowing you to open the inode. Some attributes are changed during the edit session of vi, and when you have finished and typed :wq, the inode is closed and released. This way, if two users were to try to edit the same file, the inode would already have been assigned to another user ID (UID) in the edit session, and the second editor would have to wait for the inode to be released.

------------------------------
THE INODE STRUCTURE

The inode structure is relatively straightforward for seasoned UNIX developers or administrators, but there may still be some surprising information you don’t already know about the insides of the inode. The following definitions provide just some of the important information contained in the inode that UNIX users employ constantly:

	1 - Inode number
	2 - Mode information to discern file type and also for the stat C function
	3 - Number of links to the file
	4 - UID of the owner
	5 - Group ID (GID) of the owner
	6 - Size of the file
	7 - Actual number of blocks that the file uses
	8 - Time last modified
	9 - Time last accessed
	10 - Time last changed

Basically, the inode contains all information about a file outside of the actual name of the file and the actual data content of the file. The full inode structure can be found in the header file /usr/include/jfs/ino.h in AIX or on the Web at http://publib.boulder.ibm.com/infocenter/systems/index.jsp?topic=/com.ibm.aix.files/doc/aixfiles/inode.h.htm.

The information listed above is important to files and is used heavily in UNIX. Without this information, a file would appear corrupt and unusable.

Directories and files may appear different on UNIX systems compared to other operating systems, but they aren’t. In UNIX, directories are actually files that have a few additional settings in their inodes. A directory is basically a file containing other files. Also, the mode information has flags set to inform the system that the file is actually a directory.
----------------------------------------
================================================================================
LOGIN LOCKDOWNS:
Lockdowns can happen in 2 cases:
	1 - Taking too long to input a password - NEEDS TO CHECK
	2 - Inputting the Wrong Password too many times
	3 - Too many requests for inputting password made

Solution A)
	in both cases the account will be locked down from the system for a given period of time configured in:
	/etc/pam.d/password-auth & /etc/pam.d/system-auth

After making changes to those files, make sure to: systemctl restart sshd

Solution B)
	1 - login as root using 'su', since sudo is already locked.
	2 - type in: #faillock --reset --user <username>


------------------------------
================================================================================
INTRODUCTION TO SHELL SESSIONS:

	Each user in the system can have their own interactive shell session,
linux defaults to using BASH(Bourne-Again Shell), but it's possible to use other shell 

Shells are a UNIX Command interpreter, they're executed as soon as you make your first login 
on the system.

All user shells are maintained in the following file: 
	/etc/passwd


.RC Files are used to load local user settings everytime a shell session is open,
for example, by oppening a terminal program like xterm, gnome-terminal, or even cool-retro-term,
a user shell session is initiated and all it's .rc files are loaded every single time.


.Profile File settings are also maintained to enable certain settings to be loaded only upon the first login in the system!

Each Shell Program has their own naming convention for those files, and might as well differ in location:

	1 - Bash 
		uses ~/.bashrc and ~/.bash_profile
	2 - ZSH
		uses ~/.zshrc, ~/.zprofile
		optionally provides ~/.zshenv for setting up Environment Variables.

Upon modifications of the .rc files, the user needs to restart computer, or invoke the source command:
	source ~/.zshrc or ~/.bashrc

Very Important: NEVER SOURCE THE .PROFILE FILES, SINCE THEY NEED TO BE INITIALIZED AT BOOT ONLY ONCE
for testing .Profile files, switch your virtual terminal by pressing: 
	ctrl+alt+f3

To return use ctrl+alt+f1.
---------------------
SHELL TROUBLESHOOTING:


1. All default User Shells can be changed on /etc/passwd

2. Some programs like NVIM might default to using the BASH Shell instead of using whatever the user
might be using in /etc/passwd, to circumvent this append the following line to your main .rc file:
	<shellname>

example: on ~/.bashrc the last line should look like: zsh
obs.: if your system defaults to a different shell, you must use the proper .rc file instead of ~/.bashrc
(although nvim doesn't do that, read the following scheme)

3. NVIM defaults to using a different Shel Session, because when loading saved sessions using either nvim -S session_name or :source session_name,
the session actually saves what shell you were using before! in order to change that, you'll need to change the session file for vim!

4. Some programs like youtube-dl might stop working after you move in to a new shell,
that's because those programs likely created a Environment Variable as a shell script for the default linux shell(bash),
to circumvent this issue, you can create another Environment Variable in your rc. files of your shell application.
ZSH offers an optional ~/.zshenv file just for setting and sourcing environment variables

5. Programs like Youtube-dl might also present as a problem itself not only to different shell environments, but also to different OSes.
the problem happens because youtube uses the following '?' in all their links, which might be a reserved character is some shells and OSes.
To solve this problem, make sure to add single or double quotes when inputing a link to youtube-dl.

------------------------------
================================================================================
TERMINAL EMULATOR
A terminal emulator, terminal application, or term, is a computer program that emulates a video terminal within some other display architecture. Though typically synonymous with a shell or text terminal, the term terminal covers all remote terminals, including graphical interfaces. A terminal emulator inside a graphical user interface is often called a terminal window.

A terminal window allows the user access to a text terminal and all its applications such as command-line interfaces (CLI) and text user interface (TUI) applications. These may be running either on the same machine or on a different one via telnet, ssh, or dial-up. On Unix-like operating systems, it is common to have one or more terminal windows connected to the local machine.

Terminals usually support a set of escape sequences for controlling color, cursor position, etc. Examples include the family of terminal control sequence standards known as ECMA-48, ANSI X3.64 or ISO/IEC 6429.

----List of terminal emulators:
Gnome-Terminal, xterm(XFCE), xfce4-terminal(XFCE4).

COMMAND-LINE INTERFACE:

	1. Linux console – implements a large subset of the VT102 and ECMA-48/ISO 6429/ANSI X3.64 escape sequences.
	   The following terminal emulators run inside of other terminals, utilizing libraries such as Curses and Termcap:
	2. GNU Screen – Terminal multiplexer with VT100/ANSI terminal emulation
	3. Minicom – text-based modem control and terminal emulation program for Unix-like operating systems
	4. tmux – Terminal multiplexer with a feature set similar to GNU Screen

GRAPHICAL:
Terminal emulators used in combination with X Window System and Wayland:

	xterm – standard terminal for X11
	Alacritty – GPU accelerated, without tabs
	Kitty – GPU accelerated, with tabs, tiling, image viewing, interactive unicode character input
	GNOME Terminal – default terminal for GNOME with native Wayland support
	guake – drop-down terminal for GNOME
	konsole – default terminal for KDE
	xfce4-terminal – default terminal for Xfce with drop-down support
	Terminator – written in Java with many novel or experimental features
	Terminology – enhanced terminal supportive of multimedia and text manipulation for X11 and Linux framebuffer
	Tilda – a drop-down terminal
	Yakuake – (Yet Another Kuake) a drop-down terminal for KDE
	rxvt – lightweight X11 terminal emulator
		aterm (from rxvt 2.4.8) created for use with the AfterStep window manager (no longer maintained)
		Eterm (from rxvt 2.21) created for use with Enlightenment
		mrxvt (from rxvt 2.7.11) created for multiple tabs and additional features (latest version released in 2008-09-10)
		urxvt (from rxvt 2.7.11) created to support Unicode, also known as rxvt-unicode
		Wterm – created for NeXTSTEP style window managers such as Window Maker


MAC OS:
Terminal emulators used on macOS

	Terminal – default macOS terminal
	iTerm2 – open-source terminal specifically for macOS
	xterm – default terminal when X11.app starts
	MacWise
	SecureCRT
	Terminator
	ZOC
	ZTerm – serial line terminal

APPLE CLASSIC MAC OS:

	MacTerminal
	ZTerm

MICROSOFT WINDOWS
	AbsoluteTelnet
	AlphaCom
	ConEmu – local terminal window that can host console application developed either for WinAPI (cmd, powershell, far) or 
	         Unix PTY (cygwin, msys, wsl bash)
	HyperACCESS (commercial) and HyperTerminal (included free with Windows XP and earlier, but not included with Windows Vista and later)
	Kermit 95
	mintty – Cygwin terminal
	Procomm Plus
	PuTTY
	Qmodem Pro
	RUMBA
	SecureCRT
	Tera Term
	TtyEmulator
	Windows Console – Windows command line terminal
	Windows Terminal
	ZOC

MICROSOFT MS-DOS:
	Crosstalk
	Kermit
	ProComm
	Qmodem and Qmodem Pro
	Synchronet
	Telix
	Terminate

IBM OS/2:
	Kermit 95
	ZOC – discontinued support for OS/2

COMMODORE AMIGA:
	NComm

COMMODORE 64:
	CBterm/C64
------------------------------
================================================================================
VIRTUAL TERMINAL(VT) / VIRTUAL CONSOLE(VC):
	A terminal emulator, terminal application, or term, is a computer program that emulates a video terminal within some other display architecture. Though typically synonymous with a shell or text terminal, the term terminal covers all remote terminals, including graphical interfaces. A terminal emulator inside a graphical USER INTERFACE runs as a new TERMINAL WINDOW. - (To summon it up, VTs/VCs are not Terminal Emulators)

A terminal window allows the user access to a text terminal and all its applications such as command-line interfaces (CLI) and text user interface (TUI) applications. These may be running either on the same machine or on a different one via telnet, ssh, or dial-up. On Unix-like operating systems, it is common to have one or more TERMINAL WINDOWS connected to the local machine.

Terminals usually support a set of escape sequences for controlling color, cursor position, etc. Examples include the family of terminal control sequence standards known as ECMA-48, ANSI X3.64 or ISO/IEC 6429.

------------------------------
CHANGING BETWEEN VIRTUAL TERMINALS:
(Very Important)

	In the Linux console and other platforms, usually the first six virtual consoles provide a text terminal with a login 
prompt to a Unix shell. THE GRAPHICAL X WINDOW SYSTEM STARTS IN THE SEVENTH VIRTUAL CONSOLE: 

In Linux, the user switches between them by pressing the Alt key combined with a function key:
	1 - Alt + F1 to access the virtual console number 1. 
	2 - Alt + ← changes to the previous virtual console and Alt + → to the next virtual console. 

To switch from the X Window System or a Wayland compositor:
	1 - Ctrl + Alt + F3 works. (Note that users can redefine these default key combinations.)


------------------------------
GNU SCREEN:
	If several sessions of the X Window System are required to run in parallel, such as in the case of fast user switching or when debugging X programs on a separate X server, each X session usually runs in a separate virtual console. GNU Screen is a program that can change between several text-mode programs in one textual login.

----------------------------------------
VIRTUAL CONSOLE INTERFACES:
The virtual consoles are represented by device special files like /dev/tty1, /dev/tty2, etc. 
There are also other special files like /dev/console, /dev/tty and /dev/tty0. 
(Compare the devices using the patterns vcs ("virtual console screen") and vcsa ("virtual console screen with attributes") such as /dev/vcs1 and /dev/vcsa1) 

The virtual consoles can be configured in the file /etc/inittab read by INIT -- typically it starts the text mode login process 
getty for several virtual consoles. X Window System can be configured in /etc/inittab or by an X display manager. 

A number of Linux distributions use SYSTEMD instead of INIT, which also allows virtual console configuration.

--------------------------
PROGRAMS USED TO ACCESS THE VIRTUAL CONSOLES:
(Very Important)

	1 - chvt to switch the current virtual console
		Linux command chvt to switch vt from cmdline
	2 - openvt to run a program on a new virtual console
	3 - deallocvt to close a currently unused virtual console

The program startx starts the X Window System on a new virtual console. There are also other graphical programs 
that can start from the console, such as LinuxTV and MPlayer.

------------------------------
PURPOSE OF VIRTUAL TERMINALS:
Unix workstations, such as those manufactured by Sun or Silicon Graphics, did not include virtual consoles. 
The only purpose of a console would be to fix the system so that the graphical environment could start.

------------------------------
PSEUDOTERMINAL / PSEUDOTTY / PTY / PTS / PTYS:
PTYs is a pair of pseudo-devices:
	
	1 - Slave:
		Emulates a hardware text-terminal device

	2 - Master:
		Provides the means by which a terminal emulator process controls the slave.

The PTY feature is part of POSIX and the Single Unix Specification in the form of a posix_openpt() function since 1998.

----PRACTICAL APPLICATIONS OF PTTY:
The role of the terminal emulator process is:

	1 - to interact with the user,
	2 - to feed text input to the master pseudo-device for use by the shell (such as bash), which is connected to the slave pseudo-device,
	3 - to read text output from the master pseudo-device and show it to the user.

The terminal emulator process must also handle terminal control commands, e.g., for resizing the screen. 
Widely used terminal emulator programs include xterm, GNOME Terminal, Konsole, and Terminal (macOS). 
Remote login handlers such as ssh and telnet servers play the same role but communicate with a remote user instead of a local one. 
Also consider programs such as expect.

Screen and Tmux are used to add a session context to a pseudoterminal, making for a much more robust and versatile solution. 
For example, each provides terminal persistence, allowing a user to disconnect from one computer and then connect later from another computer.

----DIFFERENCE BETWEEN TTY/PTS:
PTS is usually ran by Terminal Emulators,
TTY is a local virtual console emulated by the Kernel.

Most of the terminals you'll see is a PTS and live in /dev/pts. These are also called ptys. Any sort of software terminal lives here, whether be it a
remote login via SSH or a local terminal emulator.

A TTY is a local console manipulated by the Linux kernel. This is the kind of terminal you'll use in Linux's Virtual Terminal.
Getty is the program used to show a login prompt and start a shell on these terminals.
----
TODO LATER:
	1 - Use screen or tmux to access virtual terminals.
	2 - Learn to use chvt openvt deallocvt
	3 - LinuxTV and MPlayer can also be started from command-line
	4 - Research into Graphical Terminals
----------------------------------------
TERMINAL MODES:
Terminals can operate in various modes, relating to when they send input typed by the user on the keyboard to the receiving system (whatever that may be):
(Very Important)

1. - Character mode (a.k.a. character-at-a-time mode): In this mode, typed input is sent immediately to the receiving system.

2. - Line mode (a.k.a. line-at-a-time mode): In this mode, the terminal provides a local line editing function, and sends an entire input line, 
after it has been locally edited, when the user presses a return key. A so-called "line mode terminal" operates solely in this mode.

2. - Block mode (a.k.a. screen-at-a-time mode): In this mode (also called block-oriented), the terminal provides a local full-screen data function. The user can enter input into multiple fields in a form on the screen (defined to the terminal by the receiving system), moving the cursor around the screen using keys such as Tab ↹ and the arrow keys and performing editing functions locally using insert, delete, ← Backspace and so forth. The terminal sends only the completed form, consisting of all the data entered on the screen, to the receiving system when the user presses an ↵ Enter key.

There is a distinction between the return and the ↵ Enter keys. In some multiple-mode terminals, that can switch between modes, 
pressing the ↵ Enter key when not in block mode does not do the same thing as pressing the return key. Whilst the return key will 
cause an input line to be sent to the host in line-at-a-time mode, the ↵ Enter key will rather cause the terminal to transmit 
the contents of the character row where the cursor is currently positioned to the host, host-issued prompts and all.

Different computer operating systems require different degrees of mode support when terminals are used as computer terminals. 
The POSIX terminal interface, as provided by Unix and POSIX-compliant operating systems, does not accommodate block-mode terminals 
at all, and only rarely requires the terminal itself to be in line-at-a-time mode, since the operating system is required to 
provide canonical input mode, where the terminal device driver in the operating system emulates local echo in the terminal, 
and performs line editing functions at the host end. Most usually, and especially so that the host system can support non-canonical 
input mode, terminals for POSIX-compliant systems are always in character-at-a-time mode. In contrast, IBM 3270 terminals connected 
to MVS systems are always required to be in block mode.

------------------------------
================================================================================
MY CUSTOM GDM CONF:
/etc/gdm/custom.conf

has the following custom line: 
[code]
	[daemon]
	WaylandEnable=false
[/code]

this helps with faster boot
------------------------------
================================================================================
MY CUSTOM I3 CONF:
~/.config/i3/config    or  ~/.i3/config

Changed this line:
	bindsym $mod+Return exec i3-sensible-terminal

To this line:
	bindsym $mod+Return exec terminology

since this is a GPU Accelerated Terminal Emulator, it can be broken if there is no proprietary drivers around
------------------------------
================================================================================
MY CUSTOM UDEV RULE:

1 - archlinx% cat /etc/udev/rules.d/50-usb_power_save.rules
ACTION=="add", SUBSYSTEM=="usb", TEST=="power/control", ATTR{power/control}="on"

2 - archlinx% cat /etc/udev/rules.d/99-zram.rules
KERNEL=="zram0", ATTR{priority}="100", ATTR{disksize}="6GB", RUN="/usr/bin/mkswap /dev/zram0", TAG+="systemd"
^ not used anymore(OLD), 4GB is the ideal

	New One(2023):
		ACTION=="add", KERNEL=="zram0", ATTR{comp_algorithm}="lz4", ATTR{disksize}="4G", RUN="/usr/bin/mkswap -U clear /dev/%k", TAG+="systemd"


Udev Attributes for zram devices can be found on /sys/block/zram0/
Use lz4 for performance, zstd for compression.
Both are VERY good at what they do, just pick one you want the most.

----------------------------------------
MY CUSTOM MODULES-LOAD.D:

1 - archlinx% cat /etc/modules-load.d/zram.conf
    zram


2 - archlinx% cat /etc/modprobe.d/zram.conf
    options zram num_devices=1
================================================================================
LIST OF LIGHTWEIGHT OSES:
	Kolibri OS:
		This power requires only a few megabyte disk space and 8MB of RAM to run. Kolibri features a rich set of applications that include word processor, image viewer, 
		graphical editor, web browser and well over 30 exciting games. Full FAT12/16/32 support is implemented, as well as read-only support for NTFS, ISO9660 and Ext2/3/4. 
		Drivers are written for popular sound, network and graphics cards.

		http://kolibrios.org/en/index

		KolibriOS is an open source operating system for 32-bit x86 computers based on MenuetOS, written entirely in Assembler/FASM.

	MenuetOS
		MenuetOS is an operating system with a monolithic preemptive, real-time kernel written in FASM assembly language. The system also includes video drivers. 
		It runs on 64-bit and 32-bit x86 architecture computers. Its author is Ville M. Turjanmaa. It has a graphical desktop, games, and networking abilities (TCP/IP stack). 
		One distinctive feature is that it fits on one 1.44 MB floppy disk. On an Intel Pentium MMX 200 MHz it has been known to boot in 5 seconds.


------------------------------
================================================================================
EXPLAINNING THE BIN FOLDER:

Once you copy new applications to /bin, they won't be instantly available through the commandline,
all program name references from /bin are loaded into memory during the initramfs, and new applications will require booting in order to be seen from commandline.

To circumvent this issue, try typing full name path, ex.: /usr/bin/myapplication
if the binary file you want to execute is on your current path, you can try ./myapplication in order to execute it,
or else your shell session might not be able to see it as if you had type the full application name as in: $myapplication
this won't work, because the shell will look for the namespace reference, and return the execution as an invalid operation.

./ will let the shell know you actually want to execute the binary that is in your current folder/directory and not the one on /bin/ or /usr/bin/

Very Important:
Because of this, it's important to copy binary to the /bin/ using the ./ operator, otherwise...:
cp mybin.out /bin/

...doing this will just copy the content(s) of /bin/mybin.out into /bin/ again!
so the correct form is:
	cp ./mybin.out /bin/

Note that this is only necessary if & when there's already a bin with the same name on /bin/ directory AND you want to copy a new instance of bin that you might've compiled/downloaded
from somewhere else. Deleting the /bin/mybin.out before replacing it with cp will also mitigate this issue.
------------------------------
================================================================================
------------------------------
================================================================================
TELLING PROGRAMS TO INIT WITH THE SYSTEM

Just enable them as a service first: 
ex.: sudo systemctl enable rngd		#Enables rng-tools to boot with the system
					^ As of kernel 6.0, performance is better off by letting the kernel handle the RNG.
					^ Disable & UNINSTALL RNGD!

if a service doesn't exist, you may be able to create one

------------------------------
================================================================================
------------------------------
================================================================================
HOW TO MAKE KDE RUN FAST

	1. Use kde/openbox
	2. System Settings > Display and Monitor > Compositor
		2.1 - Tearing Prevention: Only When Cheap
		2.2 - Latency: Force Lowest Latency
		2.3 - Rendering Backend: OpenGL 3
	
------------------------------
================================================================================
LINUX DESKTOP ENVIRONMENT COMPOSITION

Windowing System => Window Manager => Login Manager/Display Manager => Desktop Environment

1. Windowing System
	ex.: Xorg, Wayland, Mir, Surface Flinger, Quartz Compositor, Desktop Window Manager(Windows OS). 

2. Display Manager/Login Manager:
	ex.: GDM, SDDM

3. Window Manager:
	kwin, It is released as a part of KDE Plasma 5;
	openbox;
	xfwm is the window manager for the Xfce environment.
	i3-wm, has an user interface of it's own without creating a full Desktop Environment.
	dwm;
	cwm.

4. Desktop Environments:
	ex.: kfce, lxde, gnome 40, kde, enlightment,

------------------------------
================================================================================
AKONADI ERROR ON KDE DESKTOPS
	Just remove the entire local user folder for akonadi in:
		~/.local/share/akonadi

	ex.: rm -R ~/.local/share/akonadi

KDE should be able to recreate the files as needed.

------------------------------
================================================================================
VERY USEFUL HINTS:
	If you want to check for any errors on post-install on your system in order to avoid
possible performance issues, you can let journalctl run under a terminal or inside nvim using the following command:
	sudo journalctl -f

this should allow you to check for any errors as your system progress into execution.

IF possible, you should allow journalctl -f to run inside a different Virtual Terminal using the following keys:
	alt+<F4> or ctrl+alt+<F4>

just becareful with alt+f2, because that's usually reserved by the xorg displayer.


1 - WHEN BOOTING THE SYSTEM DESKTOP ENVIRONEMENT: you should still be able to use your computer through another TTY(virtual Terminal) while it's still loading
and then go back to your graphical environment once it's fully loaded!

2 - CHECKING BOOT TIMES: you can use systemd-analyze or journalctl -b-0! - VERY IMPORTANT
for using journalctl -b-0 you have to check timestamps from the following line:
	May 16 01:46:14 archlinx kernel: Command line: BOOT_IMAGE=/boot/vmlinuz-linux-zen root=UUID=555d2084-401a-4002-8217-61547c0eedab rw quiet acpi=off mitigations=off pcie_bus_perf zswap.enabled=0

UNTIL the first user login in the system:
	May 16 01:46:50 archlinx audit[359]: USER_ACCT pid=359 uid=0 auid=4294967295 ses=4294967295 msg='op=PAM:accounting grantors=pam_access,pam_unix,pam_permit,pam_time acct="<username>" exe="/usr/bin/login" hostname=archlinx addr=? terminal=/dev/>


ALTERNATIVELY, you can use $systemd-analyze: - VERY IMPORTANT
	Startup finished in 3.801s (kernel) + 1min 35.707s (userspace) = 1min 39.508s
	graphical.target reached after 20.823s in userspace


This line will let you know how long it took for the user to reach on the first graphical target:
	>>graphical.target reached after 20.823s in userspace


VERY IMPORTANT: by typing systemd-analyze blame you can check which services are taking longer in the boot process!!!!


------------------------------
================================================================================
MY SYSTEM SCRIPTS:
	
1 - Disabling USB Autosuspend, not required on desktop PCs: 

The Linux kernel can automatically suspend USB devices when they are not in use. This can sometimes save quite a bit of power, however some USB devices are not compatible with USB power saving and start to misbehave (common for USB mice/keyboards). udev rules based on whitelist or blacklist filtering can help to mitigate the problem.

The most simple and likely useless example is enabling autosuspend for all USB devices:
		/etc/udev/rules.d/50-usb_power_save.rules:
			ACTION=="add", SUBSYSTEM=="usb", TEST=="power/control", ATTR{power/control}="auto"

For disabling it:
		/etc/udev/rules.d/50-usb_power_save.rules:
			ACTION=="add", SUBSYSTEM=="usb", TEST=="power/control", ATTR{power/control}="on"

Valid values are auto for automatic autosuspend and on for disabling autosuspend (keeping your device on all the time). If you have problems, you want to set this value to on like this:
-----------



------------------------------
================================================================================
FIND/GREP:
FINDS ALL CONTROL RELATED VARIABLES IN /PROC/SYS:
	find /sys/ -name "control" -exec grep -H on {} \;
------------------------------
================================================================================
USING ZRAM AS SWAP DISK:
	ZRAM can be used as an alternative Swap Cache system

zram is a Linux kernel module that allows you to create RAM-backed block devices with up to 5:1 compression. zram devices can be used like any other block device. They are typically used to make compressed swap devices using a machines RAM. They can also be used to store /tmp and anything else one might want to store on a compressed RAM-drive.

You can find out more about the zram Kernel Module in: https://www.kernel.org/doc/html/latest/vm/zswap.html

------
ZSWAP:
	Zswap is a lightweight compressed cache for swap pages. It takes pages that are in the process of being swapped out to disk and attempts to compress them into a dynamically allocated RAM-based memory pool. Once the pool is full, data is then uncompressed and sent the actual swap device which could be a swap file or swap partition.

	Zswap basically trades CPU cycles for potentially reduced swap I/O on the disk.  This trade-off can also result in a significant performance improvement if reads from the compressed cache are faster than reads from a swap device.
------
THE DIFFERENCE BETWEEN ZRAM AND ZSWAP
	The difference between ZRAM and ZSWAP is that ZRAM is an actual Swap Device that stores swap data in RAM.
ZSWAP works like a buffer cache for a given swap device installed on the system.

There is no advantages to using ZSWAP for a given ZRAM Block Device. Because of this, it is important to disable ZSWAP,
which comes enabled by default. To disable zswap permanently, add zswap.enabled=0 to your kernel parameters in:
	/etc/boot/grub/grub.cfg

ZSWAP shall only be used with actual Swap Files or Swap Partitions.
----------------
A FEW COMMANDS:
zramctl								#Lists all zswap devices and displays how much memory it's using
								obs.: ZRAM IS NOT ZSWAP!
zramctl --output-all
ls /sys/kernel/debug/zswap					#Shows zswap debugging parameters
sudo dmesg | grep zswap						#Shows the currently in kernel loaded zswap settings: compressor / compressed memory pool alocator(compressor manager)
grep -R . /sys/module/zswap/parameters				#Shows all parameters taken by zswap
cat /proc/sys/swapiness						#Shows how much desirable swap data is before moving them into swap, the more desirable it is, the more they'll get moved into cache.
cat /sys/module/zswap/parameters/compressor			#Shows default compressor
cat /sys/module/zswap/parameters/zpool				#Shows the compressed memory manager
									^ Zswap makes use of zpool for managing the compressed memory pool.
cat /sys/module/zswap/parameters/enabled			#Shows whether Zswap is enabled or disabled

									Zswap evicts pages from compressed cache on an LRU basis to the backing swap device when the compressed pool reaches its size limit. 
									This requirement had been identified in prior community discussions.

									Whether Zswap is enabled at the boot time depends on whether the CONFIG_ZSWAP_DEFAULT_ON Kconfig option is enabled or not. 
									This setting can then be overridden by providing the kernel command line zswap.enabled= option, for example zswap.enabled=0. 
									Zswap can also be enabled and disabled at runtime using the sysfs interface. An example command to enable zswap at runtime, 
									assuming sysfs is mounted at /sys, is: echo 1 > /sys/module/zswap/parameters/enabled

cat /sys/module/zswap/parameters/accept_threshold_percent	#Shows the Hysteresis threshold


cat /sys/module/zswap/parameters/same_filled_pages_enabled      #Shows whether same filled pages are enabled or not
									^ Some of the pages in zswap are same-value filled pages (i.e. contents of the page have same value or repetitive pattern). 
									These pages include zero-filled pages and they are handled differently. During store operation, a page is checked if it is a same-value 
									filled page before compressing it. If true, the compressed length of the page is set to zero and the pattern or same-filled value is stored.

									Same-value filled pages identification feature is enabled by default and can be disabled at boot time by setting the same_filled_pages_enabled attribute to 0, 
									e.g. zswap.same_filled_pages_enabled=0. It can also be enabled and disabled at runtime using the sysfs same_filled_pages_enabled attribute.

echo zstd > /sys/module/zswap/parameters/compressor		#Sets zstd as default compressor(on the current session)
									^ When the zpool and/or compressor parameter is changed at runtime, any existing compressed pages are not modified; 
									they are left in their own zpool. When a request is made for a page in an old zpool, it is uncompressed using its original compressor. 
									Once all pages are removed from an old zpool, the zpool and its compressor are freed.
echo zbud > /sys/module/zswap/parameters/zpool			#Sets zbud as the zpool type compressor(on the current session)
echo 1 > /sys/module/zswap/parameters/enabled			#Sets swap to enabled (on the current session)
echo 80 > /sys/module/zswap/parameters/accept_threshold_percent #To set the threshold at which zswap would start accepting pages again after it becomes full, use the sysfs accept_threshold_percent attribute.
								Setting this parameter to 100 will disable the hysteresis.
									^ To prevent zswap from shrinking pool when zswap is full and there’s a high pressure on swap (this will result in flipping pages in and 
								out zswap pool without any real benefit but with a performance drop for the system), a special parameter has been introduced to implement a sort of 
								hysteresis to refuse taking pages into zswap pool until it has sufficient space if the limit has been hit. 

									^ In other words, the hysteresis is used to allow enough free space in the pool for the uncompressing of data to take place.

echo 1 > /sys/module/zswap/parameters/same_filled_pages_enabled #Enables same filled pages



cat /proc/swaps							#Shows a list of all swap partitions and their usage in the system
								^ Also shows priority
free -m								#Shows all memory block devices along with their usage in the system
---------------
ZPOOL TYPE / COMPRESSOR MANAGERS:
zbud, z3fold... etc
---------------
USING MORE THAN 1 SWAP PARTITION/DISK:

	1 - AUTOMATICALLY BIND SWAP DEVICE TO NUMA NODE
		If the system has more than one swap device AND swap device has the NODE INFORMATION, we can make use of this information to decide which swap device 
		to use in get_swap_pages() to get better performance.

		Swap devices with the same priority(a priority can be set to each SWAP Device! - VERY IMPORTANT) are create on he same NUMA Node.
		They are then used on a round-robin mode. Swap Devices on different NUMA Nodes will be set as 2nd or Nth nodes depending on their priority.

	Example: Assume 6 swap devices are going to be swapped on: 
			1 - swapA and swapB are attached to node 0, 
			2 - swapC is attached to node 1, 
			3 - swapD and swapE are attached to node 2 
			4 - swapF is attached to node3. 

		 The way to swap them on is the same as above:

			# swapon /dev/swapA
			# swapon /dev/swapB
			# swapon /dev/swapC
			# swapon /dev/swapD
			# swapon /dev/swapE
			# swapon /dev/swapF

	Then node 0 will use them in the order of:
		swapA/swapB -> swapC -> swapD -> swapE -> swapF

	node 1 will use them in the order of:
		swapC -> swapA -> swapB -> swapD -> swapE -> swapF

	node 2 will use them in the order of:
		swapD/swapE -> swapA -> swapB -> swapC -> swapF
	
	node 3 will use them in the order of:
		swapF -> swapA -> swapB -> swapC -> swapD -> swapE

	2 - IMPLEMENTATION DETAILS
		The current code uses a priority based list, swap_avail_list, to decide which swap device to use and if multiple swap devices share the same priority, they are used round robin. 
	This change here replaces the single global swap_avail_list with a per-numa-node list, i.e. for each numa node, it sees its own priority based list of available swap devices. Swap device’s 
	priority can be promoted on its matching node’s swap_avail_list.

	The current swap device’s priority is set as: user can set a >=0 value, or the system will pick one starting from -1 then downwards. The priority value in the swap_avail_list is the negated value 
	of the swap device’s due to plist being sorted from low to high. The new policy doesn’t change the semantics for priority >=0 cases, the previous starting from -1 then downwards now becomes starting 
	from -2 then downwards and -1 is reserved as the promoted value. So if multiple swap devices are attached to the same node, they will all be promoted to priority -1 on that node’s plist and will be used 
	round robin before any other swap devices.

	In other words, the priority is either set by the user, or automatically set by the system. It's just that the system uses negative values, -1 being highest priority as opposed to User Priority Set
	where 100 would be highest priority. - VERY IMPORTANT.

SOURCE:
https://www.kernel.org/doc/html/latest/vm/zswap.html
---------------
COMPRESSOR TYPES:
zstd, lfo, lzo... etc

---------------
CONTROLLED USER POLICIES:
Zswap seeks to be simple in its policies. Sysfs attributes allow for one user controlled policy:
	max_pool_percent - The maximum percentage of memory that the compressed pool can occupy.


------------------------------------------
WHY ZSWAP IS IMPORTANT --------BEGIN:
In personal experimentation working towards "Using the Raspberry Pi 3B/3B+ as a light duty desktop," I've discovered that fronting my swap file with zswap makes a huge difference in system capability, most notably in how Chromium functions. With stock settings, Chromium on the Pi 3B cannot load Google Inbox (https://inbox.google.com, assuming one's account is enabled) or Google Docs properly. With zswap enabled, I can load both, simultaneously, and still have a usable system.

Under normal operation, with Chromium running, I have a fully responsive system with 300-500MB of memory swapped out - this being memory that, while not able to be discarded, is not actively in use.

I'm aware of the concerns about thrashing the SD card (and the glacial performance of said SD card under swap use), which is why zswap works so well.

zswap, in a nutshell, is a compressed frontend for swap. It's quite configurable, with multiple compression options (lzo and lz4 being the most useful), several ways of storing compressed pages in memory (two and three "slots" per 4k page for compressed data), configurable in terms of percent of total system memory it will cache, etc.

It also includes a LRU (Least Recently Used) algorithm for evicting pages from compressed swap to physical disk swap when the cache is full, which prevents the priority inversion issues one can run into when using zram and physical swap with priorities set (zram fills up with the first stuff swapped out, which is typically least important, leaving physical disk to handle the later, higher priority swap that you'd like to keep in RAM).

Enabling zswap in the kernel requires the following changes to .config:
CONFIG_ZSWAP=y
CONFIG_ZPOOL=y
CONFIG_ZBUD=y
CONFIG_Z3FOLD=y

And, optionally, if you want lzo compression (somewhat better than lz4, but somewhat slower):
CONFIG_CRYPTO_LZO=y

I believe these can be built as modules as well, with no loss of functionality. If the defaults are not zswap, they should be modules, but if the decision is made to use zswap on all installs, these should be built in.

To enable zswap, there needs to be a backing swapfile (already the case, though 100MB is a bit small in 2018), and the kernel needs to have zswap configured. I've done this in /boot/cmdline.txt, though other locations would probably work as well.

At a minimum, this requires: zswap.enabled=1

I've also set the following on my install, though a smaller value may be a better default initially. With Chromium being as memory hungry as it is, I normally raise this at runtime.
zswap.max_pool_percent=15

One can also set:
zswap.zpool=z3fold

However, while this worked properly on 4.9, with 4.14, I've seen a few kernel oopses related to this (buddy ID of 0 - I haven't worked out the details on this bug), so I have reverted to using zbud for now. The effective compression is worse than using z3fold, but the stability is better.

Current zswap parameters on my light desktop:

root@raspberrypi:/sys/kernel/debug/zswap# grep -R .
stored_pages:68856
pool_total_size:151773184
duplicate_entry:0
written_back_pages:0
reject_compress_poor:1091
reject_kmemcache_fail:0
reject_alloc_fail:0
reject_reclaim_fail:0
pool_limit_hit:0

This works out to 282MB of data swapped into 151MB, for a compression ratio of 1.86:1. z3fold is better, but, as previously noted, seems somewhat unstable right now. I will investigate that further when I have time.

I encourage the maintainers to build a kernel with zswap enabled, and use Chromium for a while to observe the difference. It makes a substantial difference in what can be loaded without grinding the system to a halt. If you still run into memory pressure, try adding more swapfile. I currently have a 4GB swapfile, which is entirely excessive and mostly unused, but I'm experimenting and have no particular storage pressures at the moment. I would suggest increasing the default swapfile size to 200MB if zswap is used, although this may be something to simply note for users.

Unlike zram, zswap allows data to overflow out of RAM to physical swap, which allows forbetter system performance and a higher ratio of "Getting stuff that's actually unused out of RAM."

Some relevant documentation for reading:
https://www.kernel.org/doc/Documentation/vm/zswap.txt
https://lwn.net/Articles/537422/

Let me know what sort of benchmarking or other testing you would like to see in this thread. I understand that the maintainers are touchy about adding anything that requires additional kernel size, but experimentally, zswap is a massive win in terms of usability with the new default browser of Chromium. 
------------------------------------------------------------------------- END: Source -> https://github.com/raspberrypi/linux/issues/2649
DEFAULT DIRECTORIES:

ls -la /dev/zram0				#Displays device names/pseudo files?
ls -la /sys/block/zram0				#Displays all zram0 available variables
ls -la /proc/devices/zram0			#???	
ls -la /sys/devices				# ???
-------------------------------------
SWAP DISKS OFF:
Bottom line is that, without swap:

	1. Your system will be less stable.
	2. Your system will not be able to hibernate.
	3. Disk access speed in your system will be slower compared to a system that has swap partition. Moreover, disk access speed will drop in the course of time.
---------------
SOURCE:

https://medium.com/for-linux-users/linux-tip-higher-performance-with-zswap-2a4654b935de
	If you have lots of memory, you probably won’t get much benefit from ZSWAP. It still pays to have a swap file or partition (modern kernels work fine with either), 
	though, because it allows the kernel to unload infrequently used programs which gives better performance to programs you are using. 
	The kicker is when everything you do requires something else to be evicted.

http://www.alexonlinux.com/swap-vs-no-swap - SPSPSP

https://www.kernel.org/doc/Documentation/vm/zswap.txt
https://lwn.net/Articles/537422/
---------------
================================================================================
UDEV RULES:
	Udev is a device manager for linux kernel which is able to manage the device nodes in the /dev directory.

In a GNU/Linux system, while devices low level support is handled at the kernel level, the management of events related to them is managed in userspace by udev, and more precisely by the udevd daemon. 
Learning how to write rules to be applied on the occurring of those events can be really useful to modify the behavior of the system and adapt it to our needs.

For example, an udev rule can be used to initalize a system device or disable it entirely as soon as possible at system boot.

for example: 
	/etc/udev/rules.d/99-togglemouse.rules
		ACTION=="add" \
		, ATTRS{idProduct}=="c52f" \
		, ATTRS{idVendor}=="046d" \
		, ENV{DISPLAY}=":0" \
		, ENV{XAUTHORITY}="/run/user/1000/gdm/Xauthority" \
		, RUN+="/usr/bin/xinput --disable 16"


The udev rule above will disable the touchpad as soon as it detects a connected mouse device at system boot.
----
UDEV OPERATORS:

==   and   +=		#these operators are used as equality and inequality operators.

=    and   :=		#These are used as assignment and forced-assignment operators.
			^ the forced assignment ensures the data can't be overwritten by other udevs or parts of the system.
+=   and   -=		#Can be used to add/remove values from multi-valued paramater/variable/attribute.


---
EVENTS:
	In the script above there is an ACTION event where we can decide when/how the rest of the script is going to be executed.
For example: ACTION event can take add, remove and change as valid values.

---
ATTRS:
	Describe attributes and can be used to assign or unassign values to given paramaters,
to find out which parameters are valid for a given device, you can use: udevadm info 
	ex.: udevadm info /dev/zram0

	ex2.:
		udevadm info -ap /devices/pci0000:00/0000:00:1d.0/usb2/2-1/2-1.2/2-1.2:1.1/0003:046D:C52F.0010/input/input39

		Output:
			Udevadm info starts with the device specified by the devpath and then
			walks up the chain of parent devices. It prints for every device
			found, all possible attributes in the udev rules key format.
			A rule to match, can be composed by the attributes of the device
			and the attributes from one single parent device.

			  looking at device '/devices/pci0000:00/0000:00:1d.0/usb2/2-1/2-1.2/2-1.2:1.1/0003:046D:C52F.0010/input/input39':
			    KERNEL=="input39"
			    SUBSYSTEM=="input"
			    DRIVER==""
			    ATTR{name}=="Logitech USB Receiver"
			    ATTR{phys}=="usb-0000:00:1d.0-1.2/input1"
			    ATTR{properties}=="0"
			    ATTR{uniq}==""

			  looking at parent device '/devices/pci0000:00/0000:00:1d.0/usb2/2-1/2-1.2/2-1.2:1.1/0003:046D:C52F.0010':
			    KERNELS=="0003:046D:C52F.0010"
			    SUBSYSTEMS=="hid"
			    DRIVERS=="hid-generic"
			    ATTRS{country}=="00"

			  looking at parent device '/devices/pci0000:00/0000:00:1d.0/usb2/2-1/2-1.2/2-1.2:1.1':
			    KERNELS=="2-1.2:1.1"
			    SUBSYSTEMS=="usb"
			    DRIVERS=="usbhid"
			    ATTRS{authorized}=="1"
			    ATTRS{bAlternateSetting}==" 0"
			    ATTRS{bInterfaceClass}=="03"
			    ATTRS{bInterfaceNumber}=="01"
			    ATTRS{bInterfaceProtocol}=="00"
			    ATTRS{bInterfaceSubClass}=="00"
			    ATTRS{bNumEndpoints}=="01"
			    ATTRS{supports_autosuspend}=="1"

			  looking at parent device '/devices/pci0000:00/0000:00:1d.0/usb2/2-1/2-1.2':
			    KERNELS=="2-1.2"
			    SUBSYSTEMS=="usb"
			    DRIVERS=="usb"
			    ATTRS{authorized}=="1"
			    ATTRS{avoid_reset_quirk}=="0"
			    ATTRS{bConfigurationValue}=="1"
			    ATTRS{bDeviceClass}=="00"
			    ATTRS{bDeviceProtocol}=="00"
			    ATTRS{bDeviceSubClass}=="00"
			    ATTRS{bMaxPacketSize0}=="8"
			    ATTRS{bMaxPower}=="98mA"
			    ATTRS{bNumConfigurations}=="1"
			    ATTRS{bNumInterfaces}==" 2"
			    ATTRS{bcdDevice}=="3000"
			    ATTRS{bmAttributes}=="a0"
			    ATTRS{busnum}=="2"
			    ATTRS{configuration}=="RQR30.00_B0009"
			    ATTRS{devnum}=="12"
			    ATTRS{devpath}=="1.2"
			    ATTRS{idProduct}=="c52f"
			    ATTRS{idVendor}=="046d"
			    ATTRS{ltm_capable}=="no"
			    ATTRS{manufacturer}=="Logitech"
			    ATTRS{maxchild}=="0"
			    ATTRS{product}=="USB Receiver"
			    ATTRS{quirks}=="0x0"
			    ATTRS{removable}=="removable"
			    ATTRS{speed}=="12"
			    ATTRS{urbnum}=="1401"
			    ATTRS{version}==" 2.00"

			    [...]
-----
ENV KEYWORD:
	ENV keyword: it can be used to both set or try to match environment variables. We assigned a value to the DISPLAY and XAUTHORITY ones. Those variables are essential when interacting with
the X server programmatically, to setup some needed information: with the DISPLAY variable, we specify on what machine the server is running, what display and what screen we are referencing, and 
with XAUTHORITY we provide the path to the file which contains Xorg authentication and authorization information. 

This file is usually located in the users "home" directory.

-----
RUN KEYWORD:
The RUN keyword is used to run external programs. In this case we used the xinput utility to change the status of the touchpad. Notice that 16 is the id of the touchpad.

-----
UDEV TEST:
	Once our rule is set in /etc/udev/rules.d/, we can debug it by using the udevadm test command. 
This is useful for debugging but it doesn't really run commands specified using the RUN key:
	$ udevadm test --action="add" /devices/pci0000:00/0000:00:1d.0/usb2/2-1/2-1.2/2-1.2:1.1/0003:046D:C52F.0010/input/input39

By providing the action key in the test, we stabilish which event must be tested, and also the syfs path of the device.
If no errors are reported, our rules are ready to go.

	SYNTAX:
		udevadm test syfs_path				#sysfs_path is the actual device path, not device names under /dev/ (check FINAL CONSIDERATIONS section)
----
EXECUTING THE UDEV RULE:
To run it in the real world, we must either reboot the system or reload the rules:
	# udevadm control --reload

This command will reload the rules files, however, will have effect only on new generated events.
---
MORE COMMANDS:
udevadm info /dev/zram0 -a		#Displays all properties available for /dev/zram0 - VERY IMPORTANT
zramctl					#Lists all zswap devices and displays how much memory it's using
---
FINAL CONSIDERATIONS
/dev/ folder only contain device names! The true device paths are located in /sys/devices/ - VERY IMPORTANT !!!!

to find out where your device path is located you can do:
	1 - sudo ls -laR /sys/devices/ | grep 'device_name'
		OR
	2 - udevadm info /dev/zram0    # BETTER, using this command you'll find your device path under DEVPATH variable!


------------------------------
================================================================================
SWAP, CACHE, RAM, ZRAM, ZSWAP, ZCACHE, TMPFS, KERNEL, FURTHER NOTES:

RAM:
Physical Memory Storage, also known as primary data storage for programs still in execution.

SWAP MEMORY:
SWAP memory is used for paging/swapping of virtual memory and also as a backing store(Disk),
the virtual memory is a point reference to the actual data in RAM.

The BACKING STORE is used as safety storage for the SWAPPED-OUT PAGES, acessing a swapped-out page will result on page-fault which happens when a given reference to memory returns data
that doesn't exist anymore in RAM. 

The SWAPPED-IN Pages are references to Memory Data that still exist in RAM.

Accessing An UNMAPED PAGE causes a page fault which results in a segmentation violation.
UNMAPED PAGE are references that have not yet been mapped/referenced to anything.

The SWAP Memory usually gets stored in the Disk/Hard Drive, either as swap file or as a swap partitions.

SWAP Memory will have it's own allocated free space, 
and will send data to Cache under pressure of running out of free available space.

CACHE:
Cache is where all soon-to-be-removed data gets stored before they are actually removed from RAM.
Cache usually is stored in RAM physical module.

A Cache will send data to SWAP Disk/Block Device in case it runs out of Physical Memory(RAM).
HOWEVER, programs will usually be referenced by SWAP Device at the OS level, not the cache!
It's the job of the SWAP to point where programs are located in memory and therefore also it's responsibility. - VERY IMPORTANT

SWAP:
Swap can be RAM Block Devices, Files and Partitions.
When making swap devices, the user will be given the choice of setting priority: 

	1 - User Priority
		uses numbers from 0 to 100, 100 being the highest priority.
	2 - Auto/Default Priority set:
		uses negative numbers; In older kernels, auto-set priorities would start at -1,
		newer kernels begin at -2.

The swap priorities will help to set the device on the proper NUMA Node on kernel.
Swap Devices with same priorities will work in a round-robin mode!

CACHE & SWAP:
As seen previously, Cache & Swap have a bi-directional relationship.
The CACHE uses the RAM and will send data back to SWAP as soon as it starts running out of RAM Disk Space.
The SWAP will send data to the Cache as soon as it runs out of the allocated Pool Space.

ZRAM: 
This used to be called 'compcache' in the old days.
It is a kernel module that creates a block device that allows both storage and compression of data in RAM.

ZSWAP:
Cache for the Swap File or Swap Partition.
Beware! Zswap is not Zram, never use Zswap and Zram together! In fact, zswap has worse performance, always use ZRAM as swap, never zswap. Also, zswap is a cache for actual swap devices, not a swap in on itself. - VERY IMPORTANT

Zram occupies part of memory with compressed pool(-s) which are used as in-memory swap devices. 
Zswap uses similar in-memory pool with compression but also can flush its content to a real swap file/partition.

To disable ZSWAP, add zswap.enabled=0 to your kernel parameters in /boot/grub/grub.cfg! - VERY IMPORTANT

ZCACHE:
It does what ZSWAP does and ALSO compresses and speeds the filesystem page cache. (It is internally much more complicated and is not in the mainline kernel as it is still under development).

source: https://askubuntu.com/questions/471912/zram-vs-zswap-vs-zcache-ultimate-guide-when-to-use-which-one

TMPFS:
Temporary Filesystem that uses RAM as Persistent Storage.

KERNEL:
In the abscence of a SWAP Partition Disk/Block Device, the kernel will create it's own Paging/Swapping and backing store on the Hard Drive Disk.
everything else is stored as Cache permanently, because no SWAP Devices in existance for the Kernel to Manage and evict pages/files from memory.
Evicting Pages/Data from memory requires Swap. - VERY IMPORTANT

-------
NOTES ABOUT USING CACHE(TMPFS) AND ZSWAP:
If zswap gets set to use a small portion of memory(small pool), as soon as it runs out of space, it'll send all data back to Cache.
Whenever data gets re-required, it'll be copied from CACHE back into the small SWAP POOL. this is bad, because re-writtens/re-reads will be executed too many times
before any of the requested data can be used by the computer.

HOWEVER, setting a very large ZSWAP pool will cause the CPU to become slower, making some programs take too long to open.
This is due to the nature of ZRAM block devices require to compress/uncompress their requested datas.

The secret here is to set ZSWAP not so big, that will cause slowdowns due to all the compressed/uncompressed data,
and not too small that can cause too many read/write requests from cache data.

------------------------------
================================================================================
TIPS ON READING JOURNALCTL AND DMESG:

The numbers in the left side indicate seconds.milliseconds on dmesg.
On journalctl it indicates a full timestamp! - VERY IMPORTANT.
------------------------------
================================================================================
INTRODUCTION TO INITRAMFS:

INITRAMFS 


---DEFAULT PATH:
/etc/mkinitcpio.conf			#Default file configuration for all preset files in /etc/mkinitcpio.d/
					^ All changes to this file will require sudo mkinitcpio -P 
					to rebuild each of the kernels' initramfs image on /boot/. - OBS#1!!

---DKMS Modules:
	You can set DKMS modules on mkinitcpio.conf - OBS#1
ex.: MODULES=(nvidia nvidia_modeset nvidia_drm nvidia_uvm)

You'll need the referenced DKMS Modules Installed!

-----COMPRESSION METHODS:
	ZSTD compresses better
	LZ4 is FASTER

you can set either of them on /etc/mkinitcpio.conf  !!!  - ATTENTION TO OBS#1!
beware this only sets compressor for initramfs images, 
the kernel uses it's own compressor settings, you can set in:
zgrep CONFIG_KERNEL /proc/config.gz 	#NOT SAFE YET
------------------------------
================================================================================
---------------
========================================
ABOUT ENTROPY, RNG-TOOLS AND HAVEGED:
1 - archlinx% grep 'AnonHugePages:\s\+[^ 0]' /proc/*/smaps
		^ This actually printed all my opera tabs

2 - cat /proc/meminfo | grep -i huge

AnonHugePages:    874496 kB
ShmemHugePages:        0 kB
FileHugePages:      2048 kB
HugePages_Total:       0   
HugePages_Free:        0   
HugePages_Rsvd:        0   
HugePages_Surp:        0   
Hugepagesize:       2048 kB
Hugetlb:               0 kB
--------------
ekd_ovmf firmware
--------------
New GRUB Option set!

Check if it's working:
rngd -f

A simple test to see if everything is working as it should is to run (in another terminal) the following dd command:

$ dd if=/dev/random of=/dev/null bs=1024 count=1 iflag=fullblock
Without rngd, the above command will take lots of time to run. With rngd working properly, the result should be almost instantaneous:
---------------------
Dictionary:
2.1 Random Number Generators (RNGs)
An RNG is a utility or device of some type that produces a sequence of numbers on an interval [min, max] such that values appear unpredictable. Stated a little more technically, we are looking for the following characteristics:

Each new value must be statistically independent of the previous value. That is, given a generated sequence of values, a particular value is not more likely to follow after it as the next value in the RNG's random sequence.
The overall distribution of numbers chosen from the interval is uniformly distributed. In other words, all numbers are equally likely and none are more "popular" or appear more frequently within the RNG’s output than others.
The sequence is unpredictable. An attacker cannot guess some or all of the values in a generated sequence. Predictability may take the form of forward prediction (future values) and backtracking (past values).
Since computing systems are by nature deterministic, producing quality random numbers that have these properties (statistical independence, uniform distribution, and unpredictability) is much more difficult than it might seem. Sampling the seconds value from the system clock, a common approach, may seem random enough, but process scheduling and other system effects may result in some values occurring far more frequently than others. External entropy sources like the time between a user's keystrokes or mouse movements may likewise, upon further analysis, show that values do not distribute evenly across the space of all possible values; some values are more likely to occur than others, and certain values almost never occur in practice.

Beyond these requirements, some other desirable RNG properties include:

The RNG is fast in returning a value (i.e., low response time) and can service a large number of requests within a short time interval (i.e., highly scalable).
The RNG is secure against attackers who might observe or change its underlying state in order to predict or influence its output or otherwise interfere with its operation.

2.2 Pseudo-Random Number Generators (PRNGs)
One widely used approach for achieving good RNG statistical behavior is to leverage mathematical modeling in the creation of a Pseudo-Random Number Generator. A PRNG is a deterministic algorithm, typically implemented in software that computes a sequence of numbers that "look" random. A PRNG requires a seed value that is used to initialize the state of the underlying model. Once seeded, it can then generate a sequence of numbers that exhibit good statistical behavior.

2.3 True Random Number Generators (TRNGs)
For contexts where the deterministic nature of PRNGs is a problem to be avoided (e.g., gaming and computer security), a better approach is that of True Random Number Generators.

Instead of using a mathematical model to deterministically generate numbers that look random and have the right statistical properties, a TRNG extracts randomness (entropy) from a physical source of some type and then uses it to generate random numbers. The physical source is also referred to as an entropy source and can be selected among a wide variety of physical phenomenon naturally available, or made available, to the computing system using the TRNG. For example, one can attempt to use the time between user key strokes or mouse movements as an entropy source. As pointed out earlier, this technique is crude in practice and resulting value sequences generally fail to meet desired statistical properties with rigor. What to use as an entropy source in a TRNG is a key challenge facing TRNG designers.


2.4 Cascade Construction RNGs
A common approach used in modern operating systems (e.g., Linux* (2)) and cryptographic libraries is to take input from an entropy source in order to supply a buffer or pool of entropy (refer to Figure 1). This entropy pool is then used to provide nondeterministic random numbers that periodically seed a cryptographically secure PRNG (CSPRNG). This CSPRNG provides cryptographically secure random numbers that appear truly random and exhibit a well-defined level of computational attack resistance:
	Entropy Pool: Non-Deterministic Random Numbers
	Entropy Seed: Non-Deterministic Random Seeds
	CSPRNG:	Cryptographically Secure Random Numbers

2.5 Introducing the Digital Random Number Generator (DRNG)
The Digital Random Number Generator (DRNG) is an innovative hardware approach to high-quality, high-performance entropy and random number generation. It is composed of the new Intel 64 Architecture instructions RDRAND and RDSEED and an underlying DRNG hardware implementation.

With respect to the RNG taxonomy discussed above, the DRNG follows the cascade construction RNG model, using a processor resident entropy source to repeatedly seed a hardware-implemented CSPRNG. Unlike software approaches, it includes a high-quality entropy source implementation that can be sampled quickly to repeatedly seed the CSPRNG with high-quality entropy. Furthermore, it represents a self-contained hardware module that is isolated from software attacks on its internal state. The result is a solution that achieves RNG objectives with considerable robustness: statistical quality (independence, uniform distribution), highly unpredictable random number sequences, high performance, and protection against attack.

This method of digital random number generation is unique in its approach to true random number generation in that it is implemented in the processor’s hardware and can be utilized through instructions added to the Intel 64 instruction set. As such, response times are comparable to those of competing PRNG approaches implemented in software. The approach is scalable enough for even demanding applications to use it as an exclusive source of random numbers and not merely a high quality seed for a software-based PRNG. Software running at all privilege levels can access random numbers through the instruction set, bypassing intermediate software stacks, libraries, or operating system handling.
---------------------------------
SYSCT KERNEL.RANDOM:
	Typing sysctl kernel.random will show a few interesting /proc/ variables,
specially write-wakup-threshold and read-wakeup-threshold:

		1 - write-wakeup-threshold
			The write-wakeup-threshold is rarely used but is good for sequencing; the performance gain it provides should be minimal. What it does is wake up devices blocking to write to 
		the entropy pool (i.e. sources which will use the aforementioned ioctl to add entropy to the pool) when the pool gets low. It will not necessarily have the effect of adding entropy, obviously.

		2 - The read-wakeup-threshold is opposite; this is the number of bits of entropy required to be available (i.e. the number indicated in entropy_avail) 
		before we allow anything to read from /dev/random. /dev/urandom ignores this parameter (since reads from it are nonblocking and don't wait for entropy, instead allowing pseudorandom data to be read).
		^ 2023 NOTE: read 'KERNEL NO LONGER SHOWS AVAILABLE ENTROPY' topic

---------------------------------
DEFAULT DIRECTORIES FOR ENTROPY:
/proc/sys/kernel/random/entropy_avail		#Kernel available entropy 
						^ 2023 NOTE: read 'KERNEL NO LONGER SHOWS AVAILABLE ENTROPY' topic
/dev/random					#All block random number generated
/dev/urandom					#All unblocking random number generated 
/dev/hwrng					#All hardware generated entropy


---------------------------------
Checking SPEED on Kernel.Random Generator:
		-doesn't counts hardware generator
sysctl kernel.random, ex.: 

	#sysctl kernel.random

	[output]
		kernel.random.boot_id = 21fb6d3e-fac6-4fa8-beaf-fac48483a90b
		kernel.random.entropy_avail = 3950
		kernel.random.poolsize = 4096
		kernel.random.urandom_min_reseed_secs = 60
		kernel.random.uuid = d22b6c20-3165-4add-847b-50f454f79607
		kernel.random.write_wakeup_threshold = 60
	[/output]
	^ 2023 NOTE: read 'KERNEL NO LONGER SHOWS AVAILABLE ENTROPY' topic
	^ As of kernel 6.0, performance is better off by letting the kernel handle the RNG.
	^ Disable & UNINSTALL RNGD!

Testing RNG-TOOLS:
	rngtest -c 1000 < /dev/random

	The Number of Failures must be MINIMUM! it's okay if it fails just once.

	2nd test: $ dd if=/dev/random of=/dev/null bs=4096 count=1024 iflag=fullblock
	A speed of around 50 kB/s in dd's output shows that everything is working properly. For comparison, without rngd you probably would get 0.0 kB/s (since the speed is too low).

	my optimal results:
		archlinx% dd if=/dev/random of=/dev/null bs=4096 count=1024 iflag=fullblock
		1024+0 records in
		1024+0 records out
		4194304 bytes (4.2 MB, 4.0 MiB) copied, 0.0189523 s, 221 MB/s

	results should always be less than 1 second!

	Watch entropy accumulation in real time. Type:
		watch -n 1 cat /proc/sys/kernel/random/entropy_avail
		^ 2023 NOTE: read 'KERNEL NO LONGER SHOWS AVAILABLE ENTROPY' topic
		^ As of kernel 6.0, performance is better off by letting the kernel handle the RNG.
		^ Disable & UNINSTALL RNGD!

ABOUT RNG_TOOLS MAIN PART:

RNGD / RNG-TOOLS is used to gather entropy from TRNG/HRNG(Hardware Random Number Generator/True Random Number Generator) devices in /dev/random,
by default /dev/random is very slow because it gathers entropy from DEVICE DRIVERS and other slow sources.

rngd allows the use of faster entropy sources, mainly hardware random number generators (TRNG), present in modern hardware like recent AMD/Intel processors, Via Nano or even Raspberry Pi.
VERY IMPORTANT: ex.: by default rngd/rng-tools try to capture random numbers from PKCS11(Card reader when available), JITTER(Entropy gathered by measuring timing variance of operations on the local cpu), 
rdrand(CPU RDRAND - when enabled on kernel/grub), /dev/hwrng (from other hardware sources that linux kernel may automatically capture)! - ALWAYS READ MAN PAGE

Personal Note: Sandy Bridges CPU like intel 2500k have no RDRAN, Ivy Bridge does. However, old cpu users can still feed from rdtscp

While Linux itself uses the result from TRNG in /dev/random, if available, they are only used as a XOR after the entropy is collected by kernel. So /dev/random, by default, is slow even if you do have a TRNG.
rngd feeds /dev/random itself, increasing the available entropy by far.

CUSTOM CONFIGURATION RNGD:
	Configuration file is located in /etc/conf.d/rngd

By default, RNGD Only grabs hardware entropy to /dev/random until 50% is filled, that's the default --fill-watermark=2056, since 100% is 4096,
That means that RNGD will stop grabbing entropy from hardware sources if entropy is already above the watermark fill, 
which in turn means it'll only grab entropy before the first 50% fill.

To Solve this, add this to the configuration file:
RNGD_OPTS="--fill-watermark=4096"
 	or
RNGD_OPTS="--fill-watermark=100%"

Only do this if you 100% trust your hardware device as entropy source, it'll fill al the /dev/random pool with the available hardware device's entropy.

A more safe approach would be to allow it to consume 60% of the entropy pool, leaving the rest of the job to the linux kernel, this will ensure that
your entropy is always 100% filled.

VERY IMPORTANT: To automate this process, add sudo systemctl start rngd(ONLY AFTER TESTING IT!) on either your ~/.bash_profile or ~/.zprofile
	or
#systemctl start rngd
#systemctl enable rngd
	^ As of kernel 6.0, performance is better off by letting the kernel handle the RNG.
	^ Disable & UNINSTALL RNGD!

MORE ABOUT RNG-TOOLS / RNGD:
By default rngd fills the entropy pool until at least 2048 bits of entropy are available. This is to avoid the TRNG to dominate the contents of the pool. You can override this setting if you really trust your TRNG. To do this, pass "--fill-watermark=4096" to RNGD_OPTS, for example (4096 is the maximum size of kernel's entropy pool by default, you shouldn't pass a value greater than the maximum either). Doing so may increase the performance of /dev/random even further, at the expense of maybe lower random number quality. However, it should be noted that the default setting is already sufficient for the majority of user cases.


Personal Note: To use rngd, you'll need to enable it as a service: 
	$ sudo systemctl start rngd
	^ As of kernel 6.0, performance is better off by letting the kernel handle the RNG.
	^ Disable & UNINSTALL RNGD!

trying to manually fire rng-tools using "sudo rngd -f" - as advised on arch wiki page - will lock your shell session.

MODPROBING / MODINFO KERNEL MODULES FOR RNG:
	# modinfo rng-core
	# modinfo via-rng

note that modprobe and modinfo are different things!
modprobe allows you to load a kernel module when needed, even if it was turned off somewhere else! - VERY IMPORTANT

LSCPU TO CHECK FOR RNG MODULES:
	type lscpu, and find out if the rng flag is there on flags/cpu flags

TESTING QUALITY OF HARDWARE RANDOMNESS:
	rngtest -c 4096 < /dev/hwrng

Checking Number of Entropy Available:
	cat /proc/sys/kernel/random/entropy_avail
	^ READ NOTE BELOW:

	---NOTE 2023.1: KERNEL NO LONGER SHOWS AVAILABLE ENTROPY ---
		as of this date, entropy_avail shall always return '256' as entropy value:

			"Note that because /dev/random no longer aims to provide true 
		random data, but rather "just" cryptographically secure random data
		(which is sufficient for all real-world use cases), reading from it
		no longer causes the kernel's entropy pool to deplete. Therefore, 
		/proc/sys/kernel/random/entropy_avail should always contain 256, 
		which is the size of a ChaCha20 key in bits. 

		Historical documentation that expected larger values in this file, 
		or expected the user to take actions if the value was getting "too low", can be disregarded."


		source: https://wiki.archlinux.org/title/Random_number_generation
	---
	A Good Entropy value must be around 3900!
	Otherwise system can get slow.

	Low Entropy is bad!

	source:
	https://wiki.archlinux.org/index.php/Rng-tools

Read Later:
	https://blog.cloudflare.com/ensuring-randomness-with-linuxs-random-number-generator/

	Read0:
	This explains why some system becomes slow after some time:

	"In computing, entropy is the randomness collected by an operating system or application for use in cryptography or other uses that require random data. 
	This randomness is often collected from hardware sources (variance in fan noise or HDD), either pre-existing ones such as mouse movements or specially provided randomness generators. 
	A lack of entropy can have a negative impact on performance and security."
		https://en.wikipedia.org/wiki/Entropy_(computing)


	OpenBSD has integrated cryptography as one of its main goals and has always worked on increasing its entropy for encryption but also for randomising many parts of the OS, including various internal 
	operations of its kernel. Around 2011, two of the random devices were dropped and linked into a single source as it could produce hundreds of megabytes per second of high quality random data on an 
	average system. This made depletion of random data by userland programs impossible on OpenBSD once enough entropy has initially been gathered.


	Read 0.1: - WINDOWS ENTROPY -
		Microsoft Windows releases newer than Windows 95 use CryptoAPI to gather entropy in a similar fashion to Linux kernel's /dev/random.

		Windows's CryptoAPI uses the binary registry key HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Cryptography\RNG\Seed to store a seeded value from all of its entropy sources.

		Because CryptoAPI is closed-source, some free and open source software applications running on the Windows platform use other measures to get randomness. For example, GnuPG, as of 
		version 1.06, uses a variety of sources such as the number of free bytes in memory that combined with a random seed generates desired randomness it needs.

		Programmers using CAPI can get entropy by calling CAPI's CryptGenRandom(), after properly initializing it.

		CryptoAPI was deprecated from Windows Vista and higher. New API is called Cryptography API: Next Generation (CNG). Windows's CNG uses the binary registry key HKEY_LOCAL_MACHINE\SYSTEM\RNG\Seed to store a seeded value.

		NEWER VERSIONS OF WINDOWS are able to use a variety of entropy sources:

			TPM if available and enabled on motherboard
			Entropy from UEFI interface (if booted from UEFI)
			RDRAND CPU instruction if available
			Hardware system clock (RTC)
			OEM0 ACPI table content
			Interrupt timings
			Keyboard timings and Mouse movements

	Read1:
		Recent Intel processors have the RDRAND machine instruction, and /dev/random is using it (and some other things) as random source. 
		So I guess that lack of entropy on /dev/random don't affect (i.e. never happens for) them.


---
	Read2:
 Unfortunately, Linux only has two interfaces to get random numbers: /dev/random, which blocks when it shouldn't, and /dev/urandom, which never blocks. Fortunately, in practice, /dev/urandom is almost always correct, because a system quickly gathers enough entropy, after which point /dev/urandom is ok forever (including uses such as generating cryptographic keys).
 
Even if you believe in Linux's definition of entropy, low entropy isn't a security problem. /dev/random blocks until it's satisfied that it has enough entropy. With low entropy, you'll get applications sitting around waiting for you to wiggle the mouse, but not a loss of randomness.

------------------------------
================================================================================
AUR & ALA REPOSITORY:
	Besides the official repository,
Archlinux users can also have access to AUR(Arch User Repository) and ALA Repository.
While AUR have all the cool optional user-made packages & applications,
ALA is a repository that keeps old versions of all packages in the system.

This can be useful, because neither the Official Arch Repo nor AUR keeps support for old packages hanging around.
So when a newly update package breaks something in your system, the only way to fallback to the old package is to
access ALA Repository and download the older ones again.

https://archive.archlinux.org/
https://wiki.archlinux.org/index.php/arch_linux_archive
https://bbs.archlinux.org/viewtopic.php?id=265563

example:
# pacman -U https://archive.archlinux.org/packages/path/packagename.pkg.tar.xz

	^ All you have to do is fetch the link and parse it to pacman -U


Important Nvidia Packages(old archlinux, check 2023):
	These were fetch from the list below: 
		nvidia, nvidia-utils, nvidia-dkms, nvidia-settings, lib32-nvidia-utils, lib32-opencl-nvidia, libxnvctrl, opencl-vidia

FIXING NVIDIA UPDATE ISSUE(old):
sudo pacman -U https://archive.archlinux.org/packages/n/nvidia-utils/nvidia-utils-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/n/nvidia-dkms/nvidia-dkms-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/n/nvidia-settings/nvidia-settings-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/n/nvidia/nvidia-460.67-1-x86_64.pkg.tar.zst

2nd BATCH COMMANDS(old):
sudo pacman -U https://archive.archlinux.org/packages/l/lib32-nvidia-utils/lib32-nvidia-utils-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/l/lib32-opencl-nvidia/lib32-opencl-nvidia-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/l/libxnvctrl/libxnvctrl-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/o/opencl-nvidia/opencl-nvidia-460.67-1-x86_64.pkg.tar.zst

1st and 2nd - FULL COMMAND(old):
sudo pacman -U https://archive.archlinux.org/packages/n/nvidia-utils/nvidia-utils-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/n/nvidia-dkms/nvidia-dkms-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/n/nvidia-settings/nvidia-settings-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/n/nvidia/nvidia-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/l/lib32-nvidia-utils/lib32-nvidia-utils-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/l/lib32-opencl-nvidia/lib32-opencl-nvidia-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/l/libxnvctrl/libxnvctrl-460.67-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/o/opencl-nvidia/opencl-nvidia-460.67-1-x86_64.pkg.tar.zst

TEST LATER:
	cat nvidia-bug-report.log | grep Coolbits


LAST KNOWN WORKING NVIDIA DRIVERS
Important Nvidia Drivers(2023 - NEW):
	Use these drivers if you want to revert installation to a previous version:
		extra/opencl-nvidia 545.29.06-1 (2.8 MiB 23.2 MiB) (Installed)
		extra/libxnvctrl 545.29.06-1 (76.3 KiB 456.9 KiB) (Installed)
		extra/nvidia-settings 545.29.06-1 (766.4 KiB 1.6 MiB) (Installed)
		extra/nvidia-utils 545.29.06-1 (239.9 MiB 639.8 MiB) (Installed)
		extra/nvidia-dkms 545.29.06-1 (41.9 MiB 69.8 MiB) (Installed)
		multilib/lib32-nvidia-utils 545.29.06-1 (38.7 MiB 198.8 MiB) (Installed)


Obs.: Note that lib32-opencl-nvidia isn't needed anylonger.

Unimportant apps(2023 - NEW):
	These Drivers don't need to be reverted:
		multilib/lib32-libvdpau 1.5-2 (22.6 KiB 86.7 KiB) (Installed)
		extra/libva-nvidia-driver 0.0.11-1 (37.8 KiB 90.7 KiB) (Installed) * KIND OF IMPORTANT
		extra/cuda 12.3.0-6 (1.5 GiB 4.3 GiB) (Installed)
		extra/egl-wayland 2:1.1.13-1 (34.2 KiB 93.4 KiB) (Installed)
		extra/libvdpau 1.5-2 (63.4 KiB 288.3 KiB) (Installed)
		extra/nvtop 3.0.2-1 (62.1 KiB 157.0 KiB) (Installed)

---NEW BETA DRIVERS(Old info, previous to 2023):
	Missing: nvidia(not needed), libxnvctrl(not needed)

NVIDIA DKMS PERFORMANCE DRIVERS: | NEW INFO - As of 2023, performance drivers DON'T EXIST anymore
yay -Syu nvidia-utils-performance opencl-nvidia-performance nvidia-dkms-performance lib32-nvidia-utils-performance nvidia-settings-performance lib32-opencl-nvidia-performance 
---

OBS.: IF YOU RUN INTO TROUBLE WITH CUDA NOT BEING AVAILABLE, RUN NVIDIA-MODPROBE FIRST
LINK:
https://archive.archlinux.org/packages/n/nvidia-dkms/nvidia-dkms-460.67-1-x86_64.pkg.tar.zst
https://forums.developer.nvidia.com/t/unable-to-set-fan-speed-on-the-newest-465-drivers/176816
------------------------------
================================================================================
Packages (2) nvidia-utils-beta-465.24.02-1 [removal]  nvidia-utils-465.24.02-2


-----
nvidia-settings -q all | grep Fan
nvidia-settings -a "[gpu:0]/GPUFanControlState=1" -a "[fan:0]/GPUTargetFanSpeed=97"

(WW) xf86CloseConsole: KDSETMODE failed: Bad file descriptor
(WW) xf86CloseConsole: VT_GETMODE failed: Bad file descriptor

------------------------------
================================================================================
------------------------------
================================================================================
UNOFFICIAL ARCHLINUX USER REPOSITORIES | 2024
	"The official Arch Linux Developers and the Trusted Users do not perform tests of any sort to verify the contents of these repositories. It is your decision whether to trust their maintainers, and you take full responsibility for any consequences of using any unofficial repository."
	^ source: https://wiki.archlinux.org/title/unofficial_user_repositories

	on each [file] section below you'll need to append the corresponding data to your /etc/pacman.conf.

	RECOMENDATIONS
	1 - ALHP
		Maintainer: Giovanni Harting
		Description: official repositories compiled with LTO, -march=x86-64-vN and -O3.
		Issue Tracker: https://somegit.dev/ALHP/ALHP.GO/issues
		Keyring: alhp-keyringAUR
		Mirrorlist: alhp-mirrorlistAUR
		Upstream page: https://somegit.dev/ALHP/ALHP.GO
		Debuginfod: https://debuginfod.alhp.dev

	 	[file: /etc/pacman.conf] 
			[core-x86-64-v3]
			Include = /etc/pacman.d/alhp-mirrorlist

			[extra-x86-64-v3]
			Include = /etc/pacman.d/alhp-mirrorlist
		[/file]


	2 - ANDONTIE-AUR

		Maintainer: Holly M.
		Description: A repository containing the most popular AUR packages, 
		as well as some I use all the time. New packages can be requested on the upstream website.
		Key-ID: B545E9B7CD906FE3
		Upstream page: https://aur.andontie.net

		[file: /etc/pacman.conf]
			[andontie-aur]
			Server = https://aur.andontie.net/$arch
		[/file]
	
	3 - 

source: https://wiki.archlinux.org/title/unofficial_user_repositories
------------------------------
================================================================================
------------------------------
================================================================================
CHECKING WHICH MOUNT OPTIONS ARE ACTIVE

command: grep "<mount_name>" /proc/mounts
	ex.: grep "/mnt/usb-Samsung_M3_Portable_923D7DF1040000B9-0:0-part1" /proc/mounts


^ results.: /dev/sdd1 /mnt/usb-Samsung_M3_Portable_923D7DF1040000B9-0:0-part1 btrfs rw,nosuid,nodev,relatime,compress-force=zstd:15,space_cache,subvolid=5,subvol=/ 0 0
------------------------------
================================================================================
CREATING A ZFS PARTITION

	Use a whole Disk instead of a partition, since ZFS automatically partitions the whole Disk Device,
if you ever use a partition for ZFS, make sure to create only one pool for each disk.
2nd option is to create a loop device that will look like a disk for ZFS, check following section on how to create a loop device first:

	Read Topic: CREATING AND MOUNTING LOOP DEVICES
		Make sure to create a loop device using qemu-img, and format it as a GPT Disk, writting the changes to the loop device.
		Finally use ZFS as bellow to create both a pool and partition it.

	VERY IMPORTANT: do not allow 'journaling filesystems' on loop devices, it will break write order.

1. Install ZFS DKMS Modules and ZFS Utils
	1.1 - Install ZFS DKMS Modules and ZFS Utils
		# yay -Sy zfs-dkms zfs-utils

	1.2 - (Optional) ZFS Zpool Scrub
		# yay -Sy systemd-zpool-scrub

	1.3 - (Optional) ZFS Boot Environment manager.
		# yay -Sy zectl

	1.4 - (Optional) GRUB ZFS Libraries
		# yay -Sy grub-libzfs 

		1.4.1 - (Optional) Utility to manage ZFS Boot Environments
			# yay -Sy zedenv

	1.5 - (Optional) Storage load analysis tool for OpenZFS
		# yay -Sy ioztat

2. Make sure to add a zfs reference on file /etc/mkinitcpio.conf on line MODULES=():
	ex.: MODULES=(nvidia, nvidia_modeset nvidia_drm, nvidia_uvm zfs)

	2.1 - Build Initial ram filesystem:
		# mkinitcpio -P

3. (Optional) - Add a modules-load configuration file in /etc/modules-load.d/zfs.conf:
	[content]
		zfs
	[/content]

	this will make sure zfs module is loaded at boot time.

	3.1 - reboot and check if modules have been loaded: 
		ex.: lsmod | grep zfs

3.0 - (Alternative) - You can instantly load the module by typing:
	#modprobe zfs

	3.1 - You can disable zfs module by typing:
		#modprobe -r zfs
			or
		#modprobe -r zfs zzstd zunicode zlua zavl zcommon znvpair icp
		^ the latter disables all zfs modules.

4. type in: 
	# zpool create -f ZFS_PARTITION /dev/sdd2

5. finally test to see if it's all running:
	# zpool status

5.1 - alternativefor checking:
	# df -h			#ZFS_PARTITION should be listed here

6 - Use ZFS Create(man zfs-create) to manage & set ZFS datasets within a ZFS storage pool:
	sudo zfs create -o mountpoint=/mnt/ZFS_PARTITION/ -o compression=on ZFS_PARTITION/<username>

7 - Type in & check: 
	zfs list 

---- COMPRESSING:

1. # zfs set recordsize=256k atime=off ZFS_PARTITION
2. # zfs set compression=zstd-3 ZFS_PARTITION
3. # zfs inherit recordsize ZFS_PARTITION/<username>
4. # zfs set compression=zstd-3 ZFS_PARTITION/<username>

5. check if changes have been applied:
	# zfs get all property ZFS_PARTITION/<username>


note.: atime=off was optional, can be turn on again.
VERY IMPORTANT: do not use zstd compression rate higher than 4 or 5! - EXPLAIN LATER - VERY SLOW!


---- CHANGING MOUNTING POINTS:
# zfs set mountpoint=/mnt/ZFS_PARTITION_POOL ZFS_PARTITION

note/very important: DO NOT MOUNT THE POOL/PARTITION ON TOP OF OTHER PARTITIONS!

---- CHECKING ON ARC SIZE:
This will give you a summary of all ZFS Arc Stats:
	$ arc_summary -s arc

		or
	
	$ arcstat
		AND
	$ arc_summary -s archits

---- CHECKING ZPOOL SETTINGS:

Checking property/features of a pool:
	$ zpool get all property ZFS_PART


---- SETTING MAXIMUM SIZE OF ARC:
Arc Parameter defines how much RAM Memory ZFS will use for compress/decompress files on the go!
Create a zfs.conf file as /etc/modprobe.d/zfs.conf:
[CONTENT]
	options zfs zfs_arc_max=1073741824
[/CONTENT]


To change arc size on the go: 
	1 - sudo -iu root
	2 - echo 2147483648 > /sys/module/zfs/parameters/zfs_arc_max

2147483648 for 2GB = 1024^3*2 ( 2GB TO KB )

1GB = 1073741824
2(alternate) - echo 1073741824 > /sys/module/zfs/parameters/zfs_arc_max
---- CHECKING ON SMART FEATURE IN HARD DRIVE:
	zpool status -c smart

		or
	smartctl -A /dev/sdd		#MORE RELIABLE | VERY IMPORTANT | WORKS ON ALL DISKS
		or
	smartctl -A -i /dev/sdd		
		or
	smartctl -a -i /dev/sdd

---- CHECKING DISK STATUS:
	zpool status -s

--- REBALANCEMENT:
The only way to rebalance is to copy & paste the files elsewhere on the same partition!
Rebalance helps when there's a change on compression method on the given pool/partition!

--- CHECKING PHYSICAL DISK HEALTH STATUS:
       ZED (ZFS Event Daemon) monitors events generated by the ZFS kernel module.  When  a  zevent
       (ZFS  Event)  is  posted, ZED will run any ZEDLETs (ZFS Event Daemon Linkage for Executable
       Tasks) that have been enabled for the corresponding zevent class.(MAN ZED)

---- OTHER COMMANDS:
# zpool destroy mypool							#Destroys/Deletes ZFS Partition
# zpool create -f mypool raidz sdb sdc sdd sde sdf			#Creates a Raidz VFS Partition 
									 ^ Raidz is a variaton of Raid5
# zpool create -f mypool mirror sdb sdc mirror sdd sde			#Creates a mirrored vpool!
									sdd and sde will mirror at sdb and sdc

# zpool export mypool							#Exports/Imports zpools
# zpool import mypool							Useful for transferring pools between systems

# zpool iostat -v mypool							#Views zpool I/O Statistics

# zpool replace mypool sde sdf						#Replaces a corrupted Disk in ZFS Pool

# zpool add -f mypool spare sde						#Adds a spare disk, the spare disk will
									automatically replace the failed one(s)

# zfs list								#Lists ZFS Pools/Partitions
# zfs destroy mypool/home						#Destroys ZFS partition
# zfs -o compression=on mypool/home					#Creates ZFS partition from given pool with 
									 ^ compression on.
# zfs create -o mountpoint=/mnt/zfs_part mypool/home			#Creates ZFS Partition from given pool and
									 ^ mounts it on /mnt/zfspart

# zfs create -o mountpount=/mnt/zfs_part -o compression=on mypool/home   #Does the same thing above twice

# zfs set quota=10G tank/home/bonwick 					#Assigns quota to a previously created user 
									pool called bonwick!

full ex. of zfs create: 
# zfs create -o mountpoint=/export/zfs -o sharenfs=on -o compression=on my_zpool/home

# zfs set compression=zstd-19 ZFS_PART/<username> 			#Sets compression rate of zstd-19 to
									ZFS_PART/<username>

# zfs get compression ZFS_PART/<username>				#Gets compression rate on pool ZFS_PART/<username>


----PLUS:
$ zfs get all property ZFS_PART/<username>				#Lists all attributes available within ZFS System

source:
https://www.thegeekstuff.com/2015/07/zfs-on-linux-zpool/
https://docs.oracle.com/cd/E19253-01/819-5461/gayog/index.html
https://wiki.archlinux.org/title/ZFS#Automatic_Start		#For installation/configuration on archlinux

----------------------------------------
================================================================================
USING PROGRAMS AS ANOTHER USER
sudo -iu <username> <program_name>

----------------------------------------
================================================================================
ABOUT MAKEPKG
	MAKEPKG is a script to automate the building of packages. The
requirements for using the script are a build-capable *nix platform and
a custom build script for each package you wish to build (known as a
PKGBUILD). See PKGBUILD(5) for details on creating your own build
scripts.


The advantage to a script-based build is that the work is only done
once. Once you have the build script for a package, makepkg will do the
rest: download and validate source files, check dependencies, configure
the build-time settings, build the package, install the package into a
temporary root, make customizations, generate meta-info, and package
the whole thing up for pacman to use.

--------------------EXAMPLE.: Installing YAY
1 - mkdir ~/.yay
2 - cd ~/.yay
3 - git clone https://aur.archlinux.org/yay.git
4 - cd yay
5 - makepkg -si			##Installs packages along with it's dependencies
6 - cd ../ ; sudo rm -R ~/.yay


----------------------------------------
================================================================================
DAILY CHECKING YOUR DISK HEALTH WITH SMARTCTL AND GIT

A) First create a folder for your disk health logs:

	1 - mkdir ~/.health_status
	2 - cd ~/.health_status

B) Now create a log file for each of the disks you own,
as follows:

	1 - sudo smartctl -a -i /dev/sda > sda_disk.log
		or
		smartctl -A /dev/sda (for simple output)


C) Now initialize git repository:
	
	1 - git init

D) Add all files to the staging area:
	
	1 - git add .
		or 
	    git add *

E) Make your first commit:

	1 - git commit -a -m "1st day"


Finished! From now on, you'll only need to do steps B) and E)!!!
IF YOU EVER add a NEW DISK, just follow steps B) to E)!!

Final Considerations:
	To check disk health differences you can type in:
		1 - git log
		2 - git diff <commit_id>

this will show you existing differences between commits


THE END

----------------------------------------
================================================================================
OLD KERNELS
Linux zen docs, headers, kernel:
	https://archive.archlinux.org/packages/l/linux-zen/linux-zen-5.12.8.zen1-1-x86_64.pkg.tar.zst
	https://archive.archlinux.org/packages/l/linux-zen-docs/linux-zen-docs-5.12.8.zen1-1-x86_64.pkg.tar.zst
	https://archive.archlinux.org/packages/l/linux-zen-headers/linux-zen-headers-5.12.8.zen1-1-x86_64.pkg.tar.zst

sudo pacman -U https://archive.archlinux.org/packages/l/linux-zen/linux-zen-5.12.8.zen1-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/l/linux-zen-docs/linux-zen-docs-5.12.8.zen1-1-x86_64.pkg.tar.zst https://archive.archlinux.org/packages/l/linux-zen-headers/linux-zen-headers-5.12.8.zen1-1-x86_64.pkg.tar.zst
----------------------------------------
================================================================================
MAKING YOUR SYSTEM BOOT UP FASTER

---DISABLE STAGGERED SPIN-UP:

This feature allows notebooks/laptops to save battery by turning on & mounting 1 hard drive at a time.
Disabling it should boost your boot up time significantly(Explanation Below):

	1 - Check if you have it enabled:
			# dmesg | grep 'SSS'

	2 - Add this to your OS kernelentry parameters on /boot/grub/grub.cfg:
			libahci.ignore_sss=1
	 ^ It should look like this:

	linux /boot/vmlinuz-linux-zen root=UUID=555d2084-401a-4002-8217-61547c0eedab rw quiet acpi=off mitigations=off pcie_bus_perf zswap.enabled=0 libahci.ignore_sss=1


VERY IMPORTANT: However, disabling this function isn't recommended, 
because it's also meant to protect the PSU from Power Surges, for example:
the energy used to start spinning an HDD at 7200rpm is way higher than the energy used to keep it spinning.

In other words, having too much HDDs connected and starting to work all at once can cause the PSU to either 
fail from outtages or suffer from Power Surges which could lead to permanent damage of your PSU over the time.

However, this isn't a problem if you have a low ammount of HDD Disks, this will only be a problem if you have
10 or more disks.

----------------------------------------
================================================================================
ON GOING ISSUE:

Jun 08 23:26:04 archlinx /usr/lib/gdm-x-session[2075]: wine: Unhandled page fault on read access to FFFFFFFFFFFFFFFF at address 00007FD31F1DCD73 (thread 019c), starting debugger...

----------------------------------------
================================================================================
NVIDIA ONGOING ISSUE 2: - SOLVED: HOW TO START NVIDIA FAN
nvidia fan not working on latest Drivers:
Xorg needs to be running as root!

Check that you're not running Xorg as an user:
	ps -o user $(pgrep Xorg)

----------------------------------------
================================================================================
HOW TO START NVIDIA FAN

1 - First, start your system as you normally would:

	1.1 - sudo systemctl start firewalld
	1.2 - sudo systemctl start dhcpcd
	1.3 - sudo systemctl start gdm

2 - Then press ctrl+alt+f2 and login as root!
	2.1 - startx
	2.2 - nvidia-settings
	2.3 - set your fan speed

3 - ctrl+alt+f1 and login!


If necessary, copy a file called .nvidia-settings-rc from user into /root. This will normally be created
by nvidia-settings, so don't worry.

Pre-Requirements: you need to have coolbits enabled!

Note.: This issue only occurs when starting GDM.
on SDDM this doesn't occurs(according to few users), however Nvidia GPUs don't work properly on wayland yet.
----------------------------------------
================================================================================
HOW TO USE MANUAL PAGES

$ man --path						#Displays the location of the manpages
$ man -a <command>					#Displays all manual sections for that manpage, sequentially.

$ man <command>						#Shows the 1st manual page for that command.

$ man <command>. 7					#Shows 7th section for a given manual page. - DEPRECATED?
$ man 7 man						 ^ Same as above - WORKS?
$ man man(7)						 ^ Same as above - WORKS?

$ man -w man						#Displays the location of a manpage rather than the manpage
							itself

$ man -k "search_string"				#Searches manpages containing a search string.
							 ^ VERY USEFUL

For more help, type in: man man
---------------------------
HOW TO USE TLDR
	This requires TLDR installed on the system.

$ tldr <command> 			#Print the tldr page for a specific command (hint: this is how you got here!).

$ tldr <command> <subcommand> 		#Print the tldr page for a specific subcommand.

$ tldr --language <language_code> <command> 	#Print the tldr page for a command in the given 
						[L]anguage (if available, otherwise fall back to English).

$ tldr --platform <android|common|freebsd|linux|windows> <command> 	#Print the tldr page for a command 
								 	for a specific [p]latform.

$ tldr --update 				#[u]pdate the local cache of tldr pages.

$ tldr --list 				#List all pages for the current platform and `common`.
----------------------------------------
HOW TO USE INFO PAGES
$ info <program>						#Displays info documents for given program, if available

Example:
info make
================================================================================
DECREASING/CAPPING/LIMITING FPS IN GAMES

Capping FPS in games can increase performance & input delay in some games that would otherwise run slow when trying to output full 60FPS.

----------------------------------------
================================================================================
HOW TO CHECK GNOME SETTINGS:

--- LISTING SCHEMAS:
	gsettings list-schemas

--- FETCHING KEYS FROM SCHEMAS:
	gsettings list-keys org.gnome.desktop.screensaver

--- GETTING AND SETTINGS KEYS:
	gsettings get org.nome.desktop.screensaver lock-enabled
	gsettings set org.nome.desktop.screensaver lock-enabled false  

----------------------------------------
================================================================================
HOW TO SYSTEMCTL:
# systemctl status --all			#This will show all status data to the current system
						 ^ including failed services and succesfull ones
# systemctl --failed				#Shows systems that failed.


Personal Note.: State - Degraded just means some of your services failed to start.

	HINT:
		# systemctl <tab-key press>			##Lists available commands
		# systemctl --<tab-key press>			##Lists available commands that start with --

		Service Files located in /usr/lib/systemd (/lib/systemd).

		Finding and reading service files is as easy as:
			# find /usr/lib/systemd/ -iname '*service_name*'


	ADVANCED SYSTEMCTL:

		# systemctl --all			##Lists all available service units in the system
		# systemctl --failed			##Show all failed units
		# systemctl --firmware-setup		##Tells the firmware to display setup menu
							(SHOW BIOS ON RESTART/STARTUP)
		
		$ systemctl --user status <service_name> #Display status of the service for current user only.
		# systemctl status <service_name> 	#Display status of the service, system-wide.

		$ systemctl --user show <service_name>	#Show details about a given service name
							^ User-only, not system wide.
		# systemctl show <service_name>		#Same as above, system-wide

		# systemtl list-machines
		# systemctl list-unit-files		#(Very Useful) Lists available(enabled/disabled) system wide services
		$ systemctl --user list-unit-files	#(Very Useful) Lists available(enabled/disabled) user-only services
							^ Always current runnning user.

		# systemctl list-jobs			#
		# systemctl list-timers			#
		# systemctl list-automount		#

		# systemctl --all list-dependencies 	#Lists enabled/disables/unloaded services
		# systemctl list-dependencies		#(Useful) Lists service dependencies
							^ Lists only active services and it's dependencies
		$ systemctl --user list-dependencies	#(Useful) Lists service dependencies for current user
							^ Lists only active services
		# systemctl --user --all list-dependencies 	#Lists enabled/disables/unloaded services for current user

		# systemctl rescue			##Enters system rescue mode
							^WARNING:Log-off all users, shutdowns every running program/service.

		# systemctl emergency			##Enters system emergency mode
							^WARNING: Same as above

		# systemctl daemon-reload		##Reload systemd manager configuration 
		# systemctl daemon-reexec		##Reexecute systemd manager


HOW TO CREATE A USER SERVICE
	This is untested and might be incomplete:
		1) Open the service editor where 'maxvolume.service' is the filename:
			$ systemctl --user edit --full --force maxvolume.service

		2) Copy and paste the code below:
			[code]
				[Unit]
				After=wireplumber.service
				Description=Set pipewire audio level to 100%

				[Service]
				ExecStart=/bin/amixer set Master 100%
				Type=OneShot

				[Install]
				WantedBy=default.target
			[/code]
	
		3) Enable Service:
			$ systemctl --user enable --now maxvolume.service
----------------------------------------
================================================================================
USING PS, DBUS-MONITOR, TOP, PKILL, PGREP TO HANDLE SYSTEM & USER PROCESSES:
# ps ax						#Lists all system/user processes
						 ^ reports a snapshot of the current processes

# top						#Persistently does what ps ax does as snapshots
						 ^ keeps running showing current system processes only

# pgrep -a <name>					#Shows id, path, arguments of a given system service/program 
						 ^ by it's name

# pgrep -u root sshd				#Displays all running processes owned by user root group sshd

# pgrep -a dbus 					#Shows all services ran by dbus at some point

# pgrep -au <username> 				#Shows all services ran by username

dbus-monitor | dbus-cleanup-socket | dbus-launch  


MONITORING GNOME SCREENSAVER CHANGES:
	dbus-monitor --address $DBUS_SESSION_BUS_ADDRESS --monitor interface='org.gnome.ScreenSaver'

WATCHING FOR A SPECIFIC SCREENSAVER SIGNAL:
	dbus-monitor --address $DBUS_SESSION_BUS_ADDRESS --monitor interface='org.gnome.ScreenSaver' member='SessionIdleChanged'


	----Known Signals/Members:
		ActiveChanged, SessionIdleChanged, AuthenticationRequestBegin, AuthenticationRequestEnd
SEND DISABLING GNOME SCREENSAVER:
dbus-send --bus=$DBUS_SESSION_BUS_ADDRESS --type=method_call --print-reply --reply-timeout=20000 /org/gnome/ScreenSaver org.gnome.ScreenSaver.SetActive boolean:false                


Signature:
dbus-send [--help] [--system | --session | --bus=ADDRESS | --peer=ADDRESS] [--dest=NAME] [--type=TYPE] [--print-reply[=literal]] [--reply-timeout=MSEC] <destination object path> <message name> [contents ...]

^ THE LAST ONE WORKS!

SOURCE: https://people.gnome.org/~mccann/gnome-screensaver/docs/gnome-screensaver.html#dbus-interface
----------------------------------------
================================================================================
How to disable a dbus inerface?

----------------------------------------
================================================================================
INCREASE STEAM PERFORMANCE:
Disable FSYNC in your proton folder, on the following file: user_settings.py

----------------------------------------
================================================================================
HTOP EXPLAINNING:
At the top of HTOP, there will be RES | SHR | S,
S means the state of the running process, which can be: Running, Sleeping, Zombie, and 'D'

----------------------------------------
================================================================================
interrupt took too long (2502 > 2500), lowering kernel.perf_event_max_sample_rate to 79000

----------------------------------------
================================================================================
TURN OFF I/O SCHEDULER:
Check if it's on by default with:

1 - cat sys/block/{DEVICE-NAME}/queue/scheduler 

Turn I/O Scheduler OFF (OLD):
	1 - sudo -iu root
	2 - echo "none" > /sys/block/{DEVICE-NAME}/queue/scheduler

Make it Permanent (OLD):
vim /etc/default/grub
GRUB_CMDLINE_LINUX_DEFAULT="quiet elevator=noop"		#DEPRECATED
VERY IMPORTANT: this requires generating a new grub configuration file:
	# grub-mkconfig -O /boot/grub/grub.cfg

ex.:
archlinx% cat /sys/block/sdb/queue/scheduler
mq-deadline kyber [bfq] none


https://unix.stackexchange.com/questions/362679/perf-interrupt-took-too-long-but-perf-not-being-installed
https://www.netcup-wiki.de/wiki/KVM_Tuning

ISSUE:
archlinx kernel: perf: interrupt took too long (2502 > 2500), lowering kernel.perf_event_max_sample_rate to 79000

I/O Scheduler can cause performance issues
----------------------------------------
================================================================================
Kernel Things:
BPF PROGRAM

       The  bpf()  system  call  performs a range of operations related to extended Berkeley Packet Filters.  Extended BPF (or eBPF) is similar to the original ("classic") BPF (cBPF) used to filter network packets.  For both cBPF and
       eBPF programs, the kernel statically analyzes the programs before loading them, in order to ensure that they cannot harm the running system.

       eBPF extends cBPF in multiple ways, including the ability to call a fixed set of in-kernel helper functions (via the BPF_CALL opcode extension provided by eBPF) and access shared data structures such as eBPF maps.
----------------------------------------
================================================================================
LISTING ALL LINUX KERNEL MODULES CURRENTLY IN USE:

$ lsmod 			#Show the status of modules in the Linux Kernel
				^ Nicely formats the contents of the /proc/modules
				showing all currently loaded modules.

---Blacklisting Kernel Modules:
You can blacklist modules from being loaded at boot 
by adding them to: etc/modprobe.d/blacklists.conf

ex.: 
[CONTENTS]
	blacklist iTCO_wdt
	blacklist pcspkr
	blacklist joydev
	blacklist mousedev
	blacklist mac_hid
	blacklist uvcvideo
[/CONTENTS]
	
Hint2: You can try removing modules that are installed by have no use by any of your currently running programs.
----------------------------------------
================================================================================
FAST VIDEO PLAYBACK (ONLY FOR INTEL GRAPHIC CARD USERS):
You might face slow loading 4K videos on VLC even on a high end desktop let alone laptops, which can't even load HD.
Install:
A) intel-media-driver
B) intel-media-sdk

pacman -S intel-media-driver intel-media-sdk


----------------------------------------
================================================================================
I/O SCHEDULER / DISK SCHEDULERS
Schedulers can help reduce your systems latency, but will require more processing power.

If you are an HDD user, use BFQ IO scheduler.
If you are an SSD user, use mq-deadline IO scheduler.
If you use NVME SSD, use none IO scheduler.

Create an UDEV Rules as followed:
--- FOR SSD(2023):
[file /etc/udev/rules.d/60-ssd.rules]
ACTION=="add|change", KERNEL=="sdb", ATTR{queue/rotational]=="0", ATTR{queue/scheduler}="none"
[/file]

----FOR SSDs(2022):
/etc/udev/rules.d/60-ssd.rules

[CONTENTS]
ACTION=="add|change", KERNEL=="sd[a-z]*", ATTR{queue/rotational}=="0", ATTR{queue/scheduler}="mq-deadline"
[/CONTENTS]

----FOR NVM SSDs:
/etc/udev/rules.d/60-nvm.rules

[CONTENTS]
ACTION=="add|change", KERNEL=="nvme[0-9]*", ATTR{queue/scheduler}="none"
[/CONTENTS]

----FOR HDDs:
/etc/udev/rules.d/60-hdd.rules

[CONTENTS]
ACTION=="add|change", KERNEL=="sd[a-z]*", ATTR{queue/rotational}=="1", ATTR{queue/scheduler}="bfq"
[/CONTENTS]

----USING A SINGLE FILE FOR ALL OF THEM:
/etc/udev/rules.d/60-ioschedulers.rules

[CONTENTS]
# SSDs
ACTION=="add|change", KERNEL=="sd[a-z]*", ATTR{queue/rotational}=="0", ATTR{queue/scheduler}="mq-deadline"

# NVME
ACTION=="add|change", KERNEL=="nvme[0-9]*", ATTR{queue/scheduler}="none"

# HDD
ACTION=="add|change", KERNEL=="sd[a-z]*", ATTR{queue/rotational}=="1", ATTR{queue/scheduler}="bfq"
[/CONTENTS]

----------------------------------------
================================================================================
TURNING OFF SLEEP MODE FROM USB DEVICES:

1 - Type cat /sys/modules/usbcore/parameters/autosuspend
Output should be 2 if sleep mode is enabled.

2 - add usbcore.autosuspend=-1 as one of your kernel options on /boot/grub/grub.cfg

3 - Reboot and check the output of step 1

----------------------------------------
================================================================================
PRELOADING PROGRAMS INTO MEMORY (AVOID THIS, USE ZICRORAM INSTEAD, LOOK ON GITHUB):
Preloading is the action of putting and keeping target files into the RAM. The benefit is that preloaded applications start more quickly because reading from the RAM is always quicker than from the hard drive. However, part of your RAM will be dedicated to this task, but no more than if you kept the application open. Therefore preloading is best used with large and often-used applications like Firefox and LibreOffice.


gopreload-gitAUR is a small daemon created in the Gentoo forum. To use it, first run this command in a terminal for each program you want to preload at boot:

1 - gopreload-prepare program

1.2(optional) - For regular users, take ownership of /usr/share/gopreload/enabled and /usr/share/gopreload/disabled, type:
	#chown username:users /usr/share/gopreload/enabled /usr/share/gopreload/disabled

	^Then follow step 1 again.

2 - Then Open the Program, and as instructed, ONLY press Enter when the program is fully loaded.
This will add a list of files needed by the program in /usr/share/gopreload/enabled. 

3 - To load all lists at boot, enable the systemd service file gopreload.service.

4 - To disable the loading of a program, remove the appropriate list in /usr/share/gopreload/enabled or move it to /usr/share/gopreload/disabled

5 - It is advised to run gopreload-prepare after system upgrades to refresh the file lists. For the task, the following batch tool come handy: 
	# gopreload-batch-refresh.sh

6 - Further configurations are found on: /etc/gopreload.conf

Source: https://wiki.archlinux.org/title/Preload


GUIA: https://shop.westerndigital.com/pt-br/products/portable-drives/wd-elements-portable-usb-3-0-hdd#WDBUZG0010BBK-WESN

https://www.youtube.com/watch?v=afUdR6c8ikY
----------------------------------------
================================================================================
CHECKING WHICH KERNEL MODULES ARE LOADED FOR WHICH DEVICES:
$ lscpi -k				#VERY IMPORTANT

$ glxinfo -B				#SHOWS GPU FEATURES

$ sensors					#CHECKS CPU TEMPERATURE
# sensors-detect				#HELPS YOU DETERMINE WHICH KERNEL MODULES YOU NEED
					mines.: coretemp, it87

					note: capable of displaying cpu fan speed / cpu fan rotation


----------------------------------------
================================================================================
NEW FROM BELOW
----------------------------------------
================================================================================
SETTING MAXIMUM CAPACITY(LIMIT) CPU PERFORMANCE FOR PROCESSES/SERVICES/PROGRAMS ON LINUX:
Install: cputool

cputool --cpu-limit 20 -p 8275 


syntax: 
	cputool --cpu-limit <percent> -p <process_id>


if a given process/service has more than on program associated with it,
then follow the below steps:

	1 - pidof <program_name>
	outputs: 9592 3643 3642 3641 3640 3638 3637 1780

	2 - type: 
		# cputool --cpu-limit <percent> -P <process_id>

	note.: Use capital -P instead of -p


OBS.: Note that the shell doesn’t expect any user input while cputool is running; therefore becomes unresponsive. To kill it (this will terminate the CPU usage limitation operation), press Ctrl + C.

OBS2.: Usually a good value would be around 250 or 200 depending on the task.
----------------------------------------
================================================================================
SETTING CPU SCALING GOVERNOR TO MAXIMUM PERFORMANCE:
---Showing your current scale governor settings:
	# cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

---Setting your current scale governor to PERFORMANCE:
	$ echo "performance" | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

---Setting your current scale governor to POWERSAVING:
	$ echo "powersave" | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
----------------------------------------
================================================================================
AVOIDING HEATING UP YOUR CPU TOO MUCH:

---Showing your current max_perf_pct settings:
	$ cat /sys/devices/system/cpu/intel_pstate/max_perf_pct

---Setting your current max_perf_pct settings:
	$ echo 70 | sudo tee /sys/devices/system/cpu/intel_pstate/max_perf_pct


THERMALD:
Advice: Enable thermald, to control maximum CPU frequency automatically, thereby not universally limiting the upper 
bound. 

# systemctl enable thermald
# systemctl start thermald

NOTE: Thermald will only work after restarting your computer! VERY IMPORTANT
	1 - Install thermald
	2 - Enable thermald
		# systemctl enable thermald
		# systemctl start thermald
	3 - Restart computer!

Successfully restarting after enabling termald will display this:

	[OUTPUT]
		# systemctl status thermald

		● thermald.service - Thermal Daemon Service
		     Loaded: loaded (/usr/lib/systemd/system/thermald.service; enabled; preset: disabled)
		     Active: active (running) since Wed 2023-03-01 14:17:47 -03; 2h 5min ago
		   Main PID: 555 (thermald)
		      Tasks: 3 (limit: 18925)
		     Memory: 9.1M
			CPU: 903ms
		     CGroup: /system.slice/thermald.service
			     └─555 /usr/bin/thermald --systemd --dbus-enable --adaptive

		Mar 01 14:17:47 archlinux systemd[1]: Started Thermal Daemon Service.
		Mar 01 14:17:47 archlinux thermald[555]: 13 CPUID levels; family:model:stepping 0x6:3a:9 (6:58:9)
		Mar 01 14:17:47 archlinux thermald[555]: 13 CPUID levels; family:model:stepping 0x6:3a:9 (6:58:9)
		Mar 01 14:17:47 archlinux thermald[555]: sensor id 8 : No temp sysfs for reading raw temp
		Mar 01 14:17:47 archlinux thermald[555]: sensor id 8 : No temp sysfs for reading raw temp
		Mar 01 14:17:47 archlinux thermald[555]: sensor id 8 : No temp sysfs for reading raw temp
		Mar 01 14:17:47 archlinux thermald[555]: Config file /etc/thermald/thermal-conf.xml does not exist
		Mar 01 14:17:47 archlinux thermald[555]: Config file /etc/thermald/thermal-conf.xml does not exist
		Mar 01 14:17:47 archlinux thermald[555]: Config file /etc/thermald/thermal-conf.xml does not exist
		Mar 01 14:17:47 archlinux thermald[555]: Polling mode is enabled: 4
	[/OUTPUT]
----------------------------------------
================================================================================
MY BOOTUP COMMANDS ( VERY OLD, NOT USING ANYLONGER) :
echo "performance" | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor;
echo 70 | sudo tee /sys/devices/system/cpu/intel_pstate/max_perf_pct;
echo -1 | sudo tee /sys/modules/usbcore/parameters/autosuspend;
sudo systemctl start firewalld;
sudo systemctl start dhcpcd;
----------------------------------------
================================================================================
OVERCLOCKING AND UNDERCLOCKING:
Overclocking is dangerous, avoid it at all costs,
even with a water cooling system it's still dangerous! Things can go catratrophic and you might have to buy an entire new PC from scratch! Even if your overclock settings didn't cause any apparent CPU/GPU overheating, it'll still burn internal motherboard components which lack protection for the Higher Required Voltage, not all components are protected from higher voltage, and you'll end up with either a broken PC or worse: a defective PC with intermitent untraceable problems/errors that will leave you scratching your head for weeks.

Underclocking(Downclocking) is a better safe option, as long as you're not changing voltages!
Underclocking will actually protect both your CPU lifetime and protect your CPU from overheating, giving you a better boost in games that rely too much on the CPU/GPU. Overheating is a well known cause of slowdowns!!!


----------------------------------------
================================================================================
INSTALL THERMALD:
Thermald will help auto-manage CPU Thermal Status so it doesn't burn itself!

	On archlinux:
		#sudo pacman -Syu thermald
		#systemctl enable thermald
		#systemctl start thermald


NOTE: Thermald will only work after restarting your computer! VERY IMPORTANT
	1 - Install thermald
	2 - Enable thermald
		#systemctl enable thermald
		#systemctl start thermald
	3 - Restart computer!

Successfully running termald will display this:

	[OUTPUT]
		#systemctl status thermald

		● thermald.service - Thermal Daemon Service
		     Loaded: loaded (/usr/lib/systemd/system/thermald.service; enabled; preset: disabled)
		     Active: active (running) since Wed 2023-03-01 14:17:47 -03; 2h 5min ago
		   Main PID: 555 (thermald)
		      Tasks: 3 (limit: 18925)
		     Memory: 9.1M
			CPU: 903ms
		     CGroup: /system.slice/thermald.service
			     └─555 /usr/bin/thermald --systemd --dbus-enable --adaptive

		Mar 01 14:17:47 archlinux systemd[1]: Started Thermal Daemon Service.
		Mar 01 14:17:47 archlinux thermald[555]: 13 CPUID levels; family:model:stepping 0x6:3a:9 (6:58:9)
		Mar 01 14:17:47 archlinux thermald[555]: 13 CPUID levels; family:model:stepping 0x6:3a:9 (6:58:9)
		Mar 01 14:17:47 archlinux thermald[555]: sensor id 8 : No temp sysfs for reading raw temp
		Mar 01 14:17:47 archlinux thermald[555]: sensor id 8 : No temp sysfs for reading raw temp
		Mar 01 14:17:47 archlinux thermald[555]: sensor id 8 : No temp sysfs for reading raw temp
		Mar 01 14:17:47 archlinux thermald[555]: Config file /etc/thermald/thermal-conf.xml does not exist
		Mar 01 14:17:47 archlinux thermald[555]: Config file /etc/thermald/thermal-conf.xml does not exist
		Mar 01 14:17:47 archlinux thermald[555]: Config file /etc/thermald/thermal-conf.xml does not exist
		Mar 01 14:17:47 archlinux thermald[555]: Polling mode is enabled: 4
	[/OUTPUT]
----------------------------------------
================================================================================
DO NOT EVER USE LINUX WITHOUT READING THIS:
Read Topics Mentioned Below:
	SETTING CPU SCALING GOVERNOR TO MAXIMUM PERFORMANCE
	AVOIDING HEATING UP YOUR CPU TOO MUCH
	INSTALL THERMALD
	INSTALL CLAMAV
	INSTALL FIREWALLD

Also install GPU proprietary drivers so Linux Generic ones doesn't burn your GPUs,
don't let linux handle your GPU without proprietary drivers if you're doing any sort of 
HEAVY INTENSIVE GPU APPLICATION!!!(Same goes for CPUs)

Also ALWAYS use well-known File Systems like EXT, BTRFS, XFS, avoid using beta/git development versions of these,
always use the stable ones! Avoid File Systems made by specific 'Niches' of people, specially if they're Closed-Source/Not-Free/Proprietary on linux!(SEE REASON BELOW)

---Avoid Proprietary or Close-Source Code:
Last but not least, avoid proprietary software like the plague(Unless the guys making it are big boys like Nvidia, AMD or Intel), the reason of this is because Linux exposes all of it's Kernel Functionalities to the users, hackers, script kids, and all old bad people around the world by letting it OPEN AND FREE! That in turn of course means anyone can 
exploit the security holes left open by Linux Installations made by Linux Junior Users. All they have to do is read 
about the exploits themselves, either through linux very own official documentation and source-code, 
or by following instructions on 3rd Party Sites which will usually teach how to exploit those Linux Junior User 
Installations.

Supporting proprietary software on linux means no one will be able to review bad code or malicious code inserted by 
those people in their own program(s)! A simple "echo '0' > sudo tee ..."can turn off your CPU/GPU Fans from working
and therefore cause it to burn from overheating without you noticing it, simply because the Linux Kernel documentation 
is there along with the open-source code allowing anyone one with bad intentions to learn and exploit less 
experienced users. This in turn, brings to my last advice... 

---Do not install programs without knowing how trustful they are:
Never ever install packages/programs/scripts from 3rd parties without knowing if they're trustful enough.
As a golden thumb rule, companies that make Close-Source code or Proprietary Software AREN'T trustful at all!

Even when no malicious code are involved, these will still often break your system! Because people can't review
changes & features made to the project, that means users can't make complains or sugest fix solutions about 
bad new features that cause bugs, or just plain bad code that cause performance issues, and those new features 
will remain broken for weeks, months, years without anyone fixing it AND some of these issues can cause perma-damage 
to your system without the user's notice.

The reason this happens more often on linux than windows(or other closed systems) is because of the nature of 
the Open-Source Linux Kernel: most closed-source linux software developers/coders may try tuning/changing your 
fully-functional kernel settings so their programs will run faster/better compared to competitors and the problem with 
this is that these "tunes" can cause permanent damage to BOTH your Hardware and Filesystems AND they can't 
been seen or traced back on closed-source code and proprietary software, these "tunes" can damage your hardware 
permanently without you ever noticing what has happened on your system or what has caused it.

you've been advised!


----------------------------------------
================================================================================
WHAT'S VIM EX MODE?

----------------------------------------
================================================================================
GREPPING DEVICES FROM DMESG(KERNEL):
# dmesg | grep -i scsi                                #Greps all SCSI devices from kernel
# journalctl -k					 ^ Same as above

----------------------------------------
================================================================================
Intel Powerclamp is mostly used by thermald for preventing CPU overheat.

Jun 14 23:20:41 archlinx kernel: intel_powerclamp: Stop forced idle injection
Jun 14 23:21:25 archlinx kernel: intel_powerclamp: Start idle injection to reduce power

NOHZ tick-stop error: Non-RCU local softirq work is pending, handler #02!!!

THIS IS ON MY HTOP: /usr/bin/efreetd

vivaldi-bin[1517]: segfault at c ip 00007fb94fae1f2d sp 00007ffe884d0c68 error 6 in libnvidia-glcore.so.465.31[7fb94eb68000+11c7000]

NOTE: Thermald will only work after restarting your computer! VERY IMPORTANT
	1 - Install thermald
	2 - Enable thermald
		# systemctl enable thermald
		# systemctl start thermald
	3 - Restart computer!

Successfully running termald will display this:

	[OUTPUT]
		#systemctl status thermald

		● thermald.service - Thermal Daemon Service
		     Loaded: loaded (/usr/lib/systemd/system/thermald.service; enabled; preset: disabled)
		     Active: active (running) since Wed 2023-03-01 14:17:47 -03; 2h 5min ago
		   Main PID: 555 (thermald)
		      Tasks: 3 (limit: 18925)
		     Memory: 9.1M
			CPU: 903ms
		     CGroup: /system.slice/thermald.service
			     └─555 /usr/bin/thermald --systemd --dbus-enable --adaptive

		Mar 01 14:17:47 archlinux systemd[1]: Started Thermal Daemon Service.
		Mar 01 14:17:47 archlinux thermald[555]: 13 CPUID levels; family:model:stepping 0x6:3a:9 (6:58:9)
		Mar 01 14:17:47 archlinux thermald[555]: 13 CPUID levels; family:model:stepping 0x6:3a:9 (6:58:9)
		Mar 01 14:17:47 archlinux thermald[555]: sensor id 8 : No temp sysfs for reading raw temp
		Mar 01 14:17:47 archlinux thermald[555]: sensor id 8 : No temp sysfs for reading raw temp
		Mar 01 14:17:47 archlinux thermald[555]: sensor id 8 : No temp sysfs for reading raw temp
		Mar 01 14:17:47 archlinux thermald[555]: Config file /etc/thermald/thermal-conf.xml does not exist
		Mar 01 14:17:47 archlinux thermald[555]: Config file /etc/thermald/thermal-conf.xml does not exist
		Mar 01 14:17:47 archlinux thermald[555]: Config file /etc/thermald/thermal-conf.xml does not exist
		Mar 01 14:17:47 archlinux thermald[555]: Polling mode is enabled: 4
	[/OUTPUT]
----------------------------------------
================================================================================
ENABLING NOHZ FOR PERFORMANCE

1 - Check if you have HZ enabled first
	$ getconf CLK_TCK

2 - If you have it enabled, disable it at boot time with: nohz=on
	on your grub.cfg @ /etc/boot/grub/grub.cfg as an actual kernel parameter

3 - reboot!


Enabling NOHZ will actually Disable HZ!

---Checking if HZ is at Regular Normal Mode:
1 - # cat /proc/interrupts | grep -i time; sleep 10; cat /proc/interrupts | grep time

This above will output the following data on a single core system:
 16:   10404858      INTC  68 Level     gp_timer
 16:   10514798      INTC  68 Level     gp_timer

On a multi-core system this might get confusing, it'll be a per core listing.

2 - Next divide the value by 10!
10404858 / 10 = 1040485.8 HZ

That means HZ is set to Mode 10000HZ

3 - Alternatively, you can check the actual options by unpacking the file /proc/config.gz or by doing:
	$ zcat /proc/config.gz | grep CONFIG_HZ
		or
	$ zgrep CONFIG_HZ /proc/config.gz

4 - Finally, Check for parts that look like this:

CONFIG_HZ_FIXED=0
# CONFIG_HZ_100 is not set
# CONFIG_HZ_200 is not set
# CONFIG_HZ_250 is not set
# CONFIG_HZ_300 is not set
# CONFIG_HZ_500 is not set
# CONFIG_HZ_1000 is not set
CONFIG_HZ_10000=y
CONFIG_HZ=10000
CONFIG_SCHED_HRTICK=y


10000 is way off charts! READ FINAL CONSIDERATIONS!
---Checking your real frequencies:

1 - Get the code below and compile it with: gcc frequency-test.c

2 - Run it sudo ./a.out

3 - Code Below:

[code]

	#include <signal.h>
	#include <stdio.h>
	#include <stdlib.h>
	#include <string.h>
	#include <sys/time.h>

	#define USECREQ 250
	#define LOOPS 1000

	void event_handler(int signum)
	{
	    static unsigned long cnt = 0;
	    static struct timeval tsFirst;
	    if (cnt == 0)
	    {
		gettimeofday(&tsFirst, 0);
	    }
	    cnt++;
	    if (cnt >= LOOPS)
	    {
		struct timeval tsNow;
		struct timeval diff;
		setitimer(ITIMER_REAL, NULL, NULL);
		gettimeofday(&tsNow, 0);
		timersub(&tsNow, &tsFirst, &diff);
		unsigned long long udiff = (diff.tv_sec * 1000000) + diff.tv_usec;
		double delta = (double)(udiff / cnt) / 1000000;
		int hz = (unsigned)(1.0 / delta);
		printf("kernel timer interrupt frequency is approx. %d Hz", hz);
		if (hz >= (int)(1.0 / ((double)(USECREQ) / 1000000)))
		{
		    printf(" or higher");
		}
		printf("\n");
		exit(0);
	    }
	}

	int main(int argc, char **argv)
	{
	    struct sigaction sa;
	    struct itimerval timer;

	    memset(&sa, 0, sizeof(sa));
	    sa.sa_handler = &event_handler;
	    sigaction(SIGALRM, &sa, NULL);
	    timer.it_value.tv_sec = 0;
	    timer.it_value.tv_usec = USECREQ;
	    timer.it_interval.tv_sec = 0;
	    timer.it_interval.tv_usec = USECREQ;
	    setitimer(ITIMER_REAL, &timer, NULL);
	    while (1) ;
	}

[/code]

4 - VERY DANGEROUS: READ FINAL CONSIDERATIONS!

---FINAL CONSIDERATIONS:
10'000HZ isn't normal settings! It's way off the charts!

1st) If the timer interrupt frequency determined on your system is for example around 250Hz, 
it will be very difficult for any near to real-time or multimedia application to send out an isochronous data 
stream of 250 packets per second (pps).

When configuring your kernel you can choose a timer frequency of either 100Hz, 250Hz, 300Hz or 1000Hz. All of these are supported, and although 1000Hz is the default it’s not always the best.

2nd) People will generally choose a high value when they value latency (a desktop or a webserver) and a low value when they value throughput (HPC).

3rd) The higher the value, the less GPU dependent your system will become. This means some games might become full
CPU Dependent, thus making some games slow!

4th) Higher values will increase CPU Usage, therefore making the CPU heat up a bit more!
Always check your temperatures with SENSOR!

5th) Games that depend on more CPU will have less CPU available for them!

6th) It's impossible to disable HZ_CONFIG or set it to different values without recompiling the kernel.
Older CPUs also make that task impossible, since they have no better alternate route making the use of 
HZ_CONFIG is mandatory!

---Finally:
	To make permanent changes, you'll need to add the to your /etc/mkinitcpio.conf
and finally rebuild all your kernel current initramfs it with: #mkinitcpio -P

once it's done, reboot!

The values you should add to mkinitcpio.conf are the ones LISTED from the following command: 
	$zgrep HZ /proc/config.gz

Add the changes you want to the last line and REBUILD with mkinitcpio -P as previously stated!
----------------------------------------
================================================================================
DISABLING HPET FOR INCREASED GPU USAGE
	PERFORMANCE INCREASE 
Notice.: This shouldn't be required unless you're having GPU Usage Issues, which is usually
correlated to other problems.

Pre-requisites: HPET is only available on intel

1 - Finding out if you have HPET Enabled:
	$ zcat /proc/config.gz | grep HPET
		or
	$ zgrep HPET /proc/config.gz 

2 - Add: nohpet
	or
    Add: hpet=disable

^ On your grub.cfg file @ /etc/boot/grub.cfg as an actual kernel parameter

*Note.: ENABLING HPET on actually let the Computer better in general.
----------------------------------------
================================================================================
	LISTING WHICH KERNEL MODULES AND FEATURES YOU HAVE AVAILABLE IN YOUR SYSTEM:
mkinitcpio -M			#Lists all Kernel modules available on your system

lsmod				#Shows which Kernel Modules are in use

lspci -k			#Shows which Kernel Modules are being used by each system devices.

cat /proc/cpuinfo | grep -E "vmx|svm" --color  	#Shows if you have a vmx or svm feature enabled in kernel using cat

---CHECKING LISTING IF KERNEL FEATURE(S) ARE ENABLED AND WHAT ARE THEIR SETTINGS:

1 - $ zcat /proc/config.gz | grep <KERNEL_FEATURE_NAME>
		or
    $ zgrep <KERNEL_FEATURE> /proc/config.gz

----------------------------------------
================================================================================
LOADING PROGRAM COMMANDS AS SOON AS POSSIBLE AT BOOT TIME

1 - Create a .service file at /usr/lib/systemd/system/ as follows
	ex.: # touch /usr/lib/systemd/system/<service_name>.service

	^ file should have -rw-r--r-- as permissions:
	ex.: # chmod u=rw-,g=r--,o=r-- /usr/lib/systemd/system/<service_name>.service
	     # chown root:root /usr/lib/systemd/system/<service_name>.service

2 -  Create a symlink to that service at: /etc/systemd/system/<service_name>.service


---MORE ABOUT SYSTEMD:

DESCRIPTION(from man pages):
       systemd is a system and service manager for Linux operating systems. When run as first
       process on boot (as PID 1), it acts as init system that brings up and maintains userspace
       services. Separate instances are started for logged-in users to start their services.

       systemd is usually not invoked directly by the user, but is installed as the /sbin/init
       symlink and started during early boot. The user manager instances are started automatically
       through the user@.service(5) service.


----------------------------------------
================================================================================
SHORTCUT/HOTKEYS
COMMON TEXT EDITOR SHORTCUT KEYS:
ctrl+a				#Start of Line 
ctrl+e 				#End of Line

TERMINOLOGY/YAKUAKE/SOME TERMINALS SHORTCUT/HOTKEYS KEYS:
	^ By default, terminology doesn't work with GPU Hardware Accelerated
	^ Go to settings > Toolkit > Launch Elementary Toolkit > MORE > Rendering > Select OpenGL and restart terminology

	KDE KONSOLE HOTKEYS
		ctrl+shift+m	#Displays Menu Options
				Note: Useful if your tab gets locked into read-only mode by accident.

ctrl+shift+page_up		#[TERMINOLOGY] Creates a new window, splits terminal on the vertical
ctrl+shift+page_down		#[TERMINOLOGY] Creates a new window, splits terminal on the horizontal.
ctrl+shift+(			#[YAKUAKE] Splits terminal Horizontally
ctrl+shift+)			#[YAKUAKE] Splits terminal Vertically

shift+page_up			#Scroll up
shift+page_down			#Scroll down
ctrl+shift+t			#Creates a new tab
ctrl+shift+arrow_keys		#Moves Tab left or right
shift+arrow_keys		#Switches selected tab

ZSH VI KEYS:
ctrl+[				#Normal Mode
esc				#Normal Mode

$man zshzle 			#Hotkey manual for zsh
				^vi-cmd-mode (^[)
				^clear-screen (^L)
				^list-choices (^D)
				^

AWESOME WM SHORTCUT/HOTKEYS:
system_key+tab			#Moves to previous program that was being used in the current window
----------------------------------------
================================================================================
ONGOING PROTON / WINE ISSUE | ONGOING ISSUE | STUDY LATER:
cat /sys/devices/system/clocksource/clocksource0/current_clocksource

----------------------------------------
================================================================================
CHECK LATER:
https://www.netcup-wiki.de/wiki/KVM_Tuning
----------------------------------------
================================================================================
MORE ABOUT ENCRYPTION:
---HINT: CHECK STEGANOGRAPHY!
---HINT2: HINTS ON HOW TO HIDE YOUR KEYS
----WHAT IS ENCRYPTION?
	Encryption is a technique wherein the data to be transferred is scrambled or translated into a different format that only the receiver of the data who has the decryption key, or secret key, can access it.

Before data is encrypted, it’s referred to as plaintext, and after encryption it’s called ciphertext. The technique is majorly used to securely transfer data from one computer to another, and there are two basic categories of encryption – symmetric encryption and asymmetric encryption.

----ASYMMETRIC ENCRYPTION
	Also referred to as public-key encryption, asymmetric encryption is a technique where two keys are used for the encryption process, wherein one key will be used for encrypting the data to be transferred and the other for decrypting it at the receiver’s end. This is a slower encryption technique as it uses longer keys to provide better security. However, it’s considered to be the best way there is to share data or information between two individuals. Here’s how it works:

The sender will encrypt the data to be transferred using the public key of the intended recipient

The receiver will use their private key (one that’s associated with the public key used to encrypt the data) to decrypt the data

----SYMMETRIC ENCRYPTION
	As the name indicates, in symmetric encryption a single key is shared by both the sender and receiver of a message or data to encrypt and decrypt it. This process is pretty straightforward, and the key used remains secret or confidential, which means the person on the receiving end won’t be able to access the data until the sender shares the key with them. While this method is simpler and faster than asymmetric encryption, it may not be suitable for organizations constantly transferring huge amounts of data, as they’ll have to manage and keep the same number of keys.


----ENCRYPTION ALGORITHMS(2021):

1 - DES:
	Too Old Don't Use!

1 - RSA:
	Uses Asymmetric Key, needs to be customized, default options is easy to break.

2 - DES:
	Do not use DES, it has been broken several times

3 - TDES / 3DES:
	Do not use 3DES, Substitute for DES, also has been broken.

4 - Advanced Encryption Standard (AES):
	Substitute/Alternative for DES, made by the US, avoid using it.

5 - Blowfish:
	The key lengths in this algorithm can be anywhere between 32 and 448 bits, and the facts that it’s readily 
available in the public domain for anyone to use and that it hasn’t been cracked so far make this a go-to encryption 
method for industries and businesses.

6. Twofish
	Twofish is the successor to Blowfish, but both are quite popular and still being implemented into many 
systems today. Like DES and Blowfish, Twofish uses symmetric encryption which means the same key is used for 
encryption and decryption. 

One of the major differences between the two is that Twofish divides data into 128-bit blocks instead of 64-bit. But Twofish only goes up to 256-bit keys. It also applies the key to all of the blocks simultaneously.

---SUMMARY:
	All of them are successor to the old DES Encrytion algorithm. Save for RSA.
RSA uses Asymmetric Key(Public & Private) the public being used for encrypting someoene else's files, while the
private key is used for decrypting someone's file. So only use RSA when you need to verify file's integrity & ownership while keeping data privacy. RSA is mostly used by web-browsers, this is done to guarantee that that has not been
seen or changed by 3rd parties that may be sitting between the sender & receiver.

---NEXT STEP(STEGANOGRAPHY):
	How to hide your keys in images/files/etc using steganography tools!
Steganography (/ˌstɛɡəˈnɒɡrəfi/ (About this soundlisten) STEG-ə-NOG-rə-fee) is the practice of concealing a message 
within another message or a physical object. In computing/electronic contexts, a computer file, message, image, or 
video is concealed within another file, message, image, or video. The word steganography comes from Greek 
steganographia, which combines the words steganós (στεγανός), meaning "covered or concealed", and -graphia (γραφή) 
meaning "writing".


---HINTS ON HOW TO HIDE YOUR KEYS:
1) Make a program that asks for pin, login, password and is able to store keys for use with gpg.
Once gpg is summoned, all data required by gpg will be forwarded by the user.

2) Use steganography to hide the password/pin for the program in step_1 INSIDE THE OWN PROGRAM as heuristic rules.
only the program can decode it by following STRICT HEURISTIC RULES set into it, That way Even if the key is found out,
people would still need to study,read & understand the algorithm in order to look into the following data:
	2.1) Make map, steganograph the password/pin or key into the map.
	2.2) Generate a key and key_password on the go
		2.2.1) gpg --store to mask the key and key_password as something else.
		2.2.2) alternative: gpg --armor
	2.3) Cryptograph the map with the unmasked key and it's password.
	2.4) Mask the map key and key_password back.

3) That way, having the map decrypted won't give out the real key and key_password to the encrypted files,
because only the program will be able to look and search for tha password in the map by following it's own strict
heuristic rules.


----------------------------------------
SPSPSP NEW FROM BELOW
================================================================================
VERIFYING THERE'S SOMETHING WRONG WITH PROCESSES USING POWERTOP:

	powertop is a power consumption and power management diagnosis tool.
powertop  is a program that helps to diagnose various issues with power consumption and power management.  It
also has an interactive mode allowing one to experiment with various power management settings.  When  invok‐
ing powertop without arguments powertop starts in interactive mode.
----------------------------------------
================================================================================
ENABLING IOMMU:
Just add intel_iommu=on on your grub.cfg at /boot/grub/grub.cfg if you're an intel owner.
If you're on AMD CPU, do this instead: add iommu=on


From Wikipedia:
AMD has published a specification for IOMMU technology, called AMD-Vi.
Intel has published a specification for IOMMU technology as Virtualization Technology for Directed I/O, abbreviated VT-d.
Information about the Sun IOMMU has been published in the Device Virtual Memory Access (DVMA) section of the Solaris Developer Connection.
The IBM Translation Control Entry (TCE) has been described in a document entitled Logical Partition Security in the IBM eServer pSeries 690.
The PCI-SIG has relevant work under the terms I/O Virtualization (IOV) and Address Translation Services (ATS).
ARM defines its version of IOMMU as System Memory Management Unit (SMMU) to complement its Virtualization architecture.

----------------------------------------
================================================================================
INTEL TECHNOLOGY(TRADUZIR)
TECNOLOGIA DE VIRTUALIZAÇÃO INTEL® PARA E/S DIRIGIDA (VT-D):
	Permite redirecionar I/O de dispositivos hardware para máquinas virtuais.
ex.: é possível passar a GPU para acesso por uma máquina virtual de forma direta.

Add to /etc/mkinitcpio.conf:
	MODULES=(vfio vfio_iommu_type1 vfio_pci vfio_virqfd nvidia nvidia_modeset nvidia_drm nvidia_uvm wd719x)

Only add modules: vfio vfio_iommu_type1 vfio_pci vfio_virqfd

INTEL VT-X:
	Permite a utilização de EPTs(Extended Page Tables). 

	Intel® VT-x com Tabelas de página estendida (EPT), também conhecidas como SLATs (tradução de endereço de segundo nível), proporciona aceleração para aplicativos virtualizados de uso intensivo de memória. As tabelas de página estendida em plataformas da Tecnologia de virtualização Intel® reduzem os gastos de memória e energia, o que aumenta a vida útil da bateria por meio da otimização de hardware do gerenciamento de tabela de página.

We evaluated EPT performance by comparing it to the performance of our software-only shadow page table
technique on an EPT-enabled Intel system. From our studies we conclude that EPT-enabled systems can
improve performance compared to using shadow paging for MMU virtualization. EPT provides performance
gains of up to 48% for MMU-intensive benchmarks and up to 600% for MMU-intensive microbenchmarks. We
have also observed that although EPT increases memory access latencies for a few workloads, this cost can be
reduced by effectively using large pages in the guest and the hypervisor.

----------------------------------------
================================================================================
LINUX EXT4 DEFRAG

Use 'e4defrag' for defragging ext4 filesystem partitions.

The syntax is as follows:

	0) #e4defrag -c <device>		#Calculates fragmentation on the disk
	1) #e4defrag "path to file"		#For defragmenting a file
	2) #e4defrag -r "directory/"	  	#For defragmenting a directory
	3) #e4defrag /dev/sda1	 		#For defragmenting a partition


--more about defragmentation on EXT4 partitions/disks:

	The fourth extended file system was developed as the successor of the commonly used ext3 journaled file system. The ext4 file system has significant advantages over the ext3 and ext2 file systems.

Support for the ext4 file system has been available from the Linux kernel version 2.6.19 onwards and was officially declared stable in the Linux kernel version 2.6.28. All the latest versions of distributions like Ubuntu (9.04), Fedora (11) etc. include the ext4 file system.

 
The ext4 file system includes the “extent” approach to block management features used in JFS, and the delayed allocation feature of XFS.

--more about EXT Filesystem:
EXT3 Filesystem is just EXT2 with Journaling System.

Ext3 was mostly about adding journaling to Ext2, but Ext4 modifies important data structures of the filesystem such as the ones destined to store the file data. The result is a filesystem with an improved design, better performance, reliability, and features.
----------------------------------------
================================================================================
LINUX EXT4 REPAIRING

Use 'fsck' for repairing ext4 filesystem partitions/disks.
----------------------------------------
================================================================================
IMPROVING EXT4 PERFORMANCE

---E4RAT
E4rat is a preload application designed for the ext4 filesystem. It monitors files opened during boot, optimizes their placement on the partition to improve access time, and preloads them at the very beginning of the boot process. E4rat does not offer improvements with SSDs, whose access time is negligible compared to hard disks.

---DISABLING ACCESS TIME UPDATE
The ext4 file system records information about when a file was last accessed and there is a cost associated with recording it. With the noatime option, the access timestamps on the filesystem are not updated.

example: FILE: /etc/fstab
               /dev/sda5    /    ext4    defaults,noatime,commit=60    0    1

Doing so breaks applications that rely on access time update.

---INCREASING COMMIT INTERVAL
The sync interval for data and metadata can be increased by providing a higher time delay to the commit option.

The default 5 sec means that if the power is lost, one will lose as much as the latest 5 seconds of work. It forces a full sync of all data/journal to physical media every 5 seconds. The filesystem will not be damaged though, thanks to the journaling. The following fstab illustrates the use of commit:

FILE:	/etc/fstab
	/dev/sda5    /    ext4    defaults,noatime,commit=60    0    1

---TURNING BARRIERS OFF - DANGEROUS
DANGEROUS - VERY IMPORTANT: Warning: Disabling barriers for disks without battery-backed cache is not recommended and can lead to severe file system corruption and data loss.

Ext4 enables write barriers by default. It ensures that file system metadata is correctly written and ordered on disk, even when write caches lose power. This goes with a performance cost especially for applications that use fsync heavily or create and delete many small files. For disks that have a write cache that is battery-backed in one way or another, disabling barriers may safely improve performance.

To turn barriers off, add the option barrier=0 to the desired filesystem. For example:

FILE:	/etc/fstab
	/dev/sda5    /    ext4    noatime,barrier=0    0    1

---DISABLING JOURNALING - DANGEROUS
VERY IMPORTANT: Warning: Using a filesystem without journaling can result in data loss in case of sudden dismount like power failure or kernel lockup.

Disabling the journal with ext4 can be done with the following command on an unmounted disk:

#tune2fs -O "^has_journal" /dev/sdXN  # DANGEROUS DISABLE JOURNALING!!!!

---Source:
	https://wiki.archlinux.org/title/Ext4#Improving_performance
----------------------------------------
================================================================================
EXT4 FILESYSTEM ENCRYPTION

	Since Linux 4.1, ext4 natively supports file encryption, see the 'fscrypt' article. Encryption is applied at the directory level, and different directories can use different encryption keys. This is different from both 'dm-crypt', which is block-device level encryption, and from 'eCryptfs', which is a stacked cryptographic filesystem.

---FSCRYPT:
	The ext4, F2FS, and UBIFS file systems natively support file encryption via a common API called fscrypt 
(originally called "ext4 encryption"). With fscrypt, encryption is applied at the directory level. Different 
directories can use different encryption keys. In an encrypted directory, all file contents, filenames, and 
symlinks are encrypted. All subdirectories are encrypted too. Non-filename metadata, such as timestamps, the sizes 
and number of files, and extended attributes, is not encrypted.

"FSCRYPT" is also the name of a userspace tool to use the kernel feature of the same name. 
This article shows how to use the fscrypt tool to encrypt directories, including how to encrypt a home directory.

---PROTECTING AN ENTIRE FILESYSTEM:
	To protect an entire file system with one password, block device encryption with DM-CRYPT (LUKS) is generally a better option, as it ensures that all files on the file system are encrypted, and also that all file system metadata is encrypted. fscrypt is most useful to encrypt specific directories, or to enable different encrypted directories to be unlockable independently—for example, per-user encrypted home directories.

Unlike eCryptfs, fscrypt is not a stacked file system, i.e. it is supported by file systems natively. This makes fscrypt more memory-efficient. fscrypt also uses more up-to-date cryptography than eCryptfs, and it does not require setuid binaries. Also, eCryptfs is no longer being actively developed, and its largest users have migrated to dm-crypt (Ubuntu) or to fscrypt (Chrome OS).

See data-at-rest encryption for more information about other encryption solutions, and about what encryption does and does not do.

---KERNEL SUPPORT TO EXT4 ENCRYPTION
All officially supported kernels support fscrypt on ext4, F2FS, and UBIFS.

Users of custom kernels version 5.1 or later, make sure CONFIG_FS_ENCRYPTION=y is set. For older kernels, see the documentation.

It is also highly recommended for the kernel version to be 5.4 or later, as this allows the use of v2 encryption policies. There are several security and usability issues with v1 encryption policies.

Personal Note.: This is why you should stick to EXT4 instead of other filesystem partition. 

---DATA AT REST ENCRYPTION:
	Data-at-rest encryption ensures that files are always stored on disk in an encrypted form. The files only become available to the operating system and applications in readable form while the system is running and unlocked by a trusted user (data in use or in transit). An unauthorized person looking at the disk contents directly, will only find garbled random-looking data instead of the actual files.

For example, this can prevent unauthorized viewing of the data when the computer or hard-disk is:

1) located in a place to which non-trusted people might gain access while you are away
2) lost or stolen, as with laptops, netbooks or external storage devices
3) in the repair shop
4) discarded after its end-of-life
5) In addition, data-at-rest encryption can also be used to add SOME security against unauthorized attempts to tamper with your operating system – for example, the installation of keyloggers or Trojan horses by attackers who can gain physical access to the system while you are away.


---source:
	https://wiki.archlinux.org/title/Data-at-rest_encryption
	https://wiki.archlinux.org/title/Ext4#Improving_performance
	https://wiki.archlinux.org/title/Fscrypt
----------------------------------------
================================================================================
EXT4 FILESYSTEM FEATURES

---FILESYSTEM SIZE:
Ext4 permits filesystems up to 1 exbibyte (2^60 bytes) size and files up to 16 tebibytes size (16 * 2^40 bytes). Whereas, the ext3 file system supports only a maximum filesystem size of 16 TB and a maximum file size of 2 TB. Another advantage of ext4 is its support for the creation of enormous number of sub directories under a directory. More than 64000 sub directories can be created under a directory(unlimited) in ext4, compared to 32000 in ext3.

---EXTENT
EXTENTS is a known EXT4 feature; The commonly used file systems of Unix/Linux like ext2 and ext3 use direct, indirect, double indirect, and triple indirect block mapping scheme to map file offsets to on-disk blocks. This is ideal for small files. However, in the case of large files, there will be a large number of mappings which will reduce performance, especially while seeking and deleting data. So extents were introduced to replace the block mapping scheme.

The concept extent means “a contiguous sequence of physical blocks”. Large files are split into several “extents”. The files are allocated to a ’single extent’ instead of a particular size, thus avoiding the indirect mapping of blocks. The inode stores up to 4 extents of a file and indexes the rest in an Htree (This is turned on by default in ext4). Thus extents allow less fragmentation (due to a sequential block allocation) and improves performance.

Summary: Extents allows for less fragmentation of the disk.

---DELAYED AND MULTIBLOCK ALLOCATION
In ext4 file systems, multiblock allocation (mballoc) allocates multiple blocks for a file in a single operation, instead of allocating it one by one, as is the case in ext3. This will reduce the overhead of calling the “block allocator” several times and will optimize the allocation of memory.

In delayed allocation, if a function writes data onto a disk instead of allocating it at once, it will get stored in the cache. All data in the cache will be written only after “flush”-ing the cache. This technique is called “allocate-on-flush”. Thus the block allocator gets an opportunity to optimize the allocation using the “extents” concept.

Summary: The above mentioned technique will also reduce disk fragmentation.

---ONLINE DEFRAGMENTATION AND FSCK SPEED
The fragmentation rate is lesser in ext4 systems due to the utilization of techniques mentioned above. However, that does not mean zero percent fragmentation. Defragmentation, when required, can be done online(wihout unmounting the disk) using the tool “e4defrag”.

The syntax is as follows:
	e4defrag "path to file" : For defragmenting a file
	e4defrag -r "directory/" : For defragmenting a directory
	e4defrag /dev/sda1 : For defragmenting a partition

---JOURNAL CHECKSUMMING
Ext4 uses the checksum of the journal to find out health of the journal blocks. This is used to avoid massive data corruption.

---PERSISTENT PREALLOCATION
Persistent preallocation allows the application to allocate contiguous blocks with a fixed size, before writing the data. This will ensure the following:

1) Lesser fragmentation (because blocks are allocated as contiguously as possible).
2) Ensure that applications have enough space to work.

This feature is appropriate for real time applications, databases and content streaming.

Summary: Less fragmentation of the disk, faster read speeds.

---INODES AND TIMESTAMPS
The ext4 filesystem has a large inode size of 256 bytes by default whereas ext3 has only 128 bytes for inodes. The accuracy of the timestamp (Eg:- the modified time of a file) stored, is nano seconds instead of seconds in the case of ext3.

Summary: Accurate System Timestamp


---BACKWARDS COMPATIBILITY
Ext3 file systems can be migrated to ext4 easily without formatting or reinstalling the OS, provided the kernel supports the ext4 file system.
----------------------------------------
================================================================================
ENCRYPTING DIRECTORIES WITH FSCRYPT

	The ext4, F2FS, and UBIFS file systems natively support file encryption via a common API called fscrypt (originally called "ext4 encryption"). With fscrypt, encryption is applied at the directory level. Different directories can use different encryption keys. In an encrypted directory, all file contents, filenames, and symlinks are encrypted. All subdirectories are encrypted too. Non-filename metadata, such as timestamps, the sizes and number of files, and extended attributes ARE NOT ENCRYPTED!

fscrypt is also the name of a userspace tool to use the kernel feature of the same name. This article shows how to use the fscrypt tool to encrypt directories, including how to encrypt a home directory.
https://wiki.archlinux.org/title/Fscrypt

--INFORMATION TO CONSIDER | 2024
	as of 2024, Fscrypt does not yet support nested encrypted directories,
	allowing only the parent directory to be encrypted with either a single passsword or security key.

	If you have important directories that you do not want to decrypt all at once,
	consider using symlinks, bind mounts, loop-devices to prevent nested directories that may require
	to be unlocked at different moments.

--ALTERNATIVES TO CONSIDER
	To protect an entire file system with one password, block device encryption with dm-crypt (LUKS) is generally a better option, as it ensures that all files on the file system are encrypted, and also that all file system metadata is encrypted. fscrypt is most useful to encrypt specific directories, or to enable different encrypted directories to be unlockable independently—for example, per-user encrypted home directories.

--KERNEL SUPPORT

Kernel
All officially supported kernels support fscrypt on ext4, F2FS, and UBIFS.

Users of custom kernels version 5.1 or later, make sure CONFIG_FS_ENCRYPTION=y is set. For older kernels, see the documentation.

It is also highly recommended for the kernel version to be 5.4 or later, as this allows the use of v2 encryption policies. There are several security and usability issues with v1 encryption policies.

--CREATING NEW FILESYSTEMS:
	When creating a new file system, one can enable the encrypt feature immediately with mkfs.ext4 -O encrypt.

--INSTRUCTIONS & ENABLING FSCRYPT ENCRYPTION ON EXT4:
For ext4, the file system on which encryption is to be used must have the encrypt feature flag enabled. To enable it, run:

Step 1  - #tune2fs -O encrypt /dev/device
		#tune2fs -O encrypt /dev/sdc1
			or
		#tune2fs -O encrypt <loop_device_file>	

The operation can be undone with debugfs -w -R "feature -encrypt" /dev/device. 
Also run fsck before and after to ensure the integrity of the file system on both cases.

Step 2 - Finally - #pacman -Syu fscrypt


Step 3 - #fscrypt setup
	^ This creates the file /etc/fscrypt.conf and the directory /.fscrypt.
	^ This will need to be done for each different filesystem you want to use with fscrypt!!!! - VERY IMPORTANT
	ex.: #fscrypt setup /dev/sdx2
	
Step 4(Optional) - Then, if the file system on which encryption is to be used is not the root file system, also run:
			#fscrypt setup /mnt/sdx


VERY IMPORTANT: Encryption can only be enabled on an empty directory. 
To encrypt an existing directory, first copy the contents elsewhere and then delete the original files, e.g.:
DIRECTORY RENAMING & FILE MOVING will also work BUT NOT ADVISED ALWAYS COPY FILES AND THEN USER SHRED TO SAFELY ERASE THE UNCRYPTED FILES!

 ^ Beware that the original unencrypted files may still be forensically recoverable from disk even after being shredded and deleted, especially if using an SSD. It is better to encrypt data from the start. Use 'shred' to safely remove files afterwards! READ MORE ON SECTION: SHRED SAFELY REMOVING FILES FROM LINUX


VERY IMPORTANT: DO NOT ENCRYPT NON-EMPTY DIRECTORIES, THIS WILL CAUSE FILES TO BE DELETED!!!

--ENCRYPTING NEW DIRECTORY WITH FSCRYPT:
	0 - Installs fscrypt on a given mounted filesystem:
		#fscrypt setup /mnt/usb_disk

	1 - Sets encryption on given directory:
		VERY IMPORTANT: DO NOT ENCRYPT NON-EMPTY DIRECTORIES, THIS WILL CAUSE FILES TO BE DELETED!!!
		$fscrypt encrypt dir/

	2 - Locks directory:
		$fscrypt lock dir/
	
	3 - Unlock directory:
		$fscrypt unlock dir/

If SUDO is used, it will sudo lock or sudo unlock the given user in sudo -iu <user>, default is sudo user.

------------------
---ENCRYPTING DIRECTORIES WITH SECURITY KEY INSTEAD OF PASSWORD | 2024
		Fscrypt requires creating a new protector and also a 256-bit(32 bytes) Raw-Key 
	for encrypting directories with a Security Key instead of a password.

	furhter observations: fscrypt doesn't encrypt metadata, making it possible for data to be found
	by looking into the underlaying filesystem. Consider using dm-crypt and loop-devices
	if you plan on encrypting data and sending it to cloud storage.

	source: https://docs.kernel.org/filesystems/fscrypt.html

	1) GPG can't create 32 bytes keys, to manually create one, you have to use 'dd' and 'nvim':
		#dd if=/dev/zero of=/32bitkey.key bs=32 count=1
			or
		read more on: 
			"HOW TO CREATE SECURITY KEYS / KEYFILES ON LINUX"

	2) now go to nvim and replace all '^@' character with actual numbers and letters:
		#nvim /32bitkey.key
	
	3) Finally, make sure the file is 32bits:
		$ls -lah /32bitkey.key
			or
		$ls -la /32bitkey.key

		Output for the 'size' field should be '32':
			-rw-r--r-- 1 root root 32 Feb 18 18:38 /32bitkey.key

	4) remember to store the key somewhere safe as a read-only file:
		#chmod u-w,g-w,o-w /32bitkey.key
	
	5) also, since this is a very important file, make a backup!

	6) To use this key on fscrypt, a new directory has to be encrypted!
		#fscrypt encrypt <new_dir>
		^ if an /.fscrypt directory already exists, create a new protector for it instead
		of using the current one.
		^ YOU CAN CREATE DIRS PROTECTED BY KEYS HERE ON THIS STEP

	7) Unlocking directory is only possible with both this key and the newly generated files under /.fscrypt.

	8) Ensure that the new files on /.fscrypt are saved and backed up as well
	or else data will be permanently lost!

	9) MAKING THINGS EVEN MORE SAFE
	The steps described here replace step #6 to #8, 
	it requires you to create a loop device and enable cryptography for it using:
		#tune2fs -O encrypt <loop_device_file>
	
		9.1) mount and create a new dir. inside this loop device:
			#mount --mkdir <loop_device_file> /mnt/<loop_device_mount_name>
			#cd /mnt/<loop_device_mount_name>; 
			#mkdir ./<new_dir_name>

		9.2) enable enryption for new dir:
			#fscrypt encrypt ./<new_dir_name>
			^ new fscrypt directory will be created into the loop device from doing this
			^ chose to encrypt with security key instead of password.

		9.3) Copy all current system keys into the newly encrypted directory:
	
	#ls /.fscrypt/ | sudo awk '{system("mv \"/.fscrypt/"$0"\" \"/mnt/<loop_device_mount_name>/<new_dir_name>/\"")}'
	

		9.4) DONE! All current system keys can now only be decrypted with the key file.

--MORE INFO ON FSCRYPT:
	1) Nested directory encryption isn't possible; so keep this in mind if you ever need different encrypted
	nested directories. Specially if you need different encrypted directories to be unlocked at different moments.
	make use of symlinks, bind mounts, mounts, loop-devices if you ever need this to be accomplished.

	2) After a directory has been encrypted and unlocked, it can both be moved and renamed to other 
	places in the same filesystem. This will not cause loss of cryptographic features.

	3) When fscrypt is requested by the user to create a new encrypted directory,
	it will ask the user if it wants to create a new protector or if it wants to use the same one,
	and will always create new policies under /.fscrypt directory using the same configurations set
	on /etc/fscrypt.conf; always make sure to back these files elsewhere to prevent data loss.

	4) Further obfuscation enhancement is possible through loop devices, storing the data described on #3 on an 
	encrypted directory sinde a loop device will enhance security even further.

	^ in this case, unlocking directories is now only possible by mounting the loop-device and
	unlocking it's fscrypt contents with another fscrypt data.
	(READ MORE ON "ENCRYPTING DIRECTORIES WITH SECURITY KEY INSTEAD OF PASSWORD")

	^ if you ever do this, make sure you backup all data!

	Finally, you can copy the security-key to a removable disk like an USB Device,
	and even the loop-devices containning the fscrypt data themselves.

	FSCRYPT VS LUKS OR DM-CRYPT ON LOOP DEVICE
		The advantages are many, 1st loop-devices don't work well when they're over 100GB,
		they become super slow. Fscrypt runs using the underlaying filesystem encryption.

		Performance wise, fscrypt is much faster since it uses the underlying filesystem instead of 
		simulating stuff inside a loop device file. So it should be faster even when using a loop-device.

		In terms of security, there's no password recovery/change on fscrypt, it's almost always assured
		to be a permanent loss and likely no one can recover it in case you lose your data.

--READ MORE ON SECTIONS:
	ENCRYPTING FILES WITH GPG
	SHRED SAFELY REMOVING FILES FROM LINUX

--SOURCE:
https://wiki.archlinux.org/title/Fscrypt
----------------------------------------
----------------------------------------
----------------------------------------
================================================================================
================================================================================
================================================================================
ADDING/CREATING NEW FSCRYPT PROTECTOR | CHANGING FSCRYPT PASSWORD
	Adding/creating new fscrypt protector is very important,
sinze it allows a recovery password to be set.

	1)(Optional) Get the status of the fscrypt installation on the filesystem
		cd <disk_containning_fscrypt_dirs>;
		#fscrypt status;
	
		note: remember fscrypt can only be setup 1 time per disk;
		because of that it's not necessary to 'cd' into the fscrypt directory,
		just 'cd' into the hard drive/filesystem containning the fscrypt dir.
	
	This step will reveal all disks at which encryption has been enabled
	as well all disks in which fscrypt has been setup.

	3)(Optional) Finding out protectors of a given encrypted directory
		cd <directory containning encrypted dir>;
		#fscrypt status ./;

		note: this will reveal the protector and policy combination of
		a given directory after it's been encrypted on an fscrypted enabled disk.
	
	4) Fetching all existing policies and protectors on a filesystem/disk.
		cd <fscrypt_dir>
		#fscrypt status ./;
	
		note: every filesystem or disk in which fscrypt has been setup with,
		will contain a /.fscrypt directory which contains policies and protectors.
		Never delete this file, and if possible back it up somewhere.

		important: you'll need to remember the policy ID to which you need to add a
		new protector. ctrl+c on the ID Number here.

	5) Create a new protector
		#fscrypt metadata create protector <fscrypt_filesystem> 
			example:
				cd <filesystem_at_which_fscrypt_has_been_setup>;
				#fscrypt metadata create protector ./
	
	5.1) Adding protector to Policy / Linking new Protector to existing Policy:
		#fscrypt metadata add-protector-to-policy --protector=./:<Protector_ID> \
		--policy=./:<Policy_ID>

		note: you can fetch IDs on the step #4
		the protector ID must be the newly created one on step #5, 
		so you'll need to do step #4 again.

		note: you'll need to remember password/keys for existing protectors for the given policy of choice.
		in order to add a new protector.

	5.2) Finally, you can use both protectors for accessing the same fscrypt dir.
	Remember to always make sure you know at least one of the passwords.

	It is, when desirable, possible to delete the old protector, so that only the newer one can be used.
	If this is the case follow the below steps (CHANGING PASSWORDS):

		5.2.1) Deleting old Protector from policy
			#fscrypt metadata --remove-protector-from-policy --protector=./:<protector_ID> \
			--policy=./:<policy_ID>

		5.2.2) Removing protector from fscrypt
			#fscrypt metadata destroy --protector=./:<protector_ID>

		5.2.3) DONE! Note that these are also the steps required for changing password.
		However, without knowing the password/keys for the old protector, it's not possible 
		to change/replace anything. Because of that ALWAYS KEEP BACKUPS of the FSCRYPT DIRS.
		If you're using passwords/keys, also make sure you keep them saved somewhere.

		The best way to do this is to use a loop device for storing both fscrypt dirs and keys.
		Finally, you can back the loop device file anywhere without worrying too much.

================================================================================
================================================================================
================================================================================
CREATING A LUKS ENCRYPTED LOOP DEVICE USING DM-CRYPT

VERY IMPORTANT: do not allow 'journaling filesystems' on loop devices, it will break write order.

1) Create a file with a size larger than 20MB for storage(dm-decrypt requires 13MB):
	$ cd ~;
	$ dd if=/dev/zero of=./loop-device-luks bs=20MB count=1;

	^ Setting up a 20MB device will leave you with only 7MB of actual free disk space on the loop device.
	^ From this point forward, you can skip partitioning with 'fdisk', 
	^ this is because cryptsetup will do the partitioning on step #2 below.

2) Use cryptsetup for setting up a luks encrypted loop device:
	$ cryptsetup luksFormat ./loop-device-luks
	^ This will encrypt with a user passphrase/password

	$ cryptsetup luksFormat ./loop-device-luks <path_to_key_file>
	^ This will encrypt it with a given key file.

3) Open the encrypted device with cryptsetup:
	# cryptsetup open ./loop-device-luks <dm_name>
	^ <dm_name> is a temporary identification to be used for the device manager.
		or
	# cryptsetup open ./loop-device-luks root

	3.1) (Alternate) Open encrypted device with security key file:
		# cryptsetup open ./loop-device-luks <dm_name> --key-file /etc/mykeyfile
					or
		# cryptsetup open ./loop-device-luks root --key-file /etc/mykeyfile

	^ Read more on:
		"HOW TO CREATE SECURITY KEYS / KEYFILES ON LINUX"

4) Format the device with a filesystem of your choice:
	# mkfs.ext4 /dev/mapper/<dm_name>
	^ this has to be the same <dm_name> used on step #4
		or
	# mkfs.ext4 /dev/mapper/root

	^ this will format the loop-device with an EXT4 filesystem,
	requiring the Operating System to know what it is before mounting it on #5.

5) Mount the device anywhere you want
	# mount --mkdir /dev/mapper/<dm_name> ./<mount_point_name>
	^ this has to be the same <dm_name> used on step #4
		or
	# mount --mkdir /dev/mapper/root ./mnt_luks_device

6) Once you're done using the device, you can finally umount, close device and detach loop-device.
this requires knowing the loop-device id, type "$ losetup -l" to list all mounted loop devices.

	# umount ./<mount_point_name>
	# cryptsetup close /dev/mapper/<dm_name>
	# losetup -d /dev/<loop_device_id>
		or
	# umount ./mnt_luks_device
	# cryptsetup close /dev/mapper/root
	# losetup -d /dev/loop0

topic recommendation:
	"HOW TO CREATE SECURITY KEYS ON LINUX"

source: https://wiki.archlinux.org/title/dm-crypt/Device_encryption

----
	RESIZING A LUKS ENCRYPTED LOOP-DEVICE USING DM-CRYPT
		Make sure you have followed step #6 above before attempting a resize.
	and if possible make a backup of the loop-device just in case things go wrong and data gets corrupted.

	1) Increase disk size of the loop device
		$ dd if=/dev/urandom bs=1M count=1024 | cat - >> ./loop-device-luks

	2) Open the device without mounting it:
		# cryptsetup open ./loop-device-luks <dm_name>
			or
		# cryptsetup open ./loop-device-luks root

		2) (Alternate) Open with keyfile:
			# cryptsetup open ./loop-device-luks <dm_name> --key-file /etc/mykeyfile
						or
			# cryptsetup open ./loop-device-luks root --key-file /etc/mykeyfile
	
	3) Apply disk resize
		# cryptsetup resize <dm_name>
			or
		# cryptsetup resize root

	4) Perform filesystem check and resize it(for ext2/3/4):
		# e2fsck -f /dev/mapper/<dm_name>
		# resize2fs /dev/mapper/<dm_name>
			or
		# e2fsck -f /dev/mapper/root
		# resize2fs /dev/mapper/root

	5) You can now mount and use the container
		# mount --mkdir /dev/mapper/<dm_name> ./<device_mount_point>
			or
		# mount --mkdir /dev/mapper/root ./mnt_luks_device

	6) After you're done, follow step #6 from the previous instruction guide here:
		# umount ./<mount_point_name>
		# cryptsetup close /dev/mapper/<dm_name>
		# losetup -d /dev/<loop_device_id>
			or
		# umount ./mnt_luks_device
		# cryptsetup close /dev/mapper/root
		# losetup -d /dev/loop0
----
 	HOW TO KEEP CRYPTSETUP / LUKS / DM-CRYPT DEVICE SAFE
			The only way to keep a luks device safe from access is to nuke it's header,
		since keys can be erased and then a new one can be created to access the device.

		1) BACKUP HEADER AND KEY
			Backup luks' header and keyslot area:
				# cryptsetup luksHeaderBackup /dev/device --header-backup-file ~/luksheaderbackup.img

		2) NUKE HEADER ON THE LUKS-LOOP-DEVICE
				# cryptsetup luksErase ./<luks-loop-device-file>
				^ Access to the is now restricted to restoring the header on the loop device. 
				^ It's not possible to create new keys for access at this point here.

		3) RESTORING THE HEADER ON THE LOOP DEVICE:
			Restoring the header to the loop device can permanently damage the loop device,
		because of that, the following steps need to be done carefully.

			1) Make sure the header you want to restore will work:
				# cryptsetup -v --header ~/luksheaderbackup.img open /dev/device test
					or
				# cryptsetup -v --header ~/luksheaderbackup.img open ./<luks-loop-device-file> test

				you should get an output saying:
					Key slot 0 unlocked.
					Command successful.

				VERY IMPORTANT: this step #1 can be permanently used to open and mount the device
				without the need to follow step #3.

			2) Make sure the contents are readable by mounting device on the system:
				# mount /dev/mapper/test /mnt/test && ls /mnt/test 
				# umount /mnt/test 
				# cryptsetup close test

			3) Restore the header into the device only if step #1 and #2 has been succesfull:
				# cryptsetup luksHeaderRestore /dev/device --header-backup-file ./mnt/backup/file.img

				3.1) (Alternate) you can use step #1 to keep getting access instead of restoring the header
				using step #3.


	CRYPTSETUP / DM-CRYPT KEYS MANAGEMENT
		LISTING KEYS
			# cryptsetup luksDump /dev/sda8 | grep 'Slot 6'
			^ will list Slot '6' key

		ADDING KEYS
			# cryptsetup luksAddkey ./<luks_device>
				or
			# cryptsetup luksAddKey ./<luks_device> /etc/mykeyfile

		REMOVING KEYS
			# cryptsetup luksKillSlot ./<luks_device> <slot_number>
				or
			# cryptsetup luksKillSlot ./<luks_device>
			^ when no <slot_number> is present, cryptsetup will ask for password to be deleted.
			^ and will only delete it if it exists.

		BACKING UP HEADER AND KEYS
			Cryptsetup's luksHeaderBackup action stores a binary backup of the LUKS header and keyslot area: 
				# cryptsetup luksHeaderBackup /dev/device --header-backup-file /mnt/backup/file.img

			VERY IMPORTANT: this doesn't covers plain-text mode, and is only meant for non plain-text mode.

		ERASING HEADER
			# cryptsetup luksErase ./<luks-loop-device-file>
			^ erases all key files, requiring a HEADER RESTORE action to regain access to the device.
			^ this is a required safety action, so unwanted 3rd parties don't get access to the device!
			^ VERY IMPORTANT: Do not do this before BACKING UP the HEADER to safe device(s).

		RESTORING HEADER AND KEYS
			Restoring the header to the loop device can permanently damage the loop device,
		because of that, the following steps need to be done carefully.

			1) Make sure the header you want to restore will work:
				# cryptsetup -v --header /mnt/backup/file.img open /dev/device test

				you should get an output saying:
					Key slot 0 unlocked.
					Command successful.

			2) Make sure the contents are readable by mounting device on the system:
				# mount /dev/mapper/test /mnt/test && ls /mnt/test 
				# umount /mnt/test 
				# cryptsetup close test

			3) Restore the header into the device only if step #1 and #2 has been succesfull:
				# cryptsetup luksHeaderRestore /dev/device --header-backup-file ./mnt/backup/file.img


	src: https://wiki.archlinux.org/title/dm-crypt/Device_encryption

================================================================================
================================================================================
================================================================================

HOW TO CREATE SECURITY KEYS / KEYFILES ON LINUX
	A keyfile(or key) can be an image, video, text/binary file, gpg keys, etc.
using them should be completely fine as long as no data changes happen
due to user-modification or data-corruption.

A keyfile can be of arbitrary content and size too.

It's possible to generate a keyfile with random numbers using 'dd' tool:

1) Creating a keyfile of 2048 random bytes, storing it in the file /etc/mykeyfile:
	# dd bs=512 count=4 if=/dev/random of=/etc/mykeyfile iflag=fullblock

2) Blocking access for the keyfile from users other than root:
	# chmod 600 /etc/mykeyfile

3) (Optional) keyfiles should not contain '\n' characters in it,
it's possible to filter them out of a keyfile using the following command:
	# perl -pi -e 'chomp if eof' /path/to/keyfile
	^ only use this if neccessary

src: https://wiki.archlinux.org/title/dm-crypt/Device_encryption#Keyfiles

STORING KEYFILE ON LOOP-DEVICE AND MOUNTING IT ON RAM
	It's possible to create a loop device for storing keyfile and mounting it in /tmp/
directory to prevent an easier key recovery in case you ever delete it or it gets changed.

Read topics:
	"CREATING AND MOUNTING LOOP DEVICES"
	"ENCRYPTING DIRECTORIES WITH SECURITY KEY INSTEAD OF PASSWORD | 2024


================================================================================
================================================================================
================================================================================
SHRED SAFELY REMOVING FILES FROM LINUX
	"shred" (from the coreutils package) is a Unix command that can be used to securely delete individual files or full devices so that they can be recovered only with great difficulty with specialised hardware, if at all. By default shred uses three passes, writing pseudo-random data to the device during each pass. This can be reduced or increased.

The following command invokes shred with its default settings and displays the progress.

---EXAMPLE
	#shred -v /dev/sdX					# -v: Verbose Mode for shredding

	#find <dir> -type f -exec shred {} \;			#Wipes data recursively in a given directory

	#find <dir> -type f -exec shred --remove=wipesync {} +	#Wipesyncs data with the + since shred accepts 
								 ^ multiple arguments - WIPESYNC IS VERY SLOW BUT SAFE
	
	#find <dir> -type f -exec shred --remove=wipe {} +      #Same as above, but better than wipesync

Example:
sudo find "/some/directory/" -type f -exec shred --remove=wipesync {} +

	^ VERY IMPORTANT: wipesync is very very very slow BUT IS THE SAFEST METHOD!
sudo find "/some/directory" -type f -exec shred --remove=wipesync {} +

Needs to Erase:
sudo find "/some/directory" -type f -exec shred --remove=wipesync {} +

** - VERY IMPORTANT: DO NOT SHRED THE ORIGNAL FILES BEFORE CONFIRMING FILES HAVE BEEN COPIED SOMEWHERE ELSE!
ACCIDENTS CAN HAPPEN LIKE COPYING THE FILES AND ENCRYPTING THE FOLDER WILL CAUSE THE FILES TO BE DELETED!!!
---CAUTION
CAUTION: Note that shred relies on a very important assumption: that the file system overwrites data in place. This is the traditional way to do things, but many modern file system designs do not satisfy this assumption. The following are examples of file systems on which shred is not effective, or is not guaranteed to be effective in all file system modes:

log-structured or journaled file systems, such as those supplied with AIX and Solaris (and JFS, ReiserFS, XFS, Ext3, etc.)

file systems that write redundant data and carry on even if some writes fail, such as RAID-based file systems

file systems that make snapshots, such as Network Appliance's NFS server

file systems that cache in temporary locations, such as NFS version 3 clients

compressed file systems

In the case of ext3 file systems, the above disclaimer applies (and shred is thus of limited effectiveness) only in data=journal mode, which journals file data in addition to just metadata. In both the data=ordered (default) and data=writeback modes, shred works as usual. Ext3 journaling modes can be changed by adding the data=something option to the mount options for a particular file system in the /etc/fstab file, as documented in the mount man page (man mount).

In addition, file system backups and remote mirrors may contain copies of the file that cannot be removed, and that will allow a shredded file to be recovered later.
----------------------------------------
================================================================================
PROGRAMMING HINTS:
1 - If you ever thinkg you've made a mistake of missing a closing bracket: '{' or '}'
you can check any of the outer brackes for inconsistences, since a mistake will cascade into the outer closing brackets! 

----------------------------------------
================================================================================
IMPROVING STEAM 

HARDWARE DECODING NOT AVAILABLE | INSTALING LIBVA LIB32-LIBVA: 
Remote Play hardware decoding uses vaapi, but steam requires libva2_32bit, where as Arch defaults to 64bit.

As a basic set, this is libva and lib32-libva. Intel graphics users will also require both libva-intel-driver and lib32-libva-intel-driver.

For more information about vaapi see hardware video acceleration.

It may also be necessary to remove the steam runtime version of libva, in order to force it to use system libraries. The current library in use can be found by using:
	pgrep steam | xargs -I {} cat /proc/{}/maps | grep libva

If this shows locations in ~/.local/Share/steam steam is still using its packaged version of libva. 
This can be rectified by deleting the libva library files at ~/.local/share/Steam/ubuntu12_32/steam-runtime/i386/usr/lib/i386-linux-gnu/libva*, so that steam falls back to the system libraries.

SOLVING UNKNOWN ERROR MESSAGES:
Another issue that causes this error message can be solved by removing the config.vdf file:

$ rm ~/.local/share/Steam/config/config.vdf

ALSO INSTALL FOR NVIDIA DRIVERS:
lib32-libva-vdpau-driver
----------------------------------------
================================================================================
MAN PAGES - HOW TO MAKE YOUR OWN MAN PAGE

Man pages are located in /usr/local/share/man/ on all of the following directories:
man1, man2, man3, man4, man5, man6, man7, man8(If they don't exist, they need to be created)

ex.: /usr/local/share/man/		#For user pages
	or
     /usr/share/man/			#For system pages | #Default for Archlinux systems
        or
     /usr/local/man/

the manuals are found in the .gz format

Each man-page numeration above specifies the following manuals:
   1   Executable programs or shell commands
   2   System calls (functions provided by the kernel)
   3   Library calls (functions within program libraries)
   4   Special files (usually found in /dev)
   5   File formats and conventions eg /etc/passwd
   6   Games
   7   Miscellaneous (including macro packages and conventions), e.g. man(7), groff(7)
   8   System administration commands (usually only for root)
   9   Kernel routines [Non standard]

1 - A man page file should be named according to the following format described above:
    <program_name>.<section>
ex.: nvim.1

--- ADDING/CREATING YOUR OWN MAN PAGE:
0 - First, create your file according to naming convention: <program_name>.<section>
1 - Second, Compact your man page as a .gz file:
	#gzip /usr/local/share/man/man1/nvim.1

ex.: output will now be nvim.1.gz 

2 - Copy it to: /usr/share/man/man1/
3 - Third, type in: #mandb

--- READING RAW MANUAL ENTRIES:
You can use zcat to read the raw .gz manual entries

$zcat /usr/share/man/man1/nvim.1.gz  		#This will output the raw manual entry
--- SOURCING MAN PAGES:
Manual pages can be sourced from other manual pages with the .so (lowercase) macro. Similar to a link, sourcing will instead embed the target man page into the current page and interrupt the rest of the original page processing.

This can be useful if one wishes, for instance, to properly document a program or feature that can be called by different names (grep, egrep, printf, fprintf, etc).

Example:

.TH appenv 1 "01 August 2016" "version 1.3.1" "appenv manpage"
.so man1/ppenv
The above will create a man page that simply points to the ppenv man page every time someone uses the command man appenv. The actual documentation for the appenv alias will be written in the ppenv.1 man page. This is how egrep and fprintf man pages are done, for instance.

It is important to specify the section folder (in this case man1/) in the .so macro parameter.

--- CONVERTING MAN PAGES:
groff can also be used to convert man pages to other formats, including html, postscript, plain ascii, or pdf.
However special care must be taken on some formats and the use of the col command may be required to remove certain invisible control characters that are still printed by groff.

# correctly convert a man file to ascii in UTF-8 format
$ groff -K utf8 -mandoc -Tutf8 ppenv.1 | col -bx > ppenv.txt
----------------------------------------
================================================================================
LEARNING WINDOW MANAGER - CWM:
ctrl+alt+enter				#Open XTERM
alt+left				#Toggles previous "workspace"(application group)
alt+right				#Toggles next "workspace"(application group)

alt+mouse_1(left_click)			#Move current window
alt+mouse_2(middle_click)		#Resize current Window
alt+mouse_3(right_click)		#Minimizes Window
ctrl+mouse_1				#Choose Font-Style, Font-Size for the given window
ctrl+mouse_2				#Other Setting
ctrl+mouse_3				#Even More Settings

ctrl+alt+a		#Toggles Visibility of all groups

alt+tab			#Switches between applications
alt+?			#CWM Dmenu for executing other programs
ctrl+alt+x		#Closes window - notworking?
ctrl+alt+f		#Sets active window to full screen

ctrl+alt+shift+r	#Restarts Computer
ctrl+alt+shift+q	#Logsout/Quits CWM

Don't forget to read the cwm(1) and cwmrc(5) man pages. - VERY IMPORTANT

--- SWITCHING BETWEEN WORKSPACES:
In CWM/EvilWM there are no workspaces! - VERY IMPORTANT
Instead, you can group programs together & toggle their visibility on/off:

ctrl+alt+g		#Picks window for setting a group
			ex.: ctrl+alt+g -> number
			pressing c+a+g and followed by a number will set
			the given window to the given number group

			pressing ctrl+alt+g for the first time will show blue-colored borders!
			pressing ctrl+alt+g for the second time will automatically add window to group-0 and show red-colored border!
			once the window is in group-0, pressing ctrl+alt+g a third time will show the blue-colored border again!
			^ ctrl+alt+left_click will do the same, thing is when you want to switch a window/application from a given group, you must remove it first,
			by adding it to the group-0(red colored-border), then you need to press the number you want BEFORE DOING ctrl+alt+g AGAIN FOR THE BLUE-COLORED BORDER!

			^ In other words, always press the group-number before you summon ctrl+alt+g or ctrl+alt+left_click for the blue-colored border!
			if you don't, the window will be set to the previous group it was alocated.

			HINT: the group '0' will set the application to no-groups, which means
			it's window will always render displayed. pressing ctrl+alt+g again will re-locate it to the previous groupif none was given before pressing ctrl+alt+g.

ctrl+alt+n		# n= number
			^ will toggle on/off visibility for a given group

mouse+left_click	#Will show which applications belong to which groups


-- IMPORTANT COMMANDS:
xsetroot -solid steelblue		#Sets steelblue as background color
					^ Fix's screen replication problem
	
-- AUTOMATIC WINDOW GROUPING
We can also automatically assign a window to a group based on its class and/or name properties. By using xprop we can get these properties. xterm has a useful -class switch, which I use to set my ssh connection to my mail server simply as mail instead of the default xterm so I can distinguish it. Again, this configuration is done through .cwmrc.

This should be enough to get you started. Don't forget to read the cwm(1) and cwmrc(5) man pages.

So without further ado here's my .cwmrc:

# customize some bindings (C-control M-alt M1-button1)
bind M-r        exec
bind CM-r       reload
bind CM-l       label
bind CM-t       "xterm -e \"top -s 0.6\""
bind CM-m       menusearch
mousebind M-1   window_move
mousebind CM-1  window_resize

# application menu
command pidgin  pidgin
command xchat   xchat
command silc    "xterm -class silc -e mysilc.sh"
command mail    "xterm -class mail -e \"ssh my.mail.host\""
command firefox firefox

# groups predefined, use xprop to get these from each program
autogroup 2     "Navigator,Firefox"
autogroup 3     "pidgin"
autogroup 3     "xchat"
autogroup 3     "xterm,silc"
autogroup 4     "xterm,mail"

# inherit current selected group
sticky yes

--FROM MAN PAGE:
     cwm actions are initiated either via key or mouse bindings.  The following notations are used throughout this page:

           C       Control key.
           M       Meta key.
           S       Shift key.
           4       Mod4 (windows) key.
           M1      Left mouse button.
           M2      Middle mouse button.
           M3      Right mouse button.

     The default key bindings are:

           CM-Return       Spawn a new terminal.
	   			^ Opens xterm

           CM-Delete       Lock the screen.
           M-Return        Hide current window. - WORKING
           M-Down          Lower current window.
           M-Up            Raise current window.
				^ Does not WORK!?

           M-slash         Search for windows.
	   			^ ctrl+alt+a	#Lists all windows

           C-slash         Search for applications.
	   			^ They must be user-created in configuration file.
				^ Check man-page.

           CM-n            Label current window.
				^ Gives current window a name
				  useful for terminal applications 

           M-Tab           Cycle through currently visible windows.
           MS-Tab          Reverse cycle through currently visible windows.
				^ Windows alt+tab

           CM-x            Close current window.
           CM-[n]          Toggle visibility of group n, where n is 1-9.
           CM-a            Toggle visibility of all groups.
           CM-g            Toggle group membership of current window.

           M-Right         Cycle through active groups.
           M-Left          Reverse cycle through active groups.
				^ Good for switching between application groups

           CMS-f           Toggle freezing geometry of current window.
           CM-s            Toggle stickiness of current window.

           CM-f            Toggle full-screen mode of current window.
           CM-m            Toggle maximization of current window.
           CM-equal        Toggle vertical maximization of current window.
           CMS-equal       Toggle horizontal maximization of current window.
	   			^ Toggle window size

           M-[hjkl]        Move window by a small amount.
           MS-[hjkl]       Move window by a large amount; see cwmrc(5).
           CM-[hjkl]       Resize window by a small amount.
           CMS-[hjkl]      Resize window by a large amount; see cwmrc(5).
	   			^ Use VI/VIM keys for moving resizing windows
				^ ALT or ALT+SHIFT for moving / faster moving
				^ ALT+CTRL or ALT+CTRL+SHIFT for resizing / faster resizing

           M-question      Spawn “exec program” dialog.
           M-period        Spawn “ssh to” dialog.  This parses $HOME/.ssh/known_hosts to provide host auto-completion.  ssh(1) will be executed via the configured terminal emulator.
           CM-w            Spawn “exec WindowManager” menu, allowing a switch to another window manager.
           CMS-r           Restart.
           CMS-q           Quit.

     The default mouse bindings are:

           M-M1            Move current window.
           CM-M1           Toggle group membership of current window.
           M-M2            Resize current window
           M-M3            Lower current window.
           CMS-M3          Hide current window.

     The following key bindings may be used to navigate search and exec dialogs:

           [Return]              Select item.
           [Down], C-s or M-j    Next item.
           [Up], C-r or M-k      Previous item.
           [Backspace] or C-h    Backspace.
           C-u                   Clear input.
           C-a                   List all available items.
           [Esc]                 Cancel.
----------------------------------------
================================================================================
SWITCHING JAVA VERSIONS IN ARCHLIUX:
	$archlinux-java help

----------------------------------------
================================================================================
UNDERSTANDING PAM SYSTEM:
Pluggable Authentication Modules (PAM) is an often misunderstood, and in at
least this admin’s opinion, underutilized mechanism on *nix systems. Sitting
in its little corner of the /etc directory, PAM sits overlooking its
configuration files and man pages, just waiting for someone to come along
and discover the power that it can give to administrators and developers
alike. When will they realize that PAM is not just for ‘Authentication’
anymore?

PAM is a collection of modules that essentially form a barrier between a
service on your system, and the user of the service. The modules can have
widely varying purposes, from disallowing a login to users from a particular
UNIX group (or netgroup, or subnet…), to implementing resource limits so
that your ‘research’ group can’t hog system resources.

PAM is used by major commercial UNIX flavors such as AIX, HP-UX and Solaris,
as well as the major free versions of UNIX, like FreeBSD. Almost all
distributions of Linux also implement PAM, though I believe Slackware is
still the one notable holdout in this area.

The power, flexibility and ubiquity of PAM is a boon for developers of Linux
applications, since instead of writing a full-fledged authentication layer
for their program, they can simply write a hook into PAM, and administrators
can configure it along with the rest of the system services using PAM. This,
in turn, makes the lives of administrators much easier, since we only have
to learn PAM, instead of learning and trying to remember a separate
configuration model for every system we decide to run.

How to Read A Config File

I’ll be looking at pam as it applies to Linux systems. Solaris and other
commercial UNIX systems have a slightly different configuration model,
centered around a single file, /etc/pam.conf. Though conceptually the two
implementations are just about identical, the Linux model uses a different
configuration file for each service which uses PAM. On most Linux systems,
these configuration files live in /etc/pam.d, and are named after the
service – for example, the ‘login’ configuration file is called
/etc/pam.d/login. Let’s have a quick look at a version of that file:


auth required /lib/security/pam_securetty.so
auth required /lib/security/pam_nologin.so
auth sufficient /lib/security/pam_ldap.so
auth required /lib/security/pam_unix_auth.so try_first_pass
account sufficient /lib/security/pam_ldap.so
account required /lib/security/pam_unix_acct.so
password required /lib/security/pam_cracklib.so
password required /lib/security/pam_ldap.so
password required /lib/security/pam_pwdb.so use_first_pass
session required /lib/security/pam_unix_session.so

PAM Management Realms

The first thing I’d like you to notice in the file is the leftmost column,
which, in all, contains four unique words, which represent four realms of
PAM management: auth, account, password and session. While there are many
modules which support more than one of these realms (indeed, pam_unix
supports all of them), others, like pam_cracklib for instance, are only
suited for one (the ‘password’ facility in pam_cracklib’s case). Knowing
what these four realms are responsible for is extrememly important if you
want to make effective use of PAM. Below is a quick explanation of how I
tend to think about these keywords in practice:

auth
The ‘auth’ realm (I call it a realm – the docs refer to it as a
‘management group’ or ‘facility’) is responsible for checking that
the user is who they say. The modules that can be listed in this
area generally support prompting for a password.
account
This area is responsible for a wide array of possible account
verification functionality. There are many modules available for
this facility. Constraints to the use of a service based on checking
group membership, time of day, whether a user account is local or
remote, etc., are generally enforced by modules which support this
facility.
password
The modules in this area are responsible for any functionality
needed in the course of updating passwords for a given service. Most
of the time, this section is pretty ‘ho-hum’, simply calling a
module that will prompt for a current password, and, assuming that’s
successful, prompt you for a new one. Other modules could be added
to perform password complexity or dictionary checking as well, such
as that performed by the pam_cracklib and pam_pwcheck modules.
session
Modules in this area perform any number of things that happen
either during the setup or cleanup of a service for a given user.
This may include any number of things; launching a system-wide
initialization script, performing special logging, mounting the
user’s home directory, or setting resource limits.
PAM Module Controls

The middle column holds a keyword that essentially determines what PAM
should do if the module either succeeds or fails. These keywords are called
‘controls’ in PAM-speak. Note that this keyword is not an indicator to the
module, but to the underlying PAM library. In probably 90% of the cases, you
can use one of the common keywords (requisite, required, sufficient or
optional). However, this is only the tip of the iceberg in terms of
unleashing the flexibility and power of PAM. You could actually use more
complicated syntax to account for seemingly any situation. Here are the
valid simple ‘one-word’ keywords PAM understands:

	required
If a ‘required’ module returns a status that is not ‘success’,
the operation will ultimately fail, but only after the modules below
it are invoked. This seems senseless at first glance I suppose, but
it serves the purpose of always acting the same way from the point
of view of the user trying to utilize the service. The net effect is
that it becomes impossible for a potential cracker to determine
which module caused the failure – and the less information a
malicious user has about your system, the better. Important to note
is that even if all of the modules in the stack succeed, failure of
one ‘required’ module means the operation will ultimately fail. Of
course, if a required module succeeds, the operation can still fail
if a ‘required’ module later in the stack fails.

	requisite
If a ‘requisite’ module fails, the operation not only fails, but
the operation is immediately terminated with a failure without
invoking any other modules: ‘do not pass go, do not collect $200’,
so to speak.

	sufficient
If a sufficient module succeeds, it is enough to satisfy the
requirements of sufficient modules in that realm for use of the
service, and modules below it that are also listed as ‘sufficient’
are not invoked. If it fails, the operation fails unless a module
invoked after it succeeds. Important to note is that if a ‘required’
module fails before a ‘sufficient’ one succeeds, the operation will
fail anyway, ignoring the status of any ‘sufficient’ modules.

	optional
An ‘optional’ module, according to the pam(8) manpage, will only
cause an operation to fail if it’s the only module in the stack for
that facility.

STACKING MODULES

In the rightmost column we see the actual path to the module that gets
invoked. Note that in our sample config, four separate modules are listed
for the ‘auth’ realm. This is referred to as ‘stacking’ in PAM lingo. If
you’re thinking that this implies that ordering of the stacked modules is
significant, you’re right. Also, in addition to the more complex options you
can pass to the PAM library in the center column, there are, likewise,
options that you can feed to the module itself. This can be seen in our
example config file, where the pam_unix.so module takes the ‘try_first_pass’
argument. Furthermore, many modules are meant to be used with an
accompanying configuration file of some sort, allowing for more complex
options that would make the PAM config file a big mess otherwise.

Drilling Down: What’s Going On?

Now that we have a grip on how to basically parse a configuration file at a
high level, let’s drill down and see exactly what’s going on in our example
file. We’ll have a look at the module stack for the ‘auth’ realm, detailing
what will happen given a number of scenarios.

In our example file, we have four modules stacked for the auth realm:


auth       required     /lib/security/pam_securetty.so
auth       required     /lib/security/pam_env.so
auth       sufficient   /lib/security/pam_ldap.so
auth       required     /lib/security/pam_unix.so try_first_pass

As the modules are invoked in order, here is what will happen:

The ‘pam_securetty’ module will check its config file,
/etc/securetty, and see if the terminal being used for this login is
listed in the file. If it’s not, root logins will not be permitted. If
you try to log in as root on a ‘bad’ terminal, this module will fail.
Since it’s ‘required’, it will still invoke all of the modules in the
stack. However, even if every one of them succeeds, the login will fail.
Interesting to note is that if the module were listed as ‘requisite’,
the operation would terminate with a failure immediately, without
invoking any of the other modules, no matter what their status.
The ‘pam_env’ module will set environment variables based on what
the administrator has set up in /etc/security/pam_env.conf. On a default
setup of Redhat 9, Fedora Core 1, and Mandrake 9.2, the configuration
file for this module doesn’t actually set any variables. A good use for
this might be automatically setting a DISPLAY environment variable for a
user logging in via SSH so they don’t have to set it themselves if they
want to shoot an ‘xterm’ back to their remote desktop (though this can
be taken care of by OpenSSH automagically).
The ‘pam_ldap’ module will prompt the user for a password, and then
check the ldap directory indicated in /etc/ldap.conf to authenticate the
user. If this fails, the operation can still succeed if ‘pam_unix’
succeeds in authenticating the user. If pam_ldap succeeds, ‘pam_unix’ will not
be invoked.

The ‘pam_unix’ module, in this case, will not prompt the user for a
password. The ‘try_first_pass’ argument will tell the module to use the
password given to it by the preceding module (in this case, pam_ldap).
It will try to authenticate the user using the standard getpw* system
calls. If pam_unix fails, and pam_ldap has failed, the operation will
fail. If pam_ldap fails, but pam_unix succeeds, the operation will
succeed (this is extremely helpful in cases where root is not in the
ldap directory, but is still in the local /etc/passwd file!).
Depending on the feedback and corrections submitted for this article, I’m
happy to go into PAM further or with some other focus in a future
article. If you’ve done something amazing with PAM or notice a common
problem users have with PAM that hasn’t been discussed here, please send me
email (njcajun – linuxlaboratory dot org) or post a comment.

---source: https://www.linux.com/news/understanding-pam/

----------------------------------------
================================================================================
ARCH-CHROOT:
UPDATING/FIXING LINUX SYSTEM LOCATED IN ANOTHER DISK/PARTITION WITH ARCH-CHROOT:

Read man pages on both chroot and arch-chroot,
use arch-chroot!
ex.: sudo arch-chroot /mnt/sda2/

this will allow you to use /mnt/sda2/ path as NEWROOT dir, along with some tools that are located in your current-running system.

VERY IMPORTANT: it's not advised to try updating/upgrading linux systems that are working fine, since some functions will not be available, 
like mounting efi partitions for installing/updating grub. Also, some disk dependencies might hold different names on your current system
when compared to the ones in your secondary linux system.
----------------------------------------
================================================================================
MODIFYING LINUX KEY MAPS | KEYBOARD MAPPINGS:
	XMODMAP can actually bind more than one action to a single key.

--- XMODMAP COMMANDS:
	$man xmodmap
	$xmodmap -pke	#Prints all current KEYCODES | Prints the current mod map(shows which keys are bound to which actions)
			ex.: COMMAND OUTPUT: keycode  57 = n N
			^ Each "keysym column" in the table corresponds to a particular combination of modifier keys:

				Key
				Shift+Key
				Mode_switch+Key
				Mode_switch+Shift+Key
				ISO_Level3_Shift+Key
				ISO_Level3_Shift+Shift+Key

			Not all keysyms(read below) have to be set, but to assign only a latter keysym, use the NoSymbol value.
			ex.: keycode 66 = space NoSymbol space   #means holding down shift will have no actions binded to it.

	$xmodmap -pm	#Prints all keysims | Prints all modifier keys

VERY IMPORTANT:	names listed in $xmodmap -pke are case sensitive

HINT/Tip: There are predefined descriptive keysyms for multimedia keys, e.g. XF86AudioMute or XF86Mail. These keysyms can be found in /usr/include/X11/XF86keysym.h. 
Many multimedia programs are designed to work with these keysyms out-of-the-box, without the need to configure any third-party application.

--- EXECUTING EXPRESSIONS:
	$xmodmap -e "keycode 66 = space"	#adds space_bar function to Caps_Lock key
						  without removing caps_lock function
	$xmodmap -e "clear Lock"		#clears Capslock functionality
						^ clears modifier keys:
							Lock, Control, Mod1, Mod2, Mod3, Mod4, Mod5, Shift
						^ Modifier Key names are case-sensitive
						^ can be used before/after adding/inserting new function

	$xmodmap -e "add Lock = Caps_Lock"	#Adds Caps_Lock function back to Lock(Capslock) Mod key
	$xmodmap -e "remove Lock = Caps_Lock"	#Should do the same as "clear Lock" - But Doesn't???



EXAMPLE 2:
	For replacing the key "Y" in a fighting game, with "I"(keeping the "Y" functionality:
	1 - Type xmodmap -pke and check the following output:
		keycode  29 = y Y y Y leftarrow yen leftarrow
	2 - Type in the terminal:
		2.1) xmodmap -e "keycode 29 = i"
	3 - Reseting back to default:
		3.1) xmodmap -e "keycode 29 = 0x59" 
			^ Note.: Check ASCII Table for hexadecimal values.

VERY IMPORTANT: type for making things safe before making any changes: 
	$echo "xmodmap -e keycode 29 = 0x59" > ~/xmodmap.readme

VERY IMPORTANT:	names listed in $xmodmap -pke are case sensitive

HINT: Read man pages on xmodmap for more useful examples
	$man xmodmap

--- CREATING KEY BINDINGS
	Requires xbindkeys installed on the system.

	Use 'xbindkeys' to create key shortcuts for applications.
after installing, run: 
	$xbindkeys --defaults > ~/.xbindkeysrc

	example:
		This example creates a key bind for 'control+o' that plays a song and creates a PID File for it,
		allowing the given process to be killed with 'control+p'.

		[file: ~/.xbindkeysrc] --OLDER FILE
			# play imreallygay
			"(mpv --no-video --audio-device=pulse/combined --start=0 --loop ~/'The Gay Song- Jenna Anne [tONYrIbu6cU].mp3') & echo ${!} > /tmp/imninja.pid"
			  control + l

			# play im a ninja
			"(mpv --no-video --audio-device=pulse/combined --start=0 --end=8 --loop ~/imninja.mp3) & echo ${!} > /tmp/imninja.pid; (mpv --no-video --audio-device=pipewire --start=0 --end=8 --loop ~/imninja.mp3) & echo ${!} > /tmp/imninja.pid"
			  control + o

			# stop imma ninja
			"kill -9 $(cat /tmp/imninja.pid)"
			  control + p

			# stop imma ninja
			"killall mpv"
			  control + shift + p

		[/file]


		[file: ~/.xbindkeysrc] ----NEWER FILE
			# play imreallygay
			"(mpv --no-video --audio-device=pulse/combined --start=0 --loop ~/'The Gay Song- Jenna Anne [tONYrIbu6cU].mp3') & echo ${!} > /tmp/imninja.pid; (mpv --no-video --audio-device=pipewire --start=0 --loop ~/'The Gay Song- Jenna Anne [tONYrIbu6cU].mp3') & echo ${!} > /tmp/imninja.pid"
			  control + l

			# play sonic2greenhillzone
			"(mpv --no-video --volume=60 --audio-device=pulse/combined --start=0 --loop ~/'Emerald Hill Zone Enhanced   Sonic the Hedgehog 2 Genesis Music Extended HD [o13BFRXuS2g].mp3') & echo ${!} > /tmp/imninja.pid; (mpv --no-video --volume=60 --audio-device=pipewire --start=0 --loop ~/'Emerald Hill Zone Enhanced   Sonic the Hedgehog 2 Genesis Music Extended HD [o13BFRXuS2g].mp3') & echo ${!} > /tmp/imninja.pid;"
			  control + i

			# play sonic2supersonic
			"(mpv --no-video --volume=70 --audio-device=pulse/combined --start=2 --loop ~/'Sonic The Hedgehog 2 Super Sonic Theme Extended [Vz9lLwJPq1U].mp3') & echo ${!} > /tmp/imninja.pid; (mpv --no-video --volume=70 --audio-device=pipewire --start=2 --loop ~/'Sonic The Hedgehog 2 Super Sonic Theme Extended [Vz9lLwJPq1U].mp3') & echo ${!} > /tmp/imninja.pid;"
			  control + k

			# play BigShot 
			"(mpv --no-video --volume=80 --audio-device=pulse/combined --start=0 --loop ~/'The King of Fighters '\''96 - Big Shot! (Arranged) [Xq2hNfPmCQQ].mp3') & echo ${!} > /tmp/imninja.pid; (mpv --no-video --volume=80 --audio-device=pipewire --start=0 --loop ~/'The King of Fighters '\''96 - Big Shot! (Arranged) [Xq2hNfPmCQQ].mp3') & echo ${!} > /tmp/imninja.pid;"
			  control + j

			# play DataSelectScreen
			"(mpv --no-video --volume=60 --audio-device=pulse/combined --start=8 --loop ~/'Sonic 3 Re-Imagined - Data Select Screen [VU2BImK9v9w].mp3') & echo ${!} > /tmp/imninja.pid; (mpv --no-video --volume=60 --audio-device=pipewire --start=8 --loop ~/'Sonic 3 Re-Imagined - Data Select Screen [VU2BImK9v9w].mp3') & echo ${!} > /tmp/imninja.pid;"
			  control + u

			# thomas the tank engine
			"(mpv --no-video --volume=70 --audio-device=pulse/combined --start=8 --loop ~/'Thomas The Tank Engine Theme but the main piano is missing [-lDJ__y3gyY].mp3') & echo ${!} > /tmp/imninja.pid; (mpv --no-video --volume=70 --audio-device=pipewire --start=8 --loop ~/'Thomas The Tank Engine Theme but the main piano is missing [-lDJ__y3gyY].mp3') & echo ${!} > /tmp/imninja.pid;"
			  control + y

			# play im a ninja
			"(mpv --no-video --audio-device=pulse/combined --start=0 --end=8 --loop ~/imninja.mp3) & echo ${!} > /tmp/imninja.pid; (mpv --no-video --audio-device=pipewire --start=0 --end=8 --loop ~/imninja.mp3) & echo ${!} > /tmp/imninja.pid"
			  control + o

			# play hateme.mp3
			"(mpv --audio-device=pipewire/combined --start=0 --loop ~/'Johnny Rebel Nigger Hating Me [WPRshb4HCXE].mp3') & echo ${!} > /tmp/imninja.pid; (mpv --audio-device=pipewire --start=0 --loop ~/'Johnny Rebel Nigger Hating Me [WPRshb4HCXE].mp3') & echo ${!} > /tmp/imninja.pid;"
			 F5

			# play white noise
			"(mpv --volume-gain=15 --audio-device=pulse/combined --loop --audio-samplerate=400 ~/16000.wav) & echo ${!} > /tmp/imninja.pid;pactl set-source-mute alsa_input.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.mono-fallback on"
			  F9

			# stop imma ninja
			"kill -9 $(cat /tmp/imninja.pid)"
			  control + p

			# stop imma ninja
			"killall mpv; pactl set-source-mute alsa_input.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.mono-fallback off"
			  control + shift + p
		[/file]



		HINT #1:
		^ Pressing control+o will play audio on both the virtual pulse/combined audio device and the default device
		handled by pipewire, allowing song to be streamed through the default audio device and played to other 
		players in-game through the virtual pulse/combined microphone.

	check file for more examples: ~/.xbindkeysrc
	Note that some keys might be undesirebale, then if so uncomment them and
	reload xbindkeys if it's already running:
		#killall -SIGKILL xbindkeys

		A)(Alternative): #killall -HUP xbindkeys
			^ if you need, make sure to check on 'pstree | less' that the process isn't there.
			^ Note: -SIGKILL is superior.

		B) Reload/Restart xbindkeys to apply any made changes:
			$ xbindkeys

	if you have no keys working, make sure to initialize it with:
		$ xbindkeys
	

	Follow the two topics below if you want to use it with games:
	------------------------
	SETTING UP THINGS FOR STREAMING
		SETTING UP VIRTUAL AUDIO DEVICE
			This will create a virtual input/output audio device called combine:
				$ pactl load-module module-combine-sink

		Read more on topic:
			CONNECTING OUTPUT SOUND DEVICE TO VIRTUAL INPUT DEVICE

		SETTING UP OBS-STUDIO
			Add newly created 'combine' audio device to the list of devices in obs-studio,
		right-click > 'Advanced Audio Properties' > Start monitoring desired device

		The best option tho is to disable monitoring 'pulse/combined' device and remove it from the obs list,
		and have the audio playing through different devices at the same time as described in the HINT #1 above.
	------------------------
	--- CREATING MACROS
		Requires both xbindkeys and xdotool installed!
		
		VERY IMPORTANT: always make combinations between keys!
		keys are always active regardless of active windows on the screen.

		A) Only add sleep 0.2 if sleep 0.1 if the macro doesn't work!
		^ sleep command is mandatory.

		B) The "--window $(xdotool getwindowfocus)" isn't mandatory
		should work flawlessly without it.

		C) Default delay between each key press is 12ms

		Add the following code for a game macro:

		[file: ~/.xbindkeysrc ]
			"xdotool sleep 0.1 key --delay 10 a+b"
			  alt+d

			"xdotool sleep 0.1 key --window $(xdotool getwindowfocus) a+b"
			  alt+f

			"xdotool sleep 0.2 key --window $(xdotool getwindowfocus) a+b"
			  Shift+f
		[/file]



		Finally kill and run 'xbindkeys' again and wait a few seconds:
			A) #killall -HUP xbindkeys
			B) $xbindkeys

	--- MACRO PERSISTENCE
		If you want these macros to be permanently set on the system,
		you add "xbindkeys" on any ~/.bashrc file or ~./.zshrc file.

		Disabling them is as easy as doing:
			#killall -HUP xbindkeys

--- CUSTOM TABLE:
To make your own custom table, make the changes using xmodmap and then do the following:

	$touch ~/.Xmodmap
	$xmodmap -pke >> ~/.Xmodmap
	$xmodmap ~/.Xmodmap

HINT/SUGGESTION: You can use ! character as comment-delimiter

--- MAKING CUSTOM TABLE PERMANENT:
Just add the following command into your ~/.xinitrc(Just for startx),
if you use something different than startx, you can still add this to your shell profile,
which is eiher ~/.bashrc or ~/.zprofile if you're using either bash or zsh respectively.

	xmodmap ~/.Xmodmap

Or you can give it another name if you feel like it.

----------------------------------------
================================================================================
CREATING MODULAR KEYBINDINGS
	Modular keybinds will allow you to switch which keys do what during runtime,
without changing anything.
 
For this, follow this step:
	1 - Create a folder called xbindkeys_files
		$mkdir -p ~/.hidden/xbindkeys_files/ ;

	2 - Create a script file on ~/.xbindkeysrc

		[file: ~/.xbindkeysrc]
			# Switches to song keys:
			"killall -SIGKILL xbindkeys; cp -f ~/.hidden/xbindkeys_files/.xbindkeysrc_music ~/.xbindkeysrc; xbindkeys;"
			  shift + F11

			# Switches to default keys:
			"killall -SIGKILL xbindkeys; cp -f ~/.hidden/xbindkeys_files/.xbindkeysrc_default ~/.xbindkeysrc; xbindkeys;"
			  shift + F12

			# Closes current window:
			"xdotool sleep 0.2 key  super+shift+c ;"
			  shift + F2

			# Checking  clock time:
			"SHELL=/bin/sh xterm  -bg black  -fg yellow -fa 'Serif' -fs 8 -geometry +200+20 -e 'tclock & sleep 4'"
			  shift + F3


		[/file]

	3 - Copy step #2 file to ~/.hidden/xbindkeys_files/.xbindkeysrc_default

	4 - Suppose you have keybinds for songs, you can just enable them now with 'shift+F11' and disable them with shift+F12

	5 - When creating new Files, Make sure you have step #2 keybindings in all files, so you can freely switch between them.
	
	CREATING AN ALERT WINDOW
		If you need an alert window for a given macro, you can just use 'konsole':
			1 - (Simple Example) Invoke console as in:
				$ konsole --hold -e "sudo pacman -Syu"

			2 - (Alternate), Example: Changing background and font colors as well as font type:
				$ xterm -T "Alert Screen" -bg black  -fg yellow -fa 'Serif' -fs 12

			3 - (Alternate, Example): Changing window geometry:
				$ xterm -bg black  -fg yellow -fa 'Serif' -fs 12 -geometry 50x35+400+20
				^ 50 characters per line, 35 lines, position-x=400, position-y=20 

		CREATING A TEMPORARY 4 SECONDS WINDOW
			This is a more practical approach, creating a temporary window for only 4 seconds:
				SHELL=/bin/sh xterm  -bg black  -fg yellow -fa 'Serif' -fs 8 -e 'tclock & sleep 4'

			
		^ this is just a minor example, it's useful if you ever need to type in password or give any type of user input.


----------------------------------------
================================================================================
================================================================================
================================================================================
CREATING MAPS USING HWDB
	HWDB allows mappings to be persistent permanently,
so resetting to default will require owning 2 different files:
	1 - One containning original default values
	2 - One containning the edited map


	READING INPUTS FROM KEYBOARD AND OTHER DEVICES
		'evtest' should allow reading input from a range of different devices.
	after selecting an input device, the user should press a key, by pressing a key this will be shown:

		-----------------------------SYN REPORT-----------------------------------------
		Event: time 1726518230.346591, type 4 (EV_MSC), code 4 (MSC_SCAN), value 7001c
		Event: time 1726518230.346591, type 1 (EV_KEY), code 21 (KEY_Y), value 0
		-----------------------------SYN REPORT-----------------------------------------
		Event: time 1726518230.474627, type 4 (EV_MSC), code 4 (MSC_SCAN), value 7000c
		Event: time 1726518230.474627, type 1 (EV_KEY), code 23 (KEY_I), value 1
		-----------------------------SYN REPORT-----------------------------------------
	
	The value 7000c refers to the code of the given key press 'I'
	The value 7001c refers to the code of the given key press 'Y'

	FETCHING ACTUAL DEVICE NAME
		'evtest' can return the real device name, but you can also fetch it using the below command:
			$ cat /proc/bus/input/devices

	CREATING DEFAULT MAPPING
		This will help saving the default value for the 'y' key on the keyboard.

		[file ~/scripts/99-keyboard-default.hwdb]
			evdev:name:SEM USB Keyboard:*
			 KEYBOARD_KEY_7001c=y
		[/file]


	CREATING CUSTOM EDITED MAPPING
		
		[file ~/scripts/99-keyboard-custom.hwdb]
			evdev:name:SEM USB Keyboard:*
			 KEYBOARD_KEY_7001c=i
		[/file]
	
	ENABLING A GIVEN CREATED MAPPING
		Copy one of the the custom maps to: 
			/etc/udev/hwdb.d/ 

		then run the following command:
			$ run0 systemd-hwdb update; run0 udevadm trigger
	
	This should work over wayland, x11 or any other display manager.

----------------------------------------
================================================================================
THE NUMLOCK KEY:
The numlock key can allow the numpad to tbe used for scrolling when deactivated,
plus if one wants to, numlock also works as a modifier key for adding functionality
to other keys(that can be done using xmodmap and will not be covered here)


ex.:	Key 1	#Scrolls to the end of the page
	Key 7	#Scrolls to the start of the page
	Key 8	#Slowly up-scrolls the page
	Key 2	#Slowly bottom-scrolls the page
	Key 9	#Fast scrolls to the top-page
	Key 3	#Fast scrolls to the bottom-page

Number 5 is a neutral-key and can be used as a Key-Modifier!

In fact 7,1,9,3 are actually page_up,page_down,home,end!
All it's required to use them is to actually turn off numlock! :))) 
----------------------------------------
================================================================================
HINT ON MODIFYING TEXT FILES:
$echo 'xmodmap -e "remove lock = Caps_Lock"' >> ~/.bash_profile

^ command above will add "remove lock = Caps_Lock" to your bash profile file.

----------------------------------------
================================================================================
SUDO BASIC
SUDO - SUDOERS TUTORIAL
Next, add the line below:
	john ALL=(root) /bin/systemctl restart NetworkManager
		(the 1st ALL indicates hostname)

To run specific commands with sudo as ANY TARGET USER, for example to allow user john to restart only Apache service using sudo:
	john ALL=(ALL) /bin/systemctl restart apache2

To allow a specific user to run multiple specific commands AS ANY USER with sudo:
	john ALL=(ALL) /path/to/command1, /path/to/command2, /path/to/command3

To enable user called john to restart Network Manager on an Ubuntu system as any user without being prompted for password, at the line below to sudoers file:
	john ALL=(ALL) NOPASSWD: /bin/systemctl restart NetworkManager

This will also work: it sets the configuration line in /etc/sudoers.d/ instead of /etc/sudoers file and works the same way!
	echo "john ALL=(ALL) NOPASSWD: /bin/systemctl restart NetworkManager" >> /etc/sudoers.d/john

To run all sudo commands without password prompt as any user,group on all hosts, enter the line below in sudoers file:
	username ALL=(ALL:ALL) NOPASSWD:ALL

(all:all defines user:group, the reason is sudo allows users to execute allowed programs AS another user/group.)

ADDING USER TO SUDOERS LIST
	First, you'll need to add yourself as SUDOERS:
	
	1) Add yourself as sudo by using 'visudo' and editing /etc/sudoers,
		It is recommended to use visudo!
			$ visudo

	2) Add yourself to wheel and sudo groups
		$ usermod -aG wheel,sudo <username>
	
	3) You can now finally use 'sudo' test it with:
		$ sudo ls -la ~/
	
		TROUBLESHOOTING ADDING USER TO SUDO
			A) You can not add yourself to /etc/sudoers without administrative privileges on the system
			B) You can check which groups your user already belongs to:
				$ groups
			C) You can check which user you are:
				$ whoami
				   or
				$ users


RUNNING PROGRAMS AS ROOT WITH SUDO
	A) Use sudo to run commandline applications as root or another user(default is root):
		$sudo cp <file2> /tmp/
	
	B) You can also run applications as a different user:
		$ sudo -iu <username> <program_name>

	C) Use gksudo to run GUI applications as root or another user(default is root):
		$gksudo teams

SUDO / GKSUDO OR GKSU COMMAND OPTIONS
	
	Invoking the login shell:
		sudo -i
	
	Executing programs as another user(default is root)
		gksudo -u teamuser cat /etc/sudoers
			or
		gksu -u teamuser cat /etc/sudoers
----------------------------------------
================================================================================
DIAGNOSING STEAM ERRORS / FIXING BROKEN PROGRAMS THAT WERE WORKING BEFORE AN UPDATE:
https://archive.archlinux.org/packages/

ex.:
	[2021-09-18 09:33:02] Downloading manifest: https://cdn.cloudflare.steamstatic.com/client/steam_client_publicbeta_ubuntu12
	Installing breakpad exception handler for appid(steam)/version(1631912732)
	[2021-09-18 09:33:02] Download skipped by HTTP 304 Not Modified
	[2021-09-18 09:33:02] Nothing to do
	[2021-09-18 09:33:02] Verifying installation...
	[2021-09-18 09:33:02] Performing checksum verification of executable files
	[2021-09-18 09:33:03] Verification complete
	Failed to load steamui.so - dlerror(): /usr/lib32/libffi.so.7: undefined symbol: ffi_prep_closure_loc
	[2021-09-18 09:33:08] Shutdown
	Installing breakpad exception handler for appid(steam)/version(1631912732)
	Installing breakpad exception handler for appid(steam)/version(1631912732)
	archlinx%

Pay attention to this line: 
	Failed to load steamui.so - dlerror(): /usr/lib32/libffi.so.7: undefined symbol: ffi_prep_closure_loc

This means that the current lib32-libffi has an undefined symbol called ffi_prep_closure_loc which steam is unable to handdle with.
It's very likely that an updated lib32-libffi package has broken steam; if this is truly the this case you'll need to downgrade 
to an earlier lib32-libffi through the ALA Repository.

this can happen not only to steam, but it can happen as well as to other applications like nvidia, etc.

ALA REPOSITORY:
https://archive.archlinux.org/packages/
----------------------------------------
================================================================================

Troque o Windows por Linux (recomendamos o Linux Mint, Zorin ou Garuda).
Troque o Facebook pelo Discord.
Troque o Whatsapp pelo Telegram.
Troque o Gmail e Hotmail pelo Protonmail, Tutanota ou Disroot.
Troque o Youtube pelo Odysee.
Troque o Google pelo DuckduckGo, Startpage, Searx ou Qwnat.
Troque o Chrome pelo Firefox (bem configurado) ou Brave ou DuckDuckGo (celular).
Troque o GoogleDrive, OneDrive e ICloud pelas alternativas NextCloud, Sync, CryptPad, OwnCloud, Syncthing ou Seafile.
Troque o Android padrão do celular ou seu iOS por uma rom voltada para a privacidade e segurança como, GrapheneOS, LineageOS ou CalyxOS.Use Samourai (Android) e Electrum no PC (Linux), 
como carteiras de bitcoin.Use os plugins Privacy Badger ou DuckDuckgo + Https Everywhere (pode ser ativo nativamente no Firefox) + Ublock Origin.

https://odysee.com/@AlphaNerd:8
----------------------------------------
================================================================================
aur/rapiddisk-dkms 7.2.1-1 (+4 0.79)
	Limitations:
		Not Headless, rapiddisk is actually offered as sys service.
		https://github.com/pkoutoupis/rapiddisk/issues - 
		https://github.com/pkoutoupis/rapiddisk/issues/59 - RapidDisk-Cache does not support 4K Logical Drives
		https://github.com/pkoutoupis/rapiddisk/issues/14 - Implement OpenSUSE Kernel Module Packaging - Not Supported on All Linuxes

community/anything-sync-daemon 5.85-3 (16.2 KiB 30.1 KiB)
	1 - Not Headless, anything-sync-daemon is actually offered as sys service.
	2 - Automatically Flushes back data to disk using rsync; this can cause overhead in small embedded systems.
	3 - It accepts the risk of data not being synced back to disk with r-sync and will therefore attempt to recover original files in place.
		https://wiki.archlinux.org/title/Anything-sync-daemon

gopreload
	preload runs as a daemon to record statistics about usage of files by more frequently-used programs. 
	as a result of this, it might not include ALL of the user desirable data into RAM.
	it decides on it's own which files might go to RAM and which might not.
	This isn't interesting because it won't cover desired performance on all 100% of the situations.
----------------------------------------
================================================================================
CODEBLOCKS AND GCC COMPILER:
Settings codeblocks to work can be more of a hasle than using the raw compiler executable 
at commandline.

For example, some settings need to be set manually by the user:
	Settings > Compiler...

From there, you need to create either new compiler flags or new linker flags:
	Right Mouse Click > New Flag

This can be a bit of pain if you don't know which of the flags are for.
If you have custom flags, you need to set them properly as Linker Flag or Compiler Flag.

To make it more of a hasle, codeblocks has Global and Local Settings for each project.
The Global Settings are applied on top of the local settings, that of course means
that if you have same settings saved on both local and global, they'll be applied
twice when compiling the projects, because the Global Settings do not override the local
ones, instead they act as a complement to the local ones.

Again, some of the settings only exist as global ones, 
such as when you opt in for a specific debugger profile.

----------------------------------------
================================================================================
GOOD APPLICATIONS DESCRIPTIONS:

V4L2-CTL:	Used for controlling certain webcam settings
v4l2-ctl

examples: 
v4l2-ctl --list-ctrls			#Lists all controls available
v4l2-ctl --set-ctrl brightness=40	#Increases/Decrease Brightness
v4l2-ctl --set-ctrl saturation=70	# ^ Same as above
v4l2-ctl --set-ctrl contrast=70		# ^ Same as above
v4l2-ctl --set-ctrl gamma=150		# ^ Same as above
v4l2-ctl --set-ctrl hue=50 		# ^ Same as above
v4l2-ctl --set-ctrl contrat=10		# ^ Same as above

v4l2-ctl --set-ctrl brightness=4; v4l2-ctl --set-ctrl saturation=70; v4l2-ctl --set-ctrl contrast=70; v4l2-ctl --set-ctrl gamma=150; v4l2-ctl --set-ctrl hue=50; v4l2-ctl --set-ctrl contrat=10;
----------------------------------------
================================================================================
DD COMMANDS:
The dd command is a simple, yet versatile and powerful tool. It can be used to copy from source to destination, block-by-block, regardless of their filesystem types or operating systems. A convenient method is to use dd from a live environment, as in a Live CD.

Warning: As with any command of this type, you should be very cautious when using it; it can destroy data. Remember the order of input file (if=) and output file (of=) and do not reverse them! Always ensure that the destination drive or partition (of=) is of equal or greater size than the source (if=).

dd is part of the GNU coreutils. For other utilities in the package, please refer to Core utilities.
----------------------
CLONING A PARTITION

1) From physical disk /dev/sda, partition 1, to physical disk /dev/sdb, partition 1:
	# dd if=/dev/sda1 of=/dev/sdb1 bs=64K conv=noerror,sync status=progress

Note: Be careful that if the output partition of= (sdb1 in the example) does not exist, dd will create a file 
with this name and will start filling up your root file system.
----------------------
CLONING AN ENTIRE HARD DISK
1) From physical disk /dev/sda to physical disk /dev/sdb:
	# dd if=/dev/sda of=/dev/sdb bs=64K conv=noerror,sync status=progress

--------------------------------
CREATE DISK IMAGE:
Boot from a live medium and make sure no partitions are mounted from the source hard drive.

1) Then mount the external hard drive and backup the drive:
	# dd if=/dev/sda conv=sync,noerror bs=64K | gzip -c  > /path/to/backup.img.gz

2) If necessary (e.g. when the resulting files will be stored on a FAT32 file system) split the disk image into multiple parts (see also split(1)):
	# dd if=/dev/sda conv=sync,noerror bs=64K | gzip -c | split -a3 -b2G - /path/to/backup.img.gz

3) If there is not enough disk space locally, you may send the image through ssh:
	# dd if=/dev/sda conv=sync,noerror bs=64K | gzip -c | ssh user@local dd of=backup.img.gz

4) Finally, save extra information about the drive geometry necessary in order to interpret the partition table stored within the image. 
The most important of which is the cylinder size.
	# fdisk -l /dev/sda > /path/to/list_fdisk.info
--------------------------------
BINARY FILE PATCHING
If one wants to replace offset 0x123AB of a file with the FF C0 14 hexadecimal sequence, this can be done with the command line:

# printf '\xff\xc0\x14' | dd seek=$((0x123AB)) conv=notrunc bs=1 of=/path/to/file

--------------------------------
BENCHMARKING TMPFS | RAMFS:
# dd if=/dev/zero of=/tmp/tmp.file bs=1M count=1  			#Benchmarks MRAM 	
# dd if=/dev/zero of=/home/<username>/tmp.file bs=1G count=1		#Benchmarks DISK

sudo hdparm -Ttv /dev/sdc1						#Does ONLY work for disks not TMPFS/RAMFS

# dd if=/dev/zero of=/tmp/tmp.file bs=1G count=3			#Benchmarks MRAM
# dd if=/dev/zero of=/home/<username>/tmp.file bs=1G count=3		#Benchmarks DISK

NOTE: Make sure to erase tmp.file on both cases!
	# rm /tmp/tmp.file && rm /home/<username>/tmp.file
----------------------------------------
BACKUP AND RESTORE MBR:
Before making changes to a disk, you may want to backup the partition table and partition scheme of the drive. You can also use a backup to copy the same partition layout to numerous drives.

The MBR is stored in the the first 512 bytes of the disk. It consists of 4 parts:

The first 440 bytes contain the bootstrap code (boot loader).
The next 6 bytes contain the disk signature.
The next 64 bytes contain the partition table (4 entries of 16 bytes each, one entry for each primary partition).
The last 2 bytes contain a boot signature.


1) To save the MBR as mbr_file.img:
	# dd if=/dev/sdX of=/path/to/mbr_file.img bs=512 count=1

2) You can also extract the MBR from a full dd disk image:
	# dd if=/path/to/disk.img of=/path/to/mbr_file.img bs=512 count=1

3)To restore (be careful, this destroys the existing partition table and with it access to all data on the disk):
	# dd if=/path/to/mbr_file.img of=/dev/sdX bs=512 count=1

4)If you only want to restore the boot loader, but not the primary partition table entries, just restore the first 440 bytes of the MBR:
	# dd if=/path/to/mbr_file.img of=/dev/sdX bs=440 count=1

5)To restore only the partition table, one must use:
	# dd if=/path/to/mbr_file.img of=/dev/sdX bs=1 skip=446 count=64

6)To erase the MBR bootstrap code (may be useful if you have to do a full reinstall of another operating system) only the first 440 bytes need to be zeroed:
	# dd if=/dev/zero of=/dev/sdX bs=440 count=1

SOURCE: https://wiki.archlinux.org/title/Dd


------------------
RESTORE SYSTEM
1) To restore your system:
	# gunzip -c /path/to/backup.img.gz | dd of=/dev/sda

2) When the image has been split, use the following instead:
	# cat /path/to/backup.img.gz* | gunzip -c | dd of=/dev/sda
----------------------------------------
================================================================================
OBS STUDIO SETTINGS:
	The settings in here are mostly used for recording, for live streaming try the
"PERFECT OBS STUDIO CONFIGURATION AND SETTINGS / PERFECT LIVE STREAMING CONFIGURATION" manual section.

VERY IMPORTANT: Base (Canvas) and Output (Scale) have to be the same resolution as the desktop,
all games and applications must run under this one resolution.

Settings > Videos:
	Base (Canvas) Resolution: 1440x900
	Output (Scale) Resolution: 1440x900
	Downscale Filter: Bilinear(Fastest, but blurry if scaled)
	Common FPS Values: 60 FPS

Settings > Output:
	Output Mode: Advanced
	Recording Format: flv
	Encoder: NVIDIA NVENC H.264 | NVENC
	Rate Control: CBR
	Bitrate: 8150 Kbps
	Preset: Max Performance
	Profile high
	Psycho Visual Tunning: Unchecked

Always use application output when recording instead of screen capture!
Right Click on Source > Add > Window Capture(XComposite)

Window Capture is not only less resource consuming, it also produces
better quality output than sreen capture.

Known issues with Window Capture: 3rd party windows can't be seen;
plus if the application opens a secondary window, that window will also not be seen in Window Capture.

Install v4l2-dkms or v4l2 for Faster Webcam Output
Packages Below:
	v4l2loopback-dkms
	v4l2
----------------------------------------
================================================================================
MKINITCPIO:
MODULES=(vfio vfio_iommu_type1 vfio_pci vfio_virqfd virtio-gpu nvidia nvidia_modeset nvidia_drm nvidia_uvm wd719x v4l2loopback)
----------------------------------------
================================================================================
CUSTOM KERNELS:
If you want to install Custom Kernels, you need to grab it's header and doc files and also the kernel itself
through the package manager.

-------Stock Kernels:
Usually stock kernels come up in three files:
	1 - The kernel itself(linux)
	2 - The kernel header files(linux-header)
		These are used for installing 3rd party devices drivers as kernel modules
		like for example, nvidia-dkms and v4l2loopback-dkms modules.
	3 - The Kernel docs(linux-docs)

-------Custom Kernels
The custom kernels also come up in those 3 files/packages listed in the Stock Kernels.

Known good kernels are:
	1) Linux Zen Kernel(A Distribution of the liquorix kernel for archlinux)
		linux-zen, linux-zen-headers, linux-zen-docs

		Last Known Good Version:
			Linux-Zen 5.14.xx

	2) Linux PDS or Linux PDS TKG(Recommended)
		This Linux kernel has it's own CPU Scheduler which makes it safe & sound for gaming;
		the mainline kernels will often try to update/change the CPU Scheduler for improvements and end up causing
		performance loss. The linux-zen kernel is know to suffer from this, since it ends up being just a fork of whatever
		is done in the stock kernel without further changes.

		Requirements:
			It's required to know which CPU you own before installation/compilation and also the Timer Frequency(BCLK - Base Clock) that 
			your motherboard uses for your CPU(your CPU must support it!) which usually is 100Mhz for old computers and old CPUs.

			use "sudo dmidecode --type processor" to make sure of it.

		Known files:
			linux-pds, linux-pds-headers, linux-pds-docs

	3) Linux Hardened
		Linux Kernel used for extreme security, trades performance for system security.

		Known Files:
			linux-hardened, linux-hardened-headers, linux-hardened-docs

--------Setting up kernels with DKMS modules
	It's often required that you install your DKMS modules through the package manager,
	once that's done it's required to add them to your /etc/mkinitcpio.conf fille
	and rebuild your initramfs with "sudo mkinitcpio -P"(archlinux).

	From this point ownwards you can either reboot your system or use modprobe on all the required dkms modules.

----------------------------------------
================================================================================
CALCULATING SCREEN / MONITOR DPI:

1 - Find the Current DPI Value of your Screen
	xdpyinfo | grep -B 2 resolution

2 - How to calculate the Right DPI
	xrandr | grep -w connected

	Output ex.: 
		DVI-D-0 connected primary 1440x900+0+0 (normal left inverted right x axis y axis) 410mm x 257mm

3 - Convert 410mm and 257mm to centimeters
	410mm = 41cm
	257mm = 25,7cm

4 - Convert centimeters to inch
	41cm / 2.54 = 16.141732283 inches
	25,7cm / 2.54 = 10.118110236 inches

5 - Fetch your screen resolution and divide it with the inches value
	1440 x 900  
	
	1440 / 16.141732283 = 89.2097561
	900 / 10.118110236 = 88.949416344 

	result = 89 x 89 dots per inch

6 - Change your current DPI:
	xrandr --dpi 89

		or

6.1 - Edit the file "/home/your user name/.Xresources" and add the following line:
	Xft.dpi: 89

	Then finally: xrdb -merge ~/.Xresources

----------------------------------------
================================================================================
FINDING OUT AVAILABLE RESOLUTIONS:
	Type on terminal: 
		$xrandr

Note: as of today(2023) nvidia has disabled Interlaced Modes
for all their Turing GPU cards(Including GTX 1650 and all Raytracing enabled graphics cards), so it's not possible to list any Interlaced resolutions using xrandr or any other method.
----------------------------------------
================================================================================
SOURCE:
https://wiki.gentoo.org/wiki/Linux_firmware
----------------------------------------
================================================================================
NSCDE - Desktop Enviornment
Documentation: file:///usr/share/doc/nscde/html/Keybindings_fvwmconf.html

Hotkeys
ALT+F4 - Resizes window(must click on it before)

SHIFT+TAB+TAB - Window Manager
META+TAB - Cycles through workspaces
META+SHITT+TAB - Reverse Cycles through Workspaces

META+ALT+W - Iconifies all programs in a given window(pressing twice will reverse)

ALT+ESCAPE - Lists all programs in the given workspace 
ALT+ESCAPE+ESCAPE - Listas all programs in all workspaces
----------------------------------------
================================================================================
HOW TO MAKE A GPT PARTITION + UEFI BOOT:
VERY IMPORTANT: https://www.gnu.org/software/grub/manual/grub/html_node/BIOS-installation.html#BIOS-installation

1 - ^ "When creating a BIOS Boot Partition on a GPT system, you should make sure that it is at least 31 KiB in size. (GPT-formatted disks are not usually particularly small, so we recommend that you make it larger than the bare minimum, such as 1 MiB, to allow plenty of room for growth.) You must also make sure that it has the proper partition type. Using GNU Parted, you can set this using a command such as the following:"

According to: https://wiki.archlinux.org/title/GRUB#GUID_Partition_Table_(GPT)_specific_instructions

2 - ^ "On a BIOS/GPT configuration, a BIOS boot partition is required. GRUB embeds its core.img into this partition."

and also:
3 - "Create a mebibyte partition (+1M with fdisk or gdisk) on the disk with no file system and with partition type GUID 21686148-6449-6E6F-744E-656564454649.

Select partition type BIOS boot for fdisk.
Select partition type code ef02 for gdisk.
For parted set/activate the flag bios_grub on the partition.
This partition can be in any position order but has to be on the first 2 TiB of the disk. This partition needs to be created before GRUB installation. When the partition is ready, install the bootloader as per the instructions below.

The space before the first partition can also be used as the BIOS boot partition though it will be out of GPT alignment specification. Since the partition will not be regularly accessed performance issues can be disregarded, though some disk utilities will display a warning about it. In fdisk or gdisk create a new partition starting at sector 34 and spanning to 2047 and set the type. To have the viewable partitions begin at the base consider adding this partition last."


4 - Note about UEFI Systems:

"It is recommended to read and understand the Unified Extensible Firmware Interface, Partitioning#GUID Partition Table and Arch boot process#Under UEFI pages.

WHEN INSTALLING TO USE UEFI IT IS IMPORTANT TO BOOT THE INSTALLATION MEDIA IN UEFI MODE, OTHERWISE EFIBOOTMGR WILL NOT BE ABLE TO ADD THE GRUB UEFI BOOT ENTRY. INSTALLING TO THE FALLBACK BOOT PATH WILL STILL WORK EVEN IN BIOS MODE SINCE IT DOES NOT TOUCH THE NVRAM.
To boot from a disk using UEFI, an EFI system partition is required. Follow EFI system partition#Check for an existing partition to find out if you have one already, otherwise you need to create it."

----------
Notes to take in consideration:
1 - For RAW UEFI boot without GRUB -> https://wiki.archlinux.org/title/Arch_boot_process#Boot_loader
Some bootloaders require Filesystem Support inherited from the firmware like "Unified Kernel Image" and "EFISTUB"
Bootloaders like grub do not require firmware support for the given Filesystem created for an EFI Partition! - VERY IMPORTANT!
This is why old motherboards don't have "full-support" for UEFI system, since they don't have support for specific filesystems! - VERY IMPORTANT

^ "FILE SYSTEM SUPPORT IS INHERITED FROM THE FIRMWARE. THE UEFI SPECIFICATION MANDATES SUPPORT FOR THE FAT12, FAT16 AND FAT32 FILE SYSTEMS[6], BUT VENDORS CAN OPTIONALLY ADD SUPPORT FOR ADDITIONAL FILE SYSTEMS; FOR EXAMPLE, THE FIRMWARE IN APPLE MACS SUPPORTS THE HFS+ FILE SYSTEM. IF THE FIRMWARE PROVIDES AN INTERFACE FOR LOADING UEFI DRIVERS ON STARTUP, THEN SUPPORT FOR ADDITIONAL FILE SYSTEMS CAN BE ADDED BY LOADING (INDEPENDENTLY ACQUIRED) FILE SYSTEM DRIVERS."


MORE INFORMATION: https://wiki.archlinux.org/title/Arch_boot_process#Boot_loader - VERY IMPORTANT
----------------------------------------
================================================================================
BASIC LINUX CONCEPTS: 

WHAT'S THE KERNEL
The kernel is the core of an operating system. It functions on a low level (kernelspace) interacting between the hardware of the machine and the programs which use the hardware to run. The kernel temporarily stops programs to run other programs in the meantime, which is known as preemption. This creates the illusion of many tasks being executed simultaneously, even on single-core CPUs. The kernel uses the CPU scheduler to decide which program takes priority at any given moment.

WHAT'S INITRAMFS
After the boot loader loads the kernel and possible initramfs files and executes the kernel, the kernel unpacks the initramfs (initial RAM filesystem) archives into the (then empty) rootfs (initial root filesystem, specifically a ramfs or tmpfs). The first extracted initramfs is the one embedded in the kernel binary during the kernel build, then possible external initramfs files are extracted. Thus files in the external initramfs overwrite files with the same name in the embedded initramfs. The kernel then executes /init (in the rootfs) as the first process. The early userspace starts.

Arch Linux official kernels use an empty archive for the builtin initramfs (which is the default when building Linux). External initramfs images can be generated with mkinitcpio, dracut or booster.

The purpose of the initramfs is to bootstrap the system to the point where it can access the root filesystem (see FHS for details). This means that any modules that are required for devices like IDE, SCSI, SATA, USB/FW (if booting from an external drive) must be loadable from the initramfs if not built into the kernel; once the proper modules are loaded (either explicitly via a program or script, or implicitly via udev), the boot process continues. For this reason, the initramfs only needs to contain the modules necessary to access the root filesystem; it does not need to contain every module one would ever want to use. The majority of modules will be loaded later on by udev, during the init process.

WHAT'S INIT PROCESS
At the final stage of early userspace, the real root is mounted, and then replaces the initial root filesystem. /sbin/init is executed, replacing the /init process. Arch uses systemd as the default init.

WHAT'S GETTY
init calls getty once for each virtual terminal (typically six of them), which initializes each tty and asks for a username and password. Once the username and password are provided, getty checks them against /etc/passwd and /etc/shadow, then calls login. Alternatively, getty may start a display manager if one is present on the system.

WHAT'S A DISPLAY MANAGER
A display manager can be configured to replace the getty login prompt on a tty. - VERY IMPORTANT

In order to automatically initialize a display manager after booting, it is necessary to manually enable the service unit through systemd. For more information on enabling and starting service units, see systemd#Using units.

WHAT'S LOGIN  PROGRAM
The login program begins a session for the user by setting environment variables and starting the user's shell, based on /etc/passwd.

The login program displays the contents of /etc/motd (message of the day) after a successful login, just before it executes the login shell. It is a good place to display your Terms of Service to remind users of your local policies or anything you wish to tell them.

WHAT'S THE SHELL
Once the user's shell is started, it will typically run a runtime configuration file, such as bashrc, before presenting a prompt to the user. If the account is configured to Start X at login, the runtime configuration file will call startx or xinit.

GUI, XINIT OR WAYLAND
xinit runs the user's xinitrc runtime configuration file, which normally starts a window manager. When the user is finished and exits the window manager, xinit, startx, the shell, and login will terminate in that order, returning to getty.

source: https://wiki.archlinux.org/title/Arch_boot_process#Boot_loader
----------------------------------------
================================================================================
THINGS TO PAY ATTENTION WHEN USING LINUX:

1 - SUDO LOG AND MONITORING FILES(Requires Sudo Installed):
/var/log/ is where all linux main log files are located, pay attention because this can increase with time and fill out all your hard disk! - (date: 14/05/2022)
/root/.local/share/thrash is where all your root deleted files end up(only in cases where you don't use rm <filename>)
/etc/sudologs/ stores sudo logs, it can also fill up your entire hard drive if not properly set; by default, all sudo events are also stored in /var/log/sudo-io directory. 
sudo-io can store unlimited sudo events if you don’t limit that. A single sudo command stores event in a single directory with 7-8 separate files.

sudo-io can fill up your entire hard disk if you're not too careful, it's required that the linux user manually sets up a threshold size to sudo-io.

If you don’t limit sudo-io event creation then this will kill all your inode since a single sudo command stores its output in 7-8 files. Add the following configuration to /etc/sudoers file keep last 1000 sudo command history. sudo-io will truncate the oldest command after 1000 numbers have reached:
	Defaults  maxseq=1000

~/.bash_history file can also store bash commands even if you're on zsh or some other shell,
make sure there's a threshold by checking $HISTSIZE environment variable by doing: $cat $HISTSIZE
and also make sure to clean your bash history size from time to time with either:
	1.1 - cat /dev/null > ~/.bash_history
			or
	1.2 - history -c

2 - HOW TO USE SUDO-IO?
The user needs to have sudo installed and have the following attribute in his /etc/sudoers file:
	Defaults  log_input, log_output

sudo-io usually uses /var/log/sudo-io by default to store all sudo events.
However, You can specify a custom directory through the iolog_dir parameter.
	Defaults iolog_dir = <custom_dir>

Repeating, sudo-io WILL store unlimited sudo events if you don’t limit that. A SINGLE SUDO command stores event in a single directory with 7-8 separate files.

VERY IMPORTANT: make sure to reload your sudoers file configuration in case you have made changes to it,
usually the next sudo command will do it, but if it doesn't happen try restarting sudo service:
	#systemctl restart sudo_logsrvd.service
		or
	#service restart sudo

3 - REPLAYING SUDO-IO EVENTS

	sudoreplay -l					#Lists all sudo events
	sudoreplay -l user <username>			#Lists all sudo events for a given username
	sudoreplay -l user <username> command ls	#Lists all sudo events for a given username's command
	sudoreplay <replayid>				#Replays a given sudo session by it's ID
								ex.: sudoreplay 000001
	sudoreplay -d <sudo_log_dir> -l|head		#Replays sudo sessions from a given sudo log directory.
							ex.: sudoreplay -d /var/log/sudo-io/ -l|head
4 - LOGGING OPTIONS

The log options are set in the /etc/sudoers files:
example - Defaults log_input, log_output

4.1 - LOG_OUTPUT
	Warning do not log output from sudoreplay or reboot. To disable logging for sudoreplay Defaults! /usr/bin/sudoreplay !log_output. To disable logging for reboot Defaults! /sbin/reboot !log_output.

4.2 - LOG_INPUT
	WARNING For those trying to protect your data YOU MAY NOT WANT TO USE THIS. It may contain passwords and other sensitive information. 
You can also log input/output per command with these flags.

4.3 - NOLOG_INPUT AND NOLOG_OUTPUT
You can also log inputs/outputs per command with these flags:
Some more options:
	LOG_INPUT
	LOG_OUTPUT
	NOLOG_INPUT
	NOLOG_OUTPUT

4.4 - FURTHEROVER
Sudo also logs sudoreplay and reboot, it's important to either clean those logs places or disable them completely:

Add something like this to /etc/sudoers to log the output but not sudoreplay or reboot.

Defaults log_output

Defaults! /usr/bin/sudoreplay !log_output

Defaults! /sbin/reboot !log_output  

Very Important: You should note that flags are implicitly boolean and can be turned off using the '!' operator, 
and list attributes have two additional assignment operators, += (add to list) and -= (remove from list).

4.5 - USING VISUDO TO EDIT FILES
It's important to edit /etc/sudoers file using visudo, 
because visudo parses the file and guarantees there are no syntax errors, hence a syntax error would prevent 
you from booting into the system again.

4.6 - SET A SECURE PATH
This is the path used for every command run with sudo, it has two importances:

Used when a system administrator does not trust sudo users to have a secure PATH environment variable
To separate “root path” and “user path”, only users defined by exempt_group are not affected by this setting.

In /etc/sudoers file:
	Ex.: Defaults secure_path="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin"

4.7 - RUN SUDO COMMAND USING A PTY
A few times, attackers can run a malicious program (such as a virus or malware) using sudo, which would again fork a background process that remains on the user’s terminal device even when the main program has finished executing.

To avoid such a scenario, you can configure sudo to run other commands only from a psuedo-pty using the use_pty parameter, whether I/O logging is turned on or not as follows:
	ex.:	Defaults  use_pty

4.8 - LECTURING SUDO USERS
To lecture sudo users about password usage on the system, use the lecture parameter as below.

It has 3 possible values:

always – always lecture a user.
once – only lecture a user the first time they execute sudo command (this is used when no value is specified)
never – never lecture the user.

	ex.: Defaults  lecture="always"


Additionally, you can set a custom lecture file with the lecture_file parameter, type the appropriate message in the file:

	ex.: Defaults  lecture_file="/path/to/file"

4.9 - SET A WRONG PASSWORD MESSAGE
When a user enters a wrong password, a certain message is displayed on the command line. The default message is “sorry, try again”, you can modify the message using the badpass_message parameter as follows:

	ex.: Defaults  badpass_message="Password is wrong, please try again"

4.10 - INCREASE SUDO PASSWORD TRIES LIMIT
The parameter passwd_tries is used to specify the number of times a user can try to enter a password.

The default value is 3:

	ex.: Defaults   passwd_tries=5 

	     Defaults   passwd_timeout=2

passwd_timeout sets the threshold in minutes for password input(Default is 5mins).

4.11 - SUDO INSULTS
In case a user types a wrong password, sudo will display insults on the terminal with the insults parameter. This will automatically turn off the badpass_message parameter.

	ex.: Defaults insults

4.12 - DIFFERENCE BETWEEN SUDO, SU AND RUN0
Although optional, 'sudo' can log every single user access/use, whereas 'su' doesn't.
'sudo' also maintains a user list access of those allowed to use it; that means when a user isn't on the '/etc/sudoers' list, they can't use the 'sudo' command, 
even if they know the password. It's also possible to control which applications can be used by determined user(s) through 'sudo'.

‘su‘ forces the user to share the root password to other users who might need to use it for 'root'. Whereas ‘sudo‘ uses the user's own password for executing system commands. 
in other words, ‘sudo‘ lets you use your own password to execute system commands, delegating system responsibility without the root password.

‘sudo‘ is a root binary setuid, which executes root commands on behalf of authorized users and the USERS NEED TO ENTER THEIR OWN PASSWORD to execute system 
command followed by ‘sudo‘.

'run0' acts like 'sudo', but containerizes the executed command/user for safety. 'run0' however doesn't ever logs anything other than adding a single entry to 
/var/log/journal whenever executed. Run0 also doesn't use a file like '/etc/sudoers' for access control like 'sudo' does, Run0 instead does relies on Polkit for doing that.

5 - TOOLS FOR SECURELY REMOVING LINUX FILES
List of Programs: shred, wipe, secure-delete(or srm), sfill, sswap, sdmem

6 - LINUX HISTORY OF BASH COMMANDS
As you use linux commands, it'll store a list of used commands, make sure your $HISTSIZE variable isn't too big, or else it'll eat lots of disk space!
bash uses ~/.bashrc to control it's environment variables like $HISTSIZE and $HISTCONTROL

$HISTCONTROL = ignorespace 		#ignores saving bash commands when they have an white-space before them
						ex.: $ echo "Hello World"
$HISTCONTROL = ignoreboth		#ignores saving all bash command history

7 - RUNNING COMMANDS MULTIPLE TIMES IN LINUX | ITERATIVE LOOPS | ITERATION
Type this on terminal or a bash script .sh file:
	$ for x in {1..10}; do echo "Hello World # $x"; done

to run it from a file, type bash <file.sh>

	$ i=1; while [ $i -le 10 ]; do echo "Hello World #$i"; ((i++)); done

Another important part of the while loop is i=$(($i+1)) or (($i++)) which increments the counter until the test condition becomes false.

The command above is the same as the one bellow:
	i=1;\                                                                                                                                                                   
	while [ $i -le 10 ];
	do
		echo "hello world #$i";      
		((i++));
	done

8 - MONITORING LOG FILES IN REAL TIME
	# tail -f /var/log/apache2/access.log
		or
	# tailf /var/log/apache2/access.log

Usually, the log files are rotated frequently on a Linux server by the logrotate utility. 
TO WATCH LOG FILES THAT GET ROTATED ON A DAILY BASE you can use the -F flag to tail command.

# tail -F /var/log/apache2/access.log
tail -F will read the newly created file as soon as it's truncated

By default tail will display the last 10 lines of a given file,
you can increase/decrase that line count with : #tail -n2 -f /var/log/apache2/access.log

	8.1 MULTITAIL COMMAND – MONITOR MULTIPLE LOG FILES IN REAL TIME
		MULTITAIL utility can monitor and keep track of multiple files in real time.
	It'll also let you monitor different files back and forth.
		ex.: # multitail /var/log/apache2/access.log /var/log/apache2/error.log
	
	8.2 - MULTITAIL COMMAND - USING LNAV FOR THE SAME TASK ABOVE:
		ex.: # lnav /var/log/apache2/access.log /var/log/apache2/error.log
	
	8.2 - MULTITAIL COMMAND - USING LESS FOR THE SAME TASK ABOVE:
		ex.: # less +F /var/log/apache2/access.log
		Alternatively the user can press Shift+F by running standard less command without +F.

9 - MONITORING TOOLS
	Program Names: lynis, glances, monit, Sysstat, VnStat(comes with Sysstat?), iftop, nload, nethogs, bmon, iptraf, Iperf, NetPerf

Features of Sysstat:
It is available in all modern Linux distribution repositories by default.
Ability to create statistics about RAM, CPU, and SWAP usage. Besides the ability to monitor Linux kernel activity, NFS server, Sockets, TTY, and filesystems.
Ability to monitor input & output statistics for devices, tasks.. etc.
Ability to output reports about network interfaces and devices, with support for IPv6.
Sysstat can show you the power statistics (usage, devices, the fans’ speed.. etc) as well.
Many other features…


Features of Glance:
Glances is a monitoring tool built to present as much information as possible in any terminal size, it automatically takes the terminal window size it runs on, in other words, it’s a responsive monitoring tool.

Glances not only show information about CPU and memory usage but also monitor filesystem I/O, network I/O, hardware temperatures, fan speeds, disk usage, and logical volume.


Features of Monit:
Monit is a nice program that monitors your Linux and Unix server, it can monitor everything you have on your server, from the main server (Apache, Nginx..) to files permissions, files hashes, and web services. Plus a lot of things.

Features of vnstat(VERY VERY GOOD):
VnStat is a fully-featured, command line-based program to monitor Linux network traffic and bandwidth utilization in real-time, on Linux and BSD systems.

Features of iftop(VERY GOOD):
iftop – Displays Bandwidth Usage
iftop is a simple, easy-to-use, real-time top-like command line-based network bandwidth monitoring tool, used to get a quick overview of network activities on an interface. It displays network usage bandwidth updates every 2, 10, and 40 seconds on average.

Features of nload:
nload is another simple, easy-to-use command-line tool for monitoring network traffic and bandwidth usage in real-time. It uses graphs to help you monitor inbound and outbound traffic. In addition, it also displays information such as the total amount of transferred data and min/max network usage.

Features of Nethogs(VERY GOOD):
NetHogs – Monitor Network Traffic Bandwidth
NetHogs is a tiny top-like, text-based tool to monitor real-time network traffic bandwidth usage by each process or application running on a Linux system. It simply offers real-time statistics of your network bandwidth usage on a per-process basis.

Feature of Bmon:
bmon – Bandwidth Monitor and Rate Estimator
bmon is also a straightforward command-line tool for monitoring network bandwidth utilization and a rate estimator, in Linux. It captures network statistics and visualizes them in a human-friendly format so that you can keep an eye on your system.

Feature of IPTraf(VERY GOOD):
IPTraf is an easy-to-use, ncurses-based and configurable tool for monitoring incoming and outgoing network traffic passing through an interface. It is useful for IP traffic monitoring, and viewing general interface statistics, detailed interface statistics and so much more.

Features of Iperf:
Iperf/Iperf3 – Network Bandwidth Measurement Tool
Iperf/Iperf3 is a powerful tool for measuring network throughput over protocols such as TCP, UDP, and SCTP. It is primarily built to help in tuning TCP connections over a particular path, thus useful for testing and monitoring the maximum achievable bandwidth on IP networks (supports both IPv4 and IPv6).

It requires a server and a client to perform tests (which report the bandwidth, loss, and other useful network performance parameters).

Features of NetPerf:
Netperf – Network Bandwidth Testing
Netperf is similar to iperf, for testing network performance. It can help in monitoring network bandwidth in Linux by measuring data transfer using either TCP, UDP. It also supports measurements via Berkeley Sockets interface, DLPI, Unix Domain Sockets, and so many other interfaces. You need a server and a client to run tests.


----------------------------------------
================================================================================
DIFFERENCE BETWEEN BASH, SH AND SOURCE LINUX COMMAND
	Both are able to execute .sh script files, but
BASH will outsource your command, for example, if you 'cd' into a directory,
it WILL NOT take the user into that given directory. 

BASH is an actual shell, but will execute scripts if a script file is given as a parameter.

SH is the actual language interpreter for bash shell which is capable of running scripts as well.

SOURCE in the other hand executes the given bash script into the current shell
and once the execution flow is finished, all modified env. vars and directory changes(CD) 
will persist in the current shell environment.

----------------------------------------
================================================================================
DIFFERENCE BETWEEN TCL AND BASH SCRIPT:
The shell/s ( /bin/sh, /bin/bash, /bin/csh, /bin/ksh ... etc ) and Tcl are different interpreters. Generally shell scripting is used to run a series of commands as if you were at the terminal: "Tcl is commonly used for rapid prototyping, scripted applications, GUIs and testing"

Commands:
tclsh
man argv
----------------------------------------
================================================================================
IMPORTANT ENVIRONMENT VARIABLES AND OPTIONS:

1 - The history command can save the date and time of executed commands,
however it's required to do as followed:
	$ export HISTTIMEFORMAT='%F %T'
	$ history

2 - Permanently setting a Environment Variable value:
	$ export HISTTIMEFORMAT='%F %t' >> ~/.bash_profile

3 - Misc Commands:
$ echo $HISTFILE			#Displays Hist. File path and name
$ echo $HISTFILESIZE		#Displays Current Hist. File size
$ echo $HISTSIZE			#Displays maximum size of hist
----------------------------------------
================================================================================
WHAT DOES THE 2>&1 MEANS IN BASH SCRIPTING
	2>&1 is telling to redirect error output to the same place standard output is being redirected,
this has to happen in certain situations because by default errors output are usually redirected to a different 
pace other than the standard output, for example, sometimes you may want to redirect error output to the same output
as the standard output which could  be a file or /dev/null in case one desires to ignore both standard and error output.

ex1.: $curl http://www.google.com > ~/file.html 2>&1
	^ Normally, only standard output would be redirected to "file.html"
	and the error outputs would be ignored and not be written into the file in case errors ever happens.
	in other words, 2>&1 allows error output to be redirected into the same output as the standard output

ex2.: $curl http://www.google.com > /dev/null 2>&1 &
	^ In here we have standard output redirection to /dev/null and finally error output redirection to the 
	same place as the standard output, which in turn would be /dev/null.

Note.: The final '&' character from example 2 is telling the terminal to ignore the return of the program being called.
if there are any errors or any calls to standard output, they'll be print onto the screen regardless.

Furtheover, the number 1 denotes standard output which is stdout while the number 2 denotes standard error which is stderr. - VERY IMPORTANT
----------------------------------------
================================================================================
----------------------------------------
================================================================================
WINDOW MANAGER
	AWESOMEWM
		$ man awesome
		$ awesome --search <dir> 	#Adds directory to the library search path
		# awesome --replace		#Restarts awesomewm
----------------------------------------
================================================================================
WINE
	$ wine <binary>						#Executes the binary using wine using default wineprefix
	$ wine start <command>					#Runs a specific program in the background in the default wineprefix
	$ wine msiexec <i|x> <path_to_package.msi>		#Installs/Uninstalls a given MSI Package in the default wineprefix

	$ wine regedit|control|taskmgr				#Runs registry editor, control panel or task manager for default wineprefix

	$ wine cmd <windows_command>				#Executes the given windows command
									ex.: wine cmd dir

	$ winecfg						#Configures default/global settings for applications run through wine

	$ winetricks						#Creates and manages wine prefixes
									^ Sometimes it's necessary to create win32 wine-prefixes
									for a few applications - VERY IMPORTANT.

ABOUT WINEPREFIX
	Windows applications need a C: drive. Wine uses a virtual C: drive for this purpose. The directory 
	of this virtual C: drive is called wineprefix.

	To create a global wineprefix path, type in the terminal: 
		$ winecfg		#This creates a global wineprefix in your user directory under ~/.wine/c_drive/

	Another thing about wine, it actually reads your current linux partition installation as Z: Drive,
	so the Z:\ drive actually contains all your actual linux data.

	Note that local individual wineprefixes can also be created.

SETTING UP INVIDUAL WINEPREFIXES
	Open the terminal and type:
		$ WINEARCH=win32 WINEPREFIX=<"NEW_PREFIX_PATH"> winecfg
		^ this sets up a 32-bit wine prefix.

			or

		$ WINEPREFIX=<"NEW_PREFIX_PATH"> winecfg
		^ this sets up a 64-bit wine prefix.

	When the WINEPREFIX command finishes executing, you may see several GUI prompts appear. In these prompts, 
	you’ll be asked to install things for the software to run. Follow the GUI prompts and ensure everything is installed.

	After taking care of the tools Wine needs to install, the WINECFG tool will open. Use this tool to customize your new 
	Wine prefix and set things such as the Windows version, graphics, etc.

USING A WINE PREFIX
	If you want to use a different wine prefix, just type:
		$ WINEPREFIX=~/.wine-wow wine $HOME'/.wine-wow/drive_c/Program Files (x86)/Some Program/program.exe'
	
	The above command would select the .wine-wow prefix and run the World of Warcraft executable located in the prefix using WINE.


ABOUT WINETRICKS
	This is another important part of using Wine. Winetricks is a helper script to download and install various redistributable runtime 
	libraries needed to run some applications in Wine. These may include replacements for components of Wine using closed source libraries.

	WINECFG AND WINETRICKS
		'winecfg' is a nail and 'winetricks' is the powerdrill, they're both meant for the same thing,
		but winetricks is more powerful.


ABOUT PLAYONLINUX
	PlayOnLinux allows to create individual wineprefixes for each program/game you might have.

MYPREFIXES:

	Wine 32bit:
		$WINEPREFIX="~/.local/share/wineprefixes/<program>/" wine <path_to_binary>

	Wine 64bit:
		$WINEPREFIX="~/.local/share/wineprefixes/<program>/" wine <path_to_binary>

When the above doesn't work for a given program, try this:
		$WINEPREFIX="~/.local/share/wineprefixes/<program>/" wine cmd
			(although, not recommended: use winetricks instead to reach cmd prompt!)

	Using Wine Tricks:
		1 - type: $winetricks
		2 - create wineprefix(either 32bit or 64bit accordingly)
		3 - use wineprefix through winetricks
		4 - run commandline(debug)
		5 - execute your program typing: $ wine <binary>
			^ if you don't type "wine <binary>" to execute <binary> 
			^ then the default prefix will be used
			
----------------------------------------
================================================================================
Aug 03 17:55:52 archlinx dbus-daemon[2221]: [session uid=1000 pid=2221] Successfully activated service 'org.gnome.ScreenSaver'
Aug 03 17:56:00 archlinx kernel: perf: interrupt took too long (2507 > 2500), lowering kernel.perf_event_max_sample_rate to 79000
Aug 03 17:56:40 archlinx pulseaudio[2720]: ALSA woke us up to write new data to the device, but there was actually nothing to write.
Aug 03 17:56:40 archlinx pulseaudio[2720]: Most likely this is a bug in the ALSA driver 'snd_usb_audio'. Please report this issue to the ALSA developers.
Aug 03 17:56:40 archlinx pulseaudio[2720]: We were woken up with POLLOUT set -- however a subsequent snd_pcm_avail() returned 0 or another value < min_avail.
----
man BASH:
FILES
       /bin/bash
              The bash executable
       /etc/profile
              The systemwide initialization file, executed for login shells
       ~/.bash_profile
              The personal initialization file, executed for login shells
       ~/.bashrc
              The individual per-interactive-shell startup file
       ~/.bash_logout
              The individual login shell cleanup file, executed when a login shell exits
       ~/.inputrc
              Individual readline initialization file

----------------------------------------
================================================================================
TROUBLESHOOTING 2022.3
TIP 1(archlinux only): 
	If you're on CLI and need to re-install an older package to which you don't own in your pacman cache:

Step 1: You can get through acessing https://archive.archlinux.org/package/ which is the ALA Repository
by setting a temporary directory for download web-pages using wget, ex:
	$ mkdir ./TEMP; cd ./TEMP; wget https://archive.archlinux.org/package/l

Step 2: Then you read the source-code of the page looking for the package that you want using a text-editor like vi/vim/nvim,
in the example above the letter 'l' was choosen for the packages that start with an 'l', this includes
all linux kernel packages.

Step 3: Finally, you find the hyperlink to the package you want and repeat step 1 and step 2 until you find the desired 
package file and after you've found the package file, you can just copy the hyperlink to the file and use:
	# pacman -U https://archive.archlinux.org/package/p/pahole/pahole-1.23-1-x86_64.pkg.tar.zst

Step 4(optional): If you want to keep a copy of the given package out of pacman's cache, you can just use wget to download
the given package: $mkdir ./PACKAGES; cd ./PACKAGES; wget https://archive.archlinux.org/package/p/pahole/pahole-1.23-1-x86_64.pkg.tar.zst
By doing Step 4 you don't have to worry about cleaning your Package Manager cache anylonger.

-----
-----
-----
-----
RESTORING A BROKEN ARCHLINUX INSTALLATION

1. Insert your installation media and make sure it's connected
	1.1 - Boot your PC from archlinux installation media(cd/extern hard drive, usb pen drive)
	1.2 - Begin installation by copying data to RAM(only available through archlinux installation media)

2. Identify the hard disk containing your Broken Archlinux Operating System
	2.1 - Type "lsblk" into terminal to list all available block devices

3. Mount the block device containning your Broken Archlinux Operating System
	3.1 - Type "mount /dev/sdX /mnt"
		
4. If not connected to the internet, connect using: 
	4.1 - For Ethernet Cable Connection: dhcpcd

5. Install all basic important packages into the mounted device
	5.1 - Type: pacstrap -K /mnt <package1> <package2> <package3> <...>

		RECOMMENDED ARCHLINUX PACKAGES / RECOMMENDED PACKAGES:
	5.2 - List of known important packages(Mandatory):
		man-db man-pages texinfo dhcpcd iw iwd firewalld clamav nano tldr base base-devel systemd-ui systemdgenie linux-zen linux-zen-headers linux-zen-docs xterm terminology linux-lts linux-lts-headers linux-lts-docs linux-hardened linux-hardened-headers linux-hardened-docs sddm i3 git edk2-ovmf qemu-full sddm fuse2 os-prober lynis htop powertop linux-firmware sudo efibootmgr alsa-utils alsa-plugins git grub efibootmgr arch-install-scripts vi vim neovim e2fsprogs pcmanfm-gtk3 xorg-xkill thermald
		^ read topic on "INSTALL THERMALD"

	5.3 - List of Basic Packages(only use this if you don't need one or more of the above packages | Alternative to Mandatory)):
		man-db man-pages dhcpcd iw iwd firewalld clamav linux linux-docs linux-firmware base base-devel fuse2 os-prober sudo git efibootmgr alsa-utils alsa-plugins grub efibootmgr arch-install-scripts vi vim neovim e2fsprogs xorg-xkill thermald
		^ read topic on "INSTALL THERMALD"

	5.4 - List of Nvidia DKMS driver Package(date: 2022.2 | Nvidia Owners only):
		nvidia-settings nvidia-dkms nvidia-utils nvtop
	
	5.5 - Security Packages:
		iptables lynis clamav thermald
		^ read topic on "INSTALL THERMALD"
		^ read topic on "IPTABLES"
	
	5.6 - Audio Packages:
		alsa-plugins alsa-tools

	5.6 - Telnet Clients(optional | only available on AUR):
		syncterm netrunner

	5.5(extra info), about base base-devel package: it includes all the basic software needed for booting an archlinux system:
		filesystem  gcc-libs  glibc  bash  coreutils  file  findutils  gawk  grep  procps-ng  sed  tar  gettext  pciutils  psmisc  shadow  util-linux  bzip2  gzip  xz licenses  pacman  archlinux-keyring  systemd  systemd-sysvcompat  iputils  iproute2
		
6(Optional) - if you think your fstab might be broken, u might as well generate a new one from here:

	5.6 - Optional Packages:
		5.6.0 - xorg-xkill
			Allows killing frozen applications from mouse-click.

		5.6.1 - xorg-xinit
			^allows xinit and startx to be run, although not anylonger secure

		5.6.2 - Video Players:
			clementine vlc firefox vivaldi opera libreoffice mpv

		5.6.3 - Gstreamer Packages | Audio and Video Plugins:
			gstreamer gst-libav gst-plugins-bad gst-plugins-base gst-plugins-good gst-plugins-ugly gst-plugin-libde265

		5.6.4 - Xorg Screen Locker
			xlockmore
		
		5.6.5 - Luks Cryptography
			cryptsetup

		5.6.6 - Fscrypt Cryptography
			fscrypt

	6.0 - Backup current fstab:
		# cp /etc/fstab /etc/fstab\ backup\ 1

	6.1 - Generate a new fstab: 
		# genfstab -U /mnt > /mnt/etc/fstab

	6.2 - MISC:
		linux-tools

7. Arch-chroot into your broken operating system:
	7.1 - Type: 
		# arch-chroot /mnt

8. Rebuild your OS's initramfs:
	8.1(for NVIDIA DKMS Users Only) - Make sure the modules have been added to your /etc/mkinitcpio.conf:
		MODULES=(vfio vfio_iommu_type1 vfio_pci vfio_virqfd virtio-gpu nvidia nvidia_modeset nvidia_drm nvidia_uvm wd719x)
		#^ ignore vfio and wd719x if you're not using it

	8.2 - Type: mkinitcpio -P
		this will rebuild your initramfs for all kernels

9.(Optional) - Change user password:
	9.1 - Changing user password: "passwd <username>"
	9.2 - Changing root password: "passwd"

10.Exit arch-chroot, umount and reboot:
	10.1 - type in: exit
	10.2 - umount /mnt
	10.3 - reboot

	note: only reboot the system after you've made sure /mnt is umounted

	

Note:
	Steps not covered: 1- Connecting through wifi on step 4.
			   2- Grub installation after step 7. | READ TOPIC: HOW TO INSTALL GRUB / BOOTING FROM INSTALLATION MEDIA / CREATING INSTALLATION MEDIA
			   3- Reinstalling filesystems kernel modules like btrfs and zfs have not been covered here.
			   4- Re-setting things like zram, swap have not been covered.
			   ^ Please refer to archilinux wiki in order to do any of these steps, 
			   the steps followed for each of them may differ depending on your tastes or according to your system's architecture.

-----
-----
-----
-----
================================================================================
================================================================================
================================================================================
INCREASING LINUX PERFORMANCE
(VERY IMPORTANT: This is old, don't follow this)

1 - Remove any modules from /etc/modules-load.d/ if you have any,
add them to /etc/mkinitcpio.conf instead as in the following example:
	[file: /etc/mkinitcpio.conf]
		MODULES=(vfio vfio_iommu_type1 vfio_pci vfio_virqfd virtio-gpu nvidia nvidia_modeset nvidia_drm nvidia_uvm wd719x zram v4l2loopback)
	[/file]

2 - Remove udev from the list of loadable modules in /etc/mkinitcpio.conf
however, this may render some devices unusable!(not advisible)
	2.0 - Finding needed modules: 
		# lsmod | awk 'nf==3{print $1}'

	2.2 - Make sure it hasn't been loaded and unloaded before at boot: 
		#journalctl -b-0 | grep -P "udev"

	2.3 - Rebuild initramfs: 
		# mkinitcpio -P

---
ENABLING FAST COMMIT FOR EXT4 PARTITIONS
	This improves performance up to 20% for applications that rely on fsync() calls,
please read "ENABLING FAST COMMIT OPTION ON EXT4 FILESYSTEMS" topic!

================================================================================
================================================================================
================================================================================
CREATING AND MOUNTING LOOP DEVICES
	VERY IMPORTANT: do not allow 'journaling filesystems' on loop devices, it will break write order.

	Note.: If you want to: step #2 and #3 can be skipped and postponed,
	the fdisk in step #3 can be used as soon as step #1 is finished for partitioning disks,
	in this case step #5 can be replaced with: #losetup --partscan -f <filename>.

	Note2.: Once the loop device is created, partitioned and formated, it's not needed to use losetup for setting up a loop device anymore,
	you can just mount the file using mount -t ext4, however this is only possible if there's no more than just 1 partition in 
	the loop device. In this case, and only in this case loop devices will be automatically mounted.

	Note3.: Do not mount a loop device if you're going to use it on a virtual machine.

--
0. note: the letter '(A)' describes the easy way to create a loop-device.

1. Create a file to be used as loop-device, size 1GB (A)
	$ dd if=/dev/zero of=<filename> bs=1G count=5
		or
	$ dd if=/dev/zero of=<filename> bs=200k count=2

2. Attach the file to loop device (A - optional for files larger than 500k?)
	# losetup -f <filename>

3. (Optional) List all attached loop devices (A)
	$ losetup -a

4. Partition the attached loop-device: (A)
	notes: If you want a bootable device, you'll need to create an EFI Partition or similar at this step.
	if such is the case step #7.0 teaches you to format an EFI Partition for when you finish formatting it
	with an EFI Partition here in this step.

		# fdisk /dev/loopX
			or
		# fdisk <filename>
	
	^ both options requires going through step #2.
	
5. (Optional) Force kernel to re-scan for existing partition tables:
	# losetup --partscan
		or
	# losetup -P

6. (Optional) Lists all block devices partition tables:
	$ lsblk

7. Format Filesystems: (A) 
	7.0 - EFI Partition:
		# mkfs.fat -F 32 -t vfat /dev/<efi_loop_partition>
		^ Only meant for bootable devices

	7.1 - EXT4 Partition: (A)
		# mkfs.ext4 /dev/<ext4_loop_partition>
			or
		# mkfs.ext4 <filename>
		^ use this if you're following (A)

8. Mount loop device partitions:
	8.0 - Mount EFI Partition:
		# mount --mkdir -t vfat /dev/<efi_partition> /mnt/<directory_name>

	8.1 - Mount EXT4 Partition (A):
		# mount --mkdir -t ext4 /dev/<ext4_loop_partition> /mnt/<directory_name>
				or
		# mount <filename> /mnt/<directory_name>
		^ you can use this if device doesn't contains more than 1 filesystem and it is already formatted.
		^ use this if you're following (A) - this is also the FINAL STEP for (A)
		
	use 'lsblk' for listing all mounts to make sure it's mounted.

9. if you have more than one partition in a given loop-device and it's already formatted,
one can just mount the device by using the following steps for each partition to mount on the system:
	9.1 - List Partition table of a loop device file:
		$ fdisk -l <loop_device_file>
			ex.: fdisk -l loop_device.raw
			^ note: the loop device in /dev/ can also be used here, as long as the file has been attached using step 2.

	9.2 - Grab Sector Size and Multiply by Start Sector of the desirable partition table
		# mount --mkdir -o loop,offset=<Sector_Size * Start_Sector> <loop_device_file> <mount_directory>
			ex.: # mount --mkdir -o loop,offset=<Sector_Size * Start_Sector> ./file.raw /mnt/loopX_mnt
	
	9.3 - Final note: Mounting this way described in step 7 is not possible if the attached loop-device block isn't partitioned and formatted!

10 - If you used a loop device, when umounting, make sure to umount the filesystem first, - NOT SURE THIS IS A PROBLEM
only then umount the loop device; otherwise you don't have to worry and just 
using umount on the folder and the loop-device will be umounted automatically:

	# losetup -D
	^ only use this if you have no loop devices mounted on the system.
		or
	# losetup -d /dev/<loop_device>

	In case of doubt, use 'lsblk' to list block, loop and bind devices on the system
	to make sure you're umounting the right devices and finally dettaching the loop device.
---
Personal Notes | VERY IMPORTANT:
	Using loop devices is important, it allows the system administrator to create virtual partitions using files without unmounting the actual real disks for partitioning and formatting. This loop device file can then be stored in a USB Pen-Drive separately or copied as a block so that it's contents and characteristics become the contents and characteristics of the real disk. The virtual partitions can be used to private data which will be hidden from the rest of the system and that only the user knows where it is, what it does and how to mount it. Block Encryption of the entire loop-device file can also be used, passwords can be set for mounting/unmounting and reading/writing data to it without exposing it's interface onto the real disk(s) partition table.

	loop-devices also allow for data-obfuscation, gpg signatures, fscrypt directories and other important files
	the user may want to hide from unwanted 3rd parties that might use the system.

	it also helps protecting data from actual physical theft of your PC, Laptop, Android, etc.
	if you ever do this, make sure to backup the loop-device file.

VERY IMPORTANT: EFI may end up with different boot order this will mess up your system.
check your current efi boot order using efibootmgr before trying any of this!

	INSTALLING OPERATING SYSTEMS WITHOUT VIRTUAL MACHINE
Loop devices also allow installing entire operating systems without using a single virtual machine,
this not only speeds up the process of installation but it also uses less system resources!

	VERY IMPORTANT NOTE: However, grub-install will add a new boot device in ur main EFI Boot Partition through auto-detecting using efibootmgr,
	this will mess your bootup order which will need to be fixed using efibootmgr!

	genfstab also fails to generate a proper /etc/fstab file pointing to /dev/loop instead!

	EXPERIMENTING DIFFERENT FILESYSTEMS AND PARTITION TABLES WITHOUT USING A REAL DISK
Loop devices also allows testing disk partitionning and formatting on files instead of using entire disk/partition,
which can be later cloned, if working correctly, in a virtual machine environment like QEMU!

	SAFELY INSTALLING OPERATING SYSTEMS WITHOUT OVERWRITTING THE BOOTLOADER
Loop devices are neat because even when using virtual machines, one could install Windows 10 to a file attached to a loop device,
test it before writting/cloning it's contents to a real disk/partition without overwritting an already 
working and existing bootloader in the system, therefore making dual-boot installation safer.

	VERY IMPORTANT NOTE: However, grub-install will add a new boot device in ur main EFI Boot Partition through auto-detecting using efibootmgr,
	this will mess your bootup order which will need to be fixed using efibootmgr!

	genfstab also fails to generate a proper /etc/fstab file pointing to /dev/loop instead!


READ THIS CAREFULLY - THE GRUB/BOOTLOADER PROBLEM:
	Grub automatically tries to pickup not only EFI Settings but also MBR/GPT BOOT Settings from BIOS and automatically configure everything for the user
upon installation, so doing OS installation without virtual machine may lead to unexpected issues. Other bootloaders might do the same thing as grub.
because of this, only install through a virtual machine!

================================================================================
================================================================================
================================================================================
HOW TO INSTALL GRUB / BOOTING FROM INSTALLATION MEDIA / CREATING INSTALLATION MEDIA
	Grub installation on the disk will depend on whether the user is going to use UEFI or Legacy Boot.
Make sure you've read and understood all 4 steps before trying any of this:

	0 - (optional) - It may be required to partition the disks before installing grub according to notes on step #2 and #3.
		0.1 - Disk Paritioning requires the disks to be umounted from the system,
		      ^ so booting from an installation media is mandatory here.
			0.1.1 - After downloading the archlinux image, make a bootable disk using the following command:
				#dd bs=4M if=path/to/archlinux-version-x86_64.iso of=/dev/<Installation_Media_Drive> conv=fsync oflag=direct status=progress
					^ CREATES INSTALLATION MEDIA

			0.1.2 - Reboot your system using the disk

		0.2 - Backup any important data before using: fdisk /dev/<desired_system_disk> 

		0.3 - Create EFI Partition as described in #2 or Create a Legacy Partition as described in #3.

	1 - (optional) - after step #0 has been concluded, you must be rooted into the disk in which the system has been installed
	and you'll have to do the following, if step #0 was not necessary, then you can skip this:
		1.1 - #mount /dev/<installation_media> /mnt
		1.2 - #arch-chroot /mnt

	2 - UEFI INSTALLATION
		Note.: UEFI Boot Installation requires an EFI Partition can be in any position order but has to be on the first 2 TiB of the disk, 
	               ^ if the EFI Partition is the 5th partition on the disk, then it won't work.

		Note2.: Make sure you have efibootmgr installed on your system.
		Note3.: Make sure you have "UEFI Only" enabled in the BIOS, do not use any UEFI/LEGACY Modes, since efibootmgr and grub-install will fail

		2.0 - (optional) - mount the efi partition as follows: #mount /dev/<efi_disk_partition> /boot/efi
		^ hint: use lsblk and blkid to find out which efi partition(s) you own.

		2.1 - #grub-install --target=x86_64-efi --efi-directory=/boot/efi --bootloader-id=GRUB

		2.2 - (Alternative to 2.1) - #grub-install --target=x86_64-efi --efi-directory=/boot/efi --removable
		^ this installs the Bootloader as the main bootloader of the system, this is required on some specific motherboards, since the 2.1 will fail.

		2.3 - (Alternative to 2.1 and 2.2) - #grub-install --target=x86_64-efi --boot-directory=/boot/efi --efi-directory=/boot/efi --bootloader-id=GRUB
		^ this allows the bootloader to be installed in the same efi partition, since by default they're both installed separetely.



	3 - (Alternative to Step #2) LEGACY BOOT INSTALLATION
		Note: Legacy Boot installations require a BIOS BOOT Partition to be the 1st partition on the disk.
		Note2: Make sure your computer is running on either "Legacy" or "UEFI/LEGACY" mode, but not "UEFI Only"
		Note3.: This cover both installation on MBR and GPT partitions alike, difference is GPT requires a BOOT BIOS Partition to be manually created
		with a size of 3MB to 5MB as the 1st partition on the disk. MBR Partitions may require entire disk to be flagged as bootable, using fdisk.

		3.1 - # grub-install --target=i386-pc /dev/<system_disk>
			or
		      # grub-install /dev/<system_disk>
		        or
		      # grub-install --boot-directory=/boot/grub /dev/<system_disk>
		      	or
		      # grub-install --target=i386-pc --boot-directory=/boot/grub /dev/<system_disk>
		        
	4 - Final Step:
		# grub-mkconfig -o /boot/grub/grub.cfg

	5(optional) - Editing grub.cfg:
		You can add additional custom menu entries by editing /etc/grub.d/40_custom and re-generating /boot/grub/grub.cfg. Or you can create /boot/grub/custom.cfg
		and add them there. Changes to /boot/grub/custom.cfg do not require re-running grub-mkconfig, since /etc/grub.d/41_custom adds the necessary source statement
		to the generated configuration file. 

		Tip: /etc/grub.d/40_custom can be used as a template to create /etc/grub.d/nn_custom, where nn defines the precedence,
		indicating the order the script is executed. The order scripts are executed determine the placement in the GRUB boot menu. nn should be greater than 06 to
		ensure necessary scripts are executed first.

	6. If this is a clean install, make sure to install thermald: read 'INSTALL THERMALD' topic!

Further Notes:
^ /dev/<system_disk> is the disk, not a partition!
^ --boot-directory can be ommited, it defaults to /boot/grub
^ --target can be ommited only if Grub Installation is a Legacy Installation, it defaults to i386-pc
^ --efi-directory can NOT be ommited

VERY IMPORTANT:
These steps have to be done BEFORE installing grub using grub-install:
	1 - To install grub on EFI, restart your system with UEFI ONLY mode through the bios.
	2 - To install grub on GPT/LEGACY, GPT/MBR or GPT/BOOT BIOS, restart your pc on UEFI/LEGACY mode.
	3 - To install grub on MBR, restart your pc on BIOS Legacy mode only.

================================================================================
================================================================================
================================================================================
CONNECTING TO THE INTERNET
	Connection is important, otherwise archlinux packages installation would not be possible:

	IF YOU OWN AN ETHERNET CABLE:
		Requirements: dhcpcd package installed on the system.

		1.0 - Type: #systemctl start dhcpcd;

	IF YOU OWN WIRELESS:
		Requirements: iw and iwd packages installed on the system.

		2.0 - Type: 
			# systemctl start iwd 
		2.1 - Type: 
			# iwctl
		2.2 - Configure all your wifi settings
		2.3 - Type: 
			# dhcpcd

	MAKING CHANGES PERMANENT:
		3.0 - # systemctl start dhcpcd
		3.1 - # systemctl start iwd
		3.2 - # systemctl enable dhcpcd
		3.3 - # systemctl enable iwd

================================================================================
================================================================================
================================================================================
INSTALLING MAKE PACKAGES | INSTALLING BUILDPKG (write later) | NOT-ON-INDEX

MAKE FILES:
	# chmod -R o=rwx ./
	$ source build -r linux
	$ make install

*'build' is usually a Bash or TCL Script
*'install' is an actuall node in the make file called 'Makefile',
the named node should exist there!

BUILDPKG FILES:
	$ makepkg -si


================================================================================
================================================================================
================================================================================
DEPLOYING ALSA | SETTING UP AUDIO
	VERY IMPORTANT NOTE: 
		As of 2024, archlinux now uses pipewire, pipewire-alsa, pipewire-pulse, pipewire-jack-client by default
	instead of ALSA, PULSEAUDIO AND JACK. VERY IMPORTANT: Some files and configurations are still used by pipewire.
	So some instructions provided might still be valid.
	
	pavucontrol and alsamixer are still usable tools!

The Advanced Linux Sound Architecture (ALSA) provides kernel driven sound card drivers. 
It replaces the original Open Sound System (OSS). 

ALSA is a set of built-in Linux kernel modules. Therefore, manual installation is not necessary.

udev will automatically detect your hardware and select needed drivers at boot time, therefore, 
your sound should already be working. However, your sound may be initially muted.

Manual intervention for unmuting the channels is required:

	1 - Requires alsa-utils package:
		Type: 
			# alsamixer
			^ use <- -> to switch through channels
			^ use 'm' key to mute/unmute the channels back and forth
			^ use up and down keys to increase/decrease volume(mandatory)

	2 - Manually Unmuting chanells(Alternative):
		$ amixer set Master unmute
		$ amixer set Speaker unmute
		$ amixer set Headphone unmute
		
	3 - Make sure you check alternate sound devices, press F6 on Step 1.

VERY IMPORTANT: Something to note about Alsamixer, 
it can display Playback, Capture and All outputs for each Physical Audio Device you have,
microphone volumes can be controlled by 2 outputs, so make sure to increase/decrease volume
on BOTH OUTPUTS.
================================================================================
================================================================================
================================================================================
CREATING A BOOTABLE ARCHLINUX INSTALLATION MEDIA:

0 - Device must be the entire disk/media(/dev/sdx), not a partition(ex.: /dev/sdx2).
1 - Prepare device by umount /dev/sdx disk for cloning: 
2 - Use dd for cloning image into device: 
	#dd bs=4M if=archlinux-version-x86_64.iso of=/dev/sdx conv=fsync oflag=direct status=progress
4 - Done!

================================================================================
================================================================================
================================================================================
SETTING UP KEYBOARD | KEYBOARD LAYOUT CONFIGURATION ( 2025 - UPDATED, SOME PARTS MANDATORY )
	The following steps only cover brazillian layout settings:
please refer to archlinux wiki page for more instructions.

	VERY IMPORTANT: the file "/usr/share/X11/xkb/rules/base.lst" contain a list of all supported keyboard layouts, sections are listed in !variant, !model, !layout, !rules, !options

1 - Edit the following file, and add the following contents:
	[file /etc/vconsole.conf]
		KEYMAP=br-abnt2
	[/file; identation non-existant, not required]
	^ MANDATORY  Step: This allows keyboard layout to be applied early at boot.
	You can read man-pages: $ man vconsole.conf
	^ VERY IMPORTANT: You should use 'localectl' to query or instruct updates to /etc/vconsole.conf
	it also updates /etc/X11/xorg.conf.d/00-keyboard.conf which is important for xorg initialization.

2 - Create the following file and add the following content:
		For convenience, the tool localectl may be used instead of manually editing X configuration files. 
	It will save the configuration in /etc/X11/xorg.conf.d/00-keyboard.conf, this file should not be manually edited,
	because localectl will overwrite the changes on next start. Check Step #4 - MISC.

	[file /etc/X11/xorg.conf.d/00-keyboard.conf]
		# Written by systemd-localed(8), read by systemd-localed and Xorg. It's
		# probably wise not to edit this file manually. Use localectl(1) to
		# instruct systemd-localed to update it.
		Section "InputClass"
			Identifier "system-keyboard"
			MatchIsKeyboard "on"
			Option "XkbLayout" "br"
			Option "XkbModel" "abnt2"
			Option "XkbVariant" "abnt2"
		EndSection
	[/file; note: identation non-existant, not required]
	^ MAYBE MANDATORY, Still do this on 2023 if you go through troubles
	^ VERY IMPORTANT: Use 'localectl' for updating and creating the above file 00-keyboard.conf whenever possible.
	Only use this step if you're still going through troubles.

3 - (NOT MANDATORY ANYLOGNER) Add the following content to an already existent skel file:
	[file /etc/skel/.bash_profile]
		localectl set-x11-keymap br abnt2 abnt2
		setxkbmap -model abnt2 -layout br -variant abnt2
	[/file]
	^ AS OF 2023 THIS ISN'T MANDATORY ANY LONGER
	^ 2nd line may not be needed anylonger
	^ this directory allows skeleton files to be created whenever a new user is created on the system.
	this will allow any user to have br-abnt2 keyboard layout on the terminal.
	^ If you're using an american keyboard, use this:
		setxkbmap us


4 - FIXING SDDM AUTOMATICALLY CHANGING KEYBOARD LAYOUT TO AMERICAN LAYOUT AT BOOT:
	VERY IMPORTANT: SDDM loads the keyboard layout specified in /etc/X11/xorg.conf.d/00-keyboard.conf.
You can generate this configuration file by localectl set-x11-keymap command. Check Step #5.1.1 after reading Step #2

An alternative way of setting the keyboard layout that will only set it in SDDM and not subsequent sessions(For multi-user purposes, if ever needed) 
is to invoke a setxkbmap command in the startup script of SDDM, located at /usr/share/sddm/scripts/Xsetup

	Just add the following content into the file, WITHOUT MODIFYING THE FILE IF IT EXISTS!:
		[file /usr/share/sddm/scripts/Xsetup ]
			localectl set-x11-keymap br abnt2 abnt2
			setxkbmap -model abnt2 -layout br -variant abnt2
		[/file]
		^ 2nd line is optional, not needed anylonger
		^ AS OF 2023 1st line is mandatory
		^ If you're using an american keyboard, use this:
			localectl set-x11-keymap us
			setxkbmap us

	VERY IMPORTANT:
		4.1 - localectl has to be executed as administrator, if the user decides to run it manually, 
		it will generate the /etc/X11/xorg.conf.d/00-keyboard.conf and /etc/vconsole.conf file; 
		the changes made to the first file will only take effect when starting a xorg server; the second file vconsole.conf will boot
		the system with the same given keyboard layout.

		4.2 - setxkbmap will change keyboard layout immediately, but it doesn`t make changes persistent. 

	4.1 - Add this to /etc/X11/xinit/xinitrc if you're still find problems:
		setxkbmap -model abnt2 -layout br -variant abnt2
		^ Mandatory: add this line to /etc/X11/xinit/xinitrc as the final line

5 - MISC:
	5.1 - Checking your keyboard layout:
		ex.: $setxkbmap -print -verbose 10

		5.1.1 - About localectl
			$localectl [--no-convert] set-x11-keymap layout [model [variant [options]]]
				^ --no-convert prevents system from trying to match closest matching console keymap 
				and applied to the console configuration in vconsole.conf.

		5.1.2 - Lists available Keymaps on the system
			$localectl list-keymaps
				or
			$find /usr/share/kbd/keymaps/ -type f -name "*search_term*"
				or
			$localectl list-keymaps | less
				or
			$localectl list-keymaps | grep -P "search_term"


			^ System Locale not important


================================================================================
================================================================================
================================================================================
MOUNTING DISKS, EXTERNAL DISKS, USB PEN DRIVES
	If you have an empty Usb Device, you may want to start from step #0
If you have a known usb device, you can start from step #3 .

Mounting and using storage devices are split into 2 categories,
	0 - Unpartitioned Disks
	1 - Partitioned Disks
	
0 - Connect the storage device and check if the device is partitioned:
	# fdisk -l
	^ note that partition list should appear at this point as /dev/sdX1, /dev/sdX2, etc.

1 - (Optional) Partitioning Disks(only mandatory if disks are unpartitioned)
	1.1 - Create partitions using one of the many options(recommended: fdisk):
		# fdisk
		  or
		# cfdisk
		  or
		# parted
	
	1.2 - Pick partition table: GPT or MBR
	1.3 - Set partitions up on the disk (EXT4, NTFS, XFS, etc..)
	1.4 - Write changes to disk
	1.5 - DONE, Follow with Step #2

2 - (Optional) Formatting Disks(only mandatory if disks are unformatted)

	2.1 - (Optional) EXT4 Filesystem(Linux Default)
		# mkfs.ext4 /dev/sdX2
		^ Formats partition /dev/sdX2 into EXT4 Filesystem
	
	2.2 - (Optional) NTFS Filesystem(Windows Default)
		# mkfs.ntfs /dev/sdX2
	
	2.3 - (Optional) XFS Filesystem(Readable only in some Linux Distributions)
		# mkfs.xfs /dev/sdX2

	2.4 - (Optional) FAT Filesystem(Readable on several systems)
		# mkfs.fat -F 32 /dev/sdX2

	2.5 - (Optional) VFAT Filesystem(Readable on most modern systems)
		# mkfs.vfat -F 32 /dev/sdX2
		^ vfat supports long filenames and directories

3 - MOUNT DEVICE PARTITION(s)
	3.1 - List new partitions
		$ lsblk

	3.2 - Mount Partition
		# mount --mkdir -t <type> /dev/sdX2 /mnt/<mount_dir>	
			or
		# mount --mkdir /dev/sdX2 /mnt/<mount_dir>

	ex.: #mount --mkdir -t ext4 /dev/sdX2 /mnt/sdx2

4 - (Optional) - SETTING DEVICE FOR AUTO-MOUNTING WITH THE SYSTEM
	4.1 - Backup your /etc/fstab file
		# cp /etc/fstab /etc/fstab-backup-1

	4.2 - Generate a new fstab file:
		# genfstab -U / > /etc/fstab
	
	4.3 - Compare new file with backup file
		# diff /etc/fstab /etc/fstab-backup-1

		4.3.1 - Make sure no disks have disappeared in the new generated file
		you can add any disappearing disks back to /etc/fstab manually by hand.

	4.4 - Mount Device Partition For the First time:
		# mount /dev/sdX2
	
	^ the next time you boot, this disk will be auto-mounted in the same place as the one created in step #3.2

--------------------------------------
MOUNTING HARD DISKS
	You may need to install one or more of these tools if you don't have them available

	1 - Use either 'lsblk', 'fdisk', 'df' or 'blkid' to list all available devices:
		# lsblk
		   or
		# fdisk -l
		   or
		# df -ha
		   or
		# blkid
	
		Note: only 'lsblk' and 'df' will display whether disks are mounted or not on the system.
		On the mount column, mounted disks will be shown with a slash character '/' followed by 
		mount point on the system.

	2 - Using command above, your Hard Disks should be listed as /dev/sdX, where 'X' will be a letter from A to Z
	depending on how many Hard Disk devices you have connected, example:
		2.1 - Disk 1 will show up as:
			/dev/sda

		2.2 - Disk 2 will show up as:
			/dev/sdb

		2.3 - Disk 3 will show up as:
			/dev/sdc

		2.4 - Disk 4 will show up as:
			/dev/sdd
	
	3 - Once you know the disk you're looking for, mount it with:
		# mount -t auto --mkdir /dev/sdX /mnt/disk\ X
	
	4 - (OPTIONAL) Adding disks to system auto-mount is as easy as this:
	genfstab generates a new fstab configuration file, containning all of the current mounted devices in the system:
		# genfstab -U / > /etc/fstab
		
	5 - Once you're finished using the hard disk, you can finally umount it:
		# umount /mnt/disk\ X

		Very Important: Never umount the root partition, example:
			# umount /

		never do this above. ever.


--------------------------------------
MOUNTING .CUE AND .BIN FILES USING CDEMU
	This method emulates a cdrom drive:
		NOTE: (Optional) parts here are actually mandatory if you don't have these utils installed.

	1 (Optional) - Install cdemu-client package
		# pacman -S cdemu-client

	2 (Optional) - Install vhba-module-dkms which is a kernel module that emulates scsi drivers:
		# pacman -S vhba-module-dkms

	3 - Enable all required kernel modules:
		Note: It's possible to add those modules for system initializion on /etc/mkinitcpio.conf;
	however, this is optional. If you don't have them in your mkinitcpio.conf file, you'll have to use
	modprobe every single time you need them:

		# modprobe -a sg sr_mod vhba

	4 - Emulate cd driver:
		$ cdemu load 0 ./disk_image.cue

	5 - List available devices:
		New drives should be listed at the bottom:
			$ lsblk

	6 - Mount cd-rom
		# mount --mkdir /dev/sr0 /mnt/cdrom0
	
	7 - Umount cd-rom when finished and unload disk when finished:
		# umount /mnt/cdrom0 && cdemu unload 0

You can check more on : 
	https://wiki.archlinux.org/title/CDemu

-------------------------------------
MOUNTING .CUE AND .BIN FILES USING FUSEISO AND FUSERMOUNT
	This method doesn't emulates a cdrom drive, some program/game installations may fail:

	1 - Mounting with fuseiso
		$ fuseiso -p ~/disk_image.bin

	2 - Umounting with fusermount
		$ fusermount -u ~/cdrom0


-------------------------------------
MOUNTING .ISO FILES WITH MOUNT
	The 'mount' tool can easily mount any ISO-9660 image files:

		# mount --mkdir -t auto ./iso\ image.iso  /mnt/cdrom0

================================================================================
================================================================================
================================================================================
ENABLING FAST COMMIT OPTION ON EXT4 FILESYSTEMS || PERFORMANCE
	This should make performance better for most applications that rely on fsync()

	1. Check if partition has fast_commit enabled:
		#dumpe2fs -h /dev/sdX2 | less	

	2. Enable fast_commit using tune2fs:
		#tune2fs -O fast_commit /dev/sdX2

https://lwn.net/Articles/842385/ - ABOUT FSYNC AND EXT4 FAST_COMMIT
	https://wiki.archlinux.org/title/Ext4#Using_file-based_encryption - Enabling EXT4 Fast Commit 

================================================================================
================================================================================
================================================================================
SOMETHING IMPORTANT ABOUT JOURNALING
	Journaling is usually done on modern filesystem types by default,
the journaling happens from time to time, and the journaling system usually is
a bottleneck to system resources specially EXT4 filesystems, disabling journaling 
on non-important disks/partitions will drastically increase performance.

Furtherover, the more disks or partitions you have, the more hardware resources
will be used to keep journaling active on those disks.

================================================================================
================================================================================
================================================================================
ABOUT EXT4 FILESYSTEM FEATURES
	Source: https://ext4.wiki.kernel.org/index.php/Ext4_Howto
	Date: 11 February 2019, at 23:30

---
EXT4 EXTENTS
	Traditional, Unix-derived, file systems, like Ext3, use a indirect block mapping scheme to keep track of each block used for the blocks corresponding to the data of a file. This is inefficient for large files, especially during large file delete and truncate operations, because the mapping keeps an entry for every single block, and big files have many blocks -> huge mappings, slow to handle. Modern file systems use a different approach called "extents". An extent is basically a bunch of contiguous physical blocks. It basically says "The data is in the next n blocks". For example, a 100 MiB file can be allocated into a single extent of that size, instead of needing to create the indirect mapping for 25600 blocks (4 KiB per block). Huge files are split in several extents. Extents improve the performance and also help to reduce the fragmentation, since an extent encourages continuous layouts on the disk.

	Extents must be enabled as a filesystem feature:
		note that /dev/sdX1 is the partition, not the Disk!

		1 - # umount /mnt/<mounted_disk_or_partition>
		2 - # tune2fs -O extent /dev/sdX1
		3 - # mkfs.ext4 -Dfy /dev/sdX1


---
EXT4 MULTIBLOCK ALLOCATION
	When Ext3 needs to write new data to the disk, there's a block allocator that decides which free blocks will be used to write the data. But the Ext3 block allocator only allocates one block (4KiB) at a time. That means that if the system needs to write the 100 MiB data mentioned in the previous point, it will need to call the block allocator 25600 times (and it was just 100 MiB!). Not only this is inefficient, it doesn't allow the block allocator to optimize the allocation policy because it doesn't know how many total data is being allocated, it only knows about a single block. Ext4 uses a "multiblock allocator" (mballoc) which allocates many blocks in a single call, instead of a single block per call, avoiding a lot of overhead. This improves the performance, and it's particularly useful with delayed allocation and extents. This feature doesn't affect the disk format. Also, note that the Ext4 block/inode allocator has other improvements, described in details in this paper.


	In other words,	mballoc is a mechanism to allow many blocks to be allocated to a file in a single operation, in order to dramatically reduce the amount of CPU usage searching for many free blocks in the filesystem. Also, because many file blocks are allocated at the same time, a much better decision can be made to find a chunk of free space where all of the blocks will fit.

THE MBALLOC CODE IS ACTIVE either when using the O_DIRECT flag for writes, OR IF THE DELAYED ALLOCATION (DELALLOC) FEATURE IS BEING USED. This allows the file to have many dirty blocks submitted for writes at the same time, unlike the existing kernel mechanism of submitting each block to the filesystem separately for allocation.

---
DELAYED ALLOCATION | SPECIAL TOPIC
	Delayed allocation is enabled by Default in EXT4 filesystem, however it's only supported when partition/disk is mounted 
as data=ordered or data=writeback, and not when it's mounted as data=journal. Default value for data is 'ordered'.

Data blocks that HAVE NOT BEEN COMMITTED to disk are called “DIRTY DATA BLOCKS”.

ext4 has 3 MODES for writing files (you can set the mode while mounting the disk):

	1. Data Ordered: Ext4 commits the metadata blocks the journal. After that, ext4 commits the DIRTY BLOCKS to the disk.
		(Delayed allocation supported)

	2. Data Journal: Ext4 commits the DIRTY BLOCKS and the corresponding metadata blocks 
	to the journal before commiting them to the disk.
		(Delayed allocation not supported).

	3. Data Writeback: It is similar to the data=ordered mode. The difference is that ext4 
	may commit the DIRTY BLOCKS to the disk and after that, ext4 may commit the corresponding 
	metadata blocks to the journal.
		(Delayed Allocation Supported)

	Delayed allocation is a performance feature (it doesn't change the disk format) found in a few modern filesystems such as XFS, ZFS, btrfs or Reiser 4, and it consists in delaying the allocation of blocks as much as possible, contrary to what traditionally filesystems (such as Ext3, reiser3, etc) do: allocate the blocks as soon as possible. For example, if a process write()s, the filesystem code will allocate immediately the blocks where the data will be placed - even if the data is not being written right now to the disk and it's going to be kept in the cache for some time. This approach has disadvantages. For example when a process is writing continually to a file that grows, successive write()s allocate blocks for the data, but they don't know if the file will keep growing. Delayed allocation, on the other hand, does not allocate the blocks immediately when the process write()s, rather, it delays the allocation of the blocks while the file is kept in cache, until it is really going to be written to the disk. This gives the block allocator the opportunity to optimize the allocation in situations where the old system couldn't. Delayed allocation plays very nicely with the two previous features mentioned, extents and multiblock allocation, because in many workloads when the file is written finally to the disk it will be allocated in extents whose block allocation is done with the mballoc allocator. The performance is much better, and the fragmentation is much improved in some workloads.

	This is how to test if your disk support delayed allocation:
		1  - $ echo "test" > test_file.a
		2  - $ filefrag -v test_file.a
		3 - If delayed allocation is supported, the following line should appear:
			unknown_loc,delalloc,eof
		4(optional) - Do step #1 again and this time type: 
				$sync;
			4.1 - Do step #2 and #3 again and verify that this time the output will be:
				last, eof.
			4.2  - Conclusion: This means delayed allocation has concluded commiting the dirty blocks to disk.
	
	VERIFYING SIZE OF DIRTY BLOCKS
		# egrep -ih 'Dirty(:| )' /proc/vmstat /proc/meminfo

	Dirty: Total memory size waiting to be written to disk.
	nr_dirty: Number of dirty blocks in memory waiting to be written to disk
	pagesize: Contiguous block space of memory in bytes.

	If you need to flush the dirty blocks to the disk in your system program you must call either sys_sync, sys_fsync or 
	sys_fdatasync calls. - VERY IMPORTANT

	HOW OFTEN DIRTY BLOCKS ARE GETTING FLUSHED TO THE DISK
		1  - Type:
			# sysctl -a | grep dirty_expire_centisecs
	
	HOW TO CHANGE DIRTY BLOCK EXPIRE TIME
		1 - Type:
			# sysctl -w vm.dirty_expire_centisecs=1000
			^ sets expire time to 10 seconds.

	DELAYED ALLOCATION CAN LEAD TO POTENTIAL DATA LOSS | SPECIAL TOPIC
		The typical scenario in which this might occur is a program replacing the contents of a file without 
	forcing a write to the disk with fsync. There are two common ways of replacing the contents of a file on Unix systems:
		fd=open("file", O_TRUNC); write(fd, data); close(fd);

	In this case, an existing file is truncated at the time of open (due to O_TRUNC flag), then new data is written out. Since the write can take some time, there is an opportunity of losing contents even with ext3, but usually very small. However, because ext4 can delay allocating file data for a long time, this opportunity is much greater.

There are several problems with this approach:

	1 - If the write does not succeed (which may be due to error conditions in the writing program, or due to external conditions such as a full disk), then both the original version and the new version of the file will be lost, and the file may be corrupted because only a part of it has been written.
	2 - If other processes access the file while it is being written, they see a corrupted version.
	3 - If other processes have the file open and do not expect its contents to change, those processes may crash. One notable example is a shared library file which is mapped into running programs.

	Because of these issues, often the following idiom is preferred over the above one:
		fd=open("file.new"); write(fd, data); close(fd); rename("file.new", "file");

	A new temporary file ("file.new") is created, which initially contains the new contents. Then the new file is renamed over the old one. Replacing files by the "rename" call is guaranteed to be atomic by POSIX standards – i.e. either the old file remains, or it's overwritten with the new one. Because the ext3 default "ordered" journaling mode guarantees file data is written out on disk before metadata, this technique guarantees that either the old or the new file contents will persist on disk. ext4's delayed allocation breaks this expectation, because the file write can be delayed for a long time, and the rename is usually carried out before new file contents reach the disk.

	WHAT ARE THE MAIN ADVANTAGES?
		Delayed allocation works by deferring the mapping of newly-written file data blocks to disk blocks in the filesystem until writeback time. This helps in several ways:

	1 - Reduced filesystem fragmentation, because all (or a large number) of blocks for a single file can be allocated at the same time. Knowing the total number of blocks in each file allows the block allocator (mballoc) to find a suitable chunk of free space for each file instead of picking a free chunk that is too large or too small.
	2 - Reduced CPU cycles spent in block allocation, because the block allocator can allocate many or all of the blocks for the file at one time, instead of doing searching and locking for each block in the file as it is written without delayed allocation.
	3 - It may avoid the need for disk updates for metadata creation for short-lived files, which in turn reduces fragmentation.

This is the default allocation mode for ext4.

	VERY IMPORTANT NOTE | SPECIAL TOPIC
		Because of delayed allocation in EXT4 Filesystems, compiling/saving code from any programming/scripting language,
	may require Dirty Blocks to be commited to disk, to ensure this is done, always type: 
		# sync
		  or
		# sync <filename>

	Any other type of data that needs to be readily available will need it's dirty blocks commited to disk before being 
	used/read/executed anew. not doing so will result in the old file/code being executed instead.

---
FS-VERITY
	fs-verity is something supported by ext4 and I really like it. Let’s say you are running a SIEM application and you want to sign the log files and make log files read-only. fs-verity may help you in this scenario.

fs-verity calculates a hash value based on the file's contents and makes the file a “read-only” file and can’t be disabled.

fs-verity is not enabled by default. You should enable it while creating the filesystem:
	1 - Enables verity for a file:
		# mkfs.ext4 -O verity /dev/sdb
	2 - Enable fs-verity for a given file:
		# fsverity enable a.txt
	3 - Displays hashset of the file:
		# fsverity digest a.txt

fs-verity allows files to be read-only and not changed by any user, including root.

---
DISABLING NOSUID, NODEV FROM MOUNTED DEVICES
	nosuid and nodev are security mount options, these usually make the system slow,
you can disable it for a gaming disk device to make it faster.

================================================================================
================================================================================
================================================================================
PLAYING VIDEOS ON THE TERMINAL / VIDEOS COMMAND LINE: 
	MPV player has a mode for playing videos using ascii and unicode with colors.

1 - Playing videos using UNICODE:
	$ mpv --vo=tct <video>

2 - Playing videos using ASCII(requires mpv-caca package from AUR):
	$ mpv --vo=caca <video>



================================================================================
================================================================================
================================================================================
FETCHING EVERY SINGLE FRAME FROM VIDEO:
	MPV Player allows to fetch every single frame available from a video and
save it as an image:

	Note: Make sure you have more than enough Hard Disk Space!

1 - Fetching all images from a video:
	$ mpv --vo=image <video> <destination_directory>

================================================================================
================================================================================
================================================================================
FIXING RESOLUTION IN MPV
	You may want to read the ~/.config/mpv/mpv.conf file in this document, before continuing here:
	WARNING: This section below requires updating.

	Run mpv with the following argument:
		$ mpv --autofit=<width_pixels> <video_file>
			ex.: $ mpv --autofit=1000 <video_file>

		$ mpv --autofit <<width>x<height>> <video_file>
			ex.: $ mpv --autofit=640x480 <video_file>

		$ mpv --autofit <<width>x<height>> <video_file>
			ex.: $ mpv --autofit=50%x50% <video_file>
	
	Alternate Options:
		mpv --autofit-larger=<width>x<height> <video_file>
		^ Works pretty much the same way, but only sets video to the given width height if the actual video resolution is bigger than <width>x<height>. Videos whose resolution are smaller than the passed width and height will remain unchanged.

Write Later about:
	mpv --video-aspect-override
================================================================================
================================================================================
================================================================================
MORE MPV OPTIONS
TAGS:
{VERY GOOD}

COMMAND:
	ENABLE_VKBASALT=1 mpv --gpu-api=vulkan --target-prim=s-gamut --autofit-larger=100%x100% --autofit-smaller=100%x100% --fs <video/audio>

	[file: /bin/vulkan-vulkan]
		ENABLE_VKBASALT=1 mpv --gpu-api=vulkan --target-prim=s-gamut --keepaspect=no --autofit-larger=100%x100% --autofit-smaller=100%x100% "$@"
	[/file]

HOTKEYS:
q			#Quits player
< and >			#Plays previous and next audio/video in the queue
+ and -			#Increases/Decreases Volume
p			#Pauses
, and .			#Previous and next video frame
[ and ]			#Increases/Decreases speed
backspace		#Normalizes speed

AUDIO:
--ad=<decoder>		#Allows setting up audio decoder
--ad=help		#Lists available decoders

--ao=<driver>		#Sets up audio driver
			^ ex.: pipewire, pulse, alsa, jack, pcm, openal, null
--ao=help		#Lists available drivers

--audio-device=<device>		#Sets an available audio device
--audio-device=help		#Lists available audio devices
				^ Allows selecting a microphone as audio output device

--audio-demuxer=<demuxer>	#Sets up a demuxer for  audio demuxing
--audio-demuxer=help		#Lists available demuxers

--audio-format=<format>		#Sets up audio format
--audio-format=help		#Lists available audio formats
				^ example: u8 s16 s32 s64 float double u8p s16p s32p s64p floatp 
				^ doublep spdif-aac spdif-ac3 spdif-dts spdif-dtshd spdif-eac3 spdif-mp3 spdif-truehd

--gapless-audio=<no|yes|weak>	#When playing multiple audio files, 
				^ 'yes' ensures that the next song will  play as fast as possible.
				^ Same settings will be used for playing throughout all songs.
				^ 'weak' is the default, it ensures the audio files under the same format will be gapless executed,
				^ audio files under different formats will be re-opened, therefore not gapless.
				^ 'no' completely disables gapless audio.

--audio-samplerate=<samplerate>	#Sets an audio sample rate
				^ --audio-samplerate=40000
				^ plays audio at 40Khz


--audio-pitch-correction=<yes|no>	#Sets up pitch correction

--audio-files=<files>			#Plays Audio Files while watching a video
					^ Syncs audio with video.

--audio-file=<file>			#Appends Audio File while watching a video
					example: 
						$mpv --audio-file=audio1.mp3 --audio-file=audio2.mp3 ./video.mp4
					^ this will play the video using 2 different audio tracks.
					^ when a video is used instead, then both video and audio will be appended as a new A/V track.

--af=..					#Sets up a chain of audio filters, check --vf for reference on syntax

--av

VIDEO:
--target-prim=<primaries>		#Selects display primary colorspace.	- VERY GOOD
					^ List of good ones:
						ebu3213			#Similar to apple, better ***
						apple			#Makes colors look better
						bt.601-525		#Similar to apple
						bt.709			#Good replacement if 'apple' not available

						aces-ap1		#Makes video looks like very old
						s-gamut			#Same level or better as aces-ap1, looks better. ***
						bt.2020			#Similar to aces-ap1, maybe better.
						prophoto		#Modernized version of aces-ap1
						dci-p3			#Modernized aces-ap1, very good.

--target-prim=help			#Lists available colorspaces
					
--sharpen=<number>			#Sharpens images upon setting up a positive number, Unsharpen otherwise. - VERY GOOD
					^ Only works for --vo=gpu
					^ If your GPU is powerful enough, consider alternatives like the ewa_lanczossharp scale filter

--gpu-api=<api>				#Sets one API for rendering - VERY GOOD
					^ Unlike --gpu-context, this auto-selects the context for the user.
					example:
						ENABLE_VKBASALT=1 mpv --gpu-api=vulkan <Video Filename>

--gpu-api=help				#Lists all available APIs
						auto (autodetect)
						opengl (wayland)
						opengl (x11egl)
						opengl (x11)
						opengl (drm)
						vulkan (waylandvk)
						vulkan (x11vk)
						vulkan (displayvk)

--gpu-context=<gpu_context>		#Selects GPU Context - VERY GOOD, SAME AS --gpu-api ABOVE
					^ Requires you to select the right Context.
					#example.: 
						ENABLE_VKBASALT=1 mpv --gpu-context=x11vk <Video Filename>

--gpu-context=help			#Lists all available GPU Contexts
						auto (autodetect)
						wayland (opengl)
						x11egl (opengl)
						x11 (opengl)
						drm (opengl)
						waylandvk (vulkan)
						x11vk (vulkan)
						displayvk (vulkan


--vo=<video_output>			#Selects a video output for use
--vo=help				#Lists all available video outputs
						gpu              Shader-based GPU Renderer
						gpu-next         Video output based on libplacebo
						vdpau            VDPAU with X11
						wlshm            Wayland SHM video output (software scaling)
						xv               X11/Xv
						dmabuf-wayland   Wayland dmabuf video output
						vaapi            VA API with X11
						x11              X11 (software scaling)
						libmpv           render API for libmpv
						null             Null video output
						image            Write video frames to image files
						tct              true-color terminals
						drm              Direct Rendering Manager (software scaling)
						sixel            terminal graphics using sixels
						kitty            Kitty terminal graphics protocol

--vulkan-display-display=<number>	#Sets a display number for vulkan
--vulkan-display-display=help		#Lists all displays, modes and planes

--image-lut=<lut_image>			#Sets LUT


================================================================================
================================================================================
================================================================================
================================================================================
================================================================================
================================================================================
UNDERSTANDING WEECHAT
	Weechat is a commandline IRC Client.

USEFUL SERVER COMMANDS:
/server list								#Lists all servers available(connected & disconnected)
/server listful								#Provides full-info on all servers
/server add <alias> <server_address>[/<server-port>] [-ssl]		#Adds a given IRC server for the local user
/server connect <alias>							#Connects to a previously created server
/list									#Lists all available Channels in a given Server
									^ note: must be typen on a server buffer / channel
/list -re A*								#Lists all channels that start with letter 'A'

CHANNEL COMMANDS:
/ban
/ban [#channel_name]

================================================================================
================================================================================
================================================================================
INSTALLING YAY / AUR REPOSITORY

pacman -S --needed git base-devel
git clone https://aur.archlinux.org/yay-bin.git
cd yay-bin
makepkg -si

	FIRST YAY USE:
		Step 1 - Generates a development package database for *-git packages that were installed without yay. This command should only be run once.
		Step 2 - Will check for development package updates
		Step 3 - Makes development package updates permanently enabled (yay and yay -Syu will then always check dev packages)
			yay -Y --gendb;
			yay -Syu --devel;
			yay -Y --devel --save;
================================================================================
================================================================================
================================================================================
--------------------------------------------------
CONTENT TO READ LATER:
EXT4 Free Extents | "extents"  must be a filesistem feature in order to manage it
crontab
https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout#Blocks - EXT4 DISK LAYOUT
https://wiki.rizon.net/index.php?title=TLS - IRC TLS Connections
https://makefiletutorial.com/#multiple-targets
https://ext4.wiki.kernel.org/index.php/Ext4_Howto#EXT4_features
https://ext4.wiki.kernel.org/index.php/Frequently_Asked_Questions#Can_I_undelete_files_in_Ext4.3F
https://unix.stackexchange.com/questions/533675/meaning-of-nodev-in-mount-options-on-linux - What nodev does protect u from.
What's mknod?
https://security.archlinux.org/package/linux-zen - Security Issues on linux kernels
https://www.kernel.org/doc/html/latest/driver-api/thermal/intel_powerclamp.html - Kernel Documentation on how to use powerclamp as a cooling device and how it improves performance by letting CPU Iddle overtime. Intel Powerclamp is mostly used by thermald for preventing CPU overheat.
--------------------------------------------------------------------------------
--------------------------------------------------
INCREASING SSD LIFETIME
	1 - Do not use swap partitions
		If you really need to use swap, use a swap file instead of a partition.
	Swap files are more flexible, it can be turned on and off. Avoid using SWAP if you have enough Ram Memory, since it's not needed.

	2 - Disable Journaling(optional)
		note: If your filesystem has a journal, read the filesystem manual before disabling it.
	
	2.1 - If your PC is backed up by a Power blackout safe PSU, you can completely disable the Journaling,
	if you can manage to turn off your computer safely and without crashing, then it's not needed.

	2.2 - Use an external journaling on a secondary hard disk; this will prevent write/reads to SSD.
	2.3 - Always try to UPDATE YOUR SSD'S FIRMWARE. If you don’t update the firmware, you miss out on improvements 
	from the manufacturer that make it more efficient, read/write less and handle data better. To update your drive’s 
	firmware, it’s best to refer to the manual that comes with your SSD. Alternatively, search for the manufacturer’s 
	website, or look through your Linux distribution’s Wiki for instructions.

	2.4 - Enable Filesystem PERIODIC TRIMMING; Trimming is important for discarding unused blocks, 
	because due to the nature of Flash and NAND Memory.
		2.4.0 - Make sure you have TRIM Support before enabling it 
		or it'll cause disc DMG(VERY IMPORTANT):
			2.4.0.0 - Type:
				$lsblk --discard

			2.4.0.1 - Verify zeroed DISC-GRAN and DISC-MAX columns,
			they indicate TRIM SUPPORT.
			
			2.4.0.2 - Alternatively(MANDATORY), install hdparm and run:
				2.4.0.2.0 - Type in:
					#hdparm -I /dev/sdb | grep TRIM

		2.4.1 - Install fstrim through this package installation: util-linux
		2.4.2 - Run fstrim manually(OPTIONAL): #fstrim /dev/sdx
		2.4.3 - Enable fstream as a service(MANDATORY):
				2.4.3.1 - #systemctl enable fstrim.service
				2.4.3.2 - #systemctl enable fstrim.timer
	
	2.5 - Enable Filesystem CONTINOUS TRIMMING(Alternative to 2.4, don't use both at the same time):
		(SETTING UP TRIM / DISCARD OPTION)
		This is the same feature discussed in 2.4, This will cause the filesystem driver to
	use the trim/discard feature of some storage devices(such as SSD's and thin provisioned drivers 
	available in some enterprise storage arrays) to inform the storage device that blocks belonging 
	to deleted files can be reused for other purposes. This is preferred over 2.4's method.

	To enable this option on EXT4 Filesystems, follow these steps:

		2.5.0 - Make sure your Disk has Discard/TRIM Support
			2.5.1 - Follow Instructions on 2.4.0 topic here
			do not enable TRIM/DISCARD if it has no support!

		2.5.1 - Use tune2fs.ext4:
			#tune2fs -o discard /dev/sdx2
			^ Do not mistake this option for #fsck4.ext4 -E discard
			^ they behave differently! fsck4.ext4 may lead to data loss upon call to fsck -Dfy
			or similar commands!

		2.5.2 - Run fsck.ext4:
			#fsck.ext4 -Dfy /dev/sdx
		
	2.6 - Setting up a Disk Scheduler Options:	
		2.6 - List available disk scheduling options:
			$cat /sys/block/sdX/queue/scheduler
		
			output produced: [mq-deadline] kyber bfq none
			^ [mq-deadline] is the active disk scheduler 

		2.7 - Understanding Disk Schedulers:

			2.7.1 - noop: is often the best choice for memory-backed block devices 
			(e.g. ramdisks) and other non-rotational media (flash) where trying to 
			reschedule I/O is a waste of resources. Even if you don't have noop available,
			you can enable it just as fine.

			2.7.2 - deadline: is a lightweight scheduler which tries to put a 
			hard limit on latency 

			2.7.3 - cfq: tries to maintain system-wide fairness of I/O bandwidth

		2.8 - Make sure the disk(s) will benefit from NOOP Scheduler:
			2.8.1 - Type in:
				$for f in /sys/block/sd?/queue/rotational; do printf "$f "; cat $f; done

		2.9 - Permanently Setting noop scheduler for non-rotating disk:
			[file: /etc/udev/rules.d/60-ssd-scheduler.rules]
				ACTION=="add|change", KERNEL=="sdb", ATTR{queue/rotational}=="0", ATTR{queue/scheduler}="noop"
			[/file]

				or for multiple disks:

			[file: /etc/udev/rules.d/60-ssd-scheduler.rules]
				ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/rotational}=="0", ATTR{queue/scheduler}="noop"
			[/file]

		NOTES: READ MORE ON I/O SCHEDULERS / DISK SCHEDULERS
More Info ON:
	https://wiki.archlinux.org/title/Solid_state_drive
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
DISK SCHEDULERS / I/O SCHEDULERS

Source: https://wiki.ubuntu.com/Kernel/Reference/IOSchedulers

LINUX I/O SCHEDULERS
	I/O schedulers attempt to improve throughput by reordering request access into a linear order based on the logical addresses of the data and trying to group these together. While this may increase overall throughput it may lead to some I/O requests waiting for too long, causing latency issues. I/O schedulers attempt to balance the need for high throughput while trying to fairly share I/O requests amongst processes.

Different approaches have been taken for various I/O schedulers and each has their own set of strengths and weaknesses and the general rule is that there is no perfect default I/O scheduler for all the range of I/O demands a system may experience. 

MULTIQUEUE I/O SCHEDULERS

	The following I/O schedulers are designed for multiqueue devices. These map I/O requests to multiple queues and these are handled by kernel threads that are distributed across multiple CPUs. 

	1 - BFQ (BUDGET FAIR QUEUING) (MULTIQUEUE)
		Designed to provide good interactive response, especially for slower I/O devices. This is a complex I/O scheduler and has a relatively high per-operation overhead so it is not ideal for devices with slow CPUs or high throughput I/O devices. Fair sharing is based on the number of sectors requested and heuristics rather than a time slice. Desktop users may like to experiment with this I/O scheduler as it can be advantageous when loading large applications. 

	2 - KYBER (MULTIQUEUE)
		Designed for fast multi-queue devices and is relatively simple. Has two request queues:

		    Synchronous requests (e.g. blocked reads)
		    Asynchronous requests (e.g. writes) 

		There are strict limits on the number of request operations sent to the queues. In theory this limits the time waiting for requests to be dispatched, and hence should provide quick completion time for requests that are high priority. 

	3 - NONE (MULTIQUEUE)
		The multi-queue no-op I/O scheduler. Does no reordering of requests, 
		minimal overhead. Ideal for fast random I/O devices such as NVME. 

	4 - MQ-DEADLINE (MULTIQUEUE)
		This is an adaption of the deadline(non-multiple queue) I/O scheduler 
		but designed for Multiqueue devices. A good all-rounder with fairly low CPU overhead. 
	
NON-MULTIQUEUE I/O SCHEDULERS

	1 - DEADLINE
	This fixes starvation issues seen in other schedulers. It uses 3 queues for I/O requests:

	    1.1 - Sorted
	    1.2 - Read FIFO - read requests stored chronologically
	    1.3 - Write FIFO - write requests stored chronologically 

	Requests are issued from the sorted queue inless a read from the head of a read or write 
	FIFO expires. Read requests are preferred over write requests. Read requests have a 500ms 
	expiration time, write requests have a 5s expiration time. 

	2 - CFQ (COMPLETELY FAIR QUEUEING)

		    2.1 - Per-process sorted queues for synchronous I/O requests.
		    2.2 - Fewer queues for asynchronous I/O requests.
		    2.3 - Priorities from ionice are taken into account. 

	Each queue is allocated a time slice for fair queuing. There may be wasteful 
	idle time if a time slice quantum has not expired. 

	3 - NOOP (NO-OPERATION)
		Performs merging of I/O requests but no sorting. Good for random access devices 
		(flash, ramdisk, etc) and for devices that sort I/O requests such as advanced storage 
		controllers

BEST I/O SCHEDULER TO USE
	SSD AND NVME DRIVES
		It is worth noting that there is little difference in throughput between the mq-deadline/none/bfq I/O schedulers when using fast multi-queue SSD configurations or fast NVME devices. In these cases it may be preferable to use the 'none' I/O scheduler to reduce CPU overhead. 

HDD

Avoid using the none/noop I/O schedulers for a HDD as sorting requests on block addresses reduce the seek time latencies and neither of these I/O schedulers support this feature. mq-deadline has been shown to be advantageous for the more demanding server related I/O, however, desktop users may like to experiment with bfq as has been shown to load some applications faster.

Of course, your use-case may differ, the above are just suggestions to start with based on some synthetic tests. You may find other choices with adjustments to the I/O scheduler tunables produce better results. 
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
BUILDING INITRAMFS
	INTRODUCTION
		The initial ramdisk is in essence a very small environment (early userspace) which loads various kernel
	modules and sets up necessary things before handing over control to init. This makes it possible to have, for example,
	encrypted root filesystems and root filesystems on a software RAID array. mkinitcpio allows for easy extension with custom
	hooks, has autodetection at runtime, and many other features.

	HOW TO BUILD INITRAMFS FOR ALL AVAILABLE KERNELS IN THE SYSTEM
		Type: #mkinitcpio -P

	HOW TO BUILD INITRAMFS FOR A SPECIFIC KERNEL IN THE SYSTEM
		Ramdisk environment can be created for a specific kernel in the system through a preset file,
	preset files are located under /etc/mkinitcpio.d/ under the name of a given kernel(ex.: linux-lts.preset).

		CUSTOMIZING INITRAMFS FOR A SPECIFIC KERNEL
		Example:
			[file: /etc/mkinitcpio.d/linux-zen.preset]
				#mkinitcpio preset file for the 'linux-zen' package

				ALL_config="/etc/mkinitcpio.conf"
				ALL_kver="/boot/vmlinuz-linux-zen"

				PRESETS=('default' 'fallback')

				#default_config="/etc/mkinitcpio.conf"
				default_image="/boot/initramfs-linux-zen.img"
				#default_options=""

				#fallback_config="/etc/mkinitcpio.conf"
				fallback_image="/boot/initramfs-linux-zen-fallback.img"
				fallback_options="-S autodetect"
			[/file]
		
		Because of this it's possible to create custom mkinitcpio.conf files for each kernel in the system when desired,
		And it's also possible to create multiple Ramdisk Environments for just a single kernel.

	GENERATE A RAMDISK ENVIRONMENT BASED ON A 'LINUX' PRESET:
		#mkinitcpio --preset <preset_name>
		ex.: #mkinitcpio --preset linux-zen

	GENERATE RAMDISK ENVIRONMENT FOR ALL LINUX PRESETS:
		#mkinitcpio -P
			or
		#mkinitcpio --allpresets
	

	HOW TO LIST CURRENTLY AVAILABLE HOOKS
		Type: $mkinitcpio --listhooks

	HOW TO DISPLAY INFO ON A SPECIFIC HOOK
		type: $mkinitcpio --hookhelp <hook_name>

OPTIMIZING BOOTUP
	[file: /etc/mkinitcpio.conf]
		# Decompress kernel modules during initramfs creation.
		# Enable to speedup boot process, disable to save RAM
		# during early userspace. Switch (yes/no).
		MODULES_DECOMPRESS="yes"
	[/file]

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
STEAM NOT WORKING | NVIDIA XCONFIG
	If there are any xorg related errors when trying to start steam,
it's worthy trying to regenerate /etc/X11/xorg.conf

	Type in: #nvidia-xconfig
note: no need to restart.


--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
NVIDIA FAILING | CREATING ZRAM FOR NVIDIA

If nvidia driver fails despite being added to /etc/mkinitcpio.conf
try creating a zram for nvidia.

please read topic on: CREATING ZRAM DEVICE

RECREATING NVIDIA XCONFIG
	#nvidia-xconfig

ADDING NVIDIA MODULE REFERENCE

	[file: /etc/X11/xorg.conf]
		Section "Files"
		    ModulePath      "/usr/lib/nvidia/xorg"
		    ModulePath      "/usr/lib/xorg/modules"
		EndSection
	[/file]


NVIDIA KERNEL 6 ISSUE
Follow all of the above steps then:
	Remove acpi=off from Grub options,
	and make a custom /etc/mkinitcpio-zen.conf after editing /etc/mkinitcpio.d/linux-zen.preset for it.
	and add the following modules to mkinitcpio-zen.conf: MODULES=(nvidia nvidia-modeset nvidia-drm nvidia-uvm)

New Grub Line without acpi=off | KERNEL OPTIONS:
	rw quiet mitigations=off zswap.enabled=0 nvidia-drm.modeset=1 intel_iommu=on psi=1

If the above still doesn't fix the problem, follow the below trick:

SOLVING KERNEL 6.0 NVIDIA DKMS DRIVER ISSUE
Remove this line from /boot/grub/grub.cfg:
	nvidia-drm.modeset=1
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
EXECUTING PROGRAMS AS A DIFFERENT GROUP IN THE SYSTEM
	1. First create a group:
		#groupadd <group_name>

	2. Execute it as the different group:
		$sudo -g <group_name> <program_name>
	

	IPTABLES
	REVOKING INTERNET PERMISSION FROM AN EXISTING GROUP
		#iptables -A OUTPUT -m owner --gid-owner no-internet -j DROP

	SAVING IPTABLES CONFIGURATION
		#iptables-save 
			and 
		#iptables-restore
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
FUN LINUX STUFF
	Download figlet figlet-fonts-extra packages through AUR Repository.
Fonts: DOS\ Rebel, doubleshorts, Bloody, Larry\ 3D, 3D-ASCII

	GNU Chess, download gnuchess from AUR.
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
INSTALLING UNICODE CHARACTERS
	Below are the related packages needed to be installed on your system for UNICODE Support:

		aur/fonts-noto-hinted 20161116-1 (+1 0.00) (Installed)
		multilib/lib32-libunistring 1.1-1 (519.0 KiB 1.7 MiB) (Installed)
		multilib/lib32-icu 72.1-2 (10.3 MiB 35.5 MiB) (Installed)
		multilib/lib32-fribidi 1.0.12-1 (22.3 KiB 117.6 KiB) (Installed)
		community/python-unicodedata2 15.0.0-1 (353.7 KiB 1.2 MiB) (Installed)
		extra/libuninameslist 20221022-1 (448.5 KiB 3.2 MiB) (Installed)
		extra/fribidi 1.0.12-1 (41.9 KiB 222.8 KiB) (Installed)
		core/perl 5.36.0-3 (15.6 MiB 59.8 MiB) (Installed)
		core/libunistring 1.1-2 (673.3 KiB 2.5 MiB) (Installed)
		core/icu 72.1-2 (11.0 MiB 40.6 MiB) (Installed)

	UTF-8 Support Related Packages:
		extra/luit 20221028-1 (29.9 KiB 70.5 KiB) (Installed)

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
KDENLIVE DARK THEME SETTINGS
	extra/breeze-icons 5.102.0-1 (6.7 MiB 71.3 MiB) [kf5] (Installed)
	extra/breeze-gtk 5.26.5-1 (200.5 KiB 1.1 MiB) [plasma] (Installed)
	extra/breeze 5.26.5-1 (45.3 MiB 51.2 MiB) [plasma] (Installed)

Settings > Check "Force Breeze Icon Theme"
Settings > Style > "Breeze"
Settings > Color Scheme > "Breeze Dark"

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
INCREASING KDENLIVE PERFORMANCE
	1 - Go to Project Monitor and lower profile from 1:1 to 360p
	2 - Go to Settings > Configure Kdenlive > Environment > 
		2.1 - Checkbox "use lower CPU Priority for proxy and transcode tasks"
		2.2 - Increase number of available threads.
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
ENLIGHTENMENT DESKTOP
INCREASING VIRTUAL DESKTOPS
	Left/Right Click > Desktop > Virtual > Set Virtual Desktops

SETTING UP KEYS FOR MOVING BETWEEN DESKTOPS
	Left/Right Click > Settings > Settings Panel > Input > Key Bindings


Important KEYS:
ctrl+alt+w	#Opens menu on top of other programs
		^ Important, since it's not possible to open desktop menu from other programs
ctrl+alt+d	#Shows the desktop
ctrl+alt+f	#Full screen
		^ Important for some games, otherwise they won't work
ctrl+alt+k	#Kill program
ctrl+alt+l	#Lock System
ctrl+alt+arrow_keys #Switch terminals

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
NVIDIA AND WAYLAND 2023 | WESTON
	As of february, using nvidia gpu on wayland still has some restrictions
it's not possible to set coolbits '28' options for wayland and therefore not possible to change colors, 
fan speed, performance mode, etc.

Because of this using wayland desktop environments isn't recommended, however it's possible to execute
wayland applications from X11 Desktop Environment through weston compositor. 

Performance wise speaking it is much better when executing programs from weston.

More information: $man weston

INCREASING PERFORMANCE FOR WAYLAND 2023
	Wayland by itself doesn't provide any graphical environment,
for this to happen it's required a compositor, but for a long time nvidia didn't support
the default GBM Compositor and instead relied on EGLStreams which underperformed compared to X11(OLD INFO WARNING).

----
	NOTE ON OLD INFO:
		as of 2024.1, Nvidia has grown support for GBM and Xwayland will be dropping support for Nvidia's EGLStream.
		https://www.phoronix.com/news/XWayland-Drops-EGLStream
		
		In other words, don't use the now unsupported EGLStream.
----

Old NVIDIA GPUs( NVIDIA < 495 ) still have to rely on EGLStreams,
however it's possible to use GBM Compositors for Nvidia GPUs that are above 495.

In order to do this, you must do as follows:
	1 - Set the following Global Environment Variables:
		GBM_BACKEND=nvidia-drm
		__GLX_VENDOR_LIBRARY_NAME=nvidia

	2 - Edit the file /etc/environment and copy & paste step #1 content into it.

	3 - Restart your computer.


NVIDIA WAYDROID IMPROVEMENT 2023
	as of jan-2023, On an nvidia It's possible to switch from software renderer 
to hardware renderer.

By editing and changing the following lines to something else:
	[file: /var/lib/waydroid/waydroid_base.prop]
		ro.hardware.gralloc=default
		ro.hardware.egl=swiftshader
	[/file]


CHECKING WESTON IS RUNNING AS WAYLAND
$weston					#Wayland compositor
					^ Once weston is open, type in: $echo $WAYLAND_DISPLAY
					^ Check man page, $WAYLAND_DISPLAY shows wayland-backend.so is being used.
					^ This means wayland is working
CREATING A WESTON CONFIG FILE
My File:
[file: ~/.config/weston.ini]
	[shell]
	binding-modifier=ctrl
	num-workspaces=6
[/file - obs: workspaces not working]

Example File:
[file: ~/.config/weston.ini]
	[core]
	# xwayland support
	xwayland=true

	[libinput]
	enable-tap=true

	[shell]
	#background-image=/usr/share/backgrounds/gnome/Aqua.jpg
	background-type=scale-crop
	background-color=0xff000000
	#background-color=0xff002244
	#panel-color=0x90ff0000
	panel-color=0x00ffffff
	panel-position=bottom
	#clock-format=none
	#animation=zoom
	#startup-animation=none
	close-animation=none
	focus-animation=dim-layer
	#binding-modifier=ctrl
	num-workspaces=6
	locking=false
	cursor-theme=Adwaita
	cursor-size=24

	# tablet options
	#lockscreen-icon=/usr/share/icons/gnome/256x256/actions/lock.png
	#lockscreen=/usr/share/backgrounds/gnome/Garden.jpg
	#homescreen=/usr/share/backgrounds/gnome/Blinds.jpg
	#animation=fade

	# for Laptop displays
	[output]
	name=LVDS1
	mode=preferred
	#mode=1680x1050
	#transform=rotate-90

	#[output]
	#name=VGA1
	# The following sets the mode with a modeline, you can get modelines for your preffered resolutions using the cvt utility
	#mode=173.00 1920 2048 2248 2576 1080 1083 1088 1120 -hsync +vsync
	#transform=flipped

	#[output]
	#name=X1
	#mode=1024x768
	#transform=flipped-rotate-270

	# on screen keyboard input method
	#[input-method]
	#path=/usr/lib/weston/weston-keyboard

	[keyboard]
	keymap_rules=evdev
	#keymap_layout=us,de
	#keymap_variant=colemak,
	#keymap_options=grp:shifts_toggle
	#keymap_options=caps:ctrl_modifier,shift:both_capslock_cancel
	repeat-rate=30
	repeat-delay=300

	# keymap_options from /usr/share/X11/xkb/rules/base.lst
	#numlock-on=true

	[terminal]
	font=monospace
	font-size=18

	[launcher]
	icon=/usr/share/weston/icon_flower.png
	path=/usr/bin/weston-flower

	[launcher]
	icon=/usr/share/icons/gnome/32x32/apps/utilities-terminal.png
	path=/usr/bin/weston-terminal --shell=/usr/bin/bash

	#[launcher]
	#icon=/usr/share/icons/gnome/32x32/apps/utilities-terminal.png
	#path=/usr/bin/gnome-terminal

	[launcher]
	icon=/usr/share/icons/hicolor/32x32/apps/firefox.png
	path=MOZ_ENABLE_WAYLAND=1 /usr/bin/firefox

	#[launcher]
	#icon=/usr/share/icons/Adwaita/32x32/apps/multimedia-volume-control.png
	#path=/usr/bin/st alsamixer -c0
[/file]

WESTON HOTKEYS | WESTON SHORTCUT KEYS
These are the default keys used by weston:
	ctrl + alt + backspace 	| Kill Weston |	Keyboard
	super + scroll 	| Zoom in / out the desktop | Keyboard + mouse
	super + alt + scroll | Change activated window opacity | Keyboard + mouse
	<modifier-key> + page up / page down 	Zoom in / out the desktop 	Keyboard
	<modifier-key> + shift + f 	Put activated window fullscreen 	Keyboard
	<modifier-key> + left mouse button 	Move activated window 	Keyboard + mouse
	<modifier-key> + middle mouse button 	Rotate activated window 	Keyboard + mouse
	<modifier-key> + right mouse button 	Resize activated window 	Keyboard + mouse
	<modifier-key> + shift + left mouse button 	Resize activated window 	Keyboard + mouse
	<modifier-key> + tab 	Switch windows 	Keyboard
	<modifier-key> + k 	Kill activated window 	Keyboard
	<modifier-key> + key up / key down 	Switch to previous / next workspace 	Keyboard + workspaces
	<modifier-key> + shift + key up / key down 	Move activated window to previous / next workspace 	Keyboard + workspaces
	<modifier-key> + Fn 	Switch to workspace n 	Keyboard + workspaces
	super + s 	Capture a screenshot (see details below) 	Keyboard
	super + r 	Start / stop recording a screencast (see details below) 	Keyboard 

	^ NOTE: It's necessary to define modifier key in ~/.config/weston.ini
	Check "CREATING A WESTON CONFIG FILE" topic for binding-modifier=ctrl

CREATING WESTON DESKTOP ICONS
	Add the following to the ~/.config/weston.ini

		[launcher]
		icon=/usr/share/icons/hicolor/24x24/apps/firefox.png
		path=/usr/bin/firefox

STARTING WAYLAND AT BOOT FOR NVIDIA GPUS
	As of 2023, weston is needed
	1 - From a tty, right after login, issue weston to start
		$weston

	2 - From the weston session, open a weston-terminal(or any terminal) and start your preferred windowing system:
		ex.:
		$sudo systemctl start sddm

	3 - From this point onwards everything will be run as wayland applications

	Read more on WESTON BACKENDS below

WESTON BACKENDS
	Usually weston will automatically detect the best backend for initialization,
but it's still possible to instruct weston to start from a different backend, 
although this isn't recommended: $weston -B<backend_name.extension>

	1 - drm-backend.so:
		This is the default backend for when weston is run through a VT(Virtual terminal).

	2 - wayland-backend.so:
		This backend is only used by weston if the user is on a wayland Desktop Environment.
	it's meant for nested wayland instances.

	3 - x11-backend.so:
		This is the default backeand for running weston through an X11 based Desktop Environment.

	4 - rdp-backend.so:
		Runs in memory without the need for GPU Hardware Accelerated graphics card.

WESTON XWAYLAND MODULE
	An Xwayland module can be activated with --xwayland argument for all backends.
Althought this is usually not needed.

FIGURING OUT WHICH WESTON BACKEND IS BEING USED
	If the env. variable $DISPLAY is set and $WAYLAND_DISPLAY isn't set,
then X11-backend is in use.

INITALIZING RAW XORG | RAW XORG INITIALIZATION
	#systemctl start seatd
	#Xorg -nolisten tcp -background none -seat seat0 vt1 -noreset -displayfd 17

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
INSTALLING AND CUSTOMIZING SDDM
	SDDM is a display manager capable of instantating 
both wayland and X11 desktop environments.

	1 - Installation
		#pacman -Syu sddm
	2 - Listing Themes Directory
		$ls -la /usr/lib/sddm/themes/

	3 - SDDM Configuration File(Check MAN PAGE if needed)
		#sudo vi /usr/lib/sddm/sddm.conf.d/default.conf
	
	4 - Check var Current for theme and current theme name and change it to an existing theme in /usr/lib/sddm/themes/

Where to download Themes?
	AUR Repository has more than a few themes you can download and use.

How to Themes from GITHUB:
	1 - CD into /usr/lib/sddm/themes/ as root
	2 - git clone <url>
	3 - cd into elegant-archlinux and use mv command for moving the theme directory structure into /usr/lib/sddm/themes ( if needed )
	4 - edit file /usr/lib/sddm/sddm.conf.d/default.conf

Theme links:
https://github.com/sniper1720/elegant-sddm-archlinux-theme/tree/master/elegant-archlinux
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
HOW TO START XORG/X11 MANUALLY

	SIMPLE WAY
		$xinit
		or
		#startx

	XORG/X11 APPLICATIONS
		Avoid playing with these, as it may break your current xorg configuration files.
	it is advised to create a new user in case you really need to touch these, except for xinit and startx.

		1 - X(man Xserver)
		2 - xinit
			Starts the X Window System server on systems that are not using a display manager or in environments that use
		multiple window systems.
		3 - xrdb
		4 - startx
			It is a front-end to xinit.
		4 - Xorg
		5 - Xephyr(avoid this, don't use this)
	
	TROUBLESHOOTING: XORG STOPS WORKING FOR NO REASON
		Executing either Xorg or X without -configure option will result in a xorg.conf.new file being created on ~/ and /root/,
	the new file on ~/ will prevent Display Managers from being executed in the future, it is necessary to remove it due to it's
	miss-configuration.

		If messing with any of these programs make your actual display manager or window manager or desktop environment stop
	working for no reason, make sure to delete ~/xorg.conf.new and any ~/.Xauthority files and generate new config: #nvidia-xconfig
	restart and try again. messing with startx and xinit should be ok.

	VERY IMPORTANT: To avoid this situation, you can copy the xorg.conf file located under /etc/X11/ to both your local ~/ and /root
	directories; HOWEVER, you'll have to make sure a new copy is made each time a new xorg.conf is generated under /etc/X11/
	since it'll override the ones in /etc/X11/
		
	TROUBLESHOOTING STARTX AND XINIT FOR NVIDIA
		1 - #cp /usr/lib/nvidia /usr/lib/fakenvidia
		2 - #mkdir /etc/X11/tmp/
		3 - #cp /etc/X11/xorg.conf /etc/X11/tmp/xorg.conf-startx
			3.1 - #cp /etc/X11/xorg.conf /etc/X11/tmp/xorg.conf
		4 - Edit xorg.conf-startx as follows:
			[file: /etc/X11/tmp/xorg.conf-startx]
				Section "Files"
				    ModulePath      "/usr/lib/nvidia/fakenvidiaxorg"
				    ModulePath      "/usr/lib/xorg/modules"
				EndSection
			[/file]
		5 - Do steps #1 to #4 to the following file located in /usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf
		6 - Create the following scripts under ~/scripts/
			6.1 - mkdir ~/scripts/
			
			6.2 - [file: ~/scripts/start_xorg_conf.sh]
				sudo rm /etc/X11/xorg.conf;
				sudo rm /usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf
				sudo cp /etc/X11/tmp/xorg.conf-startx /etc/X11/xorg.conf;
				sudo cp /usr/share/X11/tmp/10-nvidia-drm-outputclass.conf-startx /usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf

			      [/file]

			6.3 - [file: ~/scripts/startx.sh]
				sudo rm /etc/X11/xorg.conf;
				sudo rm /usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf
				sudo cp /etc/X11/tmp/xorg.conf-startx /etc/X11/xorg.conf;
				sudo cp /usr/share/X11/tmp/10-nvidia-drm-outputclass.conf-startx /usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf
				startx &
				sleep 5;
				sudo rm /etc/X11/xorg.conf;
				sudo rm /usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf
				sudo cp /etc/X11/tmp/xorg.conf /etc/X11/;
				sudo cp /usr/share/X11/tmp/10-nvidia-drm-outputclass.conf /usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf

			      [/file]

			6.4 - [file: ~/scripts/stop_xorg_conf.sh]
				sudo rm /etc/X11/xorg.conf;
				sudo rm /usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf
				sudo cp /etc/X11/tmp/xorg.conf /etc/X11/;
				sudo cp /usr/share/X11/tmp/10-nvidia-drm-outputclass.conf /usr/share/X11/xorg.conf.d/10-nvidia-drm-outputclass.conf

			      [/file]
		7 - Switch to a Auxiliary VT(Virtual Terminal)
			7.0 - If necessary: $man vt
			7.1 - ctrl+alt+f2

		8 - Read notes:
			8.1 - Note1: Startx & Xinit will not work as root
			8.2 - Note2: Using startx.sh will remove xorg.conf-startx files to fit it's original ones, but will cause issues.
			8.3 - Note3: Issues will be caused if multiple instances are executed in different VTs.
				^ ex. running SDDM on VT1 and running startx in VT4,
				^ a slowdown was noticed on VT1 even after VT4 session was closed.
				^ VT4 had some issues after xorg.conf files were restored to their original ones.
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
ADDING VIM CLIPBOARD FUNCTION
	Install vim-gtk2 to add clipboard function, it'll replace default vim-runtime and vim.
This is required because vim/nvim needs to be actualli compiled with clipboard option.

ENABLING CLIPBOARD TEMPORARILY:
	1 - Linux
		:set clipboard+=unnamedplus
	2 - Windows
		:set clipboard+=unnamed

ENABLING CLIPBOARD PERMANENTLY ON NVIM:
	edit ~/.config/nvim/init.vim and add "set clipboard+=" option from "ENABLING CLIPBOARD TEMPORARILY",
	remember to remove ':' character from the file, since that is only being used here to indicate vim's 
	normal mode it's not needed in the config file.

TROUBLESHOOTING ISSUES NVIM:
	use :checkhealth to verify if the configuration you're looking for is working correctly and has
it's respective tool installed. For ex.: NVIM Clipboard usually requires a system Clipboard tool installed

CLIPBOARD TOOLS
	The presence of a working clipboard tool implicitly enables the "+" and "*"
registers. Nvim supports these clipboard tools, in order of priority:

- |g:clipboard| : User override (if set to a dict or any string "name" below;
  e.g. `g:clipboard="tmux"` forces tmux clipboard and skips auto-detection).
- "pbcopy"    : pbcopy, pbpaste (macOS)
- "wl-copy"   : wl-copy, wl-paste (if $WAYLAND_DISPLAY is set)
- "wayclip"   : waycopy, waypaste (if $WAYLAND_DISPLAY is set)
- "xsel"      : xsel (if $DISPLAY is set)
- "xclip"     : xclip (if $DISPLAY is set)
- "lemonade"  : lemonade (for SSH) https://github.com/pocke/lemonade
- "doitclient": doitclient (for SSH) https://www.chiark.greenend.org.uk/~sgtatham/doit/
- "win32yank" : *win32yank* (Windows)
- "putclip"   : putclip, getclip (Windows) https://cygwin.com/packages/summary/cygutils.html
- "clip"      : clip, powershell (Windows) https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/clip
- "termux"    : termux (via termux-clipboard-set, termux-clipboard-set)
- "tmux"      : tmux (if $TMUX is set)
- "osc52"     : |clipboard-osc52| (if supported by your terminal)

SETTING UP NVIM CLIPBOARD FOR COPY AND PASTING IN 2025

	1) Install your clipboard of choice, example:
		# pacman -S xsel xclip

		Note: If you're using WSL (Windows Linux Subsystem) use 'xsel'.

	2) Open nvim
		$ nvim

	3) Go into command mode by typing ':' and type:
		let g:clipboard = 'xsel'
			or
		let g:clipboard = 'xclip'
		
		
	Note: if needed type ':h clipboard' in nvim's command-mode for more instructions

MANUALLY SETTING UP NVIM CLIPBOARD
	create a file on "~/.config/nvim/init.vim", make directory if needed.
	add the following lines:

	let g:clipboard = {
	      \   'name': 'myClipboard',
	      \   'copy': {
	      \      '+': ['tmux', 'load-buffer', '-'],
	      \      '*': ['tmux', 'load-buffer', '-'],
	      \    },
	      \   'paste': {
	      \      '+': ['tmux', 'save-buffer', '-'],
	      \      '*': ['tmux', 'save-buffer', '-'],
	      \   },
	      \   'cache_enabled': 1,
	      \ }
	
	this allows copy/paste clipboarding to tmux.

	VERY IMPORTANT: use :checkhealth to make sure a clipboard tool has been successfully detected!

AUTOMATICALLY SETTING UP NVIM CLIPBOARD 
	This is the default behavior for NVIM, just make sure to have a clipboard tool installed on your linux system.
	as of 2023, xclip is detected automatically if installed.

	The following vim script will automatically detect the available clipboard tool if one is available,
	VERY IMPORTANT: use :checkhealth to make sure a clipboard tool has been successfully detected afterwards!

	Code Source: https://github.com/neovim/neovim/blob/master/runtime/autoload/provider/clipboard.vim
	by default this file should be on /usr/share/nvim/runtime/autoload/provider

	[file: /usr/share/nvim/clipboard.vim]
		" The clipboard provider uses shell commands to communicate with the clipboard.
		" The provider function will only be registered if a supported command is
		" available.

		if exists('g:loaded_clipboard_provider')
		  finish
		endif
		" Default to 1.  provider#clipboard#Executable() may set 2.
		" To force a reload:
		"   :unlet g:loaded_clipboard_provider
		"   :runtime autoload/provider/clipboard.vim
		let g:loaded_clipboard_provider = 1

		let s:copy = {}
		let s:paste = {}
		let s:clipboard = {}

		" When caching is enabled, store the jobid of the xclip/xsel process keeping
		" ownership of the selection, so we know how long the cache is valid.
		let s:selection = { 'owner': 0, 'data': [], 'stderr_buffered': v:true }

		function! s:selection.on_exit(jobid, data, event) abort
		  " At this point this nvim instance might already have launched
		  " a new provider instance. Don't drop ownership in this case.
		  if self.owner == a:jobid
		    let self.owner = 0
		  endif
		  if a:data != 0
		    echohl WarningMsg
		    echomsg 'clipboard: error invoking '.get(self.argv, 0, '?').': '.join(self.stderr)
		    echohl None
		  endif
		endfunction

		let s:selections = { '*': s:selection, '+': copy(s:selection) }

		function! s:try_cmd(cmd, ...) abort
		  let out = systemlist(a:cmd, (a:0 ? a:1 : ['']), 1)
		  if v:shell_error
		    if !exists('s:did_error_try_cmd')
		      echohl WarningMsg
		      echomsg "clipboard: error: ".(len(out) ? out[0] : v:shell_error)
		      echohl None
		      let s:did_error_try_cmd = 1
		    endif
		    return 0
		  endif
		  return out
		endfunction

		" Returns TRUE if `cmd` exits with success, else FALSE.
		function! s:cmd_ok(cmd) abort
		  call system(a:cmd)
		  return v:shell_error == 0
		endfunction

		function! s:split_cmd(cmd) abort
		  return (type(a:cmd) == v:t_string) ? split(a:cmd, " ") : a:cmd
		endfunction

		let s:cache_enabled = 1
		let s:err = ''

		function! provider#clipboard#Error() abort
		  return s:err
		endfunction

		function! provider#clipboard#Executable() abort
		  if exists('g:clipboard')
		    if type({}) isnot# type(g:clipboard)
			  \ || type({}) isnot# type(get(g:clipboard, 'copy', v:null))
			  \ || type({}) isnot# type(get(g:clipboard, 'paste', v:null))
		      let s:err = 'clipboard: invalid g:clipboard'
		      return ''
		    endif

		    let s:copy = {}
		    let s:copy['+'] = s:split_cmd(get(g:clipboard.copy, '+', v:null))
		    let s:copy['*'] = s:split_cmd(get(g:clipboard.copy, '*', v:null))

		    let s:paste = {}
		    let s:paste['+'] = s:split_cmd(get(g:clipboard.paste, '+', v:null))
		    let s:paste['*'] = s:split_cmd(get(g:clipboard.paste, '*', v:null))

		    let s:cache_enabled = get(g:clipboard, 'cache_enabled', 0)
		    return get(g:clipboard, 'name', 'g:clipboard')
		  elseif has('mac')
		    let s:copy['+'] = ['pbcopy']
		    let s:paste['+'] = ['pbpaste']
		    let s:copy['*'] = s:copy['+']
		    let s:paste['*'] = s:paste['+']
		    let s:cache_enabled = 0
		    return 'pbcopy'
		  elseif !empty($WAYLAND_DISPLAY) && executable('wl-copy') && executable('wl-paste')
		    let s:copy['+'] = ['wl-copy', '--foreground', '--type', 'text/plain']
		    let s:paste['+'] = ['wl-paste', '--no-newline']
		    let s:copy['*'] = ['wl-copy', '--foreground', '--primary', '--type', 'text/plain']
		    let s:paste['*'] = ['wl-paste', '--no-newline', '--primary']
		    return 'wl-copy'
		  elseif !empty($WAYLAND_DISPLAY) && executable('waycopy') && executable('waypaste')
		    let s:copy['+'] = ['waycopy', '-t', 'text/plain']
		    let s:paste['+'] = ['waypaste', '-t', 'text/plain']
		    let s:copy['*'] = s:copy['+']
		    let s:paste['*'] = s:paste['+']
		    return 'wayclip'
		  elseif !empty($DISPLAY) && executable('xsel') && s:cmd_ok('xsel -o -b')
		    let s:copy['+'] = ['xsel', '--nodetach', '-i', '-b']
		    let s:paste['+'] = ['xsel', '-o', '-b']
		    let s:copy['*'] = ['xsel', '--nodetach', '-i', '-p']
		    let s:paste['*'] = ['xsel', '-o', '-p']
		    return 'xsel'
		  elseif !empty($DISPLAY) && executable('xclip')
		    let s:copy['+'] = ['xclip', '-quiet', '-i', '-selection', 'clipboard']
		    let s:paste['+'] = ['xclip', '-o', '-selection', 'clipboard']
		    let s:copy['*'] = ['xclip', '-quiet', '-i', '-selection', 'primary']
		    let s:paste['*'] = ['xclip', '-o', '-selection', 'primary']
		    return 'xclip'
		  elseif executable('lemonade')
		    let s:copy['+'] = ['lemonade', 'copy']
		    let s:paste['+'] = ['lemonade', 'paste']
		    let s:copy['*'] = ['lemonade', 'copy']
		    let s:paste['*'] = ['lemonade', 'paste']
		    return 'lemonade'
		  elseif executable('doitclient')
		    let s:copy['+'] = ['doitclient', 'wclip']
		    let s:paste['+'] = ['doitclient', 'wclip', '-r']
		    let s:copy['*'] = s:copy['+']
		    let s:paste['*'] = s:paste['+']
		    return 'doitclient'
		  elseif executable('win32yank.exe')
		    if has('wsl') && getftype(exepath('win32yank.exe')) == 'link'
		      let win32yank = resolve(exepath('win32yank.exe'))
		    else
		      let win32yank = 'win32yank.exe'
		    endif
		    let s:copy['+'] = [win32yank, '-i', '--crlf']
		    let s:paste['+'] = [win32yank, '-o', '--lf']
		    let s:copy['*'] = s:copy['+']
		    let s:paste['*'] = s:paste['+']
		    return 'win32yank'
		  elseif executable('termux-clipboard-set')
		    let s:copy['+'] = ['termux-clipboard-set']
		    let s:paste['+'] = ['termux-clipboard-get']
		    let s:copy['*'] = s:copy['+']
		    let s:paste['*'] = s:paste['+']
		    return 'termux-clipboard'
		  elseif !empty($TMUX) && executable('tmux')
		    let ver = matchlist(systemlist(['tmux', '-V'])[0], '\vtmux %(next-)?(\d+)\.(\d+)')
		    if len(ver) >= 3 && (ver[1] > 3 || (ver[1] == 3 && ver[2] >= 2))
		      let s:copy['+'] = ['tmux', 'load-buffer', '-w', '-']
		    else
		      let s:copy['+'] = ['tmux', 'load-buffer', '-']
		    endif
		    let s:paste['+'] = ['tmux', 'save-buffer', '-']
		    let s:copy['*'] = s:copy['+']
		    let s:paste['*'] = s:paste['+']
		    return 'tmux'
		  endif

		  let s:err = 'clipboard: No clipboard tool. :help clipboard'
		  return ''
		endfunction

		function! s:clipboard.get(reg) abort
		  if type(s:paste[a:reg]) == v:t_func
		    return s:paste[a:reg]()
		  elseif s:selections[a:reg].owner > 0
		    return s:selections[a:reg].data
		  end

		  let clipboard_data = s:try_cmd(s:paste[a:reg])
		  if match(&clipboard, '\v(unnamed|unnamedplus)') >= 0
			\ && type(clipboard_data) == v:t_list
			\ && get(s:selections[a:reg].data, 0, []) ==# clipboard_data
		    " When system clipboard return is same as our cache return the cache
		    " as it contains regtype information
		    return s:selections[a:reg].data
		  end
		  return clipboard_data
		endfunction

		function! s:clipboard.set(lines, regtype, reg) abort
		  if a:reg == '"'
		    call s:clipboard.set(a:lines,a:regtype,'+')
		    if s:copy['*'] != s:copy['+']
		      call s:clipboard.set(a:lines,a:regtype,'*')
		    end
		    return 0
		  end

		  if type(s:copy[a:reg]) == v:t_func
		    call s:copy[a:reg](a:lines, a:regtype)
		    return 0
		  end

		  if s:cache_enabled == 0
		    call s:try_cmd(s:copy[a:reg], a:lines)
		    "Cache it anyway we can compare it later to get regtype of the yank
		    let s:selections[a:reg] = copy(s:selection)
		    let s:selections[a:reg].data = [a:lines, a:regtype]
		    return 0
		  end

		  if s:selections[a:reg].owner > 0
		    let prev_job = s:selections[a:reg].owner
		  end
		  let s:selections[a:reg] = copy(s:selection)
		  let selection = s:selections[a:reg]
		  let selection.data = [a:lines, a:regtype]
		  let selection.argv = s:copy[a:reg]
		  let selection.detach = s:cache_enabled
		  let selection.cwd = "/"
		  let jobid = jobstart(selection.argv, selection)
		  if jobid > 0
		    call jobsend(jobid, a:lines)
		    call jobclose(jobid, 'stdin')
		    " xclip does not close stdout when receiving input via stdin
		    if selection.argv[0] ==# 'xclip'
		      call jobclose(jobid, 'stdout')
		    endif
		    let selection.owner = jobid
		    let ret = 1
		  else
		    echohl WarningMsg
		    echomsg 'clipboard: failed to execute: '.(s:copy[a:reg])
		    echohl None
		    let ret = 1
		  endif

		  " The previous provider instance should exit when the new one takes
		  " ownership, but kill it to be sure we don't fill up the job table.
		  if exists('prev_job')
		    call timer_start(1000, {... ->
			  \ jobwait([prev_job], 0)[0] == -1
			  \ && jobstop(prev_job)})
		  endif

		  return ret
		endfunction

		function! provider#clipboard#Call(method, args) abort
		  if get(s:, 'here', v:false)  " Clipboard provider must not recurse. #7184
		    return 0
		  endif
		  let s:here = v:true
		  try
		    return call(s:clipboard[a:method],a:args,s:clipboard)
		  finally
		    let s:here = v:false
		  endtry
		endfunction

		" eval_has_provider() decides based on this variable.
		let g:loaded_clipboard_provider = empty(provider#clipboard#Executable()) ? 1 : 2
	[/file]

***: XORG CLIPBOARD TOOL
	Install package xorg-xclipboard AND xclip. #Date: 2023
	xclip alone should do the trick.

***: WAYLAND CLIPBOARD TOOL
	Install wl-clipboard and clipman and wayclip. #Date: 2023
	wayclip alone should do the trick.

CLIPBOARD MANAGER
	A Clipboard manager allows to manage multiple clipboarding.
	xorg-xclipboard is a clipboard manager for xorg.
	clipman is a clibboard manager for wayland.

NVIM BINARIES & CUSTOMIZATION PACKAGES
	powerline powerline-vim powerline-fonts
	neovim-qt(open as nvim-qt)

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
MANUALLY INSTALLING ARCHLINUX PACKAGES
	This here is only meant for manually installing packages downloaded on 3rd party websites or acquired from 3rd party applications other than the default package manager for archlinux(pacman).

	0 - Download Package to ~/Downloads/
	1 - Decompress Package on new /usr/share/ directory:
		1.1 - #7z x ~/Downloads/<package_name>.7z -o/usr/share/<package_name_directory>
		    
	2 - Create a softlink from /usr/bin/<package_name_directory>/<executable_binary_file> into /usr/bin/ :
		2.1 - #ln -s /usr/bin/<package_name_directory>/<executable_binary_filename> /usr/bin/<executable_binary_filename>
	
	3 - Create Desktop Shortcut(OPTIONAL)
		3.1 - $ln -s /usr/bin/<executable_binary_file> ~/Desktop/<shortcut_name>

	
NOTE: If you have trouble finding the proper <executable_binary_file> in a given package directory,
read topic "FINDING FILETYPES IN A GIVEN DIRECTORY".
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
FINDING FILETYPES IN A GIVEN DIRECTORY
	Dictionary:
		less							#Pages through text output using 'vi' style keys 
		# find . -type f | file -f -				#Lists all filetypes in the CURRENT DIRECTORY

	1 - Type: 
		# find . -type f | file -f - | less
			or
		$ find . -type f | file -f - | less
	
	2 - Use vi's regex search:
		2.1 Type:
			2.1.1 - Moves to 1st line of text:
				g
			2.1.2 - Display Search Function:
				/

		2.2 - Searches for executable binary files:
			2.2.1 - Type in:
				executable
			2.2.2 - Search for Next and Previous Elements:
				n or N

	3 - Exiting
		3.2 - Press 'q'

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
INCREASING EMULATOR PERFORMANCE
	Some emulators allow you to increase it's internal CPU Cycles by hand,
example:
	1 - dgen-sdl: $dgen -H 64 <rom_name>
			or
	use the best settings below:
			[file: /bin/dgen-gamescope]  
				#!/bin/sh
				ENABLE_VKBASALT=1 gamescope --backend wayland --force-grab-cursor -w 1280 -h 768 -W 1280 -H 768 -S stretch -f --force-windows-fullscreen -- /bin/dgen -X 1280 -Y 768 -G 1280x768 -R U -H 64 "$@"
			[/file]

			VERY IMPORTANT: Only use gamescope if you have trouble with running the game fullscreen.
			using scaling_startup = "scale2x stretch" solves this issue, making gamescope here useless.

			^ Must Run this  command after creating the above file: # chmod o+x /bin/dgen-gamescope
			^  -R ' ' Sets auto-region which affects framerate, 
			^ -H selects the framerate override for a given region, it must come AFTER -R ' ' option.
			^ I'm using -H 73 to speed up the game on a 73hz resolution monitor,
			^ If running on 60Hz monitor, use 66Hz to make the game faster since 73hz will look bad
			^ Runs better through wayland display server, ( Enables Vsync by default, dgen has issues with vsync on x11)

		^ Press 'F6' to cycle through dgen's image scaler
		^ Press 'F5' to cyclce through dgen's filter
		^ Default Sega Genesis CPU cycle is 60HZ (very good); However use 66HZ (Even better).
		^ cp ~/.dgen/dgenrc.auto ~/.dgen/dgenrc;
		^ nvim ~/.dgen/dgenrc
		^ edit: 
			[file: ~/.dgen/dgenrc]
				scaling_startup = "scale2x stretch"
				int_nice = 0
				int_hz = 64
				bool_pal = false
				region = ''
				str_region_order = "JUEX"
				bool_fps = false
				bool_buttons = false
				bool_fullscreen = true
				int_info_height = -1
				int_width = 1280
				int_height = 768
				int_scale = -1
				int_scale_x = -1
				int_scale_y = -1
				int_depth = 0
				bool_aspect = false 
				bool_swab = false
				bool_opengl = true
				bool_opengl_stretch = true
				bool_opengl_aspect = false
				int_opengl_width = 1280
				int_opengl_height = 768
				bool_opengl_linear = true
			[/file]
			VERY IMPORTANT 1: Setting 'int_scale', 'int_scale_x' or 'int_scale_y' to a value different than '-1'
			will disable opengl video, making graphics look worse.
			VERY IMPORTANT 2: 'int_hz' will only work when setting region to empty as followed:

				[file: ~/.dgen/dgenrc ]
					int_hz = 64
					region = ''
				[/file]
				Note1: 'int_hz' attribute will increase internal framerate. 
				Some games, however, may run it region issues and if such is the case, you can still set the framerate 
				before launching the game: dgen -H 64 <game_file>
				Note2: Setting region to ' ' instead of '' will set it to autodetect and it will override the 'int_hz' attribute
				as mentioned in note1.
				Note3: Region can be set to U as in: region = U

				Note4: 
					It's also possible to override region framerate like this:
						dgen -U J -H 64 <game_file>

				MORE ABOUT OPTIONS ON THE FOLLOWING LINK: https://github.com/clbr/dgen/blob/master/sample.dgenrc

		you can also change HZ when the game is running
			1 - press ':' to go into command-mode
			2 - type int_hz 60 for 60HZ

		some games:
			Sonic 3 Complete Edition SP/Sonic3C_130810.zip

	2 - dosbox-x: allows not only to change CPU Cycle, but also speed up and slow down the machine.(very good)
	3 - duckstation: does the same for PS1 games(very very good)
	4 - bsnes-qt5 (run as bsnes): Allows increasing CPU Cycle AND also allows increasing(CPU Cycle doesn't works, bad as of 2024, 
					avoid it and use snes9x-gtk instead)
				      ^ 1 - Go to settings > Emulator > use native file dialogs ( VERY IMPORTANT )
				      ^ 2 - Settings > Shader > None
				      ^ 3 - Settings > Driver > Video > OpenGL 3.2 > Synchronize
				      ^ 4 - Settings > Driver > Audio > Pulseaudio > Syncrhonize
				      ^ 5 - Settings > Driver > Audio > Pulseaudio > Latency 20
				      ^ 6 - Settings > Driver > Audio > Pulseaudio > Lower Frequency to 44.1k HZ (44100 HZ)
				      ^ 7 - Settings > Size > 720p
				      ^ 8 - Settings > Output > Stretch
				      ^ 9 - Settings > Output > Uncheck Aspect Ratio
				      ^ 10 - Settings > Output > HiRes Blur Emulation
				      ^ 11 - Settings > Filter > Scale2x
				      ^ 12 - Settings > Uncheck Show Status Bar
				      ^ 13 - Settings > Compatibility > CPU(Processor > Fast Math


	5 - snes9x-gtk: it's only possible to increase framerate.(very good) | SNES EMULATOR
		^ Increasing Frame Timing can only be used on XVideo Rendering, this means OpenGL and Vulkan won't work:
			5.1.1 - Options -> Preferences -> Display -> Hardware Acceleration > Choose 'XVideo'
			5.1.2 - Options -> Preferences -> Emulation -> Choose 'Timer-Based'
				^ Do not choose timer-based with automatic frame-skipping
			5.1.3 - When playing, set the shortcut/hotkeys for decreasing frame-time and lower it to 14ms for best experience!
			5.2 - Options -> Display -> Scaling -> Choose "4:3 Snes Correct Aspect Ratio, Integer Multiples"

			^ If you start to have AUDIO CRACKLING OR AUDIO STUTTERINGS this is the solution(Linux Pipewire only):
				Type this on commandline:
					$ pw-metadata -n settings 0 clock.force-rate 48000 && pw-metadata -n settings 0 clock.force-quantum 2000

					VERY IMPORTANT: This changes both audio input & audio output rate and quantum and may break games/apps that rely
					on Microphone Input!!!
					^ clock.force-quantum 2000 is the best settings as  audio delays will not be noticeable at 14ms frame time
					^ VERY IMPORTANT: The higher the quantum buffer, the more delay you'll get for hearing audio/sound effects.
					^ PIPEWIRE_QUANTUM can be used as environment var for doing the same thing.
					^ Microphone audio output may display audio cracklings and stutter with higher quantum values
					^ VERY IMPORTANT 2: Read topic "SETTING UP QUANTUM ON A PER APPLICATION BASIS" if you need it

				^  clock.force-rate should be the frequency at which your headset/speaker can play audio, never try higher values
				than the one specified on your headset.
				^ clock.force-quantum is the In PipeWire, the audio quantum is a buffer size that can be configured.
				Higher quantum will imply in bigger memory usage for buffer data, but this will solve any audio stutter/crackling issues
				you might get.

				VERY IMPORTANT: you can use 'pw-top' for monitoring audio's quantum 
				VERY IMPORTANT 2: Read topic "SETTING UP QUANTUM ON A PER APPLICATION BASIS" if you need it
				
		[file: /bin/snes9x-gamescope]
			#!/bin/sh
			#PIPEWIRE_QUANTUM doesn't works for  snes9x-gtk
			#set -r to your actual refresh rate
			PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope --backend wayland --force-grab-cursor -w 1280 -h 768 -W 1280 -H 1024 -S stretch -f --force-windows-fullscreen -r 75 -- /bin/snes9x-gtk "$@"
		[/file]
		VERY IMPORTANT: Read topic "SETTING UP QUANTUM ON A PER APPLICATION BASIS" if you need it

	6 - mupen64plus-qt(mupen64plus fork): F10 increases speed by 5% / F11 lowers speed by 5%
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
ADJUSTING HARDWARE & SYSTEM CLOCK USING HWCLOCK
	Order is 1, 2 and 3 for setting up system and hardware clock.

1 - USING TIMEDATECTL FOR SETTING TIMEZONE (SYSTEM CLOCK ONLY)
	$ timedatectl					#Returns status
	$ timedatectl status				#Returns status
	$ timedatectl list-timezones			#Lists all available timezones
	# timedatectl set-timezone Brazil/East		#Sets timezone

		1(Alternative) - MANUALLY SETTING TIMEZONE FOR LINUX
			# ln -sf /usr/share/zoneinfo/<Region>/<City> /etc/localtime		
			VERY IMPORTANT: Creates a symlink file in /etc/ directory called 'localtime'

2 - USING SYSTEMD-TIMESYNCD
	By default, archlinux installation environment comes with systemd-timesyncd enabled, and both clocks are syncd/updated
as soon as internet connection is available.

	WHAT IS SYSTEMD-TIMESYNCD
		1 - A client-only implementation of the SNTP (Simple Network Time Protocol) for synchronizing the system clock with remote NTP servers.
		2 - Writes to a local file /var/lib/systemd/timesync/clock after each synchronization and every 60 seconds.
		3 - The configuration file is located at /etc/systemd/timesyncd.conf .

	WHAT IS TIMEDATED
		1 - Controls whether the system RTC (Real-Time Clock) is in local or UTC timezone. 
		2 - Enables or disables network time synchronization services, like systemd-timesyncd.
		3 - Is automatically activated on request and terminates when unused; Can not be enabled like a service.
		4 - Uses the command-line tool timedatectl to interact with it.

	# systemctl enable systemd-timesyncd.service	#Starts systemd-timesyncd service.
							^ Once it has started, it can be turned off
	# systemctl disable systemd-timesyncd.service	#Stops systemd-timesyncd service.
		
			or
	# systemctl start systemd-timedated.service	#Starts systemd-timedated service.

	# systemctl stop systemd-timedated.service	#Stops systemd-timedated service.


	VERY IMPORTANT: It may be necessary to set the timezone first!
	Note2.: Make sure time/date is correct by typing: $timedatectl

3 - USING HWCLOCK
	# hwclock --set --date='2024-01-22 04:43:05'	#Manually sets hardware clock to the given clock
	# hwclock --hctosys				#Sets hardware clock to system clock
	# hwclock --systohc				#Sets system clock to hardware clock
							^ Useful after using SYSTEMD-TIMESYNCD
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
LINUX EXECUTABLE BINARY FILE / ELF
In computing, the Executable and Linkable Format[2] (ELF, formerly named Extensible Linking Format), is a common standard file format for executable files, object code, shared libraries, and core dumps. 

The  header  file <elf.h> defines the format of ELF executable binary files.  Amongst these files are normal executable files, relocatable object files, core files, and shared objects.

An executable file using the ELF file format consists of an ELF header, followed by a program header table or a section  header table,  or both.  The ELF header is always at offset zero of the file.  The program header table and the section header table's offset in the file are defined in the ELF header.  The two tables describe the rest of the particularities of the file.

This header file describes the above mentioned headers as C structures and also includes structures for dynamic sections, relocation sections and symbol tables.

LINUX SHARED LIBRARY / DLL
	Under windows, Shared Libraries and Executable Binary Files were named .exe(executable) and .dll(dynamic linked library); Under linux, both are ELF Filetypes. Linux Shared Libraries are more commonly known as having an .so suffix, although they're just ELF Filetypes.

READING INFORMATION FROM ELF TYPES
	It's possible to use a GUI viewer/editor like 'xelfviewer' or use a CLI tool like 'elfedit' to edit it.
'readelf' is a part of GNU Tools on linux that can read elf data from files. 'objdump' is also capable of doing the same thing.

check 'tldr' tools for more info on those tools:
	example: $tldr objdump
		
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
CUSTOMIZING GRUB THEME
Grub themes can be customized by editing the following line in the following file:

	[file: /etc/default/grub]
		# Uncomment one of them for the gfx desired, a image background or a gfxtheme
		#GRUB_THEME="/usr/share/grub/themes/<theme_name>/theme.txt"
		GRUB_THEME="/boot/grub/themes/Archlinux/theme.txt"
	[/file]

The themes directories are usually located under /usr/share/grub/themes/,
and the theme configuration file is usually a single .txt file for each of the themes.

Some themes may install under a different folder in /boot/grub/themes/.

Once a theme has been chosen and set on /etc/default/grub file,
it is necessary to generate a new grub configuration file using the following tool:
	# grub-mkconfig -O /boot/grub/grub.cfg-newfile	
	^ Very Important: Avoid using the above to generate a new file on top of an existing one,
	^ since it will overwrite the current working grub configuration file with a non-configured file
	^ which might prevent you from booting up your computer.
	^ so don't use this if you already have a grub.cfg file like this:
	^ # grub-mkconfig -O /boot/grub/grub.cfg 	AVOID THIS IF YOU HAVE AN ALREADY WORKING LINUX GRUB, UNLESS IT'S REALLY NECESSARY!

The "-newfile" prefix helps preventing overwriting the old grub.cfg configuration file,
since it will still be needed on the last step.

The Final Step here is to use nvim to cut & paste content difference from grub.cfg-newfile into grub.cfg,
but before doing this it is recommended to make a backup copy of the old grub.cfg file:
	# cp /boot/grub/grub.cfg /boot/grub/grub.cfg-backup

Now use nvim to checkout differences in both files:
	# nvim -d /boot/grub/grub.cfg /boot/grub/grub.cfg-newfile
	
The file on the left panel is your current working grub configuration file, 
the file on the right panel is the file with the new theme,

Press ctrl+h(^h) and ctrl+l(^l) to move between left and right panels.
move to the right panel(^l) and check for '+' sign on far-left,

Enter visual-mode(^v) and select all text containing '+', then copy(y) it,
and finally move to the left-panel and paste(p or P) all the contents into the exact same location.

save(press 'esc', then ':' to go into command mode and finally 'w' and exit using either zZ or ':qa!'
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
MAKING SURE DATA HAS BEEN WRITTEN TO DISK
	Useful when compiling programs for debugging, make sure to use 'sync' to make sure
saved/modified data has been written to the disk. Because some filesystems/kernel feature may cache data in RAM for a while
before actually writing it to disk, in other words, 'sync' will ensure that debugger will not use an old version of a compiled executable
that was left unchanged on disk.
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
WHAT IS THE LINUX KERNEL
	A kernel is the core of an operating system, it is the one responsible for the hardware & software intermediation.
The kernel and the sofware make up the Operating System.
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
HOW TO COMPILE YOUR OWN LINUX KERNEL
	1. Download the kernel source
	2. Read the make file
	3. Type in: #make menuconfig
	--Incomplete instructions--
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
ENABLING VSYNC FOR NVIDIA

1. Install nvidia-utils and all drivers, then type: $nvidia-settings
2. Go to 'X Server Display Configuration'
3. Go to 'Advanced Settings'
4. Check 'Force Full Compositition Pipeline'
5. Click Apply

Note: It may be required to enable coolbits first!
Note: No need to restart the computer, but if you want this settings set permanently,
the go to your ~/.nvidia-settings.rc file and add it there, either that or /etc/X11/xorg.conf.d/20-nvidia.conf,
just check nvidia documentation first!
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
PROCESS(RUNNING APPLICATION OR PROGRAM) MANAGEMENT AND CONTROL
	On linux there are several tools for controling and managing processes,
these are the few ones among them: htop, glances, kill, killall, jobs, bg, fg, pidof, disown, ps, pmap.

1 - ps: allows you to look at running and idle processes.
	$ ps aux					#Lists all running processes
	$ ps auxww				#Lists all running processes including arguments 
					 	^ that might have been passed for execution.
	$ ps aux | grep -p "process_name" 	#Lists all processes that have a given name
	$ ps --user $(id -u) -F			#Lists all processes of current user in extra full format
	$ ps --user $(id -u) f			#Lists all processes of current user in binary tree format
	$ ps --sort size				#Sorts processes by memory consuption
	$ ps -o ppid= -p <pid>			#Gets the parent PID of a given process

2 - disown: allows the current shell to disown a given process.
    $ disown 					#Disown the current job
    $ disown %<job_number> 			#Disown a specific job
    						^ use 'jobs' to list jobs by number
    $ disown -a  				#Disown all jobs
    $ disown -h %job_number 			#Keep job (do not disown it), but mark it so that 
    						^ no future SIGHUP is received on shell exit:

    note: this allows the process to keep running regardless if the current shell has been closed or not.

    VERY IMPORTANT: 'nohup' allows programs to live beyond the terminal that it's being launched from
    from the moment it's launched. HOWEVER, All that nohup does is make the process impervious to the terminal closing down and    unlike disown, 'nohup' doesn’t turn the process into a background task.

3 - jobs: lists all jobs that may have been started or suspended by a given shell.
    $ jobs  					#View jobs spawned by the current shell
    $ jobs -l  					#List jobs and their process IDs
    $ jobs -n  					#Display information about jobs with changed status
    $ jobs -p  					#Display process ID of process group leader
    $ jobs -r  					#Display running processes
    $ jobs -s  					#Display stopped processes

    note: use 'bg' to restart a given 'sleeping' process in the background or 'fg' to restart it as a foreground process.

4 - bg: resumes a given process as a background process
    $ bg						#Resumes process as a background process
    $ bg %<job id>				#Resumes given process as background process

5 - fg: restarts a given process as a foreground process
    $ fg 					#Resumes given process as foreground process
    $ fg %<job id>				#Resumes given process as foreground process

6 - pidof: retrieves the process id of a given executing program
    $ pidof bash  				#List all process IDs with given name:
    $ pidof -s bash  				#List a single process ID with given name:
    $ pidof -x script.py  			#List process IDs including scripts with given name:
    # kill $(pidof name)  			#Kill all processes with given name:

note: both 'bg' and 'fg' operate on the first process that has been set to slept as an actual stack.

7 - glances and htop: process managers in on itself, however they're not standard linux tools
and need to be installed separately.


8 - pmap: used to report memory map of a process
The pmap tool reads information from RAM  for a given Process ID(PID) and 
displays the results in the terminal window. 


The kernel exposes a lot of what it is doing with RAM through two pseudo-files in the “/proc” system 
information pseudo-filesystem. There are two files per process, named for the process ID or PID of each process: 
“/proc/maps” and “/proc//smaps.”

9 - pv and progress: displays progress bar for commands that normally don't have a progress bar. non-standard linux tools.

10 - crontabs: process scheduling. non-standard linux tools.

11 - nohup: allows process to live beyond the terminal.

-----------------------------------------
CONTROLLING SHELL PROCESSES AT RUNTIME
	When running a program on a terminal, it's possible to signal processes for control:

1 - Ctrl+C: Sends SIGINT, signal 2, to the process—if it is accepting input—and tells it to terminate.
2 - Ctrl+D: Sends SISQUIT, signal 3, to the process—if it is accepting input—and tells it to quit.
3 - Ctrl+Z: Sends SIGSTP, signal 20, to the process and tells it to stop (suspend) and become a background process

-----------------------------------------
FORCE SHUTDOWN FROZEN PROCESSES
	It's possible to either use 'xkill', given you haved it installed.
or do it manually using killall:
	
	1 - Killing all process tree using a given process name:
		# killall <process_name>

	2 - Killing a single process by it's ID:
		# kill -SIGKILL <process_id>
	
	3 - Kills all processes meeting the regex criteria:
		# killall -s KILL -r <regex_string>	

-----------------------------------------
SUSPENDING & PAUSING PROGRAMS(PROCESSES) AND RESTARTING THEM
1. For suspending a process, you can do in more than 1 way:
	1.1.0 - # killall -TSTP <process_name>
		or
	1.1.1 - # killall -STP <process_name>
		or
	1.1.2 - # kill -TSTP <PID>
		or
	1.1.3 - # kill -STOP <PID>
		or
	1.1.4 - ctrl+z  or ^z after executing the program

	note: the program must be running in order to be suspended!
	note2: -TSTP is preferred over -STOP because some programs may be able to handle it better!

2. Listing all available suspended jobs:
	2.1 - Type in: 
		$ jobs
		  or
		$ jobs -l
		^ displays suspended jobs along with their PIDs.

3. Restarting / Unsuspending a job:
	3.1 - $ fg %<job number>
	      ^you can list job number by typing: $jobs
		or
	3.2 - # kill -CONT <PID>
		or
	3.3 - # killall -CONT <process_name>
	
	note.: about 3.1, it'll only display & restart jobs suspended on the same shell/terminal/vt
	^ this means that jobs suspended on different shells/terminal/vt will not be listed.
-------------------------------
FETCHING PIDS ON A LINE-PER-LINE STYLE
	'pgrep' makes possible to feed another commandline tool with a list of PIDs,
	wherareas 'pidof' retrieves a list in a single entire line.

	1 - $ pgrep <process name>

-------------------------------
RETRIEVING PIDS IN A SINGLE ENTIRE LINE
	1 - $ pidof <process name>

-------------------------------
LISTING A PROCESS MEMORY MAP
	It's possible to use 'pmap' for listing a process memory map using the PID of a given process.

	1 - First, fetch the PID(Process id of the desired process)
		$ pidof firefox

	2 - Use pmap
		$ pmap <PID>

Also, with this method it's possible to find out which linuix and non-linux libraries a program relies on.
	1 - Type in:
		$ pmap <PID> | grep ".so"

-------------------------------
LISTING A PROCESS WORKING DIRECTORY
	It's possible to retrieve the working directory of a process by using 'pwdx'
	1 - $ pwdx <PID>
		or
	2 - # pwdx $(pgrep firefox)
-------------------------------
LISTING OPEN FILES AND THEIR RELATED PROCESS
	'lsof' is a tool that allows to list open-files that are being used by current running processes. - VERY IMPORTANT!

    # lsof path/to/file				#Find the processes that have a given file open
    # lsof -i :port				#Find the process that opened a local internet port
    						^ you can find out ports on use through a firewall like 'gufw'.
    # lsof -t path/to/file			#Only output the process ID (PID)
    # lsof -u username				#List files opened by the given user
    # lsof -c process_or_command_name		#List files opened by the given command or process
    # lsof -p PID				#List files opened by a specific process, given its PID
    # lsof +D path/to/directory			#List open files in a directory
    # lsof -i6TCP:port -sTCP:LISTEN -n -P 	#Find the process that is listening on a local IPv6 TCP port 
    						^ and don't convert network or port numbers

note: info extracted from 'tldr'
-------------------------------
OTHER TOOLS:
There are other tools, but not as important: pkill, pwait
they are very similar to kill and killall, but have more options and accept more arguments.

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
WHAT IS DBUS
	Dbus allows interprocess communication.
In practice, it allows programs to talk to one another.

INSTALLING DBUS-BROKER FOR PERFORMANCE
D-Bus is a message bus system that provides an easy way for inter-process communication. It consists of a daemon, which can be run both system-wide and for each user session, and a set of libraries to allow applications to use D-Bus. 

dbus-broker is a drop-in replacement for the libdbus reference implementation, which aims "to provide high performance and reliability, while keeping compatibility to the D-Bus reference implementation". 

Installation steps:
	1 - Download and Install it
		# pacman -S dbus-broker
	
	2 - To enable dbus-broker as system bus		
		# systemctl enable dbus-broker.service

	3(Optional) - Enables dbus-broker globally(all users):
		# systemctl --global enable dbus-broker.service

	4(Optional) - Enables dbus-broker for Local user only:
		# systemctl --user enable dbus-broker.service

	5 - Reboot system

MORE INFO:
	https://wiki.archlinux.org/title/Dbus-broker
	https://github.com/bus1/dbus-broker/wiki

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
LISTING DETAILED NVIDIA INFORMATION
	use nvidia-smi

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
SETTING UP SYSTEM(GLOBAL) AND USER SERVICES USING SYSTEMD

	1(Optional) - Enables dbus-broker globally(all users):
		# systemctl --global enable dbus-broker.service

			or

	2(Optional) - Enables dbus-broker for Local user only:
		# systemctl --user enable dbus-broker.service

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
FINDING OUT PACKAGE OWNER OF A GIVEN FILE
	Archlinux package manager allows to track which file/directory belongs to which packages.

1 - Feed pacman filename/directory:
	$ pacman -Qo <filename/directory>

------
LISTING ALL FILES OWNED BY A GIVEN PACKAGE
	Archlinux package manager allows to list all files created/managed by a given package.

1 - Feed pacman package name:
	$ pacman -Ql <package_name>


^ This is helpful to find out how to execute certain binary tools that come installed with certain packages,
for example, clamav package comes with several tools for setting up, updating and managing clamav anti-virus
under different names; normally, it would be impossible to track down to those files without a manual or tutorial.

By listing package files it's easy to know which files do what for a certain package,
since executable binary files go to /usr/bin/
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
NVIDIA NIS (NVIDIA IMAGE SCALER)
	Using NIS is a quick way to get less frame-time(which is better).
Using NIS on linux requires dkms proprietary drivers installed.

	1. Go to Nvidia Control Panel.
	2. Go to 'Application Profiles' tab.
	3. Enable 'Application Profiles', by click on the Checkbox.
	3. Add new profile, by clicking on '+'.
	4. Type a profile name of "Nvidia NIS".
	5. Enable Sharppening and set both new options to 100% using the slide.
	6. Finally, go to 'Rules' tab and add a new rule for a given process name,
		6.1 - you can use htop, glances, pidof to find out a given process name.
		6.2 - Select the previous Profile created on step 4.
		6.3 - Click on '+' to add our newly created rule.
	7(Optional) - Optionally you can save to disk all settings created so far.

ENABLING NVIDIA MULTITHREAD OPENGL
	This Environment Variable Enables OpenGL Multithreading for Nvidia GPUs in /etc/environment file
		__GL_THREADED_OPTIMIZATION=1
		^ You can simply add this to /etc/environment file

this makes not only games faster, but also Desktop Environments and Display Managers like SDDM, since they 
do not rely on vulkan for rendering.

	VERY IMPORTANT: Please note that this optimization will only work if the target application dynamically links against pthreads.
If this isn't the case, the dynamic loader can be instructed to do so at runtime by setting the LD_PRELOAD environment variable to include the pthreads
library.

	for example || NVIDIA OPENGL PERFORMANCE TWEAK:
		env LD_PRELOAD="libpthread.so.0 libGL.so.1" __GL_THREADED_OPTIMIZATIONS=1 <application/game>


		Furtherover, when launching programs you can use:
			__GL_SHADER_DISK_CACHE=1
		^ By default, caches are stored in $HOME/.nv/GLCache. Caches are persistent across runs of an application. Cached shader binaries are specific to each driver version; changing driver versions will cause binaries to be recompiled.

		Source: http://us.download.nvidia.com/XFree86/Linux-x86_64/319.32/README/openglenvvariables.html
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
CHANGING MAKEPKG.CONF FOR BUILDING APPLICATIONS FROM SCRATCH:
	/etc/makepkg.conf is a file owned by pacman, which is the default archlinux package manager tool.
yay manager also uses that file.

	0(optional) - read gcc manual if necessary: 
		$ man gcc
	1 - sudo edit the following file: /etc/makepkg.conf
	2 - make the following chage to -march and -mtunes
		-march=ivybridge -mtune=ivybridge
	
VERY IMPORTANT: Make sure your CPU Type is an Ivybridge first!

source: https://man.archlinux.org/man/makepkg.conf.5 			#MAN PAGE 

----
LISTING OPTIMIZATION FLAGS FOR GCC THAT CAN BE USED IN MAKEPKG.CONF
	Type in: $ gcc --help=optimizers
	
----
LISTING WHICH OPTIMIZATION FLAGS ARE ENABLED AT WHICH LEVEL
	1. Listing optimization flags at -Oi level:
		 $ gcc -c -Q -Oi --help=optimizers
			or
		 $ gcc -c -Q -O2 --help=optimizers

	2. Alternate(NEWER):
		2.1 - Listing all flags enabled on Optimization Level 3: 
			$ gcc -c -Q -O3 --help=optimizers | grep enabled | less

	VERY HELPFUL SOURCE: 
		https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/
		https://gcc.gnu.org/onlinedocs/gcc-13.1.0/gcc/Optimize-Options.html
		https://developers.redhat.com/blog/2018/03/21/compiler-and-linker-flags-gcc
		---
		https://linuxreviews.org/Optimized_gcc_compiling_(cflags/cxxflags,_distcc,_ccache)  
		  ^ This link is outdated and umaintained!!
----
RECOMMENDED CFLAGS
-pipe speeds up the compilation process (no gain at runtime)

-fomit-frame-pointer makes programs faster at runtime, but makes debugging impossible on i686. -O turns on -fomit-frame-pointer on machines where doing so does not interfere with debugging. x86 processors need the frame pointer for debugging, so -fomit-frame-pointer is not turned on by default.

-m3dnow -msse -mfpmath=sse -mmmx is generally not safe, required or desired. It's an optimisation analagous to -ffast-math, in that programs that are expecting 80-bit results may be confused (sse can only do 64-bit precision math). -mfpmath=sse is only the default on x86_64, not on x86. (James Harlow)

-funroll-loop will give you a bigger executable that may or may  not make it a tad faster (DON'T USE THIS)
^ -O3 automatically enables funroll-loop and it's related/associated flags!(NOT DESIRABLE)
^ in this case, and only in this case, funroll-loop is benefical and won't have any issues, becausew
^ -O3 ensures all flags that benefit funroll-loop is actually enabled too as well.

-O2 -O3 Various grades of optimization. O3 is recommended for most CPUs. NOTE: It's the letter O, not the number 0

-Os This makes the executable as small as possible, but does not optimize for CPU performance. This is a flag worth considering if you are still using a very slow HDD. It used to be a really good option in the early 2000s.

The file $HOME/.bashrc is a good place to put a CFLAGS= variable. 

source: 
		https://linuxreviews.org/Optimized_gcc_compiling_(cflags/cxxflags,_distcc,_ccache)  
		  ^ This link is outdated and umaintained!!

-----
FINDING OUT WHICH OPTIMIZATION FLAGS A BINARY HAS BEEN COMPILED WITH IN C++
	1. type in terminal:
		$readelf -wi <binary> | grep DW_AT_producer
		^ this requires the binary to be compiled with -g options for debugging
	
	2. (Alternative):
		2.1 - Compile C++ program with this flag:
			-frecord-gcc-switches
		2.2 - Type in terminal:
			$readelf -p .GCC.command.line a.out 
-----
MY CUSTOM MAKEPKG.CONF CFLAG AND CXXFLAG CHOICE
	These flags below will also work as CXXFLAG! 

	Beware, some programs may not accept some flags at all, so always check the PKGBUILD before building the program
	with any of these:

		CUSTOM: -mcpu=ivybridge -march=ivybridge -mtune=ivybridge -fomit-frame-pointer -O3

		^ CFLAGS and CXXFLAGS
		^ -mfpu=auto may cause some programs to not compile, since this is not an x86_64 compile flag.
		^ remove it manually!
		^ In order to check which flags/options are supported by your CPU, type "man gcc" and search for 'x86' string.
		^ proton-ge-custom doesn't accept "-fno-plt", remove it!!!
		^ always check packages on AUR package website because some options like "-fno-plt" may have to be disabled!

	NOTE: -mfpu=auto is the default attribute and sets up hardware float-point arithmetics by honoring what's set on
	-mcpu and -march; It only exists here, since some PKGBUILDs may change it to NEON, which does only work with
	-funssafe-math-optimizations which in turn is bad very bad. NEEDS TO CHECK: Is -mfpu honored as mentioned
	even when it's absent as a compile flag before compiling.

	NOTE2: using -ffast-math is not advisable for programs that rely on IEEE or ISO Standards. BUT it will make compiled
	code faster!

	NOTE3: -fno-plt is disabled by wine-ge!

	VERY IMPORTANT: -O3 automatically gives you -fomit-frame-pointer and -funroll-loops and it's related flags as well.
	^ while -fomit-frame-pointer is interesting, -funroll-loops and it's related flags aren't and may make programs slower.

	VERY IMPORTANT 2: ^ -O3 automatically enables funroll-loop and it's related/associated flags!(NOT DESIRABLE)
	^ in this case, and only in this case, funroll-loop is benefical and won't have any issues, becausew
	^ -O3 ensures all flags that benefit funroll-loop is actually enabled too as well.

	VERY IMPORTANT 3: If you ever compile a core package like gcc using -march, -mcpu or -mtunes, 
	booting up your PC is gonna require owning a CPU of such architecture, otherwise system boot up will fail.
	It's important to understand how to recover your system from an archlinux installation media. If you don't have 
	an Archlinux Installation Media or don't understand how to recover a broken archlinux system from installation media
	enough, avoid compiling core package tools for optimization, since you'll likely need to re-install these packages
	in the future.

	VERY IMPORTANT 4: Before installing any -git packages, make sure that users on it's Arch/AUR package webpage
	reports no problems with the package itself, or else installation may break your system in the case of compiling 
	a core package tool like gcc-git.

	VERY IMPORTANT 5: If the program you're compiling is important, you may want to add it to the IgnorePkg line
	on /etc/pacman.conf to avoid future updates, since avoiding updates may help with system stabilization, 
	always remember to check VERY IMPORTANT #4 before doing their updates.

	VERY IMPORTANT 6: If you ever add a bad/wrong flag, remember to remove it from /etc/makepkg.conf and also
	from any environment variable(s) you might've used!!!
-----
RUST FLAG TUTORIAL
	poor example, don't just copy and paste: 
		RUSTFLAGS="-C opt-level=3 -C target-cpu=ivybridge"

	^rustc is the default rustc compiler on most systems.
	^works for proton-ge-custom 
-----
CMAKE FLAG TUTORIAL
          -DCMAKE_CXX_FLAGS="$CXXFLAGS -mcpu=ivybridge -march=ivybridge -mtune=ivybridge -fomit-frame-pointer -O3 -pipe" \

	  -DCMAKE_C_FLAGS="$CFLAGS -mcpu=ivybridge -march=ivybridge -mtune=ivybridge -fomit-frame-pointer -O3 -pipe" \

	^ This should be used with cmake

	VERY IMPORTANT: -O3 automatically gives you -fomit-frame-pointer and -funroll-loops and it's related flags as well.
	^ while -fomit-frame-pointer is interesting, -funroll-loops and it's related flags aren't and may make programs slower.

	VERY IMPORTANT 2: ^ -O3 automatically enables funroll-loop and it's related/associated flags!(NOT DESIRABLE)
	^ in this case, and only in this case, funroll-loop is benefical and won't have any issues, becausew
	^ -O3 ensures all flags that benefit funroll-loop is actually enabled too as well.

Example:
	[file: ~/.cache/yay/gzdoom-git/PKGBUILD ]

	    cmake -B build \
		  -D CMAKE_BUILD_TYPE=Release \
		  -D CMAKE_CXX_FLAGS="$CXXFLAGS -mcpu=ivybridge -march=ivybridge -mtune=ivybridge -fomit-frame-pointer -O3 -pipe -fno-plt -ffile-prefix-map=\"$PWD\"=. -DSHARE_DIR=\\\"/usr/share/gzdoom\\\"" \
		  -D CMAKE_INSTALL_PREFIX=/usr \
		  -D SYSTEMINSTALL=ON \
		  -D INSTALL_PK3_PATH=share/gzdoom \
		  -D INSTALL_SOUNDFONT_PATH=share/gzdoom \
		  -D INSTALL_RPATH=/usr/lib \
		  -D DYN_GTK=OFF \
		  -D DYN_OPENAL=OFF
	    make -C build

	[/file]
-----
IMPROVING PYTHON PACKAGES PERFORMANCE: 
TODO:
	Explain how to optmize python based packages by installing nuitka3 and using nuitka3-run instead.

VERY IMPORTANT: PYPY CAN'T BE USED FOR COMPILATION!!!! SO THE FOLLOWING INSTRUCTIONS WON'T WORK!
		^ USE NUITKA3-RUN instead on the PKGBUILD script for the Python made package.
		^ By intalling nuitka package!

DO NOT USE THIS BELOW!!!:
In order to optmize python based packages, it's possible to use 'pypy'(OUTDATED):
	1 - Install pypy
		# pacman -S pypy3

	2 - Enable pip mode for pypy
		$ pypy3 -m ensurepip

		2.1 - Upgrade pypy3's PIP
			$ pypy4 -m pip install --upgrade pip

	3  - Install desired package
		$ pypy3 -m pip install <package_name>

		ex.: $pypy3 -m pip install qutebrowser

	4 - Change directory to:
		$ cd ~/.local/bin/

	5 - Execute installed program:
		$ ./<program_name>

	6(Optional) - Install required packages for running program upon failure:
		# pacman -S <required package>

		6.1 - Try step 5 again.
-----
HOW TO COMPILE YOUR PROGRAMS THROUGH YAY AND PACMAN FOR PERFORMANCE GAINS
	Compiling for performance mostly only works for C/C++ programs,
and only if they receive cpu and compiler optimization flags before being compiled.

Without cpu optimization flags, performance gains will be unnoticeable. 

	1 - Instruct pacman/yay to download your git package
		ex.: #yay -S terminology-git
	2 - Yay/Pacman will ask you to cleanBuild the package from scratch
		ex.: Packages to Clean Build? 
		^ Answer [A]ll if you've installed the package before and want to recompile it again,
		^ This is the default behavior ONLY if you haven't installed the package before. - CHECK IT LATER.
		^ If a Build already exists, it defaults to [N]one. - CHECK IT LATER

	3 - Yay/Pacman will ask you if you want to see differences
		ex.: Diffs to show?
		^ Answer [A]ll if you want to set custom options
		^ Text Editor defaults to vi in read only mode.
		^ If you haven't set a non-default text-editor option for pacman that has RW(Read-Write) modes,
		^ then you need to follow Step #4.

	4(Optional) - At the #3 screen/text, you must edit the PKGBUILD Manually
		4.1 - For Yay, go to ~/.cache/yay/terminology-git in this case
			4.1.1 - edit the PKGBUILD MANUALLY by following Step #5 instructions
		4.2 - For pacman, go to /var/cache/pacman/pkg/<package>-git/ 
			4.2.1 - edit the PKGBUILD MANUALLY by following Step #5 instructions
	
	5 - Look for CFLAG, CXXFLAG or CMAKE and add the following flags:
		-mcpu=ivybridge -march=ivybridge -mtune=ivybridge -mfpu=auto -fomit-frame-pointer -O3 -fno-plt
		#If necessary, follow the 'CMAKE FLAG TUTORIAL'

	6 - Save the file and go back to screen on Step #3 to begin compilation & installation 
	^ VERY IMPORTANT: Make sure you have enough Hard Disk Space, since compilation uses a lot of Space on Disk.

	7 - Once it's finished you may choose to keep or delete the cache for the package described on #4.
	
	8 - VERY IMPORTANT: Once installation is finished the PKGBUILD will return to it's default options,
	so if you ever need to compile it again, you'll need to edit the PKGBUILD file again 
	by following steps #1 to #8 all over again.


--------------------------------------------------
SOMETHING ABOUT COMPILE, DEBUG, TEST AND RELEASE
	Avoid using -O1, -O2 and -O3 altogether, use -O0 instead since the other three
options make debugging hard and impossible.
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
OPTIMAL CUSTOM PACMAN.CONF CONFIGURATION

[file: /etc/pacman.conf]
	# Misc options
	#UseSyslog
	Color
	#NoProgressBar
	CheckSpace
	VerbosePkgLists
	ParallelDownloads = 3
[/file]

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
PACKAGE MANAGER LOG || THE IMPORTANCE OF /VAR/LOG/PACMAN.LOG
	The importance of  pacman.log is that systems can stop working due to broken updated packages on the system.
The only way to fix it  is to identify the problem by checking which packages have been recently updated.

Keep in mind that only recent updates can break the system, since the system was fine after doing the older update.

Kernel, Video Drivers, Xorg packages can be always the culprit, although other packages can get on the way like 'glibc' or 'libc'.
After an update, if the system breaks, it's always worth checking to see if at least one of them got upgraded within the update.

2nd reason is, sometimes you might install a software you might forget the name. /var/log/pacman.log will keep it there,
you just have to date search more or less when you have installed the program.

--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
WHAT TO DO WHEN PACMAN BREAKS || INSTALLING PACKAGES WITHOUT PACMAN
	Sometimes it can happen that pacman stop working, however it's still possible to install packages manually,
hopefully you have at least one or more web browsers and a few other programs still working when that happen.

1 - If this is the case, you can easily grab packages you need on the 'ALA Repository':
	https://archive.archlinux.org/

2 - Check the contents of  the package using tar, make sure where they need to be copied:
	tar vtf package_name.tar.zst

3 - If all of the listed files and directories contain absolute paths that relate to the root directory, you can easily install packages as this:
	# tar xvf package_name.pkg.tar.zst --directory=/
		or
	# tar xvf package_name.pkg.tar.zst -C /
	^ running tar as a super user will help preserving ownership and permissions,
	^ running tar as the regular user, will change ownership to that of the user extracting those  files, keeping only permission set.
	^ since it's a system package, we want to preserve both ownership and permissions meant for the package.
--------------------------------------------------------------------------------
--------------------------------------------------
--------------------------------------------------
INSTALLING POSSIBLE MISSING FIRMWARES
	Follow the below steps to succesfully install possible missing firmwares on your system,
you can see missing firmwares when building initramfs by typing "#mkinitcpio -P" at the terminal.

	1 - Have Yay Package Manager(Gives you access to AUR Repository) Installed in your system

	2 - Type at the terminal:
		$yay -S mkinitcpio-firmware

	3 - Once it finishes installation, rebuild initramfs:
		#mkinitcpio -P

Portable Hard Drives from Western Digital may require wd719x-fimware installed,
which is already available on mkinitcpio-firmware.

VERY IMPORTANT: 
	having one or more firmwares not installed in your system may causes performances hitches.
	installing mkinitcpio-firmware may cause initramfs to be larger, but is still more optimal
	in case you're getting 'missing firmware' messages.

Helpful Sources:
	https://wiki.archlinux.org/title/Mkinitcpio#Possibly_missing_firmware_for_module_XXXX



--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
FEDORA PACKAGE MANAGER | RPM, DNF AND YUM PACKAGE MANAGER
	Fedora comes with 2 package managers, one is called 'DNF' and the other is called
'RPM' here is a few commands for both:

note.: YUM Package Manager is very similar to DNF
this is also the same package managers for Redhat, CentOS, Scientific Linux.
---
DNF COMMANDS
	DNF is the next upcoming major version of YUM package manager.

$ dnf --help | less		##Displays available DNF commands

# dnf upgrade			##Updates packages
# dnf distro-sync		##Updates packages
# dnf upgrade-minimal		##Upgrades only packages that have been affected by some problem
				^ Equivalent to updating packages that have only modified it's 'pkgrel'
				ex.: package_name 1.2.3-2 would get updated with this, but not package_name 2.3-1.


# dnf install <package_name>	##Installs the desired given package
# dnf remove <package_name>	##Uninstalls/Remove desired given package
# dnf clean			##Cleans Cache

$ dnf search <string>		##Searchers for given string on package database

$ dnf history			#Displays dnf's usage history
$ dnf info <package_name>	#Returns info on current package

---
---
RPM COMMANDS
	RPM is the official Package Manager for RPM-based distributions like Fedora Linux.

$ rpm --query <string>			##Queries/Searches local packages
$ rpm --query --list <package_name>	##Lists package owned files
					^ Same as 'pacman -Ql'
$ rpm --query --file <filename>		##Finds owner package of a given file
$ rpm --verify --all <package_name>	##Verifies package integrity
					^ Same as 'pacman -Qk'

$ rpm --query --changelog <package_name> ##Displays the changelog for a given package


-------------------
-------------------
FEDORA FUSION REPOSITORY
	Fedora's Fusion Repository is similar to Archlinux User Repository(AUR),
it has to be installed and enabled on the system separetely, since it doesn't come ready out-of-box.

1 - Enabling Free Repository:
	# dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm

2 - Enabling Non-Free Repository:
	# dnf install https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm

Source: 
	https://docs.fedoraproject.org/en-US/quick-docs/setup_rpmfusion/
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------
GENTOO PACKAGE MANAGER
source: https://wiki.gentoo.org/wiki/Binary_package_guide#Creating_binary_packages
	https://en.wikipedia.org/wiki/Portage_(software)

About:
	Gentoo always requires packages to be compiled, in other words compiling is mandatory.
	use Funtoo is a fork that provides pre-compiled packages.
	
	Only a few packages can be installed directly from binaries, like firefox:
		# emerge --ask firefox --autounmask-continue

Protips:
	1 - Install 'resolve-march-native' package
		# emerge --ask resolve-march-native
	2 - Run it: $resolve-march-native
	3 - set compile flags on: /etc/portage/make.conf
	4 - read if necessary: MY CUSTOM MAKEPKG.CONF CFLAG AND CXXFLAG CHOICE

EMERGE COMMANDS: 

# emerge --update sys-apps/portage			#Updates all portage tools - VERY IMPORTANT

# emerge --list-sets					#Displays the list of package sets
# emerge --update <package_set>				#Updates a given package set
							^ ex.: emerge --update [@world | system]
							^ using #emerge -avDuN <package_set> might be the preferred
							^ way of updating system!

# emerge --sync						#Syncs database | MANDATORY
# glsa-check -t all					#Checks security issues throught the current gentoo system
# glsa-check -f all					#Checks AND Fixes all security issues on the current gentoo system

# emerge -avDuN @world					#Updates entire system
							^ /var/lib/portage/ contains @world package list
							^ after world updates, '#emerge --depclean' is required.

#emerge --depclean					#Cleans unused dependencies
							^ VERY IMPORTANT: should be used after world updates
# emerge --clean						#Cleans old packages in case they have been replaced by newer ones

# emerge --ask <package_name>				#Installs a given package
							^ Installing new packages may require the use of 'etc-update' or
							^ 'dispatch-conf'.
# emerge --ask --noreplace <package_name>		#Installs given package while avoiding replacing packages that
							^ already exist in the system.

# emerge --config					#Run  package specific actions needed to be executed 
							^ after the emerge process has completed.
							^ VERY IMPORTANT

# emerge --resume 					#Resumes the most recent merge list that has been aborted 
							^ due to an error.
							^ VERY IMPORTANT.

# emerge --ask --autounmask-continue <package_name>	#Auto-unmasks

# emerge --cav <package_name> 				#Removes(SAFE!) package only if no other packages depend on it
							^ -c(--depclean), -a(--ask), -v(--version).

# emerge --unmerge --ask <package_name>			#Removes/Uninstall a given package WITHOUT checking dependencies
							^ UNSAFE
							^ --unmerge <-> -C

# emerge --ask --verbose --depclean <package_name>	#Removes/Uninstall package by checking if it has any dependency first
							^ --depclean <-> -c

# emerge --ask --usepkg <package_name>			#This allows package installation from it's compiled binary, ONLY if 
# emerge --ask --usepkgonly <package_name>		^ it exists on the local machine. 1st one IS -k, 2nd IS -K

# emerge --ask --buildpkgonly <package_name>		#Builds package only without merging/installing them,
							^ Built packages are stored on PKGDIR.

# emerge --getbinpkg <package_name>			#Fetches binary package for building/installation????
							^ this option implies -k(--usepkg).
							Requires: FEATURES="getbinpkg" to be set on /etc/portage/make.conf

# emerge --ask --getbinpkgonly <package_name>		#Same as above, but binaries from the remote server are 
	or						^ preferred over local packages if they are not identical.
# emerge --ask -gK
	or
# emerge --ask -G

# emerge --searchdesc <description or package_name>	#Searches description and package names in the online repository
# emerge -S						# Same as above, --fuzzy-search can be used to allow non-exact
							^searches to be performed.

# emerge --search <package_name>				#Searches available packages in the online repository
# emerge -s <package_name>				#Searches available packages in the online repository


$ equery d <package_name>				#Lists all package dependencies
$ eselect news read					#???

# fixpackages 						#???

# dispatch-conf						#Allows Configuration Files to be updated
							^ This exists due to the nature of emerge package manager of
							protecting configuration files that may be overwritten by
							new installed updates. read: $man emerge | 
							^ Search for CONFIGURATION FILES

# etc-update						#Same purpose as dispatch-conf
							^ read: $man emerge
							^ Search for CONFIGURATION FILES

# eclean-dist						#Clears gentoo's source code package cache as well as object files.
# eclean-dist -dp					#Pretends to clear gentoo's source code package cache
							^ VERY IMPORTANT

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
DEBIAN PACKAGE MANAGER
	Debian, Ubuntu, Kali Linux package manager's choice.
	
APT COMMANDS:

# apt update						#Updates package information from all configure source
							^ Similar to pacman -Sy

# apt upgrade						#Upgrades all locally installed packages
							^ Similar to pacman -Su

# apt full-upgrade					#Fully upgrades the system, removing conflicting packages if necessary

# apt < install | reinstall | remove | purge > 		#Installs/Reinstalls/Removes/Purges packages from system

$ apt search <package_name>				#Searches for given package name on the remote repos

$ apt list 						#Returns a list of packages available on the remote

$ apt show <package_name>				#Displays information about a given named package
							^ Similar to pacman -Si

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
FREEBSD PACKAGE MANAGER
	Works similarly to the Debian Package Manager, using 'pkg' instead of 'apt'.
Read more on: DEBIAN PACKAGE MANAGER

FREEBSD TIPS AND HINTS:
*sddm and slim is available as login manager
*also relies on xorg and wayland
*virtual machine images already available from the official website:
	https://www.freebsd.org/where/
*Running a service like sddm: 
	$service sddm onestart
		or
	$service sddm start
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
XBPS PACKAGE MANAGER
	This is the default package manager for Void Linux.

	ADVICE FOR USING VOID LINUX ON DISTROBOX:
		Talk about using void linux with musl and installing pipewire with xbps-install pipewire to have audio on the system.

	SOURCE:
		https://docs.voidlinux.org/xbps/repositories/index.html					#XBPS Extra Repositories
		https://docs.voidlinux.org/config/graphical-session/graphics-drivers/nvidia.html	#NVIDIA DRIVERS


	INSTALL COMMANDS:
		xbps-install -Syu

		xbps-install -S void-repo-nonfree void-repo-multilib void-repo-multilib-nonfree
		xbps-install -S steam
		xbps-install -S libgcc-32bit libstdc++-32bit libdrm-32bit libglvnd-32bit
		(Optional?) xbps-install -S nvidia nvidia-libs-32bit
		(Optional) xbps-install -S xtools


	QUERY COMMANDS:
			'xbps-query' usually performs on local package database, when -R is passed it'll check the online
		repositories.

		$ xbps-query -Rs <package>			#Searches for package in the online repositories
								^ Same as 'pacman -sS'

		# xbps-query --install <search_pattern>		
		# xbps-query -Syu				#System Update
								^ VERY IMPORTANT - For doing system updates on distrobox,
								use this instead: $ distrobox-upgrade --all

		$ xbps-query -l					#List installed packages and gives status on each:
								    ii - Package is installed.
								    uu - Package is unpacked but must be configured with 
									^ xbps-reconfigure(1).
								    hr - Package is half-removed, and can be fully 
									^ removed with xbps-remove(1).

								    ?? - Package state is unknown.

		$ xbps-query -m					#Lists user installed packages

		$ xbps-query -f <package_name>			#Lists files provided by a given package
								^ Same as 'pacman -Ql'
		$ xbps-query -s <package_name>			#Searches for a given package name
								^ Same as 'pacman -Qs'
		$ xbps-query -S					#Shows package information
								^ Same as 'pacman -Qi'

		$ xbps-query -O					#Lists orphaned packages
								^ packages that were installed as dependencies 
								^ and no package is currently depending on them directly.
		
	REMOVE COMMANDS:
		# xbps-remove <package_name>			#Removes package
		# xbps-remove -yO				#Cleans cache
		# xbps-remove -yo				#Removes orphaned packages
								^ Same as "pacman -Rns $(pacman -Qtdq)"
	EXTRA:
		# vkpurge rm all				#Removes old installed versions of Linux Kernel from the system
								^ Don't use it if you're on distrobox.


	ABOUT REPOSITORIES(2024):
		Void linux supports the following repositories:
		    1 - nonfree: contains software packages with non-free licenses
		    2 - multilib: contains 32-bit libraries for 64-bit systems (glibc only)
		    3 - multilib/nonfree: contains non-free multilib packages
		    4 - debug: contains debugging symbols for packages
		
		It's important to note some good ammmount of known software can only be installed through 
		'multilib' and 'multilib/nonfree', however only GLIBC Void Linux
		installations can use them.

		If you still desire to install some of these software on void linux, you'll need to compile them
		from the void source package repository:
			read more on: INSTALLING PROGRAMS FROM SOURCE CODE(VOID LINUX ONLY)


		xbps-install -S void-repo-nonfree void-repo-multilib void-repo-multilib-nonfree

	INSTALLING PROGRAMS FROM SOURCE CODE(VOID LINUX ONLY):
		There's a script called 'xbps-src' that can install source package from Void's Github Repository.
	of course, you'll need to have 'git' installed as well on your system.

	finally 'git clone https://github.com/void-linux/void-packages.git' the void packages into some directory of your
	choice.

	Now allow restricted packages to be installed on the system:
		echo XBPS_ALLOW_RESTRICTED=yes >> etc/conf
	
	finally:
		cd ./void-packages;
		./xbps-src <source_package_name>
		
	LINKS:
		https://github.com/void-linux/void-packages				#Void source packages
		https://docs.voidlinux.org/xbps/repositories/index.html			#Void Linux Documentation on XBPS
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
PROPERLY VERIFYING DISK ERRORS ON LINUX
	PREDICTING DISK DEATH
	How to verify if there are disk errors:
	
	1 - Type in to list all mounted/umounted disks on the system:
		#ls -l /dev/disk/by-path
		example:  pci-0000:00:1f.2-ata-1.0-part1 -> ../../sda1

	2 - Now type to list kernel boot logs of current day:
		#journalctl -b-0
		^ it's also possible to use -b-1 for a previous boot
		^ -b-0 always shows logs for current boot.

	2.1 - Type this for each "ata-number" that shows up Step #1:
		/ata1.*error
	
		2.1.1 - Type again:
			/ata1.*Error
	
	3 - Press ENTER! and finally press 'n' or 'N' to move between next and previous search result.
	^ Pressing 'G' or 'g' will bring you to the beginning and end of the log respectively.
	^ Press 'q' to exit 'journalctl'

	4 - If any message appeared on screen for the given ATA, that means the corresponding disk
	has started to display fail problems, it's very important that from this point forwards to backup 
	ANY IMPORTANT DATA IF POSSIBLE BEFORE RUNNING ANY OTHER COMMANDS HERE DESCRIBED!

	5(Optional/Warning) - At this point, if errors have been found at the logs on 'journalctl', anything
	you do from this point onwards might make the disk permanently dead!

		5.1 - It's possible to run a SMART test on top of the defective hard disk,
		VERY IMPORTANT: Don't RUN any tests if you think a backup is important,
		as tests incur lifetime penalty and may permanently damage the disks at this point.

			#smartctl --test long /dev/sdX
			^ sdX has to be the defective disk found when doing Steps #1 and #2.
			^ note that smartctl doesn't fix any filesystem or partition errors.

		5.2 - Take note of the time required for finishing the test, if you failed to see the message
		about duration or have forgotten it, you could still type: 
			#smartctl --all /dev/sdX | less

		^ this can be done both to check remainning test duration and also other health stats; 
		^ output example: 
			[SMART OUTPUT]
				Self-test execution status:      ( 242) Self-test routine in progress...
								  20% of test remaining.
			[/SMART OUTPUT]

		5.3 - Just wait for the remaining duration if needed, press 'q' to exit

		5.4 - When you feel that the test routine is done, you can try going through step #5.1 again
		and if Self-Test execution status is complete, press 'G' to go to the bottom page,
		this is the output you should see:
			[SMART OUTPUT]
				SMART Self-test log structure revision number 1
				Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error
				# 1  Extended offline    Completed without error       00%     33854         -
			[/SMART OUTPUT]

		5.5 - Even if the tests displays no errors, it's VERY important to backup all data you have in it
		or send it to a PC maintenance store so they can do the backup for you. 

		If you found issues on Step #2 this means that Disk Death is imminent at this point and that it can happen
		at any moment; if you can't make a disk backup, turn it off from your PC immediatly since permanent
		access loss could happen at any minute making both Data Backup or Recovery impossible.

	6 - FINAL STEP: read the topic on "IN SUMMARY - VERY IMPORTANT" for further important instructions!

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
IN-DEPTH ANALYSIS OF A HARD DISK / HARD DRIVE WITH SMARTCTL

well known commands:

# smartctl --scan					#Scans for Storage Devices and list each of it's type
# smartctl --scan-open					#Same as above, but listens on device type change 
							^ when openning storage devices.

# smartctl -r <ioctl_type> <disk-device>			#Outputs ioctl() call for a given device
							^ioctl types can be:
										ioctl,
										ataioctl,
										scsiioctl,
										nvmeioctl.
							
# smartctl --all <disk_device>				#Displays SMART Data on the given disk device
							^ same as: #smartctl --all --format=old

# smartctl -x <disk_device>				#Displays long list of SMART and Non-Smart data stored on disk
							^ same as: #smartctl --all --format=brief 
									or 
								   #smartctl --xall

# smartctl -i						#Displays DISK Information
							^ same as: #smartctl --info <disk_device> 

# smartctl -A <disk_device>				#Displays SMART Attributes for the given disk device.
# smartctl -H <disk_device>				#Displays SMART result for the most recent SMART Test for SCSI/SATA devices.
							^ Read man page for other devices - not very important

# smartctl -c <disk_device>				#Displays DISK SMART Capabilities and also Displays any ongoing tests
							^ Not to be mistaken with -C!!
# smartctl -lerror /dev/sda				#Lists error log for /dev/sda

# smartctl <disk_device>	--log=<log name/type> 		#Displays the given Log Data stored on the disk
							^ same as: #smartctl -l <log name/type> <disk_device> 
							log types are: 
									error, 
									error[,NUM], 
									xerror, 
									selftest,
									xselftest,
									directory,
									selective,  
									background,
									scttemp, scttempsts, scttemphist,
									scttempint,
									scterc,
									devstat,
									defects,
									envrep,
									sataphy,
									sasphy,
									tapealert,
									tapedevstat,
									zdevstat,
									gplog,ADDR[,FIRST[-LAST|+SIZE]] ,
									smartlog,ADDR[,FIRST[-LAST|+SIZE]] ,
									nvmelog,<PAGE>,<SIZE> ,
									ssd,
							^ some of these are device specific, check man-page for smartctl:
								$man smartctl

									
# smartctl --test=short <disk_device>			#Does a SHORT test on the given disk-device

							From smartctl man page: 
								"""The "Self" tests check the electrical and mechanical 
							performance  as  well as the read performance of the disk.
							Their results are reported in the Self Test Error Log, 
							readable with the '-l selftest' option.  

							Note that on some disks the progress of the  self-test  can  be
							monitored  by  watching  this  log  during  the  self-test;  
							with  other disks use the '-c' option to monitor progress."""

							VERY IMPORTANT: This test doesn't update SMART Attributes of the disk,
							it only checks for electrical, mechanic and read performance.


# smartctl --test=long <disk_device>			#Similar to --test=short, but does a longer test.
							^ This test also doesn't update SMART Attributes of the disk.

# smartctl --test=offline <disk_device>			#This test UPDATES THE SMART ATTRIBUTE VALUES, 
							^ and if errors are found they will appear in 
							the SMART error log, visible with the '-l error' option.

# smartctl -X <disk_device>				#Aborts any test!

VERY IMPORTANT: ONLY use -C or --captive TESTS if the DISK ISN'T MOUNTED!
--captive tests are pretty much the same as the default test types, the only difference being the speed they're realized.



-------
CHECKING USAGE PERCENTAGE FOR SSD DISKS
	It's important to check on usage percentage when using SSD disks:
		# smartctl --log=ssd <disk_device>

	A value of 0 indicates as new condition while 100 indicates the device is at the end of its lifetime 
	as projected by the manufacturer.  The value may or may not reach 255.
	
-------
CHECKING FOR ERRORS ON DISK
	In general, if a given disk starts showing any errors, it's always a good idea to backup all important data
to a newer disk, mainly because disk death will be iminent from this point onwards and if it happens access to data may be
lost permanently and all important data WILL be lost.

	# smartctl -l xerror <disk_device>

-------
READING AND UNDERSTANDING ATTRIBUTE COLUMNS ON SMARTCTL -A
	"Pre-Fail" and "Old Age" don't really mean your Hard Disk is about to die,
the important data is the 'VALUE', 'WORSE' and 'THRESH' attribute columns; the 'thresh' column holds the threshold
set by the Hard Disk manufacturer, 'VALUE' must be equal or below 'thresh' in order for the Hard Disk to be considered
defective/broken permanently, the 'WORSE' column will permanently store the lowest minimum value that the column 'VALUE' 
has ever reached, in other words the 'worse' column can be used as a reference in detecting Imminent Hard Disk failures.

If one or the other ever happens, the column 'WHEN FAILED' will be filled to either "FAILING_NOW" or "In_the_past" states.
"In_the_past" will be used when 'WORSE' value has reached it's given 'Thresh' at least once,
"FAILING_NOW" will be used when 'VALUE' has reached finally reached it's 'Tresh' value.

It's important to not that DISK ERRORS may occur way before any of the existing attributes actually fail on the 'WHEN FAILED'
column; this will and may happen because not all Hard Disks support listing/storing all existing Attributes that may help
understanding when a Disk is actually failing; Furtherover, attributes that have it's "UPDATE" column set to 'offline' will
actually require manual offline testing of the disk (smartctl -t offline <disk_device>) and if the tests are never realized
by the user, then the given row attribute will never display any problems on the 'WHEN FAILED' column.

Because of that, it's important to run "#smartctl -l xerror <disk_device>" for listing actual device errors that may be 
logged by SMART driver on the disk.

------
IN SUMMARY - VERY IMPORTANT
	In summary, reading and understanding attribute columns & values isn't self-sufficient on diagnosing 
Disk Failure Problems. always make sure to do these steps on both newer and old disks:
	
	1 - Execute Long/Extended test:
		# smartctl -t long <disk_device>

	2 - Execute offline test:
		# smartctl -t offline <disk_device>
	
	3 - After both tests have finished:
		3.1 - Type for results on Long/Extended test:
			# smartctl -c <disk_device>

		3.2 - Type for offline test results:
			# smartctl -A <disk_device>

		3.3 - Finally, check for the most critical error results:
			# smartctl -l error <disk_device>
			# smartctl -l xerror <disk_device>

	4 - In summary, if step #3.3 displays any error, it's self-sufficient to say that Hard Disk failure is imminent
	if it has not failed yet, it's important from this point to backup all your important data on a complete 
	different disk; you can use this same test to make sure the backup drive isn't suffering from the same issues.

-------
AUXILIARY COMMANDS
	
# lsblk							#Displays mounted/umounted disks connected to the system!
# hdparm							#Linux Kernel Only: it can set disk flags to improve performance!

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
IMPROVING FIREWALLD PERFORMANCE
	VERY IMPORTANT: Note that firewalld and  iptables  are different things,
	These settings do not apply to iptables, iptables can't be used at the same time as firewalld.
	Please, read topic on:
		FIREWALLD VS IPTABLES

	As of 2023, it's possible to enable nftables flowtable to improve traffic troughput by enabling nftables 
as follows:

Remember, restart the firewall service upon applying any changes:
	#systemctl restart firewalld.service

You may have to check all your network devices with for a full list of network devices:
	$ ifconfig -a

[file: /etc/firewalld/firewalld.conf]
	# NftablesFlowtable
	# This may improve forwarded traffic throughput by enabling nftables flowtable.
	# It is a software fastpath and avoids calling nftables rule evaluation for
	# data packets. This only works for TCP and UDP traffic.
	# The value is a space separated list of interfaces.
	# Example value "eth0 eth1".
	# Defaults to "off".
	NftablesFlowtable=enp3s0 eth0
[/file]

VERY IMPORTANT: /etc/firewalld/zones contain description of network zones and how they work.

source: https://www.phoronix.com/news/Firewalld-2.0

^ """Firewalld 2.0 also adds support for NFTables flowtable, which is a software fast-path that can 
significantly improve forwarding performance. Firewalld with NftablesFlowtable enabled has increased 
iperf performance with network forwarding by around 59%. More details on this feature via the 
Firewalld.org blog. Firewalld 2.0 also adds a new zone priorities feature."""
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
CREATING FILE ARCHIVES USING AR
	This only works for files, not directories(see TAR as an alternative option); this
Should be used, if necessary, for storing multiple static versions of the same library.
for C/C++, archives should be created with .a extensions.

$ ar -q <archive> <files>				#Adds file to end of archive; does NOT replace data.
$ ar -r <archive> <files>				#Adds file/directory into archive REPLACING data with same name
$ ar -s <archive> 					#Creates/Updates index to the archive
$ ar -d <archive> <files>				#Deletes members from the archive
$ ar -x <archive>					#Extracts members of the archive

$ ar -m <archive> <files>				#Moves members in the archive
							^ Pushes given files to the end of the queue in the archive
$ ar -p <archive> <files>				#Prints the content of a given file member in the archive
$ ar -r <archive>					#Displays a table listing the contents of the archive

$ ar -pl <archive> -L./ -l<library>			#Creates/Specifies library dependency for a given archive

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
CREATING ARCHIVE FILES USING GNU TAR, LIBARCHIVE, BSDTAR, PAX
	Allows creating archive from files and directory. Tar is also formely known as bsdtar
and is part of libarchive package on linux distros.

	VERY IMPORTANT 1:
		When executing 'tar' as root(using sudo or not) for extracting data,
		it will ensure data ownership & permission to be preserved.

		When not extracting as root, all data ownership and permission set will change to that of the user
		extracting the data, accoriding to system policy.

		This is very useful, since other tools like 7z don't save nor restore 
		neither permission set nor ownership on the extracted data.

		When downloading someone else's 'tar' file, always make sure to extract it as 'root'
		as it'll preserve the desired permissions and ownership of the extracted data.

	VERY IMPORTANT 2 - very old, not important, do not use -k - :
		By default, 'tar' has the ability of overwritting important system files as well as 
		user personal data; it can also overwrite directory metadata like permissions, ownership and ACLs
		if they already exist on the system as well as the data/files they might overwrite. 

		Because of this: FOR SAFETY, ALWAYS EXTRACT 'TAR' FILES WITH THE '-k' OPTION.
		-k option isn't mandatory, it just prevents older files from being overwritten.
	
	VERY IMPORTANT 3:
		In general, avoid extracting 'tar' files if permissions and ownership of the data isn't
		important.


SOME OLD EXAMPLES:
	$ tar cf path/to/target.tar path/to/file1 path/to/file2  	# [c]reate an archive and write it to a [f]ile:
	    

	$ tar czf path/to/target.tar.gz path/to/file1 path/to/file2 	# [c]reate a g[z]ipped archive and 
									  write it to a [f]ile.

	$ tar czf path/to/target.tar.gz --directory=path/to/directory   	# [c]reate a g[z]ipped archive from a directory using 
									  relative paths.

	$ tar xkvf path/to/source.tar[.gz|.bz2|.xz]			# E[x]tract a (compressed) archive [f]ile 
									  into the current directory [v]erbosely.

	$ tar xkf path/to/source.tar[.gz|.bz2|.xz] --directory=path/to/directory  	# E[x]tract a (compressed) archive 
											^ [f]ile into the target directory.

	$ tar caf path/to/target.tar.xz path/to/file1 path/to/file2	# [c]reate a compressed archive and write it 
									^ to a [f]ile, using [a]rchive suffix to determine 
									^ the compression program.


	$ tar tvf path/to/source.tar 					# Lis[t] the contents of a tar [f]ile [v]erbosely:

	$ tar xkf path/to/source.tar --wildcards "*.html"		# E[x]tract files matching a pattern from 
									  an archive [f]ile 	

UNIX-STYLE USAGE
       $ tar -A [OPTIONS] -f ARCHIVE ARCHIVE...

       $ tar -c [-f ARCHIVE] [OPTIONS] [FILE...]

       $ tar -d [-f ARCHIVE] [OPTIONS] [FILE...]

       $ tar -r [-f ARCHIVE] [OPTIONS] [FILE...]

       $ tar -t [-f ARCHIVE] [OPTIONS] [MEMBER...]

       $ tar -u [-f ARCHIVE] [OPTIONS] [FILE...]

       $ tar -x [-f ARCHIVE] [OPTIONS] [MEMBER...]

GNU-STYLE USAGE
       $ tar {--catenate|--concatenate} [OPTIONS] --file ARCHIVE ARCHIVE...

       $ tar --create [--file ARCHIVE] [OPTIONS] [FILE...]

       $ tar {--diff|--compare} [--file ARCHIVE] [OPTIONS] [FILE...]

       $ tar --delete [--file ARCHIVE] [OPTIONS] [MEMBER...]

       $ tar --append [--file ARCHIVE] [OPTIONS] [FILE...]

       $ tar --list [--file ARCHIVE] [OPTIONS] [MEMBER...]

       $ tar --test-label [--file ARCHIVE] [OPTIONS] [LABEL...]

       $ tar --update [--file ARCHIVE] [OPTIONS] [FILE...]

       $ tar {--extract|--get} [--file ARCHIVE] [OPTIONS] [MEMBER...]

---
---
COMPRESSION ALGORITHMS/FORMATS FOR GNU TAR
	Using one of the compression algorithms/formats will require you to have it's tools installed on your system,
	GNU TAR will usually complain when the tools fail to exist.

	Compressor 		Long 			Short
	bzip2 			.tar.bz2 		.tb2, .tbz, .tbz2, .tz2
	gzip 			.tar.gz 		.taz, .tgz
	lzip 			.tar.lz 		------------
	lzma 			.tar.lzma 		.tlz
	lzop 			.tar.lzo 		------------
	xz 			.tar.xz 		.txz
	compress 		.tar.Z 			.tZ, .taZ
	zstd 			.tar.zst 		.tzst 

	NOTE: 'xz' has the most compression rate of these algorithms, however it takes a very long time to compress and decompress 'xz' files.
	'lz' is definitively the fastest one, offering both decent compression rate and very fast speeds.
	'zstd' is the midterm between 'xz' and 'lz', it falls on speed regarding 'lz' algorithm, but it's still faster than 'xz'; 
	'zstd' offers midterm compression rate between both 'xz' and 'lz'.
---
GNU TAR EXAMPLES

EXAMPLE 0 - EXTRACTING TAR DATA ON A GIVEN DIRECTORY
	NOTE:
		Using one of the compression algorithms/formats will require you to have it's tools installed on your system,
		GNU TAR will usually complain when the tools fail to exist.

	Tar requires '--directory'(or '-C') to be used when extracting in a directory
	different than the current one, ex:

		# tar xvf ~/my_tar_file.tar.gzip	--directory=/usr/share/


	OBS1: 
		Not using -C (or --directory) will result in files being extracted in './' instead by default.

	OBS2: 
		Always use 'sudo' or 'run0' for extracting as root, this allows extracted files 
	and directories to keep the same permissions/ownership set inside the .tar file.

	You can see permission set by typing: 
		$ tar tvf <tar_file>
	
	Those are the permissions/ownership set that are stored in the tar file.

	OBS3: 
		Not using 'sudo' or extraction as root will result in ownership and permission set being lost
		for the extracted data.



EXAMPLE 1 - CREATING, UPDATING AND APPENDING TO TAR ARCHIVE
	$ tar cf ./my_filename.tar ./file_1 		# [C]reates TAR archive called 'my_filename'
							and adds 'file_1' to it.
							^ obs: it's not possible to create empty archive files

	$ tar uf ./my_filename.tar ./file_2		# [U]pdates 'file_2' to the archive
							^ If the file already exists, 
							a new index for the file will be created,
							both new and old files with same name will co-exist,
							but nothing happens if the file hasn't been changed.

							obs: [c]ompresssed files can't be updated/appended!

	$ tar rf./my_filename.tar ./file_1		# [A]ppends given file to the archive
							^ doesn't care if the file exists, just appends it anyways.

	$ tar tvf ./my_filename.tar			# [V]erbosely lists all files
	
	$ tar xkf ./my_filename.tar			# [X]tracts contents of the archive
							^ if there's a file with same-name on the same directory
							inside the archive, only the newest version of that
							file is extracted, older versions are kept in the archive.


EXAMPLE 1.1 - DELETING FROM TAR ARCHIVE
	$ tar --delete -f ./my_filename.tar ./file_1	# Deletes data from archive
							^ can not delete from compressed archives
							^ it's note possible to use both UNIX and GNU Style together:
							'tar --delete f ./my_filename.tar ...'
						
							^ when going for UNIX Style, all options have to be given
							in UNIX-STYLE; same happens for GNU-STYLE.

EXAMPLE 1.2 - AUTO COMPRESSING TAR ARCHIVE
	$ tar caf ./my_filename.tar.gz ./my_filename.tar		#[A]uto-compresses my_filename.tar into gzip format


EXAMPLE 1.3 - EXTRACTING A CHOSEN FILE WITHIN TAR ARCHIVE
	$ tar xkf path/to/source.tar --wildcards "*.html"		#Extracts only .html files/dirs

EXAMPLE 1.4 - SPLITTING A LARGE FILE INTO MANY DIFFERENT FILES | SPLITTING A FILE INTO DIFFERENT CHUNKS USING TAR
	A) Example 1.5.1 - Non-GNU Tar - :
		NOTE: This will not work on GNU Version of Tar:
		This will split a larger file called 'myfile' in chunks of 1MB and name them as my_archive using numbering from 0 to 50:
			$ tar -cav -M --tape-length=1M --file=my_archive-{00..50}.tar.gz ./myfile

	B) Example 1.5.2 - GNU Tar - :
		Using the 'split' tool is required for the GNU version of Tar:
			$ tar cvzf - ./myfile | split -b 10M - ./myfile.backup.tar.gz.
				or
			$ tar cvzf - ./myfile | split --bytes=10MB - ./myfile.backup.tar.gz.

		Note 1: -z creates 'gz' compressed file; -J creates 'xz' compressed file; check tar manual(man tar) for more options.
		Note 2: files will be split using letters.

EXAMPLE 1.5 - EXTRACTING SPLIT FILES USING TAR 
	 This will extract all files named 'myfile.backup.tar.gz' into the current dir:
		$ cat myfile.backup.tar.gz.* | tar xzvf - 
			or
		$ mkdir ./extracted && cat myfile.backup.tar.gz.* | tar xzv --directory=./extracted -f -

		Note 1: -z extracts 'gz' compressed files; -J extracts 'xz' compressed files; check tar manual(man tar) for more options.

IMPORTANT LINKS ABOUT COMPRESSION:
	https://wiki.archlinux.org/title/Archiving_and_compression

	IMPORTANT TAR RESOURCE LINKS: 
		https://www.gnu.org/software/tar/manual/html_section/tar_toc.html#SEC_Contents
		https://www.gnu.org/software/tar/manual/html_section/Styles.html#Styles
		https://www.gnu.org/software/tar/manual/html_section/extract.html#extracting-untrusted-archives

---
VERY IMPORTANT: TAR defaults to overwritting files on the disk, so becareful with TAR BOMBS who 
	        might overwrite important system files.

Observation: You can/should use 'v' (or -v, for GNU-STYLE) for 'verbose' mode.
	     This helps verifying if a chosen action has been successfully completed.
	
Observation 2: It's a good practice to always archive ALL files inside a folder, instead of using the current path(./).

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
FINDING OUT IF COMMAND IS A BINARY EXECUTABLE OR A SHELL BUILT IN COMMAND
$ whence -c <command_name>
	or
$ type -p <command_name>


--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
SYSTEM NOT BOOTING / BOOTING FROM INSTALLATION MEDIA
	Sometimes a linux system may not boot,
making an archlinux installation media and booting from it might 
be the solution to the problem.

Once your system is booting from installation media, do as follows:

0 - Execute a filesystem check to make sure everything is 'ok' before mounting
	# fsck.ext4 -Dfy /dev/sdXY
	ex.: fsck.ext4 -Dfy /dev/sdb2

1 - Mount the partition where your linux is installed:
	# mount -m /dev/sdXY /mnt/my_linux_system
	ex.: mount -m /dev/sdb2 /mnt/archlinux

2 - Now arch-chroot into your mounted partition:
	# arch-chroot /mnt/my_linux_system


3 - From this point forward you can try many options for restoring your system

	3.1(Optional) - Rebuilding initramfs on archlinux:
		# mkinitcpio -P	
	
	3.2(Optional) - Checking system package integrity:
		# pacman -Qqk > /home/<your_username>/pacman.qqk.log	#Lists only packages whose files are missing files, it uses -q for quite mode.
									^ This has to be executed as root, to check for all files in the system.
				or
		# pacman -Qkk > /home/<your_user_name>/pacman.qkk.log-<date>		#Lists all packages.

	3.2.1 - ex.: # pacman -Qkk > /home/me_myself/pacman.qkk.log-20-jul-2023

		3.2.1.1 - once it's finished, you can do the following:
			$ cat /home/me_myself/pacman.qkk.log-20-jul-2023 | less

		3.2.1.2 - press '/' character on your keyboard to start a regex search and type:
			[1..9] altered
				or
			[1..9] Altered

		3.2.2.3 - finally press 'n' and 'N' to navigate between found occurrences

		3.2.3.4 - Check how many packages have been affected and fix it with
			# pacman -S <package_name_1> <package_name_2> ... <package_name_n>
	
	3.3 (Optional) - Check if your /etc/fstab file is okay and generate a new one ONLY if necessary.
			HINT: Your fstab should contain metadata on hard disks that would be mounted at system boot.

	3.4 (Optional) - Check /boot/grub.cfg to see if everything is okay and generate a new one ONLY if necessary
			HINT: The grub.cfg contains meta about the disk UUIDs meant for boot and initram data 
			and kernel listed on /boot/ ; you need to make sure if everything is correct on ur own.

	3.5 (Optional) - Reverting suspicious updated packages if you know how to identify them and how to revert it.

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
FINDING FILES THAT HAVE BEEN ACCESSED OR MODIFIED IN LESS THAN A FEW HOURS
	Find is a very powerful tool that allows to verify files in the disk in a very fast way.
with it it's possible to verify all files that have been modified in less than an hour or accessed in less than an hour.

That allows verification for possible hackers in the system, since they can't bypass filesystem access or modification times
unless it's completely disabled.

Files that have been accessed or modified with neither the user's or the system's intent might indicate
a security issue in the system.

for ex.: if you haven't accessed your browser in the last hour, and you also made sure that there is no browser open
nor pending updates scheduled, either access or modification dates in between that last hour will indicate that there might
be a spyware/trojan on the system.

1 - Listing files MODIFICATION within the last 3h:
	$ find . -mtime +3 

2 - Listing files ACCESS within the last 3h:
	$ find . -atime +3


--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
CLAMAV ANTI-VIRUS
	source: https://wiki.archlinux.org/title/ClamAV

ENABLING CLAMAV SERVICES

	1) After installing clamav, you need to update it first:
		# freshclam

	2) Now you can enable the following services

		2.1 - Enable and then start clamav-daemon
			# systemctl enable clamav-daemon.service && systemctl start clamav-daemon.service

		2.2 - Additionally enable real time on-access protection
			Type on command line:
				# systemctl enable clamav-clamonacc.service && systemctl start clamav-clamonacc.service

			Just for real-time protection you'll need to set this option up in the following config file:
				[file: /etc/clamav/clamd.conf ]
					# Exclude the UID of the scanner itself from checking, to prevent loops
					OnAccessExcludeUname clamav
				[/file]

			Read more on:
				CLAMAV CONFIGURATION FILES
	
	3) Add clamav to sudoers list
		Use visudo to add clamav to sudoers list:
			# visudo

	4) Create default quarantine directory
		Create default quarantine directory:
			# mkdir /root/quarantine

	5) Setting up CLAMAV for Auto-update 
		The clamav-freshclam-once.timer will update clamav every 24 hours:
			# systemctl enable clamav-freshclam-once.timer && systemctl start clamav-freshclam-once.timer

TESTING CLAMAV TOOLS

	1) Testing CLAMAV Scan tool

		$ curl https://secure.eicar.org/eicar.com.txt | clamscan -
		
		If it's working, this will output the following text:
			stdin: Win.Test.EICAR_HDB-1 FOUND

	2) Testing CLAMAV Real time protection
		You can download and save a the eicar file in one of the directories you configured clamonacc to monitor. 
		For example:

			$ cd /home/${USER}/Downloads/ &&
			  wget https://secure.eicar.org/eicar.com.txt &&
			  cat eicar.com.txt;
	
		VERY IMPORTANT: If Real Time Protection IS WORKING, 'cat' WILL FAIL to output the contents of the 
		eicar.com.txt file. If cat doesn't fails, then real time protection isn't working!!!
			If this is the case, consider doing the following steps:
				MANDATORY READINGS:
					ALLOW FILE DESCRIPTOR PASS | ALLOWING CLAMAV TO SCAN DIFFERENT FILE SYSTEM TREES
					TURNING ON REAL-TIME SCANNING ON NEW, MOVED OR RENAMED FILES
					SETTING UP DIRECTORIES FOR REAL TIME SCANNING

	3) If Boths tests succeed, congratulations your CLAMAV Anti-virus is now working!

CLAMAV CONFIGURATION FILES
	The following files contain the relevant configuration options:

		A) freshclam: /etc/clamav/freshclam.conf
		B) clamd: /etc/clamav/clamd.conf
		C) clamd mail filtering: /etc/clamav/clamav-milter.conf

	You can also use 'clamconf' on commandline to list/display all attributes for all configuration files.
		$ clamconf
	
	ALLOWING CLAMAV TO WORK ON APPARMOR ENABLED ENVIRONMENTS
		Clamav requires these steps to have access to files and data in this case.

			1) Check that you have not APPARMOR enabled kernel
				$ zcat /proc/config.gz | grep -i "apparmor"
				
				Expected output if you have apparmor enabled kernel:
					CONFIG_SECURITY_APPARMOR=y

			2) If you do have APPARMOR enabled kernel, do this:
				# aa-complain clamd

				Note: after typing aa-complain clamd, if it shows that you have any configuration file containning 
				syntax errors, try isolating it in a different directory.

	ALLOW FILE DESCRIPTOR PASS | ALLOWING CLAMAV TO SCAN DIFFERENT FILE SYSTEM TREES
		Useful if clamd is running as a different  user  or  if clamd  sees  a different file system tree. 
		This is faster than streaming the file to clamd.  Only available if connected to clamd via local(unix) socket.
		
		1) You can edit the path on the service file located in:
			# nvim /etc/systemd/system/multi-user.target.wants/clamav-clamonacc.service
				or
			# systemctl edit clamav-clamonacc.service

		2) Edit and add --fdpass as described below:
             		/usr/sbin/clamonacc -F --log=/var/log/clamav/clamonacc.log --move=/root/quarantine --fdpass

	CHANGING QUARANTINE DIRECTORY FOR REAL-TIME ACCESS SCAN
		1) You can edit the path on the service file located in:
			# nvim /etc/systemd/system/multi-user.target.wants/clamav-clamonacc.service
				or
			# systemctl edit clamav-clamonacc.service


		2) You can edit and change '--move' argument as you wish:
			/usr/sbin/clamonacc -F --log=/var/log/clamav/clamonacc.log --move=/root/quarantine
		

		VERY IMPORTANT: You can replace '--move' with '--delete', but this isn't recommended,
		false positives can happen and will lead important data to be permanently removed.

	ENABLE LOGGING FOR DETECTED INFECTED DATA
		Will store data for each detected infected file on the system:

		example:
			[file: /etc/clamav/clamd.conf ]
				# Log additional information about the infected file, such as its
				# size and hash, together with the virus name.
				ExtendedDetectionInfo yes

				# Log time with each message.
				# Default: no
				LogTime yes
			[/file]
		
		NOTE: 
			/var/log/clamav/			#Default storage for CLAMAV Logs

	SETTING UP DIRECTORIES FOR REAL TIME SCANNING
		On /etc/clamav/clamd.conf you can add directories to be scanned by the real-time protection.

		example:
			[file: /etc/clamav/clamd.conf ]
				OnAccessIncludePath /home
				OnAccessIncludePath /root
				OnAccessIncludePath /etc
				OnAccessIncludePath /boot
				OnAccessIncludePath /usr
				OnAccessIncludePath /mnt
				OnAccessIncludePath /var
				OnAccessIncludePath /tmp
				OnAccessIncludePath /.fscrypt
			[/file]

	EXCLUDING DIRECTORIES FROM REAL TIME SCANNING
		Same thing, but excludes directories from being scanned:

		example:
			[file: /etc/clamav/clamd.conf ]
				OnAccessExcludePath /home/special_user/
			[/file]

	TURNING ON REAL-TIME SCANNING ON NEW, MOVED OR RENAMED FILES
		Perform scans on newly created, moved, or renamed files:

		example:
			[file: /etc/clamav/clamd.conf ]
				OnAccessExtraScanning yes
			[/file]

CLAMAV UTILITIES
	ClamAV offers 2 main tools for scanning 'clamscan' and 'clamdscan'.
	The difference is that 'clamdscan' uses the settings stored in /etc/clamav/clamd.conf for scanning

	for example:
		 # clamscan --recursive --infected --exclude-dir='^/sys|^/dev' /

		 note: even if /sys and /dev/ are excluded in clamd.conf, clamscan will completely ignore it
		 unless --exclude-dirs is explicitely used.

	Important:
		Remember that when scanning, some systems like WSL might require --fdpass to be used
		when executing either 'clamdscan' or 'clamscan'

	1 - Updating clamav
		# freshclam

	2 - Scanning & Showing only infected files, with clamav daemon
		# clamdscan -v -i <dir>

	3(Optional) - Scanning with multi-threading enabled, with clamav daemon
		# clamdscan -v -m <dir>

	4(Optional) - Recursive Scanning(scan all directories) without clamav daemon
		# clamscan -v -r <dir>

	5(Optional) - Recursive Scanning(scan all directories) without clamav daemon
		# clamscan -v -r --multiscan <dir>

	6(Optional) - Clamav Monitoring Tool for monitoring multiple instances of clamd
		# clamdtop

	CLAMAV GRAPHICAL USER INTERFACE / CLAMAV GUI
		#clamtk
		^ requires package install

	CLAMAV DIRECTORY ON ARCHLINUX
		/etc/clamav/				#Configuration Files
		/var/log/clamav/			#Default Storage Directory for Logs
		/root/quarantine			#Default Quarantine Directory


	FINDING OTHER CLAMAV DATA/FILES
		$ pacman -Ql clamav


source:
	https://wiki.archlinux.org/title/ClamAV
	https://docs.clamav.net/Introduction.html

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
NETWORK MONITORING
	Use Network Activity Viewer:
		1 - # netactview
		    ^ requires package install
FIREWALLING
	Several Options Available Out-of-box:
		1 - # systemctl start firewalld
			Firewall daemon

		2 - # firewall-config
			Requires X11 or Wayland

		3 - # gufw
			^ requires package install
			^ Firewall GUI, Requires X11 or Wayland

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
PULSEAUDIO BASICS(OLD)
	VERY IMPORTANT NOTE: 
		As of 2024, archlinux now uses pipewire, pipewire-alsa, pipewire-pulse, pipewire-jack-client by default.
		however, pavucontrol and alsamixer are still usable tools! VERY IMPORTANT: some instructins are still valid
		as pipewire still uses some pulse and alsa configuration files.

RESTART PULSEAUDIO SERVICES:
$ pulseaudio -k
$ pulseaudio --start

OPTIMIZING PULSEAUDIO FOR PERFORMANCE:
You should at least have at least these options below on /etc/pulseaudio/daemon.conf :

enable-memfd = yes
enable-shm = yes
shm-size-bytes = 200 			#Setting this 0 will use the system-default, usually 64 MiB
resample-method = soxr-vhq		#Highest Quality



Read more on MANUAL PAGE: $ man pulse-daemon.conf
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
USING PIPEWIRE INSTEAD OF PULSEAUDIO (2024)
	As of 2024, one should install wireplumber, pipewire, pipewire-alsa, pipewire-pulse instead of PULSEAUDIO or ALSA.
	however, pavucontrol and alsamixer are still usable tools! VERY IMPORTANT: Some alsa and pulseaudio files are still
	used by pipewire. So some of the instructions are still valid.

	NOTE: Pipewire/Wireplumber might only work with 'dbus-broker' at this point, so install it.

	INTRODUCTION
		ALSA AND OSS DRIVERS
			'Alsa' is the default audio driver that comes with the linux kernel.
			'Oss' is a proprietary audio driver that became open-source; however, on linux, 
			must of the audio support is on the 'alsa' driver.

			'Alsa' provides more compatibility than 'Oss'.
		
		PULSEAUDIO AND JACK
			Both are audio media servers that either rely on ALSA or OSS drivers.
			With the exception for Jack, Jack solely rely on 'ALSA' drivers,
			and only 1 driver(either ALSA or OSS) can be used at a time for 'Pulseaudio'.

			While both PULSEAUDIO and JACK can be installed at the same time,
			only one of them can use an 'audio device' per time.

			Using 'jack' would leave applications that rely on 'pulseaudio' server mute.
			Using 'pulseaudio' server would leave applications that rely on 'jack' server mute.

		PIPEWIRE AND WIREPLUMBER
			PipeWire is a new low-level multimedia framework. 
		It aims to offer capture and playback for both audio and video with minimal 
		latency and support for PulseAudio, JACK, ALSA and GStreamer-based applications. 

		Pipewire has it's own implementations of JACK and PULSEAUDIO, respectively called 'pipewire-pulse'
		and 'pipewire-jack' allowing both to be used at the same time depending on the requesting application.

		Wireplumber is a 'session manager' for Pipewire

		The WirePlumber daemon implements the session & policy management service. 
		It follows a modular design, having plugins that implement the actual management functionality.

		The WirePlumber Library provides API that allows you to extend the WirePlumber daemon, 
		to write management or status tools for PipeWire (apps that don’t do actual media streaming) 


		PipeWire can be used as an audio server, similar to PulseAudio and JACK. 
		It aims to replace both PulseAudio and JACK, by providing a PulseAudio-compatible 
		server implementation and ABI-compatible libraries for JACK clients.


	VERY IMPORTANT: 
		The PipeWire package provides an initial set of configuration files in /usr/share/pipewire. 
	YOU SHOULD NOT EDIT THESE FILES DIRECTLY, AS PACKAGE UPDATES WILL OVERWRITE YOUR CHANGES. 

	To configure PipeWire, you can copy files from /usr/share/pipewire to the alternate system-wide location 
	/etc/pipewire, or to the user location ~/.config/pipewire. 

	An equally named file in a directory with a higher precedence makes the analogous files ignored.

	INSTALLATION STEPS
		Install pipewire-alsa, pipewire-pulse, pipewire-jack-client.
	These will replace pulseaudio and alsa on the system through it's respective removals.
	Pipewire-jack-client will run atop JACK Daemon.

	Now, Increase the headroom to fix possible audio-stuttters on games that must run through wine,
	use these non-default settings:

	2024 NOTE: As of april 2024,  .lua files should be called .conf
	they need to be ported from lua to the new .conf format.

	---
		2024.2 NOTE: I HAD TO ENABLE THESE SETTINGS FOR '/usr/share/pipewire/pipewire.conf'
			     and then copy it to '/etc/pipewire/pipewire.conf'.

		PLEASE REMOVE IT FROM  /etc/wireplumber/main.lua.d and /usr/share/wireplumber/main.lua.d/
		if you have it!

		[file: /usr/share/wireplumber/main.lua.d/50-alsa-config.lua]
		[file: /usr/share/wireplumber/main.lua.d/50-alsa-config.conf]
			  ["vm.node.defaults"] = {
			    ["api.alsa.period-size"] = 2048,
			    ["api.alsa.headroom"] = 22192,
			  },
		[/file]
		^2024 note: this file doesn't exist and the new file is called pipewire.conf, you can still edit it there


		^VERY IMPORTANT: copy edited file to  /etc/wireplumber/main.lua.d/50-alsa-config.conf,
		since an wireplumber update could change the edited file.
	---
	


	INITIALIZING AUDIO SERVICES
		If migrating to pipewire from other audio services, you may need to turn off and stop/mask
	all legacy system audio services first: 
			'pulseaudio.service', 'alsa-restore.service', 'alsa-state.service'.

			ex: # systemctl mask <service_name>
			^ VERY IMPORTANT: the audio instructions here doesn't cover how to unmask any audio services.
			you're on your own if you ever decide to disable pipewire and move back to old audio services!

	If this is the first time, you may need to try rebooting or re-login after following A, B and C steps below:

	A) Initialize Pipewire Audio Services:
		$ systemctl --user restart wireplumber pipewire pipewire-pulse

		^^^ Note: Running this will reset all wire settings configured on 'helvum', 'coppwr' or 'qpwgraph'
		It'll also reset any Audio configuration settings stored on ~/.local/state/wireplumber/
		these settings are usually set when closing down 'pavucontrol' after making any changes,
		and also upon setting configurations using 'pactl'.

	B) Use this to check service status:
		$ systemctl --user status pipewire{,-pulse}.service wireplumber
				or
		$ systemctl --user status wireplumber pipewire pipewire-pulse

	C) ***Also use this to avoid the following error below:
		$ systemctl --user --now enable wireplumber pipewire pipewire-pulse
		^ use this to make settings persistent!

	D)(Optional) Restore alsa devices:
		This restores volume to alsa:
		# alsactl restore
			or
		$ alsactl --file /etc/alsa/asound.state restore
		^ You have to use this if running a PULSE_SERVER

		read more on: 
			USE ALSACTL FOR RESTORING CARD TO DEFAULT VALUES	

	If you run into problems just re-install pipewire pipewire-pulse and wireplumber and re-do the steps:
		A), C), D).
	

	Advice: In the past, i've been running PULSE_SERVER and had the trouble of restarting wireplumber
	after every single system reboot. Follow the 'SETTING UP DEFAULT AUDIO DEVICE' instructions.

	EXPLAINNING CARD PROFILES
		Different audio card profiles allow audio to be listenned to from a different audio device
		connected to the audio card.
		
		Audio cards, even when onboard, allow multiple audio devices to be connected.
		
		Audio profile, in turn, allow these different audio devices who might be plugged in to be selected.

		Pipewire supports a default 'pro-audio' profile for all sound devices(input/output).
		read more on: 
			WHAT IS PRO AUDIO || SPECIAL TOPIC
			SELECTING A CARD PROFILE

		Pipewire's Card Profiles also allow to process audio for an output and redirect to another output,
		like for example, producing audio on 'digital' and outputting it to 'analog'.
		read more on:
			REDIRECTING GPU HDMI OUTPUT TO REGULAR SPEAKER/HEADSET
			ANALOG X DIGITAL

		ANALOG X DIGITAL
			Usually in ALSA and Pulseaudio, "digital" means a soundcard with IEC 958 or SPDIF via coax 
			or optical connection to an amplifier, while "analog" means a soundcard with a D/A converter, 
			with an analog connection to speakers etc. This is selected on the sound card. 

	TROUBLESHOOTING AUDIO / AUDIO CRACKLING / AUDIO STUTTERS ON PIPEWIRE/WIREPLUMBER
		MICROPHONE GARBLED AUDIO INPUT
			1) Trying to restart audio services to give it a go, it might be required to restart application as well:
				$ systemctl --user restart wireplumber pipewire pipewire-pulse

			2) Some programs that rely too much on System Resources may garble your audio input, so increasing the quantum is a solution:
				$ pw-metadata -n settings 0 clock.force-quantum 600
				Note: This will apply 'clock.force-quantum' not only to audio input but to audio output as well.
				VERY IMPORTANT: DO NOT USE 'clock.force-rate' as specified below, since this will also force audio-input rate:
					pw-metadata -n settings 0 clock.force-rate 48000 && pw-metadata -n settings 0 clock.force-quantum 600 

			RECOMMENDED PIPEWIRE TOPIC READINGS:
				MICROPHONE UNMUTED BUT STILL GIVES NO AUDIO OR GIVES GARBLED MICROPHONE INPUT
				MICROPHONE INPUT/SOURCE AUDIO MUTE OR CRACKLING ON FEW APPLICATIONS

		MICROPHONE UNMUTED BUT STILL GIVES NO AUDIO OR GIVES GARBLED MICROPHONE INPUT
			When using 'pro-audio', my microphone wasn't working, despite other volume control applications like 'pavucontrol' 
			and 'pactl get-source-volume' saying I had volume on 100%. This was the solution:

			1 - Use 'alsamixer' to change default microphone volume:
				$ alsamixer

			2 - Press 'F6' select your audio device in 'alsamixer'

			3 - Press 'F5' to showing everything

			4 - Change volume of the microphone until it reaches 100%

			5 (Optional) - To make sure volume levels aren't overdriven, use 'pactl' to check db levels, it should say 0.0db:
				$ pactl get-source-volume <TAB-Key>

				Note: Use TAB Key on your keyboard for auto-completion using 'zsh' only.
				other shells might not have support for auto-completion.
			
			6 - Try restarting the audio services and the application to see if it works:
				Read more on: 
					MICROPHONE GARBLED AUDIO INPUT


		MICROPHONE INPUT/SOURCE AUDIO MUTE OR CRACKLING ON FEW APPLICATIONS
			I only had this issue when using 'pro-audio' card profile AND only on a few programs,
		because they abruptly refuse sample formats other than S16 for the microphone:
		Don't add any of these lines, unless it's a new application, just modify if you see something different;
		this code section already exists in the given file:
	
			[file: /etc/pipewire/pipewire-pulse.conf ]

			    {
				# skype does not want to use devices that don't have an S16 sample format.
				matches = [
				     { application.process.binary = "teams" }
				     { application.process.binary = "teams-insiders" }
				     { application.process.binary = "skypeforlinux" }
				     { application.process.binary = "wine" }
				]
				actions = { quirks = [ force-s16-info ] }
			    }
			[/file]

				Restart the following services:
					$ systemctl --user restart wireplumber pipewire pipewire-pulse

			NOTE: Some microphones have button control the allows the microphone to be muted.
			They can be pressed on mistake. Make sure it's not the case if you don't have a tray icon indicating it's off
			on your desktop.

			NOTE 2: Sleeping mode may cause the microphone to go offline/mute until turned 'on' again manually.

			NOTE 3: Sleeping mode may cause audio crackling on the microphone:
				# alsactl init
			
			NOTE 4: It's worth checking 'alsamixer' to to make sure microphone isn't muted in alsa.

			NOTE 4: Use 'helvum' or 'qpwgraph' to redirect audio from application into speaker and see if it still sounds bad.

			VERY IMPORTANT: To avoid headaches, check the files mentioned on the following topic and make sure the default audio 
			of your choice is on the 1st line of the list for each file:
				DEFAULT AUDIO SETTINGS DIRECTORY

			For all other cases, try restarting these services:
				$ systemctl --user restart wireplumber pipewire pipewire-pulse

		MODIFYING MONO AUDIO MICROPHONES TO PLAY AUDIO ON BOTH SIDES WITH EASY EFFECTS
			NOTE: Doing this may not be necessary, since some applications seems to understand what a Mono Microphone is.
			NOTE: You don't need this step if you're not being affected by microphone audio playing only from one channel.

			Some headsets may come with mono audio microphones, this means audio will likely play at only one side.
		There is an easy way checking for this: 

		1 (Preferred method) - you can either record audio using 'pw-recorder' or 'ffmpeg' and then play it back for testing.
		If you own a mono input device(mono microphone), then you'll only hear sound coming from one side.

		2 (Alternative) - Another way is to use 'pactl' to see if your input device is stereo by default: 
				$ pactl get-default-source

		3 (Alternative) - You'll need 'pavucontrol' tool installed on your system, run 'pavucontrol' then go to the 'Microphone Tab',
				  then click on the 'padlock' icon respectively to your input device(microphone); if it's stereo device, 
				  you should be able to see 2 different channels and be able to change volume for each of them separately,
				  but if it's a mono device, there will only be one channel for changing volume.


		In order to solve this, currently you'll have to download and use 'easyeffects'.

			1 - Make sure easyeffects catches all input streams:
				Run Easyeffects -> Click on 'Settings' icon -> Click on 'Preferences' -> Make sure you're on 'General' tab ->  
				-> Enable  'Process All Input Streams'.

				1.2 - Ignore streams from 'Monitor of Devices'

			2 - Close 'settings', go back and select 'Input' Tab -> Then go to 'Effects'.

			3 - In the 'Effects' page, click on 'Add Effects' -> Choose 'Stereo Tools'.
				NOTE: If necessary, you'll have to install any of the required plugins/tools.

				3.1 - Click on the 'Stereo Mix' inside the effect page
				3.2 - Select 'LR > MS (Stereo to Mid-Side)' on the scroll buton at the page.
				3.3 (Optional) - You can test how good each option sounds like using the test button inside easy effects program,
				which is located at the bottom of the page with 'a Microphone Icon and Arrows pointing to a speaker icon', 
				on mouse hover it'll say 'Enable/Disable input monitoring'

					3.3.1 (Alternatively) you can test using 'ffmpeg' or 'pw-recorder'.
				3.4 - Make sure to choose the one option that sounds best for you during the test. 
				On my case, the best choice was the one described in step #3.2 here.
			
			4 - Make sure 'easyeffects' is booting with your system at Settings -> Preferences inside easyeffects. 

			OBSERVATION: There might be other ways of doing this same task using pipewire/wireplumber,
			however this is the one method i'm going to cover from now. Using 'helvum', 'qpwgraph' and make permanent settings
			will likely solve the issue.

		SOUND OUTPUT CRACKLING / AUDIO STUTTERS WITH EASY EFFECTS + PIPEWIRE
			ELIMINATING AUDIO STUTTERS DUE TO USING EASY EFFECTS
				This is meant to fix audio crackling/stutter issues

				Use this from commandline: 
					$ pw-metadata -n settings 0 clock.force-rate 48000 && pw-metadata -n settings 0 clock.force-quantum 500
						or
					$ pw-metadata -n settings 0 clock.force-rate 48000 && pw-metadata -n settings 0 clock.force-quantum 2000
						NOTE: This changes both audio input & audio output rate and quantum and may break games that rely
						on Microphone Input!!!

						^ I'm currently using this one, emulators who are able to lower frame time may still have audio 
						^ stutters with clock.force-quantum 500
						^ VERY IMPORTANT: The higher the quantum buffer, the more delay you'll get for hearing audio/sound effects.
						^ PIPEWIRE_QUANTUM can be used as environment var for doing the same thing before executing program.
						^ Microphone audio output may display audio cracklings and stutter
						^ VERY IMPORTANT 2: Read topic "SETTING UP QUANTUM ON A PER APPLICATION BASIS" if you need it

					^  CLOCK.FORCE-RATE should be the frequency at which your headset/speaker can play audio, never try higher values
					than the one specified on your headset.
					^ CLOCK.FORCE-QUANTUM is the In PipeWire, the audio quantum is a buffer size that can be configured.
					Higher quantum will imply in bigger memory usage for buffer data, but this will solve any audio stutter/crackling 
					issues you might get.

					^ VERY IMPORTANT: you can use 'pw-top' for monitoring audio's quantum 
					^ VERY IMPORTANT 2: Read topic "SETTING UP QUANTUM ON A PER APPLICATION BASIS" if you need it

				you can add the above line to the /etc/zsh/zprofile file
				make sure ZSH is your default shell if adding on /etc/zsh/zprofile

			ELIMINATING AUDIO STUTTERS FROM  PIPEWIRE/WIREPLUMBER
				NOTE: This solution actually mitigates the problem, but it persists if 'easyeffects' is executed.
				SOURCE: https://wiki.archlinux.org/title/PipeWire#Noticeable_audio_delay_or_audible_pop/crack_when_starting_playback

				[file: /etc/wireplumber/wireplumber.conf.d/50-disabled-suspension.conf]
					monitor.alsa.rules = [
					  {
					    matches = [
					      {
						# Matches all sources
						node.name = "~alsa_input.*"
					      },
					      {
						# Matches all sinks
						node.name = "~alsa_output.*"
					      }
					    ]
					    actions = {
					      update-props = {
						session.suspend-timeout-seconds = 0
					      }
					    }
					  }
					]
					# bluetooth devices
					monitor.bluez.rules = [
					  {
					    matches = [
					      {
						# Matches all sources
						node.name = "~bluez_input.*"
					      },
					      {
						# Matches all sinks
						node.name = "~bluez_output.*"
					      }
					    ]
					    actions = {
					      update-props = {
						session.suspend-timeout-seconds = 0
					      }
					    }
					  }
					]
				[/file]

			SETTING UP QUANTUM ON A PER APPLICATION BASIS
				Sometimes, setting up a system wide global configuration for 'quantum' as described in topic " "
			may not work since it may break other applications and sometimes even microphone output from  working properly.
			Because of this, it's necessary to setup quantum on a per application basis:

				1 - Copy  /usr/share/pipewire/pipewire-pulse.conf to /etc/pipewire/pipewire-pulse.conf  if you don't
			already have one.

				2 - Insert this inside the /etc/pipewire/pipewire-pulse.conf  at 'pulse.rules' section of the file,
			becareful not to change anything else:

				[file: /etc/pipewire/pipewire-pulse.conf ]
					pulse.rules = [ #YOU DON'T HAVE TO ADD THIS LINE, SINCE IT ALREADY EXISTS!!
					    {
						# Snes9x suffers from audio stutter when frame time is set to 14ms
						matches = [ 
							{ application.process.binary = "snes9x-gamescope" } 
							{ application.process.binary = "snes9x-gtk" } 
							{ application.process.binary = "snes9x" } 
							{ application.process.binary = "alsa_playback.snes9x-gtk" } 
						]
						actions = {
						    update-props = {
							#pulse.min.quantum = 4000 #This works for other applications, but not for snes9x
							#pulse.max.quantum = 4000 #This works for other applications, but not for snes9x
							#pulse.force-quantum= 2000 #This works for other applications, but not for snes9x
							#clock.force-quantum = 2000 #not sure if this works
							pulse.min.req = 3048/48000 #works
							pulse.default.req = 3048/48000 #works
							pulse.min.quantum = 3048/48000 #works
						    }
						}
					    }
					] #DON'T ADD THIS LINE SINCE IT ALREADY EXISTS!
				[/file]
			
				3 - (OPTIONAL) For Easyeffects Users only, close easyeffects manually and kill easyeffects
				to make sure it's fully shut:
					# killall -SIGKILL easyeffects

				4 - Restart Wireplumber, Pipewire and Pipewire-pulse
					$ systemctl --user restart wireplumber pipewire pipewire-pulse            

				5 - DONE! Re-launch any closed applications if necessary.

	TROUBLESHOOTING VOLUME NOT SET TO DEFAULT
		Besides checking 'SETTING UP DEFAULT AUDIO DEVICE' and throughly following it,
	if the problem is still persistent, you may need to change audio card profile to something else:

		$ pactl set-card-profile alsa_card.usb-C-Media_Electronics_Inc._USB_Audio_Device-00 output:iec958-stereo

	in this case, using 'output:iec958-stereo' profile helped, other profiles may produce audio output
	but honor different lower volume.

	'iec958' is the actual USB Device on an usb audio host device, even tho there might be sound
	coming from profiles like 'analog-stereo'.

	as of this date(April-18-2024), pipewire stores different default settings in 2 different places:
		1 - Default devices, card profiles, channel volumes are stored in this file:
			~/.local/state/wireplumber
			^ controled/modified by 'pactl' and 'pavucontrol'

		2 - General Volume Settings
			/var/lib/alsa/asound.state
			^ controlled/modified by 'alsactl'

	If you're still finding issues, read ALL following sub-topics below.

		USE ALSACTL FOR RESTORING CARD TO DEFAULT VALUES
			pipewire-pulse by default relies on 'alsa' driver:

			This is the command for restoring to default settings:
				# alsactl restore
					or
				# alsactl --file /var/lib/alsa/asound.state restore
			
			Default alsa audio configuration is stored on:
				/var/lib/alsa/asound.state


 			AUTO INITIALIZING ALSACTL RESTORE FOR ALL USERS
				alsactl requires 'root' permissions,
			making it difficult for 'non-sudo' users to use it this way.
			Even sudo users will be required to insert password, which is a problem.

			To make things worse, without 'alsactl', 
			the general volume settings will never be 'defaulted'.

			It's possible to change this, by following these instructions:
			
			1) Copy asound.state to somewhere else:
				# cp /var/lib/alsa/asound.state /etc/alsa/asound.state

			2) Copy the following line:
				[file: /etc/zsh/zprofile]
					alsactl --file /etc/alsa/asound.state restore
				[/file]
				^ just add the lines, don't modify any content in the file

			3)(Optional) Testing to see if it works:
				3.1) Load factory settings:
					$ alsactl --file ./nofile restore
					^ it has to be a file that doesn't exist in the system

				3.2) Load settings again:
					$ alsactl --file /etc/alsa/asound.state restore

				3.3) Test by playing any song on your system

			4) Final regards:
				This solution in step #1 and #2 is an actual work-around meant for 'zsh' users.
				but should work for any other shells as long as you setup and configure 
				the right files for it on your own.

				/etc/zsh/zprofile is only executed 1 time per user.

				If you ever change default audio volume again, you'll need
				to do step #1 and #2 again.

				Recommended Readings:
					TROUBLESHOOTING ALSACTL RESTORE FOR PULSE SERVER
					MAKING SURE AUDIO CHANNELS DEFAULT TO THE SAME VOLUME
					MAKING SURE AUDIO CHANNELS DEFAULT TO THE SAME VOLUME
					USER BIND MOUNT HOME DIRECTORIES

			TROUBLESHOOTING ALSACTL RESTORE FOR PULSE SERVER
			If running pulseaudio server, you have to set server to 'unrestricted' mode
			Read more on:
				SHARING AUDIO WITH MULTIPLE LOGGED USERS ON THE SYSTEM

			if you still stuck on permission issues when using alsactl, you may have to add
			yourself to the 'audio' group and use 'setfactl' for your username:
				Read more on:
					ALSA PERMISSIONS

		MAKING SURE AUDIO CHANNELS DEFAULT TO THE SAME VOLUME
			Make sure all audio channels for the sink device are set to same levels!

			Read section:
				SETTING DEFAULT VOLUME FOR DEFAULT SINK/SOURCE DEVICE

		USER BIND MOUNT HOME DIRECTORIES
			(very important: Bind mount affects performance, so avoid it unless you really need it)

			(note: this step only sets up default channel volumes and default devices and card profiles
			general volume isn't stored by ~/.local/state/wireplumber on this step)

			Pipewire loads configuration as soon as possible when systemd is ready.
		if an user uses bind home directories, after pipewire has loaded all it's default
		settings using the files on 'DEFAULT AUDIO SETTINGS DIRECTORY' the new settings stored on the bind directory
		won't be honored.

		This will cause problems about volume/devices not being honored to it's chosen default settings.

		Solution 1)
			A temporary fix to this is to restart the services 
			by following 'INITIALIZING AUDIO SERVICES' topic.

		Solution 2)
			For a permanent fix, make sure you copy your 'bind mounted' default audio settings
			to somewhere else, ex: 
				# cp -R ~/.local/state/wireplumber/ /etc/my_wireplumber_state

			then restart your PC and copy these files to the non-bind mount, 
			before the actual 'bind mount' happens:
				# cp -R /etc/my_wireplumber_state/ ~/.local/state/wireplumber

			This requires restarting/rebooting your PC.
			However, you'll have to do this everysingle time you change something and want it to be persistent.

	TROUBLESHOOTING DAEMON NOT STARTING
			Note: Don't use this, this is rarely a problem.

		This solves the problem if the pipewire daemon refuses to start due to system
		trial limitation. If you're not being affected, then do not use this.

		0) Increase 'StartLimitBurst=' and 'Restart=' on /etc/systemd/system.conf
			
		1) Do a daemon reload to apply changes made to /etc/systemd/system.conf
			# systemctl daemon-reload 

		2) Reset failed status of a given service
			$ systemctl --user reset-failed pipewire-pulse.service

		3) Restart everything:
			$ systemctl --user restart wireplumber pipewire pipewire-pulse
	
	SETTING UP DEFAULT AUDIO DEVICE
		0)(OPTIONAL) SETTING PULSE_SERVER TO UNRESTRICTED MODE
			This allows changes to all kinds of audio device settings
			(volume, output/input device, audio profile, etc).

			You only need this step if you're running a pulseaudio server for allowing different users
			logged in at the same time to be able to listen to audio.
				Mandatory Reading : "SHARING AUDIO WITH MULTIPLE LOGGED USERS ON THE SYSTEM"

			After change is made, it's advised to copy the file /usr/share/pipewire/pipewire-pulse.conf to 
			/etc/pipewire/pipewire-pulse.conf:
				# cp /usr/share/pipewire/pipewire-pulse.conf /etc/pipewire/

			Pipewire updates will undo any changes you've made to any files located on /usr/share/pipewire
			but not on /etc/pipewire/.
			
		1) SELECTING A CARD PROFILE
			$ pactl set-card-profile <audio device> <profile_for_audio_device>
			^ press tabulation key for auto-completing options.

				0.1)(OPTIONAL) For disabling an audio device, chose the 'off' profile:
					$ pactl set-card-profile alsa_card.pci-0000_00_1b.0 off
					^ this way, the audio card will be disabled.
					^ you can verify that with 'pw-top' tool.

			example: 
				$ set-card-profile alsa_card.usb-C-Media_Electronics_Inc._USB_Audio_Device-00 pro-audio
				^ pro-audio is chosen audio profile for the given audio device.

			While you can have multiple 'audio devices' on the system,
			only one profile per device can exist. The card profile define which audio device is going
			to be used on a given single audio card.(Read more on: EXPLAINNING CARD PROFILES)

			HOWEVER, it's possible to have different profiles on different audio devices and having them working
			at the same time using 'pro-audio' profile.

				WHAT IS PRO-AUDIO? || SPECIAL TOPIC
					source: https://gitlab.freedesktop.org/pipewire/pipewire/-/wikis/FAQ#what-is-the-pro-audio-profile

					1 - Pro-audio allows different audio output devices to be streamed all at the same time.
					For example, using multiple input devices may only work when using pro-audio profile,
					but not work on other standard default profiles.

					2 - Pro-Audio is also a low-latency audio solution. 
					3 - Pro Audio exposes the raw ALSA devices without applying any logic on top.
					4 - Exposes the maximum number of channels on all devices.

					5 - Disables the hardware mixers, it only enables software volume/mute.
					6 - Pro Audio goes around the profile mechanism, and exposes the devices like they are in ALSA. 
					Without hiding anything and without applying controls, Pro-Audio also does not manage hardware 
					volume controls, so you are free to use 'alsamixer' and control the hardware in any way that you like.
					
					READ MORE ON:
						'REDIRECTING GPU HDMI OUTPUT TO REGULAR SPEAKER/HEADSET'

		2)(Optional) List existing sink/sources
			A Sink is the sound device used for listenning to sounds, musics.
			Source is a microphone device.

			$ pactl list short sinks
				or
			$ pactl list short source

		3)(Optional) Getting the default sink/source device.
			$ pactl get-default-sink <device>
			^ Sink is the device to which the PC outputs audio to.
			^ Device used for listenning to songs
				or
			$ pactl get-default-source <device>
			^ Source is usually Microphones

		4(MANDATORY) SETTING THE DEFAULT SINK/SOURCE DEVICE
			$ pactl set-default-sink <device>
			^ Sink is the device to which the PC outputs audio to.
			^ Device used for listenning to songs
				or
			$ pactl set-default-source <device>
			^ Source is usually Microphones

			hint: you can press the 'tab' key for auto-completion on devices list.

		5)(MANDATORY) SETTING DEFAULT VOLUME FOR DEFAULT SINK/SOURCE DEVICE
			You can set volume per channel, syntax is the following:
				pactl set-sink-volume @DEFAULT_SINK@ <channel_1> <channel_2> ... etc

			1) Find out how many channels you own:
				Check the 'volume' line for how many outputs you have

				$ pactl list sinks | grep -B1 -A9 State:
				^sink devices(speakers/headsets)

				$ pactl list sources | grep -B1 A9 State:
				^source devices(microphones)

			2) Set default volume on all channels(input/speakers(or headsets)):
				$ pactl set-sink-volume @DEFAULT_SINK@ 100% 100%

			3) Set default volume on all channels(source/microphone)
				$ pactl set-sink-volume @DEFAULT_SINK@ 100% 100%


		6) PERSISTENCE / PERMANENT SETTINGS
			After you've done the above settings described on step 0) 1), 2) and 3), just type
		the following command:

			$ systemctl --user --now enable wireplumber pipewire pipewire-pulse
				(note: the command right above isn't really needed)

			for users who are migrating from pulseaudio to pipewire:
				you need disable alsa-restore.service and alsa-state.service :
					# systemctl mask alsa-restore.service
					# systemctl mask alsa-state.service

			If you want you could also set permanent volume settings using: 
				$ pactl set-sink-volume <device> 120%

			VERY IMPORTANT NOTE: The same can be done with 'pavucontrol', 
			settings will be written the the DEFAULT AUDIO SETTINGS DIRECTORY
			once 'pavucontrol' is closed.

				DEFAULT AUDIO SETTINGS DIRECTORY
					The settings are saved for each user on:
						~/.local/state/wireplumber

					Files are:
						./default-nodes		#Default Sink(Output) and Source(Input) devices
						./default-profile	#Default profile for each audio device
						./default-routes	#Volume Settings for output devices
						./stream-properties	#Application Specific Settings
									^ ex.: firefox, games, etc

					If you want settings to be the same for all future users,
					you'll need to add those files and directory to "/etc/skel".

			6.0.1) Setting up default volume for a certain application(non-persistent):
				It's not possible to make this a persistent setting:
					$ pactl set-sink-input-volume <tab_key> 100%
						or
					   use 'pavucontrol'

			6.0.2) Volume Too Low
				run 'alsamix', press F6, select the audio device and correct volume.

				VERY IMPORTANT NOTE: 
					Again, 'alsamix' relies on 'alsactl', for persistent settings use
				'$ pactl set-card-profile ...' for choosing the right input-device.

				when using pipewire's pro-audio mode, you may have to change back the
				default audio profile and finally tune the volume you want using the default
				original audio device before switching back to pro-audio profile/device.

				also use pactl for setting up volume and all other settings, this will ensure
				persistent settings.
			----
			6.1)(OPTIONAL/ALTERNATE) Persistence/Permanent Settings
				VERY IMPORTANT: This doesn't work with pipewire-pulse, 
				it's only meant for the old pulseaudio driver and I haven't tested this myself
				to see if it works there.

			Create the following folder and file:
			[file: /etc/pulse/default.pa]
				set-card-profile alsa_card.usb-C-Media_Electronics_Inc._USB_Audio_Device-00 pro-audio
				set-default-sink alsa_output.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.pro-output-0
				set-default-source alsa_input.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.pro-input-0
			[/file]
		
			VERY IMPORTANT NOTE:
				The above settings uses pro-audio profile:
					pro-audio is a pipewire mode where direct access to ALSA is granted, 
					thus allowing low-latency access to audio-devices.


			6.2)(Optional/Alternate) Adding udev rules
				The above method may not work, use the one below:

				1) List available audio card devices
					$ pactl list cards

					[Output Example]
						Card #0
							Name: alsa_card.pci-0000_00_1b.0
						--- snip ----
							Properties:
						--- snip ---
								device.vendor.id = "8086" # This is a 'vendor' attribute for udev rule
								device.product.id = "1c20" # This is a 'device' attribute for udev rule
					[/Output Example]

				2) Create udev rule base on the Output Example given for the audo device card you want.

				[file: /usr/lib/udev/rules.d/91-pulseaudio-custom.rules]
					SUBSYSTEM!="sound", GOTO="pulseaudio_end"
					ACTION!="change", GOTO="pulseaudio_end"
					KERNEL!="card*", GOTO="pulseaudio_end"

					SUBSYSTEMS=="pci", ATTRS{vendor}=="0x8086", ATTRS{device}=="0x1c20", ENV{PULSE_PROFILE_SET}="custom-profile.conf"

					LABEL="pulseaudio_end"
				[/file]

		7)(Optional) REMOVING OLD CONFIGURATION SETTINGS
			This is only meant for those migrating from pulseaudio to pipewire.

			Besides disabling alsa-restore.service and alsa-state.service, you have to delete 
			some old settings files if you're migrating from pulseaudio to pipewire. 
				$ rm -R ~/.config/pulse/
				# rm -R /etc/pulse/

			this will prevent confusions and mistakes in the future in case your audio devices
			stumble upon some kind of problem

		8) TESTING DEFAULT SETTINGS
			8.1) Restart Audio Services
				$ systemctl --user restart wireplumber pipewire pipewire-pulse

			8.2) Enable Audio Services again
				$ systemctl --user --now enable wireplumber pipewire pipewire-pulse

			8.3) Make sure that audio output device is working as you'd expect:
				$ speaker-test -c 2 

			8.4) Making sure that you have the desired volume and 
			the default sink(output) device of choice:
				$ P_SINK=$(pactl get-default-sink);
				$ pactl get-sink-volume ${P_SINK};
				$ echo '\nSINK: ' ${P_SINK};

			8.5) Making sure that you have the desired volume and 
			the default source(input) device of choice:
				$ P_SOURCE=$(pactl get-default-source);
				$ pactl get-source-volume ${P_SOURCE};
				$ echo '\nSOURCE: ' ${P_SOURCE};


			8.6) Alternatively, you can run 'pavucontrol' to check on #7.4 and #7.5 steps.

			
			8.8) Final Notes: if everything is working as expected, then everything you want is set as default.
			if you're still having troubles, make sure that changes are taking effect on the configuration
			files stored on ~/.local/state/wireplumber/
				$ nvim ~/.local/state/wireplumber/*


			also make sure that any file you might have changed under /usr/share/pipewire/ directory
			hasn't been replaced due to an pipewire update! any updates will undo any changes you might
			have made to any files under this one directory. Always copy any modified files from that folder to
			/etc/pipewire/ under the same name.

			^ make sure that you followed every single step correctly

			9) RECOMMENDATIONS
				If this is your first setup, I'd recommend reading the following topics:
					TROUBLESHOOTING VOLUME NOT SET TO DEFAULT
						AUTO INITIALIZING ALSACTL RESTORE FOR ALL USERS
						MAKING SURE AUDIO CHANNELS DEFAULT TO THE SAME VOLUME
						USER BIND MOUNT HOME DIRECTORIES

	DISABLING BLUEZ(BLUETOOTH) FOR PERFORMANCE
		Believe it or not, wireplumber takes 3 to 4 seconds to load 
		bluetooth module :V
		
		On file /usr/share/wireplumber/wireplumber.conf
		you need to comment this line:

			[file: /usr/share/wireplumber/wireplumber.conf]
				#api.bluez5.*    = bluez5/libspa-bluez5
			[/file]
			^ only comment with '#' character, do nothing else!

		It's wise to copy the file to /etc/wireplumber/wireplumber.conf,
		since updates may undo any changes you've made before:
			# cp /usr/share/wireplumber/wireplumber.conf /etc/wireplumber/


		After that, restart the service:
			1) $ systemctl --user restart wireplumber pipewire pipewire-pulse
			2) $ systemctl --user --now enable wireplumber pipewire pipewire-pulse

		Also make sure everything is up and running:
			1) $ systemctl --user status wireplumber pipewire pipewire-pulse


		VERIFYING PERFORMANCE BOOST
			After restarting the system type:
				$systemd-analyze

			this trick should've saved 2~4 whole seconds, depending on the system
			from the boot process.

			If you haven't ever done this before, you could still re-enable the module
			by uncomenting it by removing '#' character, and re-doing this process.
			
			In my old rig, this saved 4 seconds from booting process.
			That's a lot of time!

	VERIFYING WHICH AUDIO DRIVER IS RUNNING
		This is going to check which audio driver is actually running by listing processes:
			# ps -e | grep -P -i "jackd|pulse|alsa|pipewire"

		You can also type:
			$ pactl info

		For checking which files are used for systemd service, read: 
			"HOW TO SYSTEMCTL"
	
	INCREASING SOUND RESAMPLING QUALITY
		source: https://wiki.archlinux.org/title/PipeWire

		If you used PulseAudio with resample-method = speex-float-10 or soxr-vhq, 
		then you might consider uncommenting and changing resample.quality = 4 to 10 or the maximum 14 
		in stream.properties block in both /etc/pipewire/client.conf and /etc/pipewire/pipewire-pulse.conf 
		(copy them from /usr/share/pipewire/ if they do not exist). 

		Do not forget to restart the pipewire.service and pipewire-pulse.socket user units
		(never forget pipewire-pulse.socket if you want your configuration changes to be applied). 

		you can use pw-conf to see which files are in use, read 'man page' for pw-conf tho.

	TROUBLESHOOTING AUDIO NOT WORKING PROPERLY || SYSTEMD ONLY
		If audio is still not working, it's worth doing the following:

		1) find all audio related service files on systemd directory:
		$ OUTPUT=$(sudo find /usr/lib/systemd/ -iname '*pipewire*' -or -iname '*jack*' -or -iname '*pulse*' -or -iname '*alsa*')
		
		2) Parse output to create service only names:
		$ AUDIO_SERVICES=$(echo $OUTPUT | grep -P -i -o "((?<=/)systemd(.*service.*|.*socket.*))" | sed -e 's/systemd\///' -e 's/user\///' -e 's/system\///' -e 's/.*\///')

		3A) Verify status for services system-wide:
			echo $AUDIO_SERVICES | awk '{system("sudo systemctl status \""$0"\"")}'

		3B) Verify status for user-only
			echo $AUDIO_SERVICES | awk '{system("systemctl --user status \""$0"\"")}'

		4A) Show Info on Service, system-wide:
			echo $AUDIO_SERVICES | awk '{system("sudo systemctl show \""$0"\"")}'

		4B) Show Info on Service, user-only
			echo $AUDIO_SERVICES | awk '{system("systemctl --user show \""$0"\"")}'


		hint #1: Some of these services will report error, because they're either system-wide only or user-only.
		system-wide only services aren't shown when 'systemctl --user' is invoked and vice-versa.

		hint #2: Some services have '@' characters in it, these services require removing the '@' character
		and only 'show' can be used, not 'status'.

		hint #3: data displayed on screen for steps 3) and 4) are in order of the list given on OUTPUT environment
		variable. if the service is located under /user/ directory, it's user-only. services located under /system/
		is system-wide only.

		Make sure everything is running as you would expect.


	SETTING UP EASYEFFECTS
		'Easyeffects' is a tool that allows system wide audio equalization and effects on both
		caputiring and outputting devices. It has the capability of noise reduction on poor microphone devices
		and audio quality improvements on poor audio device cards, like the ones found on onboard sound cards.

		It allows you to customize your own audio experience.

		AUDIO EQUALIZATION AND SOUND EFFECTS FOR PIPEWIRE
			Install package 'easyeffects', 'lsp-plugins' 'lmms'.

			Read More ON: SETTING UP EASYEFFECTS AS DEFAULT AUDIO SOURCE

			Here is a list of community presets:
				https://github.com/wwmm/easyeffects/wiki/Community-presets
				^ I've never used this, I made my own preset.

		SETTING UP AUDIO SINK AND SOURCE FOR CAPTURING SOFTWARE/DEVICES
			The instructions here only apply if you're using 'easyeffects'.

			In general, if a program captures audio sink/source for streaming it to 3rd parties
			(Other people or programs), they should not use the default audio system devices.

			Programs like 'obs' will require you to change the Global Audio Devices to 
			"Easyeffects source"(For Microphone) and "Easyeffects sink"(For Desktop Audio).
			Otherwise the effects and filters applied as well as the volume settings won't work.

			The same happens for programs like 'discord', 'teamspeak', games and programs that also capture your 
			microphone and desktop audio.

			NOTE:
				It's important to note, you shouldn't change the 'default output' settings for these programs,
				which shall always be your actual default audio device for your system.

				Only change things for capture, as already described.

			NOTE 2:
				On alsa(using pavucontrol), never change default audio device to 'Easyeffects source/sink'.
				'easyeffects' usually does this job on it's own for each running applications on your system.

				The truth is 'easyeffects' use your default audio device for applying effects, so changing it
				will break 'easyeffects'.
			

		TESTING OUTPUT/INPUT DEVICES IN OBS STUDIO
			Use 'obs studio' to test for both sink and source devices volumes, 
			right click any of the audio source/sink device in 'obs' and click 'advanced audio properties' 
			and finally set the 'audio monitoring' column to 'monitor' value.

		MONO INPUT DEVICES IN OBS STUDIO
			Only in 'obs', if you're hearing audio(from source devices) in only 1 side of your headset/speakers,
			then you have to right click the source device(in obs) and check the 'mono' option for your source 
			device.

			at this point, your obs default source device should be "Easyeffects Source"

		PERFECT AUDIO VOLUME SETTINGS
			It's best to leave 'alsa' and 'obs' at 100% volume for both input/output,
			then use easy effects to modify audio sink/source into the desirable volume.

			You should always leave 'easyeffects' to handle the desired global volume and then use
			your default audio system control(pavucontrol) for doing minor adjustements, depending on your 'mood'.
		
			This way, you can have global volume settings that sound good for all programs/games/movies that 
			output audio.

			Only making minor adjustements inside those programs/games/movies for your taste.

			If you don't follow this up, you'll end up with "spaghetti volume controls" that need to be changed 
			every single time you run a different program, game or movie.

				Side Note:
					Avoid spaghetti volume! If you need help in setting up default system volume,
					read the topic: 
						SETTING DEFAULT VOLUME FOR DEFAULT SINK/SOURCE DEVICE

			Easyeffects:
				The volume was set using the 'Loudness' filter/effect for both USB Audio Output and 
				USB Audio Input.

				Headset/Speakers(Sink Device):
					Output: 0db
					Input: -16.9db

				Microphone(Source Device):
					Output: 0db
					Input: -3.8db

			Pavucontrol:
				USB Audio Output: 100%(0db)
				Microphone: 100%(0db)


			OBS Studio:
				Output: 100% (0db)
				Input: 100%  (0db)

				NOTE: In OBS Studio select 'Default Sink Device' and 'Default Source Device' 
				when using easyeffects.

				Also remember to select 'mono' if your 'source'(microphone) is a mono device, 
				or audio will only play in one side.

		SETTING UP EASYEFFECTS AS DEFAULT AUDIO SOURCE
			SETTING UP ON SDDM DISPLAY MANAGER
				(USE THIS - NOT WORKING FOR SOME REASON THO, USE ZSH BELOW)

				Add these lines to the end of the file:

				[file: /usr/share/sddm/scripts/Xsession ]
					(easyeffects --gapplication-service > /dev/null 2>&1 &)
					(disown %easyeffects > /dev/null 2>&1 &)
				[/file]
				^ parenthesis allow any return to be ignored

			SETTING UP FOR ZSH
				VERY IMPORTANT: This only works, because easyeffects requires 
				display server/manager to be initialized.

				Add this to your zsh's zprofile:

				[file: /etc/zsh/zprofile]
					(easyeffects --gapplication-service > /dev/null 2>&1 &)
					(disown %easyeffects > /dev/null 2>&1 &)
				[/file]

			(ALTERNATIVE) SETTING UP FOR GNOME DESKTOP
				After launching 'easyeffects', simply do the following:
					Go to 'preferences' > click on 'general' tab > enable 
					"Launch service at system startup"

					The above however only works for gnome desktop environments, 
					it creates a ~/.config/autostart/ desktop file. 
					typing 'systemctl --user status easyeffects' will confirm the path
					it is actually using.

	PIPEWIRE TOOLS / UTILS
		Just a small list of tools:
			pw-top, helvum, coppwr, pavucontrol, alsamixer, pactl, speaker-test.

	ALSA PERMISSIONS
		VERY IMPORTANT: This doesn't work for pipewire-pulse, because pipewire doesn't rely on audio groups anylonger
		for permissions only the old 'pulseaudio' server relies on audio groups for audio settings.
		however, you may have to run 'alsactl' to restore audio states.

		This is only necessary if you run into problems for setting default-sink and default-source devices
	using 'pactl' as well as setting up volumes using 'pavucontrol'; this problem only arised when using PULSE_SERVER
	to allow multiple users to use audio device at the same time.

		VERY IMPORTANT: Before you run this solution, make sure you've set 'client.access = "unrestricted"'
		and that the permission problem still persists when running PULSE_SERVER.
		read the following topic for more info: "SHARING AUDIO WITH MULTIPLE LOGGED USERS ON THE SYSTEM"
		only try this, if the problem still persists:

		1) Make sure /dev/sdn uses ACL First
			$ ls -la /dev/snd/

			if a '+' symbol shows up right by the permission set, that means /dev is using ACLs.

		2) Add your user to ACL on /dev/snd
			# setfacl -m u:${USER}:rw /dev/snd/*

		3) If you still run into trouble, add yourself to audio group
			# gpasswd -a ${USER} audio


	SETTING UP PRO-AUDIO PROFILE || PERFORMANCE
		Note: Read section 'WHAT IS PRO-AUDIO?' for more info:

		1) Select your Current Audio Device and set it to pro-audio profile.
		example:
			$ pactl set-card-profile alsa_card.usb-C-Media_Electronics_Inc._USB_Audio_Device-00 pro-audio

			Note: If you're using 'ZSH' you can use TAB Key for auto-completion
				$ pactl set-card-profile <TAB-Key> <TAB-Key>

		2) Set the default Output/Sink Device:
			$ pactl set-default-sink alsa_output.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.analog-stereo    

			Note: If you're using 'ZSH' you can use TAB Key for auto-completion
				$ pactl set-default-sink <TAB-Key>

		3) Set the default Input/Source Device:
			$ pactl set-default-source alsa_input.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.pro-input-0    

			Note: If you're using 'ZSH' you can use TAB Key for auto-completion
				$ pactl set-default-source <TAB-Key>

		4) Change volume controls:
			$ alsamixer
			     or
			$ pavucontrol

		5) You can finally test audio with:
			$ speaker-test -c 2

		6) (Optional) You can check which device is in use:
			$ pw-top

		7) (Optional) More info:
			$ pactl info

	----
	REDIRECTING GPU HDMI OUTPUT TO REGULAR SPEAKER/HEADSET
		This requires setting up pro-audio profile first, which will be described below:
		Note: Read section 'WHAT IS PRO-AUDIO?' for more info on PRO-AUDIO.

		1) First, list available PCI Hardware Devices (Only meant if you own a PCI GPU)
			$ lspci

			output example:
				01:00.1 Audio device: NVIDIA Corporation Device 10fa (rev a1)

		2) Select your HDMI Audio Device and set it to pro-audio profile.
		example:
			$ pactl set-card-profile alsa_card.pci-0000_01_00.1 pro-audio

			Note: If you're using 'ZSH' you can use TAB Key for auto-completion
				$ pactl set-card-profile <TAB-Key> <TAB-Key>

		3)  Set the default Source Device: 
				$ pactl set-default-source alsa_output.pci-0000_01_00.1.pro-output-9   

				Note: If you're using 'ZSH' you can use TAB Key for auto-completion
					$ pactl set-default-source <TAB-Key>

		4) Set the default Input/Source Device:
			$ pactl set-default-source alsa_input.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.pro-input-0    

			Note: If you're using 'ZSH' you can use TAB Key for auto-completion
				$ pactl set-default-source <TAB-Key>

		5) Using 'helvum', 'qpwgraph' or 'coppwr' connect the 'monitor' outputs for the device to the
		'playback' inputs of your audio-device.
			hint: if you have trouble, try finding them by the device names given when using 'pavucontrol'.

		6) Change volume controls:
			$ alsamixer
			     or
			$ pavucontrol

		7) You can finally test audio with:
			$ speaker-test -c 2

		8) (Optional) You can check which device is in use:
			$ pw-top

		9) (Optional) More info:
			$ pactl info
	----
	AUDIO TEST || SPEAKER TEST
		This instruction is meant for pipewire-pulse only: you can make sure of this by typing 'pactl info'.
	you can check it's 'alsa' and not 'oss' driver with 'aplay -L', by going through the default device being used.

		$ pactl info				#Displays which driver is being used
							^ Makes it possible to see if pipewire is running with Alsa, etc.
		$ aplay -l				#Lists all sound devices and shows default audio device driver
		$ aplay -L				#Same as above, more details

		$ pactl get-default-sink		#Returns default sink
		$ pactl get-default-source 		#Returns default sink
		$ pactl set-default-sink <sink_device>	#Sets default sink
		$ pactl set-default-source <source_device>	#Sets default source

		$ speaker-test -c 2			#Tests left and right audio channels
		$ speaker-test -c 8			#Tests 8 different audio channels
							^ 7.1 Sorround test: Front, Center, Side, Rear and Woofer Speaker

		FETCHING VALID ALSA CARD NAMES
			VERY IMPORTANT: This is usually not needed, only use this as a last
		resource measure, make sure to read "SHARING AUDIO WITH MULTIPLE LOGGED USERS ON THE SYSTEM"
		and that you have 'client.access = "unrestricted"' if running a PULSE_SERVER.

			The name can be used for setting up the default environment variable in /etc/environment
		when running PULSE_SERVER:
			$ aplay -l | awk -F \: '/,/{print $2}' | awk '{print $1}' | uniq
		
			[file: /etc/environment]
				ALSA_CARD=Device
			[/file]

			source: https://wiki.archlinux.org/title/Advanced_Linux_Sound_Architecture#Making_sure_the_sound_modules_are_loaded

		FETCHING UNUSED DEVICES
			(this has no use)

			Fetches unused audio devices:
			$ cat /proc/asound/card*/id

			source: https://wiki.archlinux.org/title/Advanced_Linux_Sound_Architecture#Making_sure_the_sound_modules_are_loaded

	SHARING AUDIO WITH MULTIPLE LOGGED USERS ON THE SYSTEM
		This require setting up the Default Audio Device, since it becomes impossible changing it after
	the server is online.
		
		This also requires installing 'decklink' package(AUR Package) for OBS-STUDIO to work.

		obs: Just uncomment or copy-paste the line mentioned!

		[file: /usr/share/pipewire/pipewire-pulse.conf]
			pulse.properties = {
			    # the addresses this server listens on
			    server.address = [
				"unix:native"
				#"unix:/tmp/something"              # absolute paths may be used
				#"tcp:4713"                         # IPv4 and IPv6 on all addresses
				#"tcp:[::]:9999"                    # IPv6 on all addresses
				#"tcp:127.0.0.1:8888"               # IPv4 on a single address
				#
				{ address = "tcp:127.0.0.1:4713"    # address
				   max-clients = 130                 # maximum number of clients
				   listen-backlog = 130              # backlog in the server listen queue
				   client.access = "unrestricted"     # permissions for clients
				}
			    ]
			}
		[/file]
		^ Only modify file to look like this, there's no need to add anything else.
		^ by default address is set to "address = tcp:4713" allowing connections from any external network!!!
		^ make sure to setup address to 'localhost' as demonstrated above.
		^ if using iptables as firewall, you may need to enable 'localhost' connections to 4713 port.

		VERY IMPORTANT: client.access = "unrestricted" exists so that when PULSE_SERVER is running,
		connecting clients can change settings like default-sink-source or volume.

		runs program with the following environment variable:
			PULSE_SERVER=tcp:127.0.0.1:4713
				or
			PULSE_SERVER="tcp:127.0.0.1:4713"

		^ The above environment variable should be added to /etc/environment 
		NOTE: Pipewire/Wireplumber might only work with 'dbus-broker' at this point, so install it.

		VERY IMPORTANT: Updates can change the pipewire-pulse.conf file mentioned above,
		it's recommended to copy it to /etc/pipewire/ after making changes.

		After doing this you'll need to restart the service and re-enable it again:
			$ systemctl --user restart wireplumber pipewire pipewire-pulse
			$ systemctl --user --now enable wireplumber pipewire pipewire-pulse 

			READ more on: 'INITIALIZING AUDIO SERVICES'

		----
		ALTERNATE OPTION (DON'T TRY THIS, DOESN'T WORK):
			Tried this, but didn't work. 
			Server started, but The logged in user was never able to connect.

			Server use:
			$ pactl load-module module-native-protocol-tcp port=4713 listen=127.0.0.1

			Testing to see if pipewire-pulse is set for IP Port Listenning:
			$ lsof -i :4713

			Client use:
			$ env PULSE_SERVER=tcp:127.0.0.1:4713 pactl load-module module-tunnel-sink server=tcp:127.0.0.1:4713 &
				or
			$ pactl load-module module-tunnel-sink server=tcp:127.0.0.1:4713 &

			Unload Module Server Side:
			$ pactl unload-module module-native-protocol-tcp
		----

	
	CREATING VIRTUAL INPUT/OUTPUT AUDIO DEVICE
		Simply do: $ pactl load-module module-combine-sink 

	CONNECTING OUTPUT SOUND DEVICE TO VIRTUAL INPUT DEVICE
		1) Load Module
			$ pactl load-module module-combine-sink
			^ creates virtual device called 'combined'

		2) Open helvum, coppwr or qpwgraph
			$ helvum&
			^ connect/link output to combined device.

		advice to use coppwr, since it has better interface.

		3) Open pavucontrol
			$ pavucontrol&
			^ disable combined as 'output device'
			^ only leave 'combined' as input

		4) Setting up Input to Output
			In 'Helvum' look for the device with the same name as your Output Sound Device,
		make sure to have screen at 100% for doing the following:
			1) Click and Drag 'monitor_FL' and 'monitor_FR' to the 
			'Combined' device that has 'playback_FL' and 'playback_FR' respectively.

			2) In pavucontrol, Disable all 'combined' device from the 'Playback Device' list
			(no need to remove it).

			3) Play any song and go to 'pavucontrol' on the 'input devices' list to make sure it's working
			
			4) Setup your game to use 'combined' as microphone/input device.

			note: pavucontrol can be replaced by any audio device control program
			note2: 

		4.1)(ALTERNATE OPTION) Setting up Input to Output
		If you have OBS-STUDIO, it's possible to create the 'combined' device there,
		so you can listen to it. READ MORE ON: "SETTING UP THINGS FOR STREAMING"

		Unload Module When Done:
			$ pactl unload-module module-combine-sink

		PLAYING DESKTOP MUSIC TO INPUT DEVICE AND USING MICROPHONE AT THE SAME TIME | REDIRECTING DESKTOP AUDIO AS INPUT DEVICE
			1) Open 'helvum', 'coppwr', 'qpwgraph'

			2) Find your audio input(microphone) device in 'helvum' or 'coppwr' by name
				You can check the name by looking at your audio control system(pavucontrol)

			4) Load 'module-combine-sink' via 'pactl'
				$ pactl load-module module-combine-sink

			5) Connect your input-audio device to the combined sink that has the following 'playback_fl' as input 
			and 'monitor_fl' as outputs. Connect them to the playback.

			6) Read topic: "CREATING KEY BINDINGS" to learn how to make key binds for mpv.

			7) Go to game and select 'combined' as input(microphone) device.

			8) Now you can talk on the microphone and play songs at the same time.

		VOICE MODULATION THROUGH CARLA
			1) Download 'carla' and download the following plugins:
				aether.lv2, tap-plugins-lv2-git

			2) Go to Settings > Configure Carla > Engine > Set Audio Driver to 'Pulseaudio'
			
			3) Connect 'capture' nodes to 'carla' audio-in nodes and connect 'carla' audio-out to playback device.

			4) Go to patchbay > Right Click and 'Add Plugins'

			5) Add 'Carla Rack' of type internal

			6) Click Rack Tab and click on 'Carla Rack's Settings

			7) Add plugins you want.

			5) done!

	TROUBLESHOOTING
		https://gitlab.freedesktop.org/pipewire/pipewire/-/wikis/Troubleshooting#stuttering-audio-in-virtual-machine
		https://wiki.archlinux.org/title/PipeWire
		https://bbs.archlinux.org/viewtopic.php?id=280654
		https://gitlab.freedesktop.org/pipewire/pipewire/-/wikis/FAQ#what-is-the-pro-audio-profile
			
	MISC
	https://onlinetonegenerator.com/			#Use 16000HZ sine wave form, then save/download and use this:
							^mpv --volume-gain=100 --loop=inf --audio-samplerate=200 ~/16000.wav
	https://onlinetonegenerator.com/

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
RESIZING EXISTING QCOW IMAGES
	Qcow2 disk images uses thin-provisioning by default.

Very Important:
	Some filesystems can not be resized without it's Kernel Drivers installed on linux.
	Sometimes it may not be possible to resize given partition on Linux due to lack 
	of filesystem kernel module/driver!
	^ This isn't related to cqow and it also applies to any other image types like raw!
	^ MAKE SURE TO MODPROBE THE DESIRABLE FILESYSTEM FIRST AND CHECK WITH LSMOD AFTERWARDS!
	^ It's advisable to use the virtual machine to allocate the desirable free space, so you 
	don't have to deal with this.
	^ Optionally, if gparted doesn't work, try using cfdisk.


Very important: 1(A) and 2(A) Describe completely different steps that do not need to be followed in an order.
1(B) describes a follow up step to 1(A), which may or may not be followed by the user(optional step) in order to procede.
1.2(A), if it existed, would describe a mandatory follow up step to 1(A) if nothing else procedes it like 1.1(A).

0(A) - Make sure you're not running the Virtual Machine with the image given file you want to risize.
	0.1(A) - If you have it mounted elsewhere on your system, make sure to unmount everything first!
	0.2(A) - DO NOT FORGET TO UNMOUNT IT USING STEP 1.4(B)

1(A) - SHRINKING SPACE on a virtual machine image
	#virt-sparsify --in-place disk.qcow2
	^ --in-place is used to prevent virt-sparsify from creating a new copy
	^ since virtual machines can be VERY big.
	^ Very Important: if the given free available space the user wants to shrink is already allocated
	to a filesystem inside the image, virt-sparsify may not be able to shrink it,
	it may be required to unnalocate/dealocate the free space as unused disk space first!
	^ Optionally, if gparted doesn't work, try using cfdisk inside your current system, not the VM.

1(B) - Use qemu-nbd to mount qcow images on the current system for managing it with parted/gparted!
	1.1(B) - First make sure NBD Kernel Module is loaded first
		#modprobe nbd max_part=10

	1.2(B) - Mounting qcow image on the current system
		#qemu-nbd -c /dev/nbd0 disk.qcow2

	1.3(B) - Use parted/gparted - gparted preferred - to dealocate/unnalocate any unused disk space 
	from any existing partition(s).

	1.4(B) - Unmount The nbd device(DO NOT FORGET TO UNMOUNT!) - VERY IMPORTANT
		#qemu-nbd -d /dev/nbd0

	1.5(B) - Finally use virt-sparsify as described on Step 1(A) for shrinking/reclaiming unused space.
	^ virt-sparsify is only used here to reclaim unused space, shrinking down the image size,
	not actually resizing it. - VERY IMPORTANT.

2(B) - INCREASING SPACE on a virtual machine image 
	#qemu-img resize disk.qcow2 +1G
	^ This only allocates more unused free space,
	^ The new free space may need to be partitioned or added into a given partition(gparted or cfdisk)
	^ If you do this on your current machine, you will be required to follow all Steps for 1(B), 
	^ and make an exception for 1.5(B) and don't follow it.
	^ It's advisable to use the virtual machine to allocate the desirable free space, so you 
	don't have to deal with this.
	^ Optionally, if gparted doesn't work, try using cfdisk inside your current system, not the VM.


VERY IMPORTANT NOTES:

	virt-sparsify should NEVER be used for resizing images, for that use virt-resize!!!

	The virt-sparsify command-line tool can be used to make a virtual machine disk (or any disk image) sparse. This is also known as thin-provisioning. Free disk space on the disk image is converted to free space on the host. 

	In Other words, always prefer virt-sparsify for create image files for virtual machines over other methods!
	qcow2 isn't a SPARSE IMAGE FILE FORMAT!

	virt-sparsify can be installed by installing 'guestfs-tools' package(on archlinux)
	SOURCE:
		https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/virt-sparsify

---ABOUT SPARSE FILES
https://wiki.archlinux.org/title/sparse_file

Summary: 
	Always prefer sparse image files!!!!
	source: https://wiki.archlinux.org/title/sparse_file

Creating sparse files:
	$dd if=/dev/zero of=file.img bs=1 count=0 seek=512M
		or
	$truncate -s 512M file.img

These two commands are going to display different file size if file.img is a sparse file(same file):
	$du -h --apparent-size file.img
	$du -h file.img

Converting non-sparse files to sparse:
	$fallocate -d copy.img

Converting existing sparse files to non-sparse:
	$cp file.img copy.img --sparse=never

Then new_file.img will be sparse. However, cp does have a --sparse=when option. This is especially useful if a sparse file has somehow become non sparse (i.e. the empty blocks have been written out to disk in full). DISK SPACE CAN BE RECOVERED BY:

$cp --sparse=always new_file.img recovered_file.img
^ VERY IMPORTANT

---ABOUT RAW DISK IMAGES

Raw disk image format is default format in Qemu.

pros of using raw disk images:

    It is simple and portable to any other machine.
    It represents default binary format of a hard disk.
    Nearly raw performance then other formats, as it has very little overhead and no metadata and lastly,
    Only written data will occupy space, rest of space will be filled with zeros, as it is a sparse file.

Cons of using RAW Disk Images:

    Does not have other features like compression, AES encryption and snapshot.
    Backup requires full disk-up, as no incremental back-up and lastly,
    Deleted files still occupy space and have to be removed.


--ABOUT QCOW2 DISK IMAGES

qcow2 is copy on write image disk, where constant size units called clusters compose a file. A cluster holds both data as well as image metadata.

Pros of using qcow2 Disk Images:

    Smaller images are produced, as no sparse file.
    It provides zlib based compression.
    For data security, AES encryption can be used to protect disk image.
    Multiple Virtual Machine snapshots are offered, as incremental back-up.
    Small Cluster Size improve image file size, and larger can be used for better performance and,
    Larger Preallocation increases performance as image size increases to grow.

Cons of using qcow2 Disk Images::

    Very slight performance loss in comparison to raw disk image, due to metadata, compression and encryption,
    One needs to use fstrim to trim image file, as deleted files does increases image size.

--------
DELETING A FILE DOES NOT FREE UP SPACE ON DISK
On Linux or Unix systems, deleting a file via rm or through a file manager application will unlink the file from the file system's directory structure; however, if the file is still open (in use by a running process) it will still be accessible to this process and will continue to occupy space on disk. Therefore such processes may need to be restarted before that file's space will be cleared up on the filesystem. However, this case isn't related to sparse vs non-sparse files.

source: https://access.redhat.com/solutions/2316

--------
FIXING WRONG DISK SIZE
	If you've been noticing wrong disk sizes, know that some programs like gparted may display different byte sizes
when compared to programs like du, df, etc..

If you still feel that you have a wrong disk size, know that it can only happen when you clone a hard drive into another,
for ext2/ext3/ext4 partitions you'll have to use 'resize2fs' as follows:

	#resize2fs /dev/sdXY
	^ this solution can be done on mounted filesystes as long as they have 'resize_inode' as filesystem feature
	^ this can be checked using the following:
			#dumpe2fs /dev/sdXY | less
	^ you can use tune2fs to enable resize_inode as well if it's not there in the list of filesystem features
	^ if feature it's not available at all, you'll have to use resize2fs while disk is offline/umounted.

This should fix any wrong disk size issues if you have any, if it doesn't fixes then you don't have to worry about anything.

Furtherover try using a program like 'ncdu' to figure out what is taking so much hard disk space on your PC and then delete
those files.

~/.local/share/Trash/  is a known directory to store old deleted files.
/var/cache/pacman/   is also another very well known directory
/var/log/sudo/	     is used for storing each time a user calls on sudo, it can become pretty big.

'ncdu' will help you better locate these files/directories.

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
TESTING MICROPHONE ON LINUX
1 - Install pavucontrol if it doesn't exist on your system
	#pacman -S pavucontrol

2 - Turn on Audio Loopback for Microphone Device:
	$pactl load-module module-loopback latency_msec=1

3 - Turn off the loopback after testing it:
	$pactl unload-module module-loopback


IT'S ALSO POSSIBLE TO DOWNLOAD "QJACKCTL" AND "JACK-RACK":
	VERY IMPORTANT NOTE: 
		as of 2024, pipewire is the default audio media server,
	because of this, it's better using 'carla' instead of 'qjackctl'
	also downloading aether.lv2, tap-plugins-lv2-git for same plugin support.

	qjackctl mutes and disables pulseaudio. so avoid using it.

0 - Start qjackctl
1 - Click on start
2 - Click on Graph
3 - Connect Capture device to System Output

FURTHER SETTINGS OF QJACKCTL:

1 - Go to settings 
	Interface: "hw:device"
	Sample Rate: 44100
	Frames/Period: 256
	Periods/Buffer: 2

2 - The objective here is to allow delay to be as low as possible.
^ Delay value can be found in between 'started' and 'stopped' at the botom


APPLYING EFFECTS ON JACK-RACK

0 - Open qjackctl first

1 - Open Jack Rack second

2 - Apply desired effects under Jack Rack

3(Optional) - Follow "FURTHER SETTINGS OF QJACKCTL" if needed

4 - Connect Capture device to Jack Rack and connect Jack Rack to System Output device
^ follow "IT'S ALSO POSSIBLE TO DOWNLOAD "QJACKCTL" AND "JACK-RACK"" instructions if needed.

INSTALL TAP PLUGINS
	#pacman -S tap-plugins
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
HOW TO USE RSYNC FOR BACKING UP DATA
	Rsync is a great tool for backing up data
both remotely and locally.

--General Use Commands:
	-q			#Quite mode, useful if using crontab for scheduling backups
	-v			#Increase Verbosity
	--recursive, -r 	#Rescurse into directories
	--mkpath		#Create destination's missing path components
	--progress		#Shows progress while copying/backing up data.
	--inplace		#Prevents new files from being created when updating 'dest' files if they exist
				^ Really only useful if the given file is a Hard Link, this should rarely be the case.
				^ --inplace has some limitations in case files are being used by the system.
				^ files in use by the system can not be updated,
				^ in case of power blackouts, files on 'destination' may be left corrupt.

	--archive, -a		#Archive mode is -rlptgoD by default: it's directory recursive, keeps symlinks, etc...
				^ unlike tar, rsync's archive mode doesn't compress files, it's a simple file transfer/copy
				that preserves the below:

					-r recursive
					-l keep symlinks as symlinks
					-p preserve permissions
					-t preserve modification times
					-g preserve group
					-o preserve owner
					-D preserves device files(-d) and special files(--special)

				OTHER PERMISSIONS NOT INCLUDED ON --ARCHIVE:
					-A preserve ACLs
					-X preserve extended attributes
					-U preserve access times
					-N preserve creation time

	--stats			#Give some file-transfer stats
	--fsync			#Fsyncs every written files
	--list-only		#List files instead of copying them
	--log-file=<FILE>	#Logs what's going on to a given file - VERY IMPORTANT
	--itemize-changes	#Outputs a change summary for all updates
		or
		-i

	--human-readable	#Prints numbers in human readable format
		or
		-h
	
	--dry-run		#Runs the command without commiting anything, meant for testing
	    or
	    -n


--Resource Options:
	The default behavior for rsync is to update files onto 'destination' when either
modification times or data sizes have been changed in case files already exist.

	--checksum		#Changes 'default behavior' with checksumming instead,
				  HOWEVER, this will take considerably a bigger ammount of time since
				  Rsync now needs to check each files on it's entirety and generate a checksum
				  for each on both 'source' and 'destination'.
				^ This is however very useful when backing up compiled code objects, since data sizes
				  may be the same at some points, specially on systems where modification timestamps
				  can not be relied upon.

	--existing		#Only update files that exist on 'destination'.
	--ignore-times		#Update all files ignoring modification times.
	--size-only		#Update all files, disregarding modification times, only taking data size in consideration
				^ Useful when systems may not PRESERVE exact timestamps.
	--ignore-existing	#Skip updating files that already exist on 'destination'.
	--delete		#Delete files on 'destination' that do not exist in 'source'. -- needs checking
	--delete-excluded	#Same as --delete, but deletes 'destination' files if they don't 
				^ exist on 'source' anymore. - nneeds checking

	--max-size=<SIZE>	#Don't transfer any file larger than SIZE.
	--min-size=<SIZE>	#Don't transfer any file smaller than SIZE.
	--max-alloc=<SIZE>	#Change a limit relating to memory alloc.

	--sparse		#Turns sequence of nulls into sparse block.
	   or
	   -S

	--preallocate		#Allocate 'destination' files before writting them.

       --links, -l              #Copy symlinks as symlinks.
       --copy-links, -L         #Transform symlink into referent file/dir.
       --hard-links, -H		#Preserve Hard Links.

	--whole-file		#Copy files whole (w/o delta-xfer algorithm).
	   or
	   -W

--Compression Options:
       --compress				#Compress file data during the transfer
       	   or 
	   -z           

       --compress-choice=<compress_algorithm>	#Choose the compression algorithm (aka --zc)
       --compress-level=NUM			#Explicitly set compression level (aka --zl)
       --skip-compress=LIST			#Skip compressing files with suffix in LIST

--Remote Options:
	--remote-option=<OPT>	#Send options to the remote side only
		or
		-M

PRACTICAL EXAMPLES
	EXAMPLE 1 - CREATING ARCHIVES
		Rsync's archive ensures permissions, ownership, etc(CHECK ABOVE) to be preserved,
	Rsync's archive mode doesn't does any sort of compression.

		$rsync -av /path/to/source /path/to/destination			#Will copy source into destination directory.
										^ When destination is a file, it'll
										overwrite it.
	
		VERY IMPORTANT: when source is a directory and a slash '/' character is added like this:
						/path/to/source/
		
				^ then only the contents of 'source' directory will be copied!

	EXAMPLE 2 - UPDATING 
		Update allows to preserve files that are NEWER on destination folder. 
	that way newer files WILL NOT be overwritten. This is done checking "Modification Time".

		-------------
		VERY IMPORTANT: delta-xfer is already done by default and you shouldn't 
		need UPDATE unless you need to preserve files based on 'Modification Time'.
		-------------

	This is specially useful if you have updated files from a different location and now you need
	to update them from your home PC due to creating new files on your local PC.

	And also If other people are sharing the same file directory/space/server, it'll prevent replacing newer files.
	BEWARE that this is done based on 'Modification Time' of the files.

	Also note that if 2 different people modify the same file, this WILL NOT resolve conflicts as in what github does.
	Instead use 'git' if you need to resolve differences.

		$ rsync -av --update /path/to/source /path/to/destination

	EXAMPLE 3 - DELETE
		'Delete' allows files that don't exist in 'source' to be removed from 'destination'. 
	This allows directory 'cloning', making 'destination' have the exact files and folder structure as 'source'.
	this clones 'source' to 'destination'.
	
		VERY IMPORTANT: BE VERY CAREFUL, chosing or settting the WRONG DESTINATION path WILL DELETE ALL FILES 
		THAT DO NOT EXIST ON 'SOURCE', ex:
			$ rsync -av --delete /path/to/source /usr/bin/
			^ will delete the entire /usr/bin/ before cloning 'source' directory into it!

		The below is a safer approach as it will copy any files and dirs to a user specified 'backup directory'
		before overwritting and/or replacing them:
			$ rsync -av --delete --backup-dir=../Backup_Files /path/to/source/ /path/to/destination

		VERY IMPORTANT: Be very careful when using '*' wildcard along with --delete when setting up 'source' path;
		due to it's nature, rsync may delete files permanently backup files in the --backup-dir can be overwritten
		if they own the same name.

		VERY IMPORTANT: Using '*' wildcard doesn't work, because the shell will run the rsync command multiple times,
		making rsync blind to which files are being transferred.

		VERY IMPORTANT: The absence(or lack) of the slash '/' character on 'source' will tell rsync 
		to copy the entire directory instead of the files in it. --delete should work both ways,
		but always keep this in mind when copying files using rsync.
		^ Due to --delete's nature, you may lose files as well here.


		EXAMPLE 1: If you use an old hard drive or suffer from power blackouts or system crashes, 
		it's not advisable to use --delete; if your data gets removed due to data corruption,
		this will also lead to the same data to be removed from the backup folder/disk you're using.

	EXAMPLE 4 - BACKUP
		Backup is always created on the destination folder,
	it's meant to be used for backing up files that are replaced/deleted when both
	overwritting same files or deleting them through rsync.

	It is advised to use '../' for specifying the backup folder, since a 2nd subsequent --delete
	operation would delete the backup folder along with it's stored backup files otherwise:

		$ rsync -av --delete --backup-dir=../My_Backup source2/ dest

		^ VERY IMPORTANT: Always make sure the backup directory in --backup-dir is located outside
		the destination folder as already described!
	
	EXAMPLE 5 - LOGGING AND COMPRESSION
		Compression allows speeding up the transfer when it needs to be transferred online.(-z)
		Logging allows storing in a file what's being done.(--log-file=<FILE>)

		Logging is relatively important in case things go wrong, so it's completely advisable.

		$ rsync -avz --delete --log-file=my_text_file.log source/ destination
	
		
		
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
BACKTRACKING PULSEAUDIO ISSUES ( VERY OLD / PRE-PIPEWIRE ERA ):
	Pulseaudio crashes and stops working without a message on journalctl on some games,
	this is also causing unkown & untraceable system crashes sometimes.

	The only lead i got is that pulseaudio is still holding onto deleted file, using the following command:
		#lsof +L1

	Use this next time:
	systemctl -l --no-pager --user status pulseaudio

	Every line should have user name in it:
	sudo fuser -v /dev/snd/*

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
RANDOM FREEZES ON SOME PROGRAMS/GAMES | ZRAM CULPRIT
	Random freezes can be caused by low RAM memory on the system,
or temperatures being too high for disks, cpu, gpu.

1 - For GPU, you can use nvtop(for nvidia cards only).
2 - For CPU, you can use sensors, which is also capable of listing temperatures 
of the motherboard if physical sensors are available there.
3 - For Hard Disks, use smartctl or gsmartcontrol(GUI for smartctl).

If you're experiencing random system freezes/crashes not related to CPU, Disk Failure or GPU,
you can rest assured that it is not a hardware issue. Given that your PSU is 'ok' and providing
sufficient Wattage and not running bellow the system wattage requirements.


Finally, having a small ammount of memory available on a zram device may be the culprit of the issue.
Try checking if you have any zram device being used on the system, by typing: 
	$zramctl

Verify that DiskSize for the zram device is equal or above 4GB,
If you're running a game/program that relies too much on RAM Memory and they're still crashing with 4GB of Zram,
it is recommended to increase the size of the zram device way above 4GB.

If you don't have enough RAM Available for doing the above recommendation, 
then you must completely disable zram device when running those programs/games.

In this case, systems will crash/freeze even if there is actual enough Physical RAM Memory.

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
ENTROPY ON KERNEL 6.0
	As of kernel 6.0, it's best to let the kernel handle entropy on it's own,
since it's faster and safer.

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
OPTIMIZING ZRAM
	VERY IMPORTANT: May be old and outdated.

	Note: use vm.page-cluster=0 for zstd and vm.page-cluster=1 for lz4.
	there is a huge delay for zstd when using '1' instead of '0' and a minor gain on speed.
	for lz4, the delay increase is insignificant because it's already too small,
	and as for the speed gains from using '1' with lz4 is +1.2GB/s in difference,
	so it's totally worth the increased delay.

	[file: /etc/sysctl.d/99-vm-zram-parameters.conf]
		vm.swappiness = 200
		vm.watermark_boost_factor = 0
		vm.watermark_scale_factor = 125
		vm.page-cluster = 1
	[/file]

	more on: https://docs.kernel.org/admin-guide/sysctl/vm.html


	To apply the changes made above:
		#sysctl --system
	^ check man page for more info

	To apply changes made to /etc/sysctl.conf without rebooting, type:
		#sysctl -p
	^ VERY IMPORTANT: this doesn't apply to files under /etc/sysctl.d
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
ENABLING RAM (A-)XMP / DOCP PROFILE
	Go to your bios and change RAM Settings from JEDEC
to XMP or DOCP profile if available by your ramstick
and motherboard.

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
ABOVE 4GB DECODING (SAM / RBAR)
	A BIOS option that allows the CPU to access 
the GPU's entire VRAM at once.

What this actually does is allow CPU access to
a larger block of memory if your GPU is greater than
4GB.

However, this is very CPU Bound and may actually make
some CPU Bound games slower.

VERY IMPORTANT: If you don't own a GPU larger than 4GB, then this isn't ever an issue.

TROUBLESHOOTING:
If you're using VKD3D, and enabling Resizable BAR leads to lower performance, you might want to try VKD3D_CONFIG=no_upload_hvv.

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
DISABLING COMPOSITION OR ENABLING UNREDIRECTION
	Disabling composition can help to reduce latency and improve performance, either by disabling it manually (KDE Plasma) or by unredirection (GNOME and derivatives like Cinnamon etc.)

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
USING A WAYLAND DESKTOP ENVIRONMENT
	Wayland DEs will actually improve performance by 16%,
make sure your Wayland DE supports disabling vsync,
since this usually defaults to 'on'.
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
KDE PLASMA BALOO FILE INDEXING
	Kde Plasma Desktop Environment are actually very slow
because it uses baloo file indexing. Make sure to disable it.
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
FERALINTERACTIVE GAMEMODE
	VERY IMPORTANT: DO NOT USE GAMEMODE

	Gamemode assumes enabling performance mode for
GPU/CPU/MOTHERBOARD should be 100% safe.

While the performance gains are real, they might be temporary
since leaving performance overclocks 'on' all the time might
actually damage your system at long term.

Some nvidia GPUs, for example, are known to be problematic
because they'll heat up pretty fast under 'performance' mode.

Another thing, if you have thermald on your system, gamemode
might not respect it and overlload thermald's underclocking/clamping to prevent overheat; ofc, this is going to slowly damage your CPU and make it even slower.

Furthuerover, overclocking your system for performance gains, even if it's 100% safe, is still a problem because it'll arise problems like thermal paste becoming older faster.

GPUs usually have unreplacable internal thermal pastes, so "safely" overclocking for performance is still a terribly idea since it will decrease the GPU's lifetime.(See DELIDING topic for more info)

Some CPU Vendors also don't give warranty to products that might've lost it's internal thermal ability, since this proves the product has been overclocked.

If you still need "overclocking" performance, prefer using NON-STOCK or CUSTOM KERNELS; Compiling your own programs is also a better option to performance gains in game than overclocking your hardware, since overclocking will damage it and actually make it slower at the long run.

If you have a GPU or CPU that has to rely on running on 'performance' mode at 100% of times, consider buying new hardware, since it's clearly outdated.

Even watercooling solutions can entirely completely damage your equipments permanently.

If you want to stay safe, just follow the alternative options given in here.
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
NON-STOCK / CUSTOM KERNELS
	linux-zen, linux-pds, linux-tkg-pds, liquorix(linux-zen for debian users), linux-xanmod.

Avoid linux-tkg actually, it has dropped fsync support and considered it Deprecated.
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
IMPROVE CLOCK_GETTIME THROUGHPUT
	This step is not needed if you're running either linux-zen or linux-xanmod,
as this already comes as a default setting in them.

The following was copied from the given source: 
	https://wiki.archlinux.org/title/Gaming#Improving_performance

---Copy & Paste:
User space programs and especially games do many calls to clock_gettime(2) to get the current time for calculating game physics, fps and so on. The time usage can be seen by running

# perf top

and looking at the overhead of read_hpet (or acpi_pm_read).

If you are not dependent on a very precise timer you can switch from hpet (high precision event timer) or acpi_pm (ACPI Power Management Timer) to the faster TSC (time stamp counter) timer. Add the kernel parameters

	tsc=reliable clocksource=tsc

to make TSC available and enable it. After that reboot and confirm the clocksource by running

	#cat /sys/devices/system/clocksource/clocksource*/current_clocksource

You can see all currently available timers by running

	#cat /sys/devices/system/clocksource/clocksource*/available_clocksource

and change between them by echoing one into current_clocksource. On a Zen 3 system benchmarking with [3] shows a ~50 times higher throughput of tsc compared to hpet or acpi_pm.
---END OF COPY & PASTE.

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
INTRODUCTION TO KERNEL MODULES CONFIGURATION (2023)
	Always use "mkinitcpio -P" whenever updating video, dkms drivers and kernels installed on the system.
Make sure to have a new "mkinitcpio-kernel_name.conf" file for a given new installed kernel on the system.
This file must describe all dkms drivers you want to use with a given kernel when generating initramfs 
by using "mkinitcpio -P".

--IMPORTANT DIRECTORIES
/etc/mkinitcpio.conf			#This file is the default configuration file used by mkinitcpio
					for all kernels available when a given kernel doesn't uses one of it's own.
					^ this file usually describe which modules will be added into the initramfs image
					for a given kernel.

/etc/mkinitcpio.d/			#This directory contains files that describe how each of the kernel will be built,
					things like files a kernel should read/use for building initramfs image when
					mkinitcpio -P is used.
					^ This is the file mkinitcpio.conf is called from. See examples below.

--SCRIPT EXAMPLE ( 2024 ) | MKINITCPIO NVIDIA CONFIGURATION 2024:

If you're using linux-lts you might follow the same done here for linux-zen!

[file: /etc/mkinitcpio.d/linux-zen.preset]
	# mkinitcpio preset file for the 'linux-zen' package

	ALL_config="/etc/mkinitcpio-zen.conf"		##Customized by me
	ALL_kver="/boot/vmlinuz-linux-zen"

	PRESETS=('default' 'fallback')

	#default_config="/etc/mkinitcpio.conf"
	default_image="/boot/initramfs-linux-zen.img"
	#default_options=""

	#fallback_config="/etc/mkinitcpio.conf"
	fallback_image="/boot/initramfs-linux-zen-fallback.img"
	fallback_options="-S autodetect"
[/file]

[file: /etc/mkinitcpio.d/linux-xanmod.preset]
	# mkinitcpio preset file for the 'linux-xanmod' package

	ALL_config="/etc/mkinitcpio-xanmod.conf"	##Customized by me
	ALL_kver="/boot/vmlinuz-linux-xanmod"
	ALL_microcode=(/boot/*-ucode.img)

	PRESETS=('default' 'fallback')

	#default_config="/etc/mkinitcpio.conf"
	default_image="/boot/initramfs-linux-xanmod.img"
	#default_uki="/efi/EFI/Linux/arch-linux-xanmod.efi"
	#default_options="--splash /usr/share/systemd/bootctl/splash-arch.bmp"

	#fallback_config="/etc/mkinitcpio.conf"
	fallback_image="/boot/initramfs-linux-xanmod-fallback.img"
	#fallback_uki="/efi/EFI/Linux/arch-linux-xanmod-fallback.efi"
	fallback_options="-S autodetect"
[/file]

[file: /etc/mkinitcpio-zen.conf | file: /etc/mkinitcpio-xanmod.conf]
	# vim:set ft=sh
	# MODULES
	# The following modules are loaded before any boot hooks are
	# run.  Advanced users may wish to specify all system modules
	# in this array.  For instance:
	#     MODULES=(usbhid xhci_hcd)
	MODULES=(nvidia nvidia_modeset nvidia-drm nvidia_uvm) 		##Customized by me

	# BINARIES
	# This setting includes any additional binaries a given user may
	# wish into the CPIO image.  This is run last, so it may be used to
	# override the actual binaries included by a given hook
	# BINARIES are dependency parsed, so you may safely ignore libraries
	BINARIES=()

	# FILES
	# This setting is similar to BINARIES above, however, files are added
	# as-is and are not parsed in any way.  This is useful for config files.
	FILES=()

	# HOOKS
	# This is the most important setting in this file.  The HOOKS control the
	# modules and scripts added to the image, and what happens at boot time.
	# Order is important, and it is recommended that you do not change the
	# order in which HOOKS are added.  Run 'mkinitcpio -H <hook name>' for
	# help on a given hook.
	# 'base' is _required_ unless you know precisely what you are doing.
	# 'udev' is _required_ in order to automatically load modules
	# 'filesystems' is _required_ unless you specify your fs modules in MODULES
	# Examples:
	##   This setup specifies all modules in the MODULES setting above.
	##   No RAID, lvm2, or encrypted root is needed.
	#    HOOKS=(base)
	#
	##   This setup will autodetect all modules for your system and should
	##   work as a sane default
	#    HOOKS=(base udev autodetect modconf block filesystems fsck)
	#
	##   This setup will generate a 'full' image which supports most systems.
	##   No autodetection is done.
	#    HOOKS=(base udev modconf block filesystems fsck)
	#
	##   This setup assembles a mdadm array with an encrypted root file system.
	##   Note: See 'mkinitcpio -H mdadm_udev' for more information on RAID devices.
	#    HOOKS=(base udev modconf keyboard keymap consolefont block mdadm_udev encrypt filesystems fsck)
	#
	##   This setup loads an lvm2 volume group.
	#    HOOKS=(base udev modconf block lvm2 filesystems fsck)
	#
	##   NOTE: If you have /usr on a separate partition, you MUST include the
	#    usr and fsck hooks.
	HOOKS=(base udev autodetect modconf kms keyboard keymap consolefont block filesystems fsck)

	# COMPRESSION
	# Use this to compress the initramfs image. By default, zstd compression
	# is used. Use 'cat' to create an uncompressed image.
	#COMPRESSION="zstd"
	#COMPRESSION="gzip"
	#COMPRESSION="bzip2"
	#COMPRESSION="lzma"
	#COMPRESSION="xz"
	#COMPRESSION="lzop"
	#COMPRESSION="lz4"
	COMPRESSION="cat"	##Customized by me

	# COMPRESSION_OPTIONS
	# Additional options for the compressor
	#COMPRESSION_OPTIONS=()

	# MODULES_DECOMPRESS
	# Decompress kernel modules during initramfs creation.
	# Enable to speedup boot process, disable to save RAM
	# during early userspace. Switch (yes/no).
	MODULES_DECOMPRESS="yes"	##Customized by me

[/file]



--------------------------------------------------
--------------------------------------------------
ENABLING FIREFOX ENCRYPTED CLIENT HELLO (ECH)
	Encrypts HTTP Requests locally and sends it to a server that can decrypt and do the handshake for you.
This prevents 3rd parties, malicious users from seeing websites you're accessing and even preventing them
from looking at your web search results.

	Go to Settings > Search for "DNS Over HTTPS" > Set it to "Max Protection"
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
HOW TO ENABLE DXVK ON WINE-GE
	First, you need wine-ge and winetricks installed.
You also need a Vulkan Enabled GPU or iGPU and have proprietary drivers installed as well(2023 and may change).

1 - Install DXVK for the default wine prefix:
	On winetricks, select default wine prefix > Install DLL or Component > Select 'dxvk' > Click OK

2 - Once DXVK is installed for wine-ge, it's time to actually use it:
	On winetricks, select default wine prefix > Change Settings > Select 'renderer=vulkan' > Click OK

3 - Now you can run the game/application by simply typing:
	$wine <application_name>.exe

Note if you find any issues with other applications/games, you may need to revert it to 'renderer=gl' on step #2,
there's no need to uninstall dxvk in this case.

Performance-wise applications and games run much better on DXVK than GL(OpenGL) with a few rare exceptions.
Games also look much better on DXVK.
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
REDUCING BLUE LIGHT TO HELP DAILY COMPUTER USAGE / UV LIGHT
	PC screens and other digital devices are well known for emmiting Blue UV Light;
the blue UV light is 2nd most dangerous color to our eye visions, 
going as far as causing optical nerve damage.

If you use digital screens on a daily basis without a anti-blue lenses, you may find yourself
tired, have headaches and get a very hard time getting yourself to sleep.

Buying glasses with anti-blue lenses might solve the problem, but will require you to spend some money.
depending on the quality of the acquired glasses, the may also cause discomfort.

Furtherover, if you need glasses for reading in front of the computer, you'd have to order custom anti-blue glasses
specifically made for you, which is going to be even more expensive.

HOWEVER, there is a final cheap better solution, which is that you can directly reduce the UV Blue light created
by your monitor by either tweaking the options on the monitor itself and eliminating all of the blue light
OR going into your GPU's Color Management and directly set the blue light to zero.

This will allow you to use the computer for longer periods of time without having a headache, including playing games
and reading articles on the internet.

	PERFECT SETTINGS FOR NVIDIA USERS
		For the blue light, i have -0.9 for both brightness and contrast while gamma settings rests at 0.100
	for the BLUE light.

	For ALL LIGHTS(Red and Green) I have 0.0 for both brightness and contrast, while gamma sits at 0.655.

	However, the perfect settings may change from user to user, these are just my favorite.
	Going below -0.9(brightness and contrast) for blue light caused me eye strain after a while.

	Gamma should be set(for non-blue light) as needed, for playing dark games, you'll probably need more gamma.
	For reading and playing non-horror games, i find 0.655 brightness to be the ideal & perfect.

	Alternate settings:
		Use same settings for ALL LIGHTS as the one above,
		Set Blue Light to:
			-0.92 for Brightness
			-0.90 for Contrast
			0.558 for Gamma
Personal Note:
The reason i've wrote about this, is that i've spent years living with some of the issues caused by UV Blue light,
hopefully younger users don't have to go through this and are able to spare themselves from some of those issues
at 0 expenses, specially considering how expensive anti-blue amber colored glasses can be.
:) :) :)

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
PERFECT OBS STUDIO CONFIGURATION AND SETTINGS / PERFECT LIVE STREAMING CONFIGURATION
	The settings in here are meant for live streaming,
	Try "OBS STUDIO SETTINGS" manual section for setting up recording settings.

	1. Go to Settings
	2. Go to Output
		Go to Streaming Tab > Streaming Settings
			Audio Encoder: FFmpeg AAC
			Video Encoder: NVIDIA NVENC H.264(FFmpeg)
			Uncheck Rescale Output

		" " > Encoder Settings
			Rate Control: CBR
			Bitrate: 6000
			Keyframe Interval: 2 s
			Preset: P6: Slower (Better Quality)
			Tuning: High Quality
			Multipass Mode: Two Passes(Quarter Resolution)
			Profile: High
			Psycho Visual Tuning: Checked/Enabled
			GPU: 0
			Max B-Frames: 2
	3. Go to Video
		Integer FPS Value: 55
		^ There's no need to render all 60 frames for streaming viewers

	4. Go to Advanced
		> Video
			Color Format: NV12(8bit, 4:2:0, 2 planes)
			Color Spec: Rec. 709
			Color Range: Full
			SDR White Levels: 300nits
			HDR Nominal Peak Level: 4000 nits

	5. Finally, make sure you have no UNWANTED filters applied to neither SCENES nor SOURCES(VERY IMPORTANT!)

You may want to follow the "IMPROVING VULKAN GAMES EXPERIENCE / VASTLY IMPROVING GRAPHICS FOR VULKAN BASED GAMES"
section to improve graphic quality of vulkan based games.

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
IMPROVING VULKAN GAMES EXPERIENCE / VASTLY IMPROVING GRAPHICS FOR VULKAN BASED GAMES
	Install 'vkbasalt' (it's on aur repo of archlinux),
and use the following configuration file:

There are 2 files here: 2023 and 2024, the 2024 is the better one, but if it slows down your games
try using 2023 one!

[2024 Newer file: /usr/share/vkBasalt/vkBasalt.conf]
	#effects is a colon seperated list of effect to use
	#e.g.: effects = fxaa:cas
	#effects will be run in order from left to right
	#one effect can be run multiple times e.g. smaa:smaa:cas
	#cas    - Contrast Adaptive Sharpening
	#dls    - Denoised Luma Sharpening
	#fxaa   - Fast Approximate Anti-Aliasing
	#smaa   - Enhanced Subpixel Morphological Antialiasing
	#lut    - Color LookUp Table
	effects = cas:dls:smaa

	reshadeTexturePath = "/opt/reshade/textures"
	reshadeIncludePath = "/opt/reshade/shaders"
	depthCapture = off

	#toggleKey toggles the effects on/off
	toggleKey = Home

	#enableOnLaunch sets if the effects are enabled when started
	enableOnLaunch = True

	#casSharpness specifies the amount of sharpning in the CAS shader.
	#0.0 less sharp, less artefacts, but not off
	#1.0 maximum sharp more artefacts
	#Everything in between is possible
	#negative values sharpen even less, up to -1.0 make a visible difference
	#casSharpness = 0.4
	#casSharpness = 1.0 #PERFECT
	#casSharpness = 0.5 #PERFECT - Use this if already using 100% Sharpening on /etc/environment with Nvidia
	casSharpness = 1.0

	#dlsSharpness specifies the amount of sharpening in the Denoised Luma Sharpening shader.
	#Increase to sharpen details within the image.
	#0.0 less sharp, less artefacts, but not off
	#1.0 maximum sharp more artefacts
	#dlsSharpness = 0.5 #PERFECT
	dlsSharpness = 0.7 #Trying this one out, changed just now

	#dlsDenoise specifies the amount of denoising in the Denoised Luma Sharpening shader.
	#Increase to limit how intensely film grain within the image gets sharpened.
	#0.0 min
	#1.0 max
	#dlsDenoise = 0.17 #Default
	dlsDenoise = 0.27 #PERFECT
	#dlsDenoise = 0.57 #Good - but looks bad
	#dlsDenoise = 1.0 #Bad

	#fxaaQualitySubpix can effect sharpness.
	#1.00 - upper limit (softer)
	#0.75 - default amount of filtering
	#0.50 - lower limit (sharper, less sub-pixel aliasing removal)
	#0.25 - almost off
	#0.00 - completely off
	fxaaQualitySubpix = 0.75

	#fxaaQualityEdgeThreshold is the minimum amount of local contrast required to apply algorithm.
	#0.333 - too little (faster)
	#0.250 - low quality
	#0.166 - default
	#0.125 - high quality 
	#0.063 - overkill (slower)
	fxaaQualityEdgeThreshold = 0.125

	#fxaaQualityEdgeThresholdMin trims the algorithm from processing darks.
	#0.0833 - upper limit (default, the start of visible unfiltered edges)
	#0.0625 - high quality (faster)
	#0.0312 - visible limit (slower)
	#Special notes: due to the current implementation you
	#Likely want to set this to zero.
	#As colors that are mostly not-green
	#will appear very dark in the green channel!
	#Tune by looking at mostly non-green content,
	#then start at zero and increase until aliasing is a problem.
	fxaaQualityEdgeThresholdMin = 0.0312

	#smaaEdgeDetection changes the edge detection shader
	#luma  - default
	#color - might catch more edges, but is more expensive
	#smaaEdgeDetection = luma ##Default
	smaaEdgeDetection = color #Changed just now


	#smaaThreshold specifies the threshold or sensitivity to edges
	#Lowering this value you will be able to detect more edges at the expense of performance.
	#Range: [0, 0.5]
	#0.1 is a reasonable value, and allows to catch most visible edges.
	#0.05 is a rather overkill value, that allows to catch 'em all.
	smaaThreshold = 0.05

	#smaaMaxSearchSteps specifies the maximum steps performed in the horizontal/vertical pattern searches
	#Range: [0, 112]
	#4  - low
	#8  - medium
	#16 - high
	#32 - ultra
	smaaMaxSearchSteps = 32

	#smaaMaxSearchStepsDiag specifies the maximum steps performed in the diagonal pattern searches
	#Range: [0, 20]
	#0  - low, medium
	#8  - high
	#16 - ultra
	smaaMaxSearchStepsDiag = 16

	#smaaCornerRounding specifies how much sharp corners will be rounded
	#Range: [0, 100]
	#25 is a reasonable value
	smaaCornerRounding = 25 ##Default - GOOD
	#smaaCornerRounding = 50 ##Increasing this value is usually bad, makes image quality worse

	#lutFile is the path to the LUT file that will be used
	#supported are .CUBE files and .png with width == height * height
	lutFile = "/path/to/lut"
[/file]
 
[2023 file: /usr/share/vkBasalt/vkBasalt.conf]
	#effects is a colon seperated list of effect to use
	#e.g.: effects = fxaa:cas
	#effects will be run in order from left to right
	#one effect can be run multiple times e.g. smaa:smaa:cas
	#cas    - Contrast Adaptive Sharpening
	#dls    - Denoised Luma Sharpening
	#fxaa   - Fast Approximate Anti-Aliasing
	#smaa   - Enhanced Subpixel Morphological Antialiasing
	#lut    - Color LookUp Table
	effects = cas:dls

	reshadeTexturePath = "/opt/reshade/textures"
	reshadeIncludePath = "/opt/reshade/shaders"
	depthCapture = off

	#toggleKey toggles the effects on/off
	toggleKey = Home

	#enableOnLaunch sets if the effects are enabled when started
	enableOnLaunch = True

	#casSharpness specifies the amount of sharpning in the CAS shader.
	#0.0 less sharp, less artefacts, but not off
	#1.0 maximum sharp more artefacts
	#Everything in between is possible
	#negative values sharpen even less, up to -1.0 make a visible difference
	#casSharpness = 0.4
	casSharpness = 1.0 #PERFECT

	#dlsSharpness specifies the amount of sharpening in the Denoised Luma Sharpening shader.
	#Increase to sharpen details within the image.
	#0.0 less sharp, less artefacts, but not off
	#1.0 maximum sharp more artefacts
	dlsSharpness = 0.5

	#dlsDenoise specifies the amount of denoising in the Denoised Luma Sharpening shader.
	#Increase to limit how intensely film grain within the image gets sharpened.
	#0.0 min
	#1.0 max
	#dlsDenoise = 0.17 #Default
	dlsDenoise = 0.27 #PERFECT
	#dlsDenoise = 0.57 #Good - but looks bad
	#dlsDenoise = 1.0 #Bad

	#fxaaQualitySubpix can effect sharpness.
	#1.00 - upper limit (softer)
	#0.75 - default amount of filtering
	#0.50 - lower limit (sharper, less sub-pixel aliasing removal)
	#0.25 - almost off
	#0.00 - completely off
	fxaaQualitySubpix = 0.75

	#fxaaQualityEdgeThreshold is the minimum amount of local contrast required to apply algorithm.
	#0.333 - too little (faster)
	#0.250 - low quality
	#0.166 - default
	#0.125 - high quality 
	#0.063 - overkill (slower)
	fxaaQualityEdgeThreshold = 0.125

	#fxaaQualityEdgeThresholdMin trims the algorithm from processing darks.
	#0.0833 - upper limit (default, the start of visible unfiltered edges)
	#0.0625 - high quality (faster)
	#0.0312 - visible limit (slower)
	#Special notes: due to the current implementation you
	#Likely want to set this to zero.
	#As colors that are mostly not-green
	#will appear very dark in the green channel!
	#Tune by looking at mostly non-green content,
	#then start at zero and increase until aliasing is a problem.
	fxaaQualityEdgeThresholdMin = 0.0312

	#smaaEdgeDetection changes the edge detection shader
	#luma  - default
	#color - might catch more edges, but is more expensive
	smaaEdgeDetection = luma

	#smaaThreshold specifies the threshold or sensitivity to edges
	#Lowering this value you will be able to detect more edges at the expense of performance.
	#Range: [0, 0.5]
	#0.1 is a reasonable value, and allows to catch most visible edges.
	#0.05 is a rather overkill value, that allows to catch 'em all.
	smaaThreshold = 0.05

	#smaaMaxSearchSteps specifies the maximum steps performed in the horizontal/vertical pattern searches
	#Range: [0, 112]
	#4  - low
	#8  - medium
	#16 - high
	#32 - ultra
	smaaMaxSearchSteps = 32

	#smaaMaxSearchStepsDiag specifies the maximum steps performed in the diagonal pattern searches
	#Range: [0, 20]
	#0  - low, medium
	#8  - high
	#16 - ultra
	smaaMaxSearchStepsDiag = 16

	#smaaCornerRounding specifies how much sharp corners will be rounded
	#Range: [0, 100]
	#25 is a reasonable value
	smaaCornerRounding = 25

	#lutFile is the path to the LUT file that will be used
	#supported are .CUBE files and .png with width == height * height
	lutFile = "/path/to/lut"
[/file]
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
SOLVING STEAM FILE PICKING ISSUE / STEAM FILE PICKER NOT WORKING
	Installing xdg-desktop-portal for allowing steam to use file picker for uploading screenshot:
		1. download the following packages:
			kdialog
			plasma-integration
			breeze or breeze-icons
			dolphin (Kde File managaer, mandatory)
			kio-extras
			xdg-desktop-portal     (MANDATORY)
			xdg-desktop-portal-kde (MANDATORY)
			xdg-desktop-portal-gtk (MANDATORY)
			kimage-formats (optional)
			qt5-imageformats (optional)
			ffmpegthumbs (optional)
			kdegraphics-thumbnailers (optional)

	Make sure xdg-desktop-portal-kde is running before restarting the xdg-desktop-portal service:
		$ systemctl restart --user xdg-desktop-portal
	
	hint: ^ /usr/lib/xdg-desktop-portal-kde needs to be run before restarting the service above. 
	pacman -Ql xdg-desktop-portal-kde if it's not located in /usr/lib/.

	There's no need to reboot the system, maybe reboot steam.
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
WHY WAYLAND IS BETTER AND FASTER / UNDERSTANDING WAYLAND

	STACKING WINDOW MANAGERS VS COMPOSITE WINDOW MANAGERS
		The old way for applications to handle rendering each of their window was to use a 
"display memory" that was shared between other applications, a desktop environment that relies on this method 
is called Stacking Window Manager.

A Stacking window manager may be the cause of freezing/stutters and even full system crashes and
the problem happens because a single broken application is all what's needed for a freeze/stutter/crash to happen.

Even if the system doesn't freeze or crash, minor stutters may leave the user under impression their computer is too slow
when running programs under a Stacking Window Manager.

Composite Window Managers however DO NOT share display memory and allow each program to have a buffer of their own
for rendering window, thus eliminating delay, stuttering, freezing and even crash issues from happening.

The compositor can also be a Linux display server, an X application, or a special Wayland client. The Wayland project includes a demo Wayland compositor, called Weston. It’s fast, small, and runs on embedded and mobile devices.


	FURTHER IMPROVING PERFORMANCE ON LINUX WITH WESTON
		You can use weston, even if you're already running a X Desktop Environment.
	the benefits of running applications on a Composite Window Manager like weston are big.

	MAKING A WESTON AUTO-LAUNCHER FOR A GIVEN PROGRAM
		It's possible to create auto-launch for existing apps for weston:
	
	1 - Create an auto-launcher:
		[file: ~/weston-simple.ini]
			[autolaunch]
			path=/usr/bin/konsole
			watch=true

			[core]
			##Use fullscreen to let window rest outside weston
			shell=fullscreen
			##Use desktop to keep app window inside weston
			#shell=desktop

			##Not using as of yet:
			#renderer=gl
			#color-management=true
			#xwayland=true

			[output]
			mode=1280x1024
			colorimetry-mode=bt2020cycc
			eotf-mode=hdr-gamma

		[/file]

		^ 'watch=true' means weston gets closed once /bin/konsole stops running
		^ follow this link for more options:
			https://man.archlinux.org/man/weston.ini.5.en
				or
			$ man weston.ini
	
	2 - Create an executable-weston:
		[file: /bin/konsole-weston]
			#!/bin/sh
			/bin/weston -c ~/weston-konsole.ini
		[/file]

		^ you'll need to run this after creating file:
			# chmod o+x /bin/konsole-weston
	
	3 - Run weston:
		$ konsole-weston



FURTHER READINGS:
	https://download.nvidia.com/XFree86/Linux-x86_64/396.51/README/xcompositeextension.html
	^ Not advisable to enable compositing, since it cause issues for a few programs in XORG
	^ Furtherover, this is only meant for Compositing Window Managers, so stacking window managers won't
	benefit anything from doing this.

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
USING OPTIMAL GRAPHICAL ENVIRONMENT VARIABLES FOR GAMES	
	
	There are some issues when using __GL_SHARPEN_ENABLED=1,
	vlc, lutris and vp-array(from mesa-demo package) breaks and stop working:
	running glxinfo also fails with the same error;

		MESA-LOADER: Failed to load simpledrm: /usr/lib/dri/simpledrm_dri.so
		kmsro: driver missing

	SOLVED: Problem fixed by removing __GL_SHARPEN_ENABLE=1 from /etc/environment!

	FINAL SOLUTION: The solution to use __GL_SHARPEN_ENABLE is to only use it on a per-application basis,
	by calling __GL_SHARPEN_ENABLED=1 before executing a program instead of setting it on /etc/environment
	like this:

		example: 
			$ __GL_SHARPEN_ENABLED=1 vbo-tri
		
		example2:
			$ __GL_SHARPEN_ENABLED=1 ENABLE_VKBASALT=1 vbo-tri

		example3:
			$ MESA_LOADER_DRIVER_OVERRIDE=zink LD_PRELOAD="/usr/lib/libmimalloc.so ${LD_PRELOAD}" ENABLE_VKBASALT=1 vbo-tri

	This is mostly useful for games, but can also be used for web-browsers and other apps like steam itself.
	**vbo-tri is an application from 'mesa-demos' package on archlinux.
	** vp-* demos will likely crash with __GL_SHARPEN_ENABLED=1 just as some other applications would as well.


	OPTIMAL /ETC/ENVIRONMENT CONFIGURATION FOR NVIDIA
		If using /usr/share/vkBasalt/vkBasalt.conf make sure to set 
	"casSharpness = 0.5" as anything greater than 0.5 will look bad. with __GL_SHARPEN_VALUE=100.

	These nvidia options are meant for both OPENGL and Vulkan based applications.

	[file: /etc/environment]
		#
		# This file is parsed by pam_env module
		#
		# Syntax: simple "KEY=VAL" pairs on separate lines
		#

		##The Environment vars below are meant for NVIDIA GPUs Only
		##Erase this below if you don't own an NVIDIA GPU
		GBM_BACKEND=nvidia-drm
		__GLX_VENDOR_LIBRARY_NAME=nvidia
		#__GL_SHARPEN_ENABLE=1  #Default is 0, this is known to cause problems with vlc, glxinfo and nvidia-settings as well
		__GL_SHARPEN_VALUE=100 #Default is 50, 100 is maximum; only works with __GL_SHARPEN_ENABLE=1
		__GL_SHARPEN_IGNORE_FILM_GRAIN=17 #Default is 17, 100 is maximum; only works with __GL_SHARPEN_ENABLE=1
		__GL_THREADED_OPTIMIZATION=1
		LIBVA_DRIVER_NAME=nvidia #Supposedly Fixes some issue with VLC and Nvidia-Settings - AVOID USING THIS, NOT NEEDED - I'M CURRENTLY USING IT THO

		##VERY IMPORTANT: below are my personal compiler flags for C, C++ and RUST based on my ivybridge processor - Not Using Anylonger
		##Either erase or modify the below env. vars if you do not own an ivy bridge processor.
		#CFLAGS="-mcpu=ivybridge -march=ivybridge -mtune=ivybridge -fomit-frame-pointer -O3 -pipe"
		#CXXFLAGS="-mcpu=ivybridge -march=ivybridge -mtune=ivybridge -fomit-frame-pointer -O3 -pipe"
		#RUSTFLAGS="-C opt-level=3 -C target-cpu=ivybridge"
		PULSE_SERVER=tcp:127.0.0.1:4713	#Only use this if you're running a PULSE SERVER through Pipewire.
	[/file]

----
----
	OPTIMAL /USR/SHARE/VKBASALT/VKBASALT.CONF 
		This requires vkbasalt package installed on the linux system
	as well as a vulkan enabled GPU to by installed in the host machine.

	[file: /usr/share/vkBasalt/vkBasalt.conf]
		#effects is a colon seperated list of effect to use
		#e.g.: effects = fxaa:cas
		#effects will be run in order from left to right
		#one effect can be run multiple times e.g. smaa:smaa:cas
		#cas    - Contrast Adaptive Sharpening
		#dls    - Denoised Luma Sharpening
		#fxaa   - Fast Approximate Anti-Aliasing
		#smaa   - Enhanced Subpixel Morphological Antialiasing
		#lut    - Color LookUp Table
		effects = cas:dls

		reshadeTexturePath = "/opt/reshade/textures"
		reshadeIncludePath = "/opt/reshade/shaders"
		depthCapture = off

		#toggleKey toggles the effects on/off
		toggleKey = Home

		#enableOnLaunch sets if the effects are enabled when started
		enableOnLaunch = True

		#casSharpness specifies the amount of sharpning in the CAS shader.
		#0.0 less sharp, less artefacts, but not off
		#1.0 maximum sharp more artefacts
		#Everything in between is possible
		#negative values sharpen even less, up to -1.0 make a visible difference
		#casSharpness = 0.4
		#casSharpness = 1.0 #PERFECT
		#casSharpness = 0.5 #PERFECT - Use this if already using 100% Sharpening on /etc/environment with Nvidia
		casSharpness = 1.0

		#dlsSharpness specifies the amount of sharpening in the Denoised Luma Sharpening shader.
		#Increase to sharpen details within the image.
		#0.0 less sharp, less artefacts, but not off
		#1.0 maximum sharp more artefacts
		dlsSharpness = 0.5

		#dlsDenoise specifies the amount of denoising in the Denoised Luma Sharpening shader.
		#Increase to limit how intensely film grain within the image gets sharpened.
		#0.0 min
		#1.0 max
		#dlsDenoise = 0.17 #Default
		dlsDenoise = 0.27 #PERFECT
		#dlsDenoise = 0.57 #Good - but looks bad
		#dlsDenoise = 1.0 #Bad

		#fxaaQualitySubpix can effect sharpness.
		#1.00 - upper limit (softer)
		#0.75 - default amount of filtering
		#0.50 - lower limit (sharper, less sub-pixel aliasing removal)
		#0.25 - almost off
		#0.00 - completely off
		fxaaQualitySubpix = 0.75

		#fxaaQualityEdgeThreshold is the minimum amount of local contrast required to apply algorithm.
		#0.333 - too little (faster)
		#0.250 - low quality
		#0.166 - default
		#0.125 - high quality 
		#0.063 - overkill (slower)
		fxaaQualityEdgeThreshold = 0.125

		#fxaaQualityEdgeThresholdMin trims the algorithm from processing darks.
		#0.0833 - upper limit (default, the start of visible unfiltered edges)
		#0.0625 - high quality (faster)
		#0.0312 - visible limit (slower)
		#Special notes: due to the current implementation you
		#Likely want to set this to zero.
		#As colors that are mostly not-green
		#will appear very dark in the green channel!
		#Tune by looking at mostly non-green content,
		#then start at zero and increase until aliasing is a problem.
		fxaaQualityEdgeThresholdMin = 0.0312

		#smaaEdgeDetection changes the edge detection shader
		#luma  - default
		#color - might catch more edges, but is more expensive
		smaaEdgeDetection = luma

		#smaaThreshold specifies the threshold or sensitivity to edges
		#Lowering this value you will be able to detect more edges at the expense of performance.
		#Range: [0, 0.5]
		#0.1 is a reasonable value, and allows to catch most visible edges.
		#0.05 is a rather overkill value, that allows to catch 'em all.
		smaaThreshold = 0.05

		#smaaMaxSearchSteps specifies the maximum steps performed in the horizontal/vertical pattern searches
		#Range: [0, 112]
		#4  - low
		#8  - medium
		#16 - high
		#32 - ultra
		smaaMaxSearchSteps = 32

		#smaaMaxSearchStepsDiag specifies the maximum steps performed in the diagonal pattern searches
		#Range: [0, 20]
		#0  - low, medium
		#8  - high
		#16 - ultra
		smaaMaxSearchStepsDiag = 16

		#smaaCornerRounding specifies how much sharp corners will be rounded
		#Range: [0, 100]
		#25 is a reasonable value
		smaaCornerRounding = 25

		#lutFile is the path to the LUT file that will be used
		#supported are .CUBE files and .png with width == height * height
		lutFile = "/path/to/lut"
	[/file]

Source:
https://download.nvidia.com/XFree86/Linux-x86_64/510.39.01/README/openglenvvariables.html     
^ More Nvidia Environment Variables

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
HOW TO SURVIVE ON LINUX (LONGER THAN EVERYONE ELSE)
	A lot of people end up giving up on linux due to how difficult it is
the entire learning process. Unlike windows, the process of learning linux is continuous; 
in other words, the linux user has to strive on keeping learning new things for a greater period of time
than that of the windows user.

In order to survive on linux, there is a few key things you need to understand in order to survive
the great expansion that linux has been turning into over the long years of it's existence.

First, a lot of linux tools are available as CLI programs, so most of the time you'll be dealing
with commands instead of graphical user interface. In fact, most new programs on linux startup as a CLI Tool
only then to become GUI program if it survives long enough.(exampe: git and git-gui).

As a linux user, you'll have to learn & memorize a lot of commandline programs.
This is where most users decide to stop using linux, a wrong command may break things and make things stop working.
a command meant for fixing things may never fix anything.

because of their lack in understanding, a lot of users would just give up and go back to windows at this point.

Here comes the importance of understanding tools like the package manager, man pages, tldr and cheat tools.

PACKAGE MANAGER
	The package manager is the user's father of every single linux distribution.
	Understanding the package manager means you can locate installed package files, 
	verify integrity for each installed packages in the system, 
	verify package history of package removal and installation and you can
	finally fix things yourself when they break(sort of).

	Example(archlinux): 
	by installing core-utils on your system, it's possible to list every and single binary installed
	that can be used by you(the user):
		$ pacman -Ql util-linux | grep bin/ | less

	with the command above you can list every single program installed by the util-linux package.

	You could also check package meta-data(information) before installing them on your system:
		$ pacman -Si kitty

	You can also check the package integrity with:
		$ pacman -Qkk xorg-server

	Listing package meta-data after installation is also a thing,
	ofc, given they're already installed in the system:
		$ pacman -Qi xorg-server

	You could also list which file(s) in the disk are owned by which packages:
		$ pacman -Fx .bashrc

	^ this will list default file location for packages that deal in '.bashrc' named files.
	
	Same as above, but doesn't use regex:
		$ pacman -F /etc/mkinitcpio.conf
			or
		$ pacman -F /boot/grub/grub.cfg

MAN PAGES
	Man tool comes by default on linux and the man-pages themselves have to be installed
	on the system by each of the package/program developers themselves or by the system user/administrator.

	Man tool is important to understand what programs can do what and where in the system.
	Without man pages, linux wouldn't be simple; HOWEVER, man pages are sometimes too obtuse, obscure and
	too abstract to be understandable by a beginner who doesn't even understand what a certain program does.

	Example 1:
		if you have rsync package installed on your system:
			$ man rsync

		you could also try any of the bin/ programs in the "PACKAGE MANAGER" example above.

	Even then, understanding man pages using the man page tool itself can be difficult ( try $man man ),
	which is the reason you need to progress further into your readings here.

TLDR
	Using tldr requires tldr installed on the system!

	tldr tool exists to ease up the pain of reading through the whole man pages,
	remember that even the 'man pages' for the man page tool itself is way too big for any normal
	human being to know where even begin with;
	however, by using tldr things become easier:
		$ tldr man

	if you don't understand tldr enough you could even go for:
		$ tldr tldr

	even things like rsync and tar become less of a pain:
		$ tldr rsync; tldr tar

	VERY IMPORTANT: Always remember to update TLDR from time to time with: 
		$ tldr -u
EG
	Using 'eg' requires 'eg' package installed on the system.

	This is the same as TLDR, but gives brief description on what programs does.

CHEAT
	Using cheat requires cheat package installed on the system.

	What if you don't know the program's names at all?
	What if you have no internet access for google search?

	Cheat is your answer, example:
		$ cheat -l -t networking

	^ In this case above, cheat will list all available networking related tools installed on your system.
	then with a list of binary tools in your hands, you could summon MAN or TLDR for learning them.

	What if you don't know any name at all?
	here is your answer:
		$ cheat -l
		
	^ this will list all available cheatsheets.


OTHER TOOLS
	devhints, for software developers.
	kb, navi, cheat.sh

	source: https://github.com/tldr-pages/tldr
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
HOW TO FIND OUT THE REASON A PROGRAM DOESN'T WORK ON LINUX | SYSTEM FREEZE
	You can use these commands to find for error logs on the system:
		#journalctl -b --priority=3
		$journalctl -b --priority=3
		#dmesg -H --level err

Besides checking journalctl -b-0 for error logs
It's important to also check LOCAL XORG LOGS:
	$ cat ~/.local/share/xorg/Xorg.0.log | less

LOCAL XORG LOGS may explain why any application might have failed executing when the application itself
fails giving out a reason on the terminal console, like steam, lutris and even wine for example.

Reason is the global /var/log/Xorg.0.log might not explain everything in details as to what happened 
to one local user program failing to execute.

KEEPING AN EYE ON UPDATED PACKAGES
	This is important because sometimes the system may break due to a system update/upgrade.
The only way to find out is to look at the package manager history to look for upgrades, removals
and package substitutions/replacement that may have occurred and then reverting the suspicious package upgrades
to a previous version.

To check on History of Issued Pacman commands:
	$ cat /var/log/pacman.conf | less

yay manager also uses pacman.conf

Type in "-R -n -s" to look for packages that have been uninstalled
using '/' keyboard key, use 'n' and 'N' to look forward and backwards on the text.

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
HOW TO DOSBOX-X
	Additional Dosbox configuration lines:

	A) COUNTRY
		If you set COUNTRY=0 in the [config] section of the DOSBox-X config file, 
	then DOSBox-X will try to auto-detect and set the country code based on the regional 
	setting of the host system. This however only works on Windows systems.

	Alternatively, the COUNTRY command can be used from the DOSBox-X command line 
	to change the country code. For example, the following command will switch 
	the country code to 61 for international English.

	Countr 61 ensures programs use the english language.

	MACHINE TYPES DOSBOX-X CAN RUN

 		The type of machine DOSBox-X can emulate:

                	Possible values: mda, cga, cga_mono, cga_rgb, cga_composite, cga_composite2, 
		hercules, hercules_plus, hercules_incolor, hercules_color, tandy, pcjr, pcjr_composite, 
		pcjr_composite2, amstrad, ega, ega200, jega, mcga, vgaonly, svga_s3, svga_s386c928, 
		svga_s3vision864, svga_s3vision868, svga_s3vision964, svga_s3vision968, svga_s3trio32, 
		svga_s3trio64, svga_s3trio64v+, svga_s3virge, svga_s3virgevx, svga_et3000, svga_et4000, 
		svga_paradise, vesa_nolfb, vesa_oldvbe, vesa_oldvbe10, pc98, pc9801, pc9821, svga_ati_egavgawonder, 
		svga_ati_vgawonder, svga_ati_vgawonderplus, svga_ati_vgawonderxl, svga_ati_vgawonderxl24, 
		svga_ati_mach8, svga_ati_mach32, svga_ati_mach64, fm_towns.
                  
	MOUNTING DISK IMAGES
			Disk images can be  mounted through 'Drive' tab in Dosbox-x GUI,
		using the "Mount a Disk or CD Image"; it then can be accessed by accessing the mounted drive,
		by typing the mounted drive letter: 'A:' or 'cd A:'

		Supported image formats are: 
			.img, .ima, .vhd, .hdi, .iso, .cue, .bin, .chd, .mdf, .gog, .ins


		Booting from disk image is also possible but only for programs that can be actually booted!
	
		VERY IMPORTANT NOTE ABOUT MOUNTING 
			It's important to have a backup copy of the original file(s) you're mounting in the system,
		since they may get corrupted if not unmounting properly, which could happen on settings changes,
		system freezes, etc. Always have a backup copy and never mount from them.
		
		HOW TO USE FDI FILETYPES ON DOSBOX-X
			Note: 'imgmount' and 'mount' are exclusive tools to dosbox/dosbox-x,
		the 'mount' tool works completely different on linux.

			An FDI file is a floppy disk image created by various emulator programs such as Amiga, Commodore, Sinclair, and other platforms. It stores an exact copy of the complete disk and can be loaded and run on a PC with the emulator.

		To use it on Dosbox-x, mount the folder containning the FDI/FDD file using dosbox-x then go into dosbox-x 
		terminal and use IMGMOUNT: 
			$ imgmount k disk.fdi -t floppy
				or
			$ mount k disk.fdi -t floppy

		^ the image will then be mounted at K: disk!

		It's also possible to mount multiple images at the same time:
			$ imgmount k disk_a.fdi disk_b.fdi disk_c.fdi disk_d.fdi -t floppy
				or
			$ mount k disk_a.fdi disk_b.fdi disk_c.fdi disk_d.fdi -t floppy
		use $ imgswap for swapping disks!
		
		MOUNTING .HDI FILETYPES ON LINUX
			source: https://github.com/drojaazu/pc98_disks_in_linux/blob/main/README.md

			1 - Setup a loop device:
				$ IMAGE=imagefile.hdi; OFFSET=$(expr `grep -Ebaom 1 "FAT1[2|6]" ${IMAGE} | sed -E 's/:FAT1[2|6]//g'` - 54); sudo losetup --show -fL -o ${OFFSET} ${IMAGE}
			
			2 - Check which loop device your .hdi file has been attached to:
				$ losetup -a

			3 - Mount the identified loop device:
				$ mount --mkdir /dev/loopX /mnt/loopX

			4 - When done with all the tasks, finally umount and turn off the loop device:
				# umount /mnt/loopX && losetup -d /dev/loopX

		UMOUNTING DISK
			Umounts A: disk :
				$ imgmount a -u
					or
				$ mount a -u

		UMOUNTING AND MOUNTING A 2ND DISK AT THE SAME TIME
			The command below will unmount the A: disk and mount disk2.ima on A: letter:
				$ imgmount a disk2.ima -u
			
		ALTERNATE OPTION TO MOUNTING .FDI FILES ON LINUX
			A) CREATE A VIRTUAL PC-98 FLOPPY DEVICE ON THE SYSTEM
				NOTE: This only works for single .fdi files that are not split between multiple .fdi files.
				If you have multiple .fdi files, consider using 'imgmount' from dosbox-x. Refer to:
					HOW TO USE FDI FILETYPES ON DOSBOX-X

				This is actually a better and faster solution, because if you mount .fdi or any other
				floppy device inside dosbox-x, the floppy device speeds will be emulated,
				you can assume floppy devices are very slow.

				source: https://github.com/drojaazu/pc98_disks_in_linux/blob/main/README.md

				A1) (Optional) - (Mandatory only this is the first time) add the following line to /etc/fstab
				[file: /etc/fstab ]
					#VIRTUAL PC-98 FLOPPY DEVICE
					/dev/loop4  /mnt/FloppyA  auto  users,rw,exec,nofail,noauto,sync,check=strict,codepage=932,iocharset=euc-jp,x-gvfs-show  0 0
				[/file]

				A2) (Optional) - (Mandatory only if this is the first time) Reload /etc/fstab on the system:
					# systemctl daemon-reload

				A3) (Optional) - (Mandatory only if this is the first time) Mount all unmounted partitions:
					# mount -a

				A4) Attach file to newly created PC-98 Floppy loop device
					# losetup -L -o 4096 /dev/loop4 disk.fdi

				A5) Mount the Virtual PC-98 Floppy device
					# mount /dev/loop4

				A6) Access files and copy them to local hard disk:
					$ cd /mnt/FloppyA

				A7) (Optional) Access files from distrobox by mounting directory of copied files as hard disk.

				A8) Once you're done with the disk, umount & detach the loop device:
					$ sudo umount /dev/loop4 && sudo losetup -d /dev/loop4

			B) (ALTERNATE) -  using linux 'mount' tool is also possible for automating loop device setup:
				Note: Note this still requires steps A1 to A2 to be done, /dev/loop4 has to exist in the /etc/fstab.
				# mount --mkdir ./disk.fdi /mnt/FloppyA/ -o loop=/dev/loop4,offset=4096
				
				Note2: /mnt/FloppyA/ can be replaced here with anything you want.

				B1) Umounting from this point will automatically detach the used loop device at /dev/loop4:
					# umount /mnt/FloppyA

			C) (ALTERNATE - UNTESTED) -  Using linux's mtool util that comes installed with guestfs-tools package.
			but this only works with real Floppy Disks!

				source: https://dosbox-x.com/wiki/Guide%3AManaging-image-files-in-DOSBox%E2%80%90X

			D) (ALTERNATE - UNTESTED) - Using 'gnome-disk-image-mounter' on linux is also possible!
			please read the "VERY IMPORTANT" section below when going for this step.


		DOSBox-X supports mounting diskette (aka Floppy) images in the following formats:

			RAW MFM sector image format (typically ending in .ima or .img)

			Microsoft DMF (typically ending in .dmf)

			NEC PC-98 diskette images (typically ending in .fdd, .fdi, .nfd or .d88)

		However, floppy disk emulates the real speeds of floppy devices, which is very slow,
		consider following A) Solution above for extracting files from a given image
		and mounting directory as hard drive instead.
		
		more info on: 
			https://dosbox-x.com/wiki/Guide%3AManaging-image-files-in-DOSBox%E2%80%90X

		VERY IMPORTANT:
			Warning
				You should never mount a disk image simultaneously in both DOSBox-X 
				and in Linux, as that can cause corruption of files or the filesystem on the diskette image.

				Because of this, always make sure the original data you want to mount is isolated
				in a different directory and it's data is backed up elsewhere.

	RUNNING DOSBOX-X AS PC-9800

		Type on terminal:
			$ dosbox-x -set machine=pc98

		^ it's also possible to set this at dosbox-x's configuration file,
		check section "ABOUT CONFIGURATION FILE".

	SAVING AND LOADING STATES
		Saving/loading states can prevent the user from wasting a lot of time setting up 
	dosbox-x by a lot!

	Go to tab 'Capture', there you'll find options for selecting a state slot, saving and loading.
	Save states can save machine settings and mounted drives.

	There's also an option for changing pages, each page containning more than 10 state slots.


	HOTKEYS
		ctrl+C			#Machine Configuration
		ctrl+f6			#Pastes text from the host's clipboard into the dos screen
		ctrl+f9			#Closes dosbox-x
		ctrl+f10		#Captures/Uncapture the mouse into dosbox-x
	
	OPTIMIZING DOSBOX-X
		CHANGING MACHINE GRAPHICS CARDS
			Configuration Tool > Main > Machine > Choose 'svga_s3vision968'

		SETTING A HIGHER MEMSIZE
			Configuration Tool > Main > MemSize > 128
			
		TURN OFF PC-98 AND DOS/V
			Configuration Tool > PC-98 > Set PC-98 fm board to 'off'
			Configuration Tool > PC-98 > Set dosv to 'off'

		SET CYCLES TO 407000
			Configuration Tool > CPU > set cycle '407000'
			
		SET CPU-TYPE AS EXPERIMENTAL
			Configuration Tool > CPU > choose cputype 'experimental'

		SET CPU CORE TO DYNAMIC
			Configuration Tool > CPU > set core to 'dynamic'

		SET DOS TO 7.1
			Configuration Tool > DOS > set field 'ver.' to: 7.1
			
		DISABLE DISK ACCESS EMULATION
			Configuration Tool > DOS > set 'Hard Drive Data Rate Limit' to: 0
			Configuration Tool > DOS > set 'Floppy Drive Data Rate Limit' to: 0

			
	RUNNING 4DOS INSTEAD OF COMMMAND.COM FOR DOSBOX-X
		Set a SHELL environment var before launching dosbox-x:
			$ SHELL=4DOS.COM dosbox-x <DIR>
	
	RUNNING PROGRAMS ON THE HOST'S SYSTEM
		Use '-hostrun' as argument before launching the box, and start programs with the START command:
			$ dosbox-x -hostrun <DIR>

		2) Inside dosbox-x:
			$ START PROGRAM.EXE

	LAUNCHING PROGRAMS USING UPPER MEMORY
		From inside distrobox-x, run program with 'LH':
			$ LH PROGRAM.EXE

	ENABLE GLIDE PASSTHROUGH
		Some Voodoo Graphics Card games can benefit from glide passthrough,
			1) Configuration Tool > Voodoo > Check 'Glide'
			2) Configuration Tool > Voodoo > Set 'lfb' to 'full_noaux'
			3) Configuration Tool > DOS > Enable xms
			3) Configuration Tool > DOS > Enable umb

		4) All steps from 5 to 9 are only required if you want actual GLIDE to OPENGL Calls,
		Steps 5 to 9 aren't required, hence dosbox-x will use it's internal emulator for glide.
			VERY IMPORTANT: Some games might require launching a different executable,
			for example: carmageddon only runs in GLIDE through it's CARMA3DFX.BAT

		5) When playing games that use 'glide', you'll need to rename the GLIDE2X.OVL to GLIDE2X.ORG
		so that the game loads the Z:\SYSTEM\GLIDE2X.OVL instead. If game doesn't works, you'll need to copy
		from Z:\ to current game folder.

		6) On linux, you'll need libglide2x.so installed.

		7) Not all DOS games that support 3dfx use the separate (dynamically linked) GLIDE2X.OVL library. 
		Some are STATICALLY LINKED (the GLIDE library code was integrated with the game code during compilation). 
		Those games ARE NOT compatible with GLIDE PASS-THROUGH MODE, and need to be run in low-level emulation mode instead. 

		8) OpenGlide is not compatible with SDL2 Version of DOSBOX-X, as such you can only use it with the DOSBox-X SDL1 version. 
		If you do try to use it with the DOSBox-X SDL2 version, OpenGlide will segfault when trying to use the glide pass-through. 

		9) In addition to setting glide=true, DosBOX-X also checks if it finds a compatible glide library on the host. 
		If this check fails, GLIDE2X.OVL will not appear, and the menu option Video ⇒ 3dfx emulation ⇒ Glide passthrough will not be checked.

		The glide library installed on the host needs to be the same CPU architecture as the DOSBox-X executable. 
		So for instance, if you are using a 32bit x86 Glide library, such as nGlide(Windows only), you must also use a 
		32bit x86 DOSBox-X executable.
		
		SOURCE: https://dosbox-x.com/wiki/Guide%3ASetting-up-3dfx-Voodoo-in-DOSBox%E2%80%90X#_lfb

	AUTOMOUNTING DRIVES ON LAUNCH
		Just give directory when launching the program:
			$ dosbox-x <DIR>
				or
			$ cd <DIR> && dosbox-x ./
			
	ABOUT CONFIGURATION FILE
		dosbox-x has example reference files under /usr/share/dosbox-x/ directory
	in practice these are only used by dosbox when they're copied 
	to the user home directory under the following path:
		(Windows):	C:\Users\<username>\AppData\Local\DOSBox-X\dosbox-x-<version number>.conf
		(Linux):	~/.config/dosbox-x/dosbox-x-<version number>.conf
		(macOS):	~/Library/Preferences/DOSBox-X <version number> Preferences

	VERY IMPORTANT 1:
		If you still have trouble editing/finding the configuration file:
			$ dosbox-x -editconf nvim

		this will deliver the actual location for the configuration file that's currently being used.
		make a backup if you ever need to make changes.

		OBS: If needed, use the internal configuration mode, instead of editing files by hand.
		yuo can still look inside configuration files for specific settings that may not be easily found
		in the internal configuration mode.

	VERY IMPORTANT 2: as dosbox-x gets upgraded, the version number may change and dosbox-x
	may stop using the old configuration file!

	portable	configuration copies are usually saved on: ~/dosbox-x.conf
	this one file is always read first before any other configuration files!


		DOSBOX-X GUI INTERFACE FOR CONFIGURATION FILES
			Go to tab "Main" and click "Configuration Tool".
		Make the changes you want and click on 'Save'.

		Make sure you have the file saved to ~/dosbox-x.conf
		if you want to use it.

		Press F12+C to go straight into it.

	[file: /usr/share/dosbox-x/dosbox-x.reference.conf]
		country = 61
		keyboardlayout = br
		machine = pc98
		memsize = 30
		pc-98 force ibm keyboard layout = false
		dbcs = true
		dos clipboard device enable = true
		scaler = xbrz
	[/file]

	^ VERY IMPORTANT: Again, the file above must be copied to ~/dosbox-x.conf


	SOURCE:
		https://dosbox-x.com/wiki/#Home
		https://dosbox-x.com/wiki/DOSBox%E2%80%90X%E2%80%99s-Feature-Highlights
		https://dosbox-x.com/wiki/Guide%3ASetting-up-3dfx-Voodoo-in-DOSBox%E2%80%90X#_no_glide2x_ovl_on_z_drive
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
CUSTOMIZING YOUR ZSH EXPERIENCE

	INSTALLING ZSH
		source: https://wiki.archlinux.org/title/zsh

		1) As of current date, zsh package belongs to the 'extra' repository:
			# pacman -S zsh

		INSTALLING CUSTOM THEMES
			Here are a few options for customizing your ZSH Experience:

				1) It's also possible to use 'powerlevel10k' instead of 'oh-my-zsh'
				read more on:
					CHANGING ZSH THEME | INSTALLING POWERLEVEL10K | PERFORMANCE

				2) Install the oh-my-zsh-git package, it'll install themes & plugins that you can use with ZSH:
					source: https://github.com/ohmyzsh/ohmyzsh/wiki/Themes
				read more on:
					CHANGING ZSH THEME | INSTALLING OH-MY-ZSH


	SETTING UP ZSH AS YOUR DEFAULT SHELL
		After installing zsh, you'll have to change your default shell:
			#chsh -s /usr/bin/zsh
				or
			#chsh -s /usr/bin/zsh <username>
				or
			#usermod --shell /usr/bin/zsh <username>

	CLONING ZSH CONFIGURATION FOR OTHER USERS
		Follow 1st and 2nd steps! The same can be done to ~/.zshrc file after installing oh-my-zsh-git

		---- 1st STEP:
			Copy the file located in your home user directory: ~/.zshrc
			and change the following line: 
				[file: ~/.zshrc]
					zstyle :compinstall filename '/home/<user>/.zshrc'
				[/file]

			NOTE: It's also possible to copy .zshrc file to /etc/skel/ to allow all newly created users
			to copy and user the same settings.

			VERY IMPORTANT:
				Note that the <user> must be your username; as for the root home directory, 
				it is located under /root/ not in /home/ .

			(RECOMMENDED) ALTERNATE OPTION - It's also possible to use the Enviornment Variable $USER :

				[file: ~/.zshrc]
					zstyle :compinstall filename '/home/${USER}/.zshrc'
				[/file]

		---- 2nd STEP:
			Linux has a directory that is used for automatically copying files for newly created users
			it is called /etc/skel :
				
				# cp ~/.zshrc /etc/skel
					or
				$ sudo cp ~/.zshrc /etc/skel
					or
				$ run0 cp ~/.zshrc /etc/skel

	CHANGING ZSH THEME | INSTALLING POWERLEVEL10K | PERFORMANCE
		Before you procede, know that it's possible to use both 'powerlevel10k' and 'oh-my-zsh', recommended readings:
			USING BOTH POWERLEVEL10K AND OH-MY-ZSH FOR EXTRA FEATURES | PERFORMANCE | RECOMMENDED

		-----
		Unlike the some other themes, this doesn't require oh-my-zsh to be installed.
		this theme also removes the native 'zsh' lag that happens when you login into a user that also has zsh.
		the user must also be using powerlevel10k.

		Also, it's also very fast when running/instantiating new terminals.

		VERY GOOD ZSH Theme, can be easily installed with:

		1.1 - Install zsh-theme-powerlevel10k
			$ yay -S zsh-theme-powerlevel10k
		
			1.1.1) (OPTIONAL) If you don't have zsh-theme-powerlevel10k available in any of your linux distro 
			package manager repositories, it's still possible to fetch it straight from github:
				$ git clone --depth=1 'https://github.com/romkatv/powerlevel10k.git ~/powerlevel10k
				
				Note: if you've done 1.1.1 step, you can skip the 1st line in 1.2.2 step.

		1.2 - Copy the zsh theme:
			1.2.1) It's advisable to make a backup copy of your current ~/.zshrc
				$ cp ~/.zshrc ~/.zshrc-old
			
			1.2.2) Copy and then install and setup by running 'zsh' again:
				$ cp /usr/share/zsh-theme-powerlevel10k ~/powerlevel10k;
				$ echo 'source /usr/share/zsh-theme-powerlevel10k/powerlevel10k.zsh-theme' >>~/.zshrc;
				$ zsh;

			1.2.3) Check differences to make sure everything is 'OK':
				$ nvim -d ~/.zshrc ~/.zshrc-old

		If you have to, you can check my ~/.zshrc file for reference:
			MY CUSTOM POWERLEVEL10K ZSHRC CONFIGURATION FILE

		source: 
			/usr/share/zsh-theme-powerlevel10k/README.md
			https://github.com/romkatv/powerlevel10k?tab=readme-ov-file#arch-linux


		CLONING POWERLEVEL10K FOR OTHER USERS
			This will allow every single new user to have the same settings as yours on 'powerlevel10k',
		only use this AFTER setting up and installing powerlevel10k for your own user:

			# cp -R ~/{.pk10.zsh,.zshrc,powerlevel10k} /etc/skel/

		you'll need to make the necessary changes to .zshrc to allow other users to use the same file.
		example: HISTFILE var might need to be changed on the /etc/skel/ copy of the .zshrc.

	CHANGING ZSH THEME | INSTALLING OH-MY-ZSH
		IMPORTANT: AS OF 2025, i'd recommended to install powerlevel10k, read more on:
			INSTALLING POWERLEVEL10K

		however, it's also possible to install and use both for extra features, read more on:
			USING BOTH POWERLEVEL10K AND OH-MY-ZSH FOR EXTRA FEATURES | PERFORMANCE | RECOMMENDED

		----
		On archlinux, you'll need YAY Package Manager installed and running, then simply do this:
			$ yay -S oh-my-zsh-git

		OLD NOTE: After installing oh-my-zsh-git package, copy the contents of "/usr/share/oh-my-zsh/zshrc"
		into your home directory: ~/.zshrc

		UPDATE: As of 2025, Manually Copying the contents is no longer necessary, however oh-my-zsh-git installation 
		will replace your current ~/.zshrc file, make a backup copy before installing!

			1) Make a backup copy:
				$ cp ~/.zshrc ~/.zshrc-old

			2) Installing oh-my-zsh-git:
				$ yay -S oh-my-zsh-git

			2) See the differences between them and apply them:
				$ nvim -d ~/.zshrc ~/.zshrc-old
					or
				$ nvim -d /usr/share/oh-my-zsh/zshrc ~/.zshrc

			Copy & paste the new contents from the ~/.zshrc-old into the new ~/.zshrc configuration file.

				RECOMMENDED OH-MY-ZSH TOPIC READINGS:
					MY CUSTOM OH-MY-ZSH ZSHRC CONFIGURATION FILE

			TROUBLESHOOTING ZSH AND OH-MY-ZSH INSTALLATION
				If you mess setting up oh-my-zsh, you can try generating a new .zshrc file
			by simply deleting it and running "zsh" again.

			You'll then be guided into making a complete new setup.
			
			Again, make sure you backup your current ~/.zshrc file installed by oh-my-zsh first
			if it's still working:

				1A) (Optional - Only do this if the current ~/.zshrc is working) Move the old file: 
					$ mv ~/.zshrc ~/.zshrc-old

				2) Run ZSH again and complete the setup:
					$ zsh

				3A) See the differences between them and apply them to the ~/.zshrc file
					$ nvim -d ~/.zshrc ~/.zshrc-old
						or
				3B) - (Optional/Alternate - Only if the current ~/.zshrc is completely broken) 
					Fetch a complete New Copy, check differences then apply it:
						$ nvim -d ~/.zshrc /usr/share/oh-my-zsh/zshrc

		In order to enable a theme, set ZSH_THEME to the name of the theme in your ~/.zshrc, 
		before sourcing Oh My Zsh; for example: ZSH_THEME=robbyrussell If you do not want any theme enabled, 
		just set ZSH_THEME to blank: ZSH_THEME=""

	
		RECOMMENDED OH-MY-ZSH THEMES
			1) af-magic 
			2) blinks
			3) fino-time
			4) linux-only
				Vanilla theme

			5) jonathan
				this theme has been broken for a very long time, it contains a graphical glitch that has never
				been fixed to this day. Also, becareful when using the root user, the '#' symbol that shows up for root 
				user isn't shown on the far left side of the screen, the name 'root' instead shows up on the far-right 
				side, if you're not paying attention on the commandline it can lead you to cause permanent harm.


	USING BOTH POWERLEVEL10K AND OH-MY-ZSH FOR EXTRA FEATURES | PERFORMANCE | RECOMMENDED
		It's also possible to use both POWERLEVEL10K theme and OH-MY-ZSH features(without the theme!)
		First, you'll need to install and setup 'oh-my-zsh', then install 'powerlevel10k'

		MANDATORY STEPS:
			CHANGING ZSH THEME | INSTALLING OH-MY-ZSH
			CHANGING ZSH THEME | INSTALLING POWERLEVEL10K | PERFORMANCE
	
		One of the best things from 'oh-my-zsh' is the auto-complete feature after partially typing something by 
		pressing the 'up' key.

		
		CLONING POWERLEVEL10K + OH-MY-ZSH CONFIGURATION FOR OTHER USERS
			This will allow every single new user to have the same settings as yours,
			only use this AFTER setting up and installing powerlevel10k + 'oh-my-zsh' for your own user:

				# cp -R ~/{.pk10.zsh,.zshrc,powerlevel10k} /etc/skel/

			you'll need to make the necessary changes to .zshrc to allow other users to use the same file.
			example: HISTFILE var might need to be changed on the /etc/skel/ copy of the .zshrc.

		MY CUSTOM POWERLEVEL10K + OH-MY-ZSH CONFIGURATION FILE
			Only after following the 'MANDATORY STEPS', you can check this configuration 
			file for reference and make changes if you need.
			
			Note: you should disable the broot line if you DON'T plan on using broot.
			Note: Always alter the HISTFILE variable in this file, read the comments.

			[file: ~/.zshrc ]
				# Enable Powerlevel10k instant prompt. Should stay close to the top of ~/.zshrc.
				# Initialization code that may require console input (password prompts, [y/n]
				# confirmations, etc.) must go above this block; everything else may go below.
				if [[ -r "${XDG_CACHE_HOME:-$HOME/.cache}/p10k-instant-prompt-${(%):-%n}.zsh" ]]; then
				  source "${XDG_CACHE_HOME:-$HOME/.cache}/p10k-instant-prompt-${(%):-%n}.zsh"
				fi


				# If you come from bash you might have to change your $PATH.
				# export PATH=$HOME/bin:$HOME/.local/bin:/usr/local/bin:$PATH

				# Path to your Oh My Zsh installation.
				ZSH=/usr/share/oh-my-zsh/

				# Set name of the theme to load --- if set to "random", it will
				# load a random theme each time Oh My Zsh is loaded, in which case,
				# to know which specific one was loaded, run: echo $RANDOM_THEME
				# See https://github.com/ohmyzsh/ohmyzsh/wiki/Themes
				#ZSH_THEME="robbyrussell"

				# Set list of themes to pick from when loading at random
				# Setting this variable when ZSH_THEME=random will cause zsh to load
				# a theme from this variable instead of looking in $ZSH/themes/
				# If set to an empty array, this variable will have no effect.
				# ZSH_THEME_RANDOM_CANDIDATES=( "robbyrussell" "agnoster" )

				# Uncomment the following line to use case-sensitive completion.
				# CASE_SENSITIVE="true"

				# Uncomment the following line to use hyphen-insensitive completion.
				# Case-sensitive completion must be off. _ and - will be interchangeable.
				# HYPHEN_INSENSITIVE="true"

				# Uncomment one of the following lines to change the auto-update behavior
				# zstyle ':omz:update' mode disabled  # disable automatic updates
				# zstyle ':omz:update' mode auto      # update automatically without asking
				# zstyle ':omz:update' mode reminder  # just remind me to update when it's time

				# Uncomment the following line to change how often to auto-update (in days).
				# zstyle ':omz:update' frequency 13

				# Uncomment the following line if pasting URLs and other text is messed up.
				# DISABLE_MAGIC_FUNCTIONS="true"

				# Uncomment the following line to disable colors in ls.
				# DISABLE_LS_COLORS="true"

				# Uncomment the following line to disable auto-setting terminal title.
				# DISABLE_AUTO_TITLE="true"

				# Uncomment the following line to enable command auto-correction.
				# ENABLE_CORRECTION="true"

				# Uncomment the following line to display red dots whilst waiting for completion.
				# You can also set it to another string to have that shown instead of the default red dots.
				# e.g. COMPLETION_WAITING_DOTS="%F{yellow}waiting...%f"
				# Caution: this setting can cause issues with multiline prompts in zsh < 5.7.1 (see #5765)
				# COMPLETION_WAITING_DOTS="true"

				# Uncomment the following line if you want to disable marking untracked files
				# under VCS as dirty. This makes repository status check for large repositories
				# much, much faster.
				# DISABLE_UNTRACKED_FILES_DIRTY="true"

				# Uncomment the following line if you want to change the command execution time
				# stamp shown in the history command output.
				# You can set one of the optional three formats:
				# "mm/dd/yyyy"|"dd.mm.yyyy"|"yyyy-mm-dd"
				# or set a custom format using the strftime function format specifications,
				# see 'man strftime' for details.
				# HIST_STAMPS="mm/dd/yyyy"

				# Would you like to use another custom folder than $ZSH/custom?
				# ZSH_CUSTOM=/path/to/new-custom-folder

				# Which plugins would you like to load?
				# Standard plugins can be found in $ZSH/plugins/
				# Custom plugins may be added to $ZSH_CUSTOM/plugins/
				# Example format: plugins=(rails git textmate ruby lighthouse)
				# Add wisely, as too many plugins slow down shell startup.
				plugins=(git)

				# User configuration

				# export MANPATH="/usr/local/man:$MANPATH"

				# You may need to manually set your language environment
				# export LANG=en_US.UTF-8

				# Preferred editor for local and remote sessions
				# if [[ -n $SSH_CONNECTION ]]; then
				#   export EDITOR='vim'
				# else
				#   export EDITOR='nvim'
				# fi

				# Compilation flags
				# export ARCHFLAGS="-arch $(uname -m)"

				# Set personal aliases, overriding those provided by Oh My Zsh libs,
				# plugins, and themes. Aliases can be placed here, though Oh My Zsh
				# users are encouraged to define aliases within a top-level file in
				# the $ZSH_CUSTOM folder, with .zsh extension. Examples:
				# - $ZSH_CUSTOM/aliases.zsh
				# - $ZSH_CUSTOM/macos.zsh
				# For a full list of active aliases, run `alias`.
				#
				# Example aliases
				# alias zshconfig="mate ~/.zshrc"
				# alias ohmyzsh="mate ~/.oh-my-zsh"
				# Lines configured by zsh-newuser-install

				#----VERY IMPORTANT: This line from oh-my-zsh, it actually 
				#belong to the end of the script, but were moved here because 
				#they were overriding desired user options for zsh like 'bindkey -v':
				source $ZSH/oh-my-zsh.sh

				#----This will read your system-wide Environment Variables:
				source /etc/environment

				#----This is the default value for HISTFILE, 
				#----disabling since i'm using a custom one on my /etc/environmment file:
				#HISTFILE=~/.histfile
				HISTFILE=${USER_HISTFILE}
				HISTSIZE=4000
				SAVEHIST=4000
				setopt autocd extendedglob nomatch notify
				unsetopt beep
				bindkey -v
				# End of lines configured by zsh-newuser-install
				# The following lines were added by compinstall
				zstyle :compinstall filename '~/.zshrc'

				autoload -Uz compinit
				compinit
				# End of lines added by compinstall

				#-------These lines are from oh-my-zsh:
				ZSH_CACHE_DIR=$HOME/.cache/oh-my-zsh
				if [[ ! -d $ZSH_CACHE_DIR ]]; then
				  mkdir $ZSH_CACHE_DIR
				fi

				#----You need broot installed on your system and run broot at least once per user
				#----you can also disable the broot line by adding '#' to the beginning for commenting it out:
				source ~/.config/broot/launcher/bash/br

				#----powerlevel10k, leave it be:
				source ~/powerlevel10k/powerlevel10k.zsh-theme

				# To customize prompt, run `p10k configure` or edit ~/.p10k.zsh.
				[[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh
			[/file]

	----
	MY CUSTOM ZSHRC CONFIGURATION FILE BEFORE INSTALLING POWERLEVEL10K / OH-MY-ZSH

		--Old file - not updated, just here for reference:
		[file: ~/.zshrc ]
			# MY ZSHRC CUSTOM CONFIGURATION
			# Lines configured by zsh-newuser-install
			HISTFILE=~/.histfile
			HISTSIZE=1000
			SAVEHIST=1000
			setopt extendedglob
			unsetopt beep nomatch
			bindkey -v
			# End of lines configured by zsh-newuser-install
			# The following lines were added by compinstall
			# VERY IMPORTANT CHANGE THE USER NAME HERE:
			zstyle :compinstall filename '~/.zshrc'

			autoload -Uz compinit
			compinit
			# End of lines added by compinstall
		[/file]

	MY CUSTOM POWERLEVEL10K ZSHRC CONFIGURATION FILE

		[file:  ~/.zshrc ]
			# Enable Powerlevel10k instant prompt. Should stay close to the top of ~/.zshrc.
			# Initialization code that may require console input (password prompts, [y/n]
			# confirmations, etc.) must go above this block; everything else may go below.
			if [[ -r "${XDG_CACHE_HOME:-$HOME/.cache}/p10k-instant-prompt-${(%):-%n}.zsh" ]]; then
			  source "${XDG_CACHE_HOME:-$HOME/.cache}/p10k-instant-prompt-${(%):-%n}.zsh"
			fi


			# If you come from bash you might have to change your $PATH.
			# export PATH=$HOME/bin:$HOME/.local/bin:/usr/local/bin:$PATH

			# Path to your Oh My Zsh installation.
			#ZSH=/usr/share/oh-my-zsh/

			# Set name of the theme to load --- if set to "random", it will
			# load a random theme each time Oh My Zsh is loaded, in which case,
			# to know which specific one was loaded, run: echo $RANDOM_THEME
			# See https://github.com/ohmyzsh/ohmyzsh/wiki/Themes
			#ZSH_THEME="robbyrussell"

			# Set list of themes to pick from when loading at random
			# Setting this variable when ZSH_THEME=random will cause zsh to load
			# a theme from this variable instead of looking in $ZSH/themes/
			# If set to an empty array, this variable will have no effect.
			# ZSH_THEME_RANDOM_CANDIDATES=( "robbyrussell" "agnoster" )

			# Uncomment the following line to use case-sensitive completion.
			# CASE_SENSITIVE="true"

			# Uncomment the following line to use hyphen-insensitive completion.
			# Case-sensitive completion must be off. _ and - will be interchangeable.
			# HYPHEN_INSENSITIVE="true"

			# Uncomment one of the following lines to change the auto-update behavior
			# zstyle ':omz:update' mode disabled  # disable automatic updates
			# zstyle ':omz:update' mode auto      # update automatically without asking
			# zstyle ':omz:update' mode reminder  # just remind me to update when it's time

			# Uncomment the following line to change how often to auto-update (in days).
			# zstyle ':omz:update' frequency 13

			# Uncomment the following line if pasting URLs and other text is messed up.
			# DISABLE_MAGIC_FUNCTIONS="true"

			# Uncomment the following line to disable colors in ls.
			# DISABLE_LS_COLORS="true"

			# Uncomment the following line to disable auto-setting terminal title.
			# DISABLE_AUTO_TITLE="true"

			# Uncomment the following line to enable command auto-correction.
			# ENABLE_CORRECTION="true"

			# Uncomment the following line to display red dots whilst waiting for completion.
			# You can also set it to another string to have that shown instead of the default red dots.
			# e.g. COMPLETION_WAITING_DOTS="%F{yellow}waiting...%f"
			# Caution: this setting can cause issues with multiline prompts in zsh < 5.7.1 (see #5765)
			# COMPLETION_WAITING_DOTS="true"

			# Uncomment the following line if you want to disable marking untracked files
			# under VCS as dirty. This makes repository status check for large repositories
			# much, much faster.
			# DISABLE_UNTRACKED_FILES_DIRTY="true"

			# Uncomment the following line if you want to change the command execution time
			# stamp shown in the history command output.
			# You can set one of the optional three formats:
			# "mm/dd/yyyy"|"dd.mm.yyyy"|"yyyy-mm-dd"
			# or set a custom format using the strftime function format specifications,
			# see 'man strftime' for details.
			# HIST_STAMPS="mm/dd/yyyy"

			# Would you like to use another custom folder than $ZSH/custom?
			# ZSH_CUSTOM=/path/to/new-custom-folder

			# Which plugins would you like to load?
			# Standard plugins can be found in $ZSH/plugins/
			# Custom plugins may be added to $ZSH_CUSTOM/plugins/
			# Example format: plugins=(rails git textmate ruby lighthouse)
			# Add wisely, as too many plugins slow down shell startup.
			plugins=(git)

			# User configuration

			# export MANPATH="/usr/local/man:$MANPATH"

			# You may need to manually set your language environment
			# export LANG=en_US.UTF-8

			# Preferred editor for local and remote sessions
			# if [[ -n $SSH_CONNECTION ]]; then
			#   export EDITOR='vim'
			# else
			#   export EDITOR='nvim'
			# fi

			# Compilation flags
			# export ARCHFLAGS="-arch $(uname -m)"

			# Set personal aliases, overriding those provided by Oh My Zsh libs,
			# plugins, and themes. Aliases can be placed here, though Oh My Zsh
			# users are encouraged to define aliases within a top-level file in
			# the $ZSH_CUSTOM folder, with .zsh extension. Examples:
			# - $ZSH_CUSTOM/aliases.zsh
			# - $ZSH_CUSTOM/macos.zsh
			# For a full list of active aliases, run `alias`.
			#
			# Example aliases
			# alias zshconfig="mate ~/.zshrc"
			# alias ohmyzsh="mate ~/.oh-my-zsh"
			# Lines configured by zsh-newuser-install

			#VERY IMPORTANT: These 2 lines are from oh-my-zsh, they actually 
			#belong to the end of the script, but were moved here because 
			#they were overriding user options for zsh:
			#source $ZSH/oh-my-zsh.sh
			#This line is only meant for /usr/bin/broot
			source /etc/environment

			#----This is the default value for HISTFILE, 
			#----disabling since i'm using a custom one on my /etc/environmment file:
			#HISTFILE=~/.histfile
			HISTFILE=${USER_HISTFILE}
			HISTSIZE=4000
			SAVEHIST=4000
			setopt autocd extendedglob nomatch notify
			unsetopt beep
			bindkey -v
			# End of lines configured by zsh-newuser-install
			# The following lines were added by compinstall
			zstyle :compinstall filename '~/.zshrc'

			autoload -Uz compinit
			compinit
			# End of lines added by compinstall

			#-------These lines are from oh-my-zsh:
			#ZSH_CACHE_DIR=$HOME/.cache/oh-my-zsh
			#if [[ ! -d $ZSH_CACHE_DIR ]]; then
			#  mkdir $ZSH_CACHE_DIR
			#fi

			#Disabling ZSH Here:
			#source $ZSH/oh-my-zsh.sh
			source ~/.config/broot/launcher/bash/br
			source ~/powerlevel10k/powerlevel10k.zsh-theme

			# To customize prompt, run `p10k configure` or edit ~/.p10k.zsh.
			[[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh
		[/file]

	MY CUSTOM OH-MY-ZSH ZSHRC CONFIGURATION FILE
		Only use this if you have oh-my-zsh-git packages installed on your system.

		--Updated file:
		[file: ~/.zshrc]
			# If you come from bash you might have to change your $PATH.
			# export PATH=$HOME/bin:$HOME/.local/bin:/usr/local/bin:$PATH

			# Path to your Oh My Zsh installation.
			ZSH=/usr/share/oh-my-zsh/

			# Set name of the theme to load --- if set to "random", it will
			# load a random theme each time Oh My Zsh is loaded, in which case,
			# to know which specific one was loaded, run: echo $RANDOM_THEME
			# See https://github.com/ohmyzsh/ohmyzsh/wiki/Themes
			ZSH_THEME="robbyrussell"

			# Set list of themes to pick from when loading at random
			# Setting this variable when ZSH_THEME=random will cause zsh to load
			# a theme from this variable instead of looking in $ZSH/themes/
			# If set to an empty array, this variable will have no effect.
			# ZSH_THEME_RANDOM_CANDIDATES=( "robbyrussell" "agnoster" )

			# Uncomment the following line to use case-sensitive completion.
			# CASE_SENSITIVE="true"

			# Uncomment the following line to use hyphen-insensitive completion.
			# Case-sensitive completion must be off. _ and - will be interchangeable.
			# HYPHEN_INSENSITIVE="true"

			# Uncomment one of the following lines to change the auto-update behavior
			# zstyle ':omz:update' mode disabled  # disable automatic updates
			# zstyle ':omz:update' mode auto      # update automatically without asking
			# zstyle ':omz:update' mode reminder  # just remind me to update when it's time

			# Uncomment the following line to change how often to auto-update (in days).
			# zstyle ':omz:update' frequency 13

			# Uncomment the following line if pasting URLs and other text is messed up.
			# DISABLE_MAGIC_FUNCTIONS="true"

			# Uncomment the following line to disable colors in ls.
			# DISABLE_LS_COLORS="true"

			# Uncomment the following line to disable auto-setting terminal title.
			# DISABLE_AUTO_TITLE="true"

			# Uncomment the following line to enable command auto-correction.
			# ENABLE_CORRECTION="true"

			# Uncomment the following line to display red dots whilst waiting for completion.
			# You can also set it to another string to have that shown instead of the default red dots.
			# e.g. COMPLETION_WAITING_DOTS="%F{yellow}waiting...%f"
			# Caution: this setting can cause issues with multiline prompts in zsh < 5.7.1 (see #5765)
			# COMPLETION_WAITING_DOTS="true"

			# Uncomment the following line if you want to disable marking untracked files
			# under VCS as dirty. This makes repository status check for large repositories
			# much, much faster.
			# DISABLE_UNTRACKED_FILES_DIRTY="true"

			# Uncomment the following line if you want to change the command execution time
			# stamp shown in the history command output.
			# You can set one of the optional three formats:
			# "mm/dd/yyyy"|"dd.mm.yyyy"|"yyyy-mm-dd"
			# or set a custom format using the strftime function format specifications,
			# see 'man strftime' for details.
			# HIST_STAMPS="mm/dd/yyyy"

			# Would you like to use another custom folder than $ZSH/custom?
			# ZSH_CUSTOM=/path/to/new-custom-folder

			# Which plugins would you like to load?
			# Standard plugins can be found in $ZSH/plugins/
			# Custom plugins may be added to $ZSH_CUSTOM/plugins/
			# Example format: plugins=(rails git textmate ruby lighthouse)
			# Add wisely, as too many plugins slow down shell startup.
			plugins=(git)

			# User configuration

			# export MANPATH="/usr/local/man:$MANPATH"

			# You may need to manually set your language environment
			# export LANG=en_US.UTF-8

			# Preferred editor for local and remote sessions
			# if [[ -n $SSH_CONNECTION ]]; then
			#   export EDITOR='vim'
			# else
			#   export EDITOR='nvim'
			# fi

			# Compilation flags
			# export ARCHFLAGS="-arch $(uname -m)"

			# Set personal aliases, overriding those provided by Oh My Zsh libs,
			# plugins, and themes. Aliases can be placed here, though Oh My Zsh
			# users are encouraged to define aliases within a top-level file in
			# the $ZSH_CUSTOM folder, with .zsh extension. Examples:
			# - $ZSH_CUSTOM/aliases.zsh
			# - $ZSH_CUSTOM/macos.zsh
			# For a full list of active aliases, run `alias`.
			#
			# Example aliases
			# alias zshconfig="mate ~/.zshrc"
			# alias ohmyzsh="mate ~/.oh-my-zsh"
			# Lines configured by zsh-newuser-install

			#VERY IMPORTANT: These 2 lines are from oh-my-zsh, they actually 
			#belong to the end of the script, but were moved here because 
			#they were overriding user options for zsh:
			source $ZSH/oh-my-zsh.sh
			#This line is only meant for /usr/bin/broot
			#source /home/${USER}/.config/broot/launcher/bash/br
			source /etc/environment

			#----This is the default value for HISTFILE, 
			#----disabling since i'm using a custom one on my /etc/environmment file:
			#HISTFILE=~/.histfile
			HISTFILE=${USER_HISTFILE}
			HISTSIZE=4000
			SAVEHIST=4000
			setopt autocd extendedglob nomatch notify
			unsetopt beep
			bindkey -v
			# End of lines configured by zsh-newuser-install
			# The following lines were added by compinstall
			zstyle :compinstall filename '/home/${USER}/.zshrc'

			autoload -Uz compinit
			compinit
			# End of lines added by compinstall

			#-------These lines are from oh-my-zsh:
			ZSH_CACHE_DIR=$HOME/.cache/oh-my-zsh
			if [[ ! -d $ZSH_CACHE_DIR ]]; then
			  mkdir $ZSH_CACHE_DIR
			fi

			#VERY IMPORTANT: These lines were moved upwards,
			#because they were overriding VI/NVIM keys set on bindkey -v as 
			#well as the other options above it:
			#source $ZSH/oh-my-zsh.sh
			#source /home/${USER}/.config/broot/launcher/bash/br
		[/file]
	Note: Replace <username> with your username
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
LOADING ZSH MODULE
type 'zmodload' to see a list of loaded modules.

type 'zmodload zsh/pcre' to load zsh's pcre module.
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
USING ZSH WITH VI/VIM KEYS
	'ZSH' uses the some of the same features found in VI, VIM and NVIM.
It also has insertion, selection and command mode.

This option can be chosen during installation.
Simply re-install by removing ~/.zshrc and executing 'zsh' on the commandline.

Alternatively, the option can also be added by inserting the following option 'bindkey -v' in the ~/.zshrc file.

'ZSH' defaults to using Emac keys instead, so manual setup will be required as described above.

You'll have to learn VIM/NVIM on your own tho ;)
I do suggest looking at the INDEX
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
PROPERLY USING 'RENAME' FOR RENAMING FILES
	'Rename' tool has 2 different known versions:\
		1 - Util-linux version
		2 - PERL Version
	
If it's of interest, you can check which version you own by typing:
	$ rename --version

	UTIL-LINUX 'RENAME' VERSION
		This is the basic syntax of the util-linux version:
			$ rename <regular_expression> <replacement_string> <filename>
				or
			$ rename -- <regular_expression> <replacement_string> <filename>

		1 - The regular_expression is the text part you want to replace from the filename.
		2 - The replacement_string is the text that will substitute the found regular_expression
		3 - Filename is the file you want to rename.
		4 - The '--' is used to rename files in which the regular_expression start with a symbol or number.
		    A double dash ( -- ) is used in most linux commands to signify the end of command options.

		Examples: 
			1 - This will rename a Folder/File named Folder-04.inv to Folder.inv :
				$ rename -- '-04' '' Folder-04.inv

			2 - This will rename a Folder/File named -04Folder-04.inv to Folder.inv :
				$ rename -a -- '-04' '' ./-04Folder-04.inv

				Note: By default, rename will only change the first found occurrence of the regular_expression.
				however, with the -a argument it'll replace all found occurrences.

			3 - This will rename a Folder/File named -04Folder-04.inv to -04Folder.inv :
				$ rename -l -- '-04' '' ./-04Folder-04.inv
				
				Note: By default, rename will only change the first found occurrence of the regular_expression.
				however, with the -l argument it'll replace only the last found occurrence of the regular_expression.
			
			4 - Renaming multiple filenames, this will rename  all files starting with "VIDEO" in it's name from 
			.mp4 file extension to .mp3 :
				$ rename -l '.mp4' '.mp3' ./VIDEO*

				Note: -l ensures only the last occurrence found of the regular_expression '.mp4' is replaced,
				ENSURING that only the filename extension is changed to .mp3.

				without the -l argument only the first occurrence would change, example:
					'VIDEO .mp4 COLLECTION.mp4' file/dir name would change to 'VIDEO .mp3 COLLECTION.mp4'
				the '.mp4' filename extension wouldn't had been renamed, only the first occurrence would without the -l argument.

			5 - Dry-run & Verbose mode allow for checking which changes would be brought after renaming files, 
			before actually renaming anything:
				$ rename -vn '.mp4' '.mp3' ./VIDEO*

				Note: this command wouldn't actually rename anything, but would let the user know which changes would be applied.


	PERL 'RENAME' VERSION
		The Perl Version of the rename tool uses Perl Regular Expressions in the following syntax:
			$ rename <perl_regex> <filename>

		Examples:
			1 - The example below renames all files ending in .html to .php :
				$ rename 's/\.html$/.php/' ./*.html

				1.1 - the 's/' tells the perl engine this is substitution expression
				1.2 - '\.html$' is a regular_expression that tells the perl engine to look for .html string at the ent of the filename
				      if .html isn't found at the end of the filename, then renaming will not be applied at all.
				1.3 - '.php' is the replacement text that will substitute '\.html$' when found.
				1.4 - The final argument ./*.html merely indicates the filenames at which rename will be applied to.

			2 - Overwrite file if a given filename already exists under the name it's being attempted to rename:
				$ rename -f 's/\.mp3$/.mp4/' ./*.mp3

				2.1 - '-f' argument ensures that the renamed file will replace the existing one under the same filename.
				      permanently erasing it from existence.

				VERY IMPORTANT: Do not use this example, this is merely here as an example.

			3 - Rename all .JPEG and .JPG filenames to  .jpg extension:
				$ rename 's/\.jpe?g$/.jpg/i' ./*
					or
				$ rename 's/\.jpeg$|\.jpg/.jpg/i' ./*

				3.1 - '/i' sets up case-insensitive mod.

			4 - Performing a dry-run to see which changes would be made before actually commiting to it:
				$ rename -n 's/\.mp3$/.mp4/' ./*.mp3
				
				4.1 - '-n' will enable dry-run mode.

			5 - Converting filename to lowercase:
				$ rename 'y/A-Z/a-z/' ./*

			6 - Converting filename to uppercase:
				$ rename 'y/a-z/A-Z/' ./*

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
PROPERLY USING 'CP' FOR COPYING FILES
	'cp' is a binary tool installed with the GNU Coreutils package.
specially when using 'cp -r' for copying directories, it is important to specify
the '*' to ensure files and sub-directories are copied as well.

Case 0: Copying all files and sub-directories as well as hidden files and sub-directories:

	$ cp -r ~/Test\ Folder/{*,.*} /tmp
	^ This will copy all regular files and folders as well as the hidden ones into tmp
	
	VERY IMPORTANT: the character '.' for hidden file/folder isn't included when using '*' wildcard character.
	therefore hidden files/folders aren't included automatically when using the '*' wildcard character alone.
	in order to solve this, you need to explicitely let the shell know you also want the hidden files using '.*' instead.
		Example - Only copy the hidden files in the directory:
			$ cp ~/Test\ Folder/.* /tmp


	VERY IMPORTANT 2: Note that when using '*' the parent directory 'Test Folder' WILL NOT be created under /tmp,
	only the files contained inside of it will be copied to /tmp, 
	the '-r' argument allows sub-directories to be copied in this case, and thus any existing sub-dir will also be copied to /tmp.
	For copying the parent directory and not just it's contents, you need to ommit the '*' wildcard character.

	Note, this is the same as doing:
		$ cp -r ~/Test\ Folder/* ~/Test\ Folder/.* /tmp
			or
		$ cp -r ~/Test\ Folder/* /tmp && cp -r ~/Test\ Folder/.* /tmp

Case 1: Copying files:
	$ cp status/2007/november/nov12.txt /home/peter/tmp
	^ will just copy the file into /home/peter/tmp/

Case 2: Copy all 'test folder' containning a number from 0 to 9 at the end into /tmp/
	cp ./test\ folder\ [0-9] /tmp/
	
	Note: 'test folder 1', 'test folder 2', 'test folder 3' ... 'test folder 9' will all be copied to /tmp/	
	given that they exist.

Case 3: Copying multiple files/folders at the same time
	$ cp -r ~/{folder\ 1,folder\ 2,folder\ 3}/ ~/test\ folder

	Note: use this first if you want to test the example above:
		$ mkdir ~/{folder\ 1,folder\ 2,folder\ 3,test\ folder}    

		Note: you can easily remove folders using:
			rm -r ~/{folder\ 1,folder\ 2,folder\ 3,test\ folder}    
	
	VERY IMPORTANT: You can easily use 'zsh' for auto-completing names and commands using TAB Key on your keyboard.

Case 4: Copying file to multiple destinations at the same time
	This is similar to case mentioned right above:
		$ cp -r  ~/test\ folder ~/{folder\ 1,folder\ 2,folder\ 3}/

		Note: this will copy 'test folder' into 'folder 1', 'folder 2' and 'folder 3'.

Case 5: Copying parent directories and file:
	$ cp --parents status/2007/november/nov12.txt /home/peter/tmp
	^ When these folders don't exist on the destination(s),
	^ this will create all of the directories 'status', '2007', 'november' and the file 'nov12.txt' into '/home/peter/tmp' path.

Case 5: Recursive Copy:
	$ cp --recursive status/ /home/peter/tmp/
	^ will copy all the contents of status directory to /home/peter/tmp/

Case 6: Creating a symlink | Creating a shortcut:
	$ cp -s ./file_A ./file_shortcut
	^ this creates a file shortcut to 'file_A'.

Case 7: Creating a hard link:
	$ cp -l ./file_A ./file_hardlink
	^ creates a hardlinked file called 'file_hardlink"

	READ MORE ON:
		DIFFERENCES BETWEEN SOFTLINKS/SYMLINKS, HARDLINKS AND REFLINKS

Case 8: Suppose directory 'my_backup_dir' already exists, and you just want to overwrite the contents,
this is how it has to be done:
	example:
		$ cp -r ~/my_directory/* ~/my_backup_dir/
		^ not specifying '*' will result in the entire directory "my_directory" being copied under "my_backup_dir"
		^ as another entire directory instead of just copying the contents.

If data being copied is important, ALWAYS make sure to verify if all files inside sub-directories
are the same using the 'diff' tool(comes installed with 'diffutils' package).

	example:
		# diff --recursive -q ~/my_directory/ ~/my_backup_dir/
		^ -q ensures only differences are shown to the user
		^ --recursive ensures every files and every directories are checked
		^ running diff as root will ensure all files are verified due to different permission set
		if they exist.

	example2:
		# diff -rdq <dir_1> <dir_2>
		^ -d reads file contents to make sure data are the same,
		this ensures that files compared based on it's binary/textual contents.
		usually slower, but more trustful.

using 'rsync' for backing up and copying data is also an option.

Case 9: Preserving file ownership as another user:
	When running 'cp' as root through sudo/doas/run0, ALWAYS use -a option to preserve file permissions/ownership:
		$ sudo cp -ra <Path_A> <Path_B>

Case 10: Preserving only specific attributes when using cp as another user ( --preserve= ) :
	ATTR_LIST is a comma-separated list of attributes. Attributes are 
		1 - 'mode' for permissions (including any ACL  and  xattr permissions),  
		2 - 'ownership'  for user and group.
		3 - 'timestamps' for file timestamps.
		4 - 'links' for hard links.
		5 - 'context' for security context.
		6 - 'xattr' for extended attributes.
		7 - 'all' for all attributes.

	example: 
		$ cp -r --preserve=mode,ownership

	HINT:
		$ cp -a implies -dR --preserve=all, but you should avoid using it when the only true intention
		is just to use --preserve=all, example:

			Case 3.1:
				$ cp -a ~/source_dir/ ~/destination_folder/
				^ this will copy source_dir along with all contents of source_dir into destination_dir

			Case 3.2:
				$ cp --preserve=all ~/source_dir/* ~/destination_folder/
				^ this will copy only files from source dir into destination directory.

Case 11: Deciding how and which files are updated/overwritten when using 'cp' for pre-existing backup data:
	UPDATE controls which existing files in the destination are replaced. ( --update= ) 

	1 - default is 'all' when --update option is never used; in this cp will copy ALL files.
	2 - 'none' is the same as using --no-clobber in which no pre-existing files are replaced, regardless
	     if they've been changed; this ensure only files who don't exist are copied, ensuring no overwrites are done.
	3 - 'none-fail' is the same as 'none', but will fail when one or more files fail be copied due to 
	     permissions/ownership.
	4 - 'older' is default option only when --update is specified, this results in files being replaced only 
	    if they're older on destination folder. This requires a filesystem with modification timestamps support
	    on both destination and source folders.
	
	VERY IMPORTANT: note that none of these options ensure files are replaced/overwritten/updated when they're
	different! even when using 'older', files that have the same Timestamp wouldn't be updated when their
	contents are actually different. 

	This means that files previoulsy copied with -a option and also had it's content changed that they would never
	get updated/overwritten. Read 'Case 5' for more knowledge.

Case 12: Overwritting data only when they're different ( --reflink= ) 
	REFLINK creates a file that is similar to a Hardlink, but only referenced data blocks are changed 
	when file gets updated/modified instead of the entire file.

	note: EXT4 doesn't supports reflinked files. BCACHEFS, BTRFS and XFS does.
	Again, just as hardlinks, reflinks have to be created on the same filesystem.

	READ MORE ON:
		DIFFERENCES BETWEEN SOFTLINKS/SYMLINKS, HARDLINKS AND REFLINKS

	1 - if 'always' is set, it'll fail if either no 'reflink' support is found or if 'reflink' creation fails.
	2 - 'auto' does the same as 'always' but fallsback to standard copy when 'reflink' fails.
	3 - 'never' standard copy is always performed this way.


---MISC:
Read more on man pages for cp: $ man cp

As an Alternate Option, you can also use: $ rsync
	     
--------------------------------------------------
--------------------------------------------------
PROPERLY USING 'MV' AND 'AWK' FOR MOVING FILES AND SUB-DIRECTORIES
	Same as 'cp', moving files and sub-directories without moving the parent folder
around requires the use of 'awk'.

	VERY IMPORTANT: Mind you, the sole purpose of 'mv' is to move files, as such it's unable to merge directories under the same name 
	when files exist on the destination directory.
		Example: Consider "Folder A" exists in two different places on your hard disk, both of them containning different files,
		and you want to move "Folder A" into the another "Folder A" that's supposed to be destination folder, in this case, the command
		below would fail if the destination contain other folders/files under it's existing "Folder A":
					$ mv ~/Folder\ A /path\ to\ dir/

		Like stated before, supposing that the destination dir /path\ to\ dir/ has a child dir called "Folder A" containning files
		or other directories, the command above will fail, even when using arguments like '-f' to force the moving of folders.

		As a side note, only use 'mv' for moving files, never for replacing/overwriting/updating new files in another directory.
		If you need replacing, consider using 'cp' instead. Only use 'mv' for moving directories that actually do not exist.
		

	ls <source_dir> | sudo awk '{system("mv \"<source_dir>/"$0"\" \"<target_dir>\"")}'

	Note: only the contents will be moved to <target_dir>.
	the original <source_dir> will be empty and have no files nor sub-directories.


	Note2: 'sudo' will ensure contents are moved regardless of the permission 
	set in the files and directories.

	exampple 0: - mv the easy way -
		cd ~/.cache/yay/ &&
		mv ./ ~/backup\ packages/
	
	example 1: - Moving more than one folder at the same time, without using 'awk':
			$ mv ~/{folder\ 1,folder\ 2,folder\ 3}/ ~/test\ folder

			Note: use this first if you want to test the example above:
				$ mkdir ~/{folder\ 1,folder\ 2,folder\ 3,test\ folder}    

				Note: you can easily remove folders using:
					rm -r ~/{folder\ 1,folder\ 2,folder\ 3,test\ folder}    
			
			VERY IMPORTANT: You can easily use 'zsh' for auto-completing names and commands using TAB Key on your keyboard.

	example 2: - Moving yay package cache data to a different folder, using 'awk' - 
		This will move yay's cache folder to 'backup folder' in the user home directory:
			mkdir ~/backup\ packages &&
			cd ~/.cache/yay/ &&
			ls | sudo awk '{system("mv \"${PWD}/"$0"\" \"${PWD}/../../backup packages/"$0"\"")}';


		example 2.2: - Moving data back to yay package cache folder - 
			cd ~ &&
			ls ~/backup\ dir | sudo awk '{system("mv \"${PWD}/backup dir/"$0"\" \"${PWD}/.cache/yay/"$0"\"")}'
	
	note: ${PWD} ensures the correct path to which data has to be moved from/to without having to type fullpath
	of the user home directory; this is because running 'awk' as sudo prevents the use of '~' character,
	since it would now point to /root/ instead of /home/.

	WHY USE 'MV' AND 'AWK' INSTEAD OF GRAPHICAL USER INTERFACE?
		Simple. Moving files through the GUI doesn't allows the user to rename the files if so desirable,
	through the scripting above it is possible to not only change filenames but also add preffix and suffix when moving the files.
--------------------------------------------------
--------------------------------------------------
PROPERLY USING 'RM' FOR REMOVING FILES
	'rm' is a double-edged sword, it can help in easily removing unwanted files from the system
as it could also remove unwanted data!!!

A safe way to use 'rm' is to use the environment variable $PWD and pressing the <tab> key on your keyboard
when on a terminal, which will display full-path of the file you want to remove.

Always becareful when using the '*'(wildcard character) specially when calling 'rm' as root user, example:
	sudo rm ./*
^ even when you intend both running 'rm' as root and using the '*' character, making a mistake here
will permanently delete important files, so always make sure you use $PWD environment variable.

When using tools like 'awk' or even simple bash scripts for batch removal,
always make sure that you're removing the right files by using 'echo' command, example:
	find ./ -iname *.zip -iname *.7z | awk '{system("echo rm "$0)}'

^ this will delete all .zip and .7z files when not using the 'echo' command.
if you still wanna make sure 'rm' isn't run by accident in this case,
you could still use double quotes:
	find ./ -iname *.zip -iname *.7z | awk '{system("echo \"rm "$0"\")}'

VERY IMPORTANT (2024 Update):
	1 - Using 'rm ./*' will remove all files, but never remove any hidden files.
	2 - For removing hidden files 'rm ./.*' has to be used, since hidden files start with '.' character.
	3 - Using 'rm ./*' and 'rm ./.*' will also fail to remove folders, since 'rm' requires the '-r' argument to delete folders.

	For completely removing all files and hidden folders, you can use this:
		sudo rm -r ./* ./.*
	
	VERY IMPORTANT:
		Always type 'echo $PWD' to make sure you want to delete something to make sure you're deleting the right data!


VERY USEFUL: You can also use 'rm' in the following manner to remove numbered folders:
	
	1 - Removing folders/files numbered from 1 to 10:
		$ rm ./test_file[1-10]

		^ this will remove all files that start with  'test_file' in the filename in which the following character 
		contain numbers from 1 to 10 followed again by any type and number of characters.
		^ For example: if a filename called 'test_file910B' exists, it will get deleted.
	
	2  - Removing files  recursively without using -r option:
		$ rm ./desired\ folder/**/*.h

		^ This will delete all files with '.h' extension from all directories and sub-directories
		from "desired folder".
	
	3 - Removing more than one folder at the same time
		This command will remove three different folders located at the user home directory in one go:
			$ rm ~/{folder\ data,folder\ documents,folder\ tmp/}

	Note: if you're using 'zsh', then you can always use <tab-key> for auto-completing the folder's name above.

--------------------------------------------------
--------------------------------------------------
PROPERLY UNCOMPRESSING/DECOMPRESSING SEVERAL FILES USING '7Z' AND 'AWK'

	DECOMPRESSING ALL COMPRESSED FILES IN A GIVEN DIR
			GOOD EXAMPLE -- USE THIS --:

			1.1) use 'echo' for ensuring commands will run the way you want:
				cd <directory_with_files_to_decompress>;
				$ ls --file-type *.zip | awk '{system("file=\""$0"\"; echo 7z x \"./${file}\" -o\"./decompressed/${file%%.*}\"")}'

			1.2) Actually execute command:
				$ cd <directory_with_files_to_decompress>;
				$ ls --file-type *.zip | awk '{system("file=\""$0"\"; 7z x \"./${file}\" -o\"./decompressed/${file%%.*}\"")}'


			BAD EXAMPLE: 
				I'm leaving this example just as reference, this will uncompress all zipped files
				into folders named with the .zip suffix which is why it's bad.
			
			1.1) use 'echo' for ensuring commands will run the way you want:
				$ cd <directory_with_files_to_decompress>;
				$ ls --file-type *.zip | awk '{system("echo 7z x \"${PWD}/"$0"\" -o./decompressed/\""$0"\"")}'

	
			1.2) Actually execute command:
				$ cd <directory_with_files_to_decompress>;
				$ ls --file-type *.zip | awk '{system("7z x \"${PWD}/"$0"\" -o./decompressed/\""$0"\"")}'

	BATCH RENAMING FILES AND DIRECTORY

		RENAMING ALL ZIP FILES
			BEWARE, This will REMOVE the .zip suffix from all .zip files!

			1.1) Use echo to ensure the command will execute the way you want,
			before executing it:
				$ ls --file-type *.zip | awk '{system("file="$0"; echo mv \"${file}\" \"${file%%.*}\"")}'


			1.2) Actually execute command, by removing echo:
				$ ls --file-type *.zip | awk '{system("file="$0"; mv \"${file}\" \"${file%%.*}\"")}'

		RENAMING ALL DIRECTORIES CONTAINNING .ZIP SUFFIX
			This will remove the .zip suffix from all .zip suffix-named directories:

				$ ls -d *.zip | awk '{system("file="$0"; mv \"${file}\" \"${file%%.*}\"")}'

		REMOVING ALL CHARACTERS PRECEDING THE UNDERLINE CHARACTER

		the example assumes ./Test folder/ directory has all the required files below.
			04602635_b0294.DAT20120807164534
			04602637_b0297.DAT20120807164713
			04602638_b0296.DAT20120807164637
			04602639_b0299.DAT20120807164819
			04602640_b0298.DAT20120807164748
			04602641_b0300.DAT20120807164849
			04602650_b0301.DAT20120807164921
			04602652_b0302.DAT20120807164956

		
		[code]
			#!/bin/bash
			file=$(ls ./test_folder/)
			for file in * ; do
			    echo mv -v "$file" "${file#*_}"
			done
		[/code]

More INFO on: 
	https://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06_02
	^ ctrl+f for "${parameter%word}"
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
BASIC SECURITY CONCERNS
	Besides installing and enabling firewalld(firewall service) and clamav(anti-virus) and running
firewall-config or gufw(firewall applications) and also enabling directory encryption with
fscrypt encryption(EXT4 Filesystem Only) or other forms of encryption for the systemk we have the following concerns:

	PACNEW FILES - (ARCHLINUX ONLY) - 
		Files with .pacnew extension may show up over the time on the /etc/ folder,
	this always happens when a configuration file is updated by the package manager and your system contains an
	old configuration file.

	In this case, it's important to either fully upgrade the file by replacing the old file OR to bring user modifications
	done in the old configuration files to the Newer File.

	Either way, the later can be easily achieved with the following command:
		vim -d /etc/<filename_a>.pacnew /etc/<filename_a>
					or
		nvim -d /etc/<filename_a>.pacnew /etc/<filename_a>
	
	A screen showing the difference between files is gonna show up,
	you can then either copy & paste the contents from the newer .pacnew file into the old configuration file,
	or copy the modified contents(if you've ever modified it with personal settings of your own taste) into the new file.

	Whichever you do, the new file has to replace the old file. 
	Then finally .pacnew configuration file can be manually removed.

	Accepting the .pacnew files into the system is important, because they may refer to new security settings 
	or options that can be passed into a part of the system, like the firewall for example.
	

	USER HISTORY
The bash history file is a concern since it exposes the program and commands a user may call on a daily basis.
including passwords, text data that may be saved and access to all private data as well like fscrypt directories and so on.

The same for the root user, all issued commands are saved on either ~/.bash_history(for bash) or ~/.histfile(for zsh).

~/.zshrc can be used to set how much history data gets saved(ZSH Solution(ZSH Solution) and ~/.bashrc (BASH Solution)
and the user may set it to zero if he wants. Do this for all users if you don't want to get your commands tracked.

	SUDO DATA
		Sudo is a long known tool capable of storing both access to ther users and commands.
	However, sudo is a double edged sword that both serves the purpose of security and the devil at the same time.

	If unwanted users get access to root, they can spy on every singe command a user does on the system,
	exposing all private data an user has had access to in the past few months or an year.

	Sudo imposes no limit on how much log data gets saved/stored, which will also impose a huge data size on the
	hard disk, again compromising your entire life in a linux system.

	However, if you have no interest, you can completely disable sudo logging from the system.
	on the section where it says "## Uncomment to enable logging of a command's output" on the file
	/etc/sudoers you can then disable logging!(as of a while ago it should come disabled by default).

		OBSERVATION: it might not seem like a problem to a new user, but imagine you send your computer
		to a maintenance store and they call in a linux expert to solve your computer problem.

		Now he boots your Hard-Drive and arch-chroots into your linux distro,
		then as root he goes into your bash/zsh history and also check all of your sudo logs.

		He can tell all the private data you use in the system,
		he could tell which fscrypt directories you unlock on the system,
		and also know the location of all private files and disks you use and mount on the system.
	
	Furtherover, disabling sudo isn't as bad as one might think, because unaothorized access through either 
	chroot or arch-chroot will allow the perpretator to disable sudo and bash/zsh 
	logging before doing anything on the system, to finally re-enable them after he is done spying on it.

	All of this can be easily achieved with scripts and done in less than 1sec,
	since it's possible to copy all of the log data to another hard-drive.

	Also, copying the log data or cloning the hard drive will never write 
	any log data on the victim's computer.


	In other words: disabling logging isn't as bad as it looks, it's only there to track what you do 
	in case you've forgotten. 



	BIND MOUNTING AS DATA OBFUSCATION ON THE SYSTEM
		(very important: Bind mount affects performance, so avoid it unless you really need it)

		Besides using fscrypt to encrypt data, it's also important to use obfuscation to hide the encrypted data
	from likely people who might be willing to break the encryption.

	Data is never truly hidden away if people can tell it exists in the system.

	While symlinks may hide the actual data from being exposed until it's decrypted with an actual password,
	they still expose the location of the real data on the disk to a unaware user.

	However, it's possible to hide the actual true location of the data by using 'bind mounts'.
	with 'bind mounts' it's possible to have a decoy data instead of the real data at the same directory where
	the data should be.

	EXAMPLE 1:
		Let's say you have an encrypted directory in your /home/user/my_favorite_directory
		one could easily say you have fscrypt security keys on your /.fscrypt directory 
		once they realize the direcotry is unreadable they can assume you have fscrypt by looking into that
		/.fscrypt folder; even worse that's also the default location for all fscrypt keys and signatures.

		Because of this, your data can be exposed by maintenance shop clerks and possible work "friends"
		given they know how to break through it.

		However, by copying the real /.fscrypt dir into other place like /usr/lib/ with:
			# cp -r /.fscrypt/* /usr/lib/.folderZXA

		You can hide the real location of fscrypt by now creating a fake decoy fscrypt directory:
			# mkdir /.fscrypt
			# mkdir /.fscrypt/policies/
			# mkdir /.fscrypt/protectors/

		Now unlocking fscrypt data through this fake data will be impossible by 3rd parties.

		To be able to unlock it, the actual owner of the data will then bind mount the real files:
		by doing:
			# mount --bind /usr/lib/.folderZXA /.fscrypt
			use 'lsblk' to list all bind mounts on the system.

		Since mount is using the given /.fscrypt dir, the fake data never gets in the way of 
		the real data user/owner.
	
	EXAMPLE 2:
		The same can be done with other directories you might think it's important on the system,
	including home directories for user if you so desire.
	

	USING LOOP DEVICES AS DATA OBFUSCATION
		With the use of Loop Devices it's possible to further enhance the obfuscation of /.fscrypt directories
	by hiding the given directory inside a loop device. That way the real fscrypt data can't be scanned
	on the hard disk by 3rd party experts who are aware of the data being hidden.

	HOWEVER, it is recommended to make backups of the loop device, since files in it could be easily corrupted.
	2nd, resizing loop devices is risky and may corrupt files, so only use loop devices for small files/data,
	avoid using it for larger data.

	USING DM-CRYPT TO ENCRYPT LOOP DEVICES
		It's possible to use dm-crypt to encrypt loop-devices as block-devices.
	However, due to the risk of losing data due to mount corruption of loop devices, 
	this is only recommended for small sized data.

	USING USB DEVICES TO BOOT THE SYSTEM
		It's possible to use a boot device to boot an operating system.
	This is common on fully encrypted block-devices, because without a bootable USB Device it would be 
	impossible to decrypt the contents of the hard disk and boot from a 3rd party operating system.

		VERY IMPORTANT: the same can be done for ~/.fscrypt directories ;)
		you can save them to block-encrypted USB Devices.
		again, always make a backup of fscrypt directories.

	SANDBOXING APPLICATIONS WITH FIREJAIL
		If you don't trust an application, like teamspeak, discord, weechat/irssi,
	You can run them sandboxed with 'firejail' tool.
	
		example: 
			$ firejail firefox
			^ once a program is run with firejail, it can't see a few of your user home directory?
			running firejail firefox hides contents, but firejail terminology doesn't. Weird.
			explanation: firefox has firejail profiles of it's own!
	
		Read more on "FIREJAIL INTRODUCTION" and "FIREJAIL COMMANDS"

	SYMLINKING/BIND MOUNT CACHE AND CONFIGURATION DATA TO FSCRYPTED FOLDER | LIST OF UNSAFE DIRECTORIES
		Some directories in your linux system may store data that one may want to keep hidden from intrusers.
	You may want to copy these dirs to an fscrypted directory and bind mount them upon decryption only:

		~/.histfile					#VERY IMPORTANT, this file contains all
								user issued commands.
								it's possible to track down secret files,
								loop-devices by reading this file.
								^ Accessing ~/.histfile after symlinking it
								with nvim, emacs or other text editor may
								REMOVE the symlink to a text file type,
								allowing bash/zsh to use it without obfuscation.
		~/.bash_history
		~/.zsh_history

			VERY IMPORTANT: 
		If you're going to symlink ~/.histfile to an fscrypt dir, it's required to create a new directory
		and changing the filepath in ~/.zshrc if you're using zsh. 
		finally, move the new dir to the fs-encrypted directory and symlink to it.

		~/Images					#Personal data
		~/Documents					#Personal data
		~/Downloads					#Personal data
		~/.cache/					#Several program data
		~/.local/share/					#Several program data
		~/.config/					#Not really mandatory, but some chat applications
								may contain server connection data lists here.
		~/.thumbnails/					#Thumbs! :D *
		~/.thunderbird/					#EMAIL Client
		~/.mozilla/					#Firefox Profiles 
		~/.irssi/					#IRC Client, same reason as ~/.config dir


	Just mentioning a few dirs that a regular user might want to hide, ~/.config may also contain these data:

			~/.local/share/Trash			#PCMANFM Thrash folder
			~/.config/xnviewmp			#xnviewmp thumbnails
			~/.cache/gnome-desktop-thumbnailer	#Cache for Gnome Desktop
			~/.cache/qBittorrent			#Bittorrent cache data
			~/.cache/vim				#VIM Swap cache data is stored here
			~/.cache/chromium			#Chromium web browser ***REMEMBER THIS
			~/.cache/thumbnails/			#Pcmanfm and pcmanfm-qt cache folder *
			~/.cache/gimp/				#GIMP cache *
			~/.cache/amarok/			#Amarok player cache data *
			~/.cache/clementine/			#Clementine player cache data *
			~/.cache/kdenlive/			#Kdenlive Cache data *
			~/.cache/mozilla/			#Mozilla Web-Browser *
			~/.mozilla/ 				#Firefox Profiles
			~/.cache/opera/				#Opera Web-Browser *
			~/.cache/qutebrowser/			#Qutebrowser web browser *
			~/.cache/shotwell/			#Shotwell Image Viewer *
			~/.cache/weechat/			#Weechat IRC Script *
			~/.cache/vivaldi/			#Vivaldi * 
			~/.cache/thunderbird/			#Thunderbird Mail Client * 

	
	FURTHER INSTRUCTIONS ON LINUX DATA OBFUSCATION
		FURTHEROVER, some directories like the ones owned by steam may hold games way too big to be copied over to 
	a newly fscrypt directory depending on the size of your Hard-Disk; 
	The reason for this is, fscrypted directories do not accept files to be moved,
	only by copying the data it's possible to keep data in a fscrypt dir.

	In this case, move the huge directories to a new directory with a unique name inside your home directory,
	then before copying the remainder smaller data to fscrypt directory, create a symlink to that unique directory 
	for the one program you want, like steam for example.

	Doing that, you can safely copy the entire ~/.cache or ~/.config and ~/.local/share to an fscrypted directory,
	once all of that is done, you can safely bind mount everything to their real location to where it once used to be
	under your home directory, thus providing same functionality.

	If you need help in locating those huge data inside your user home directory, there is a linux program
	called NCDU.

	At some point, create a script file for decrypting and bind mounting everything.
	Make sure it's also hidden and obfuscated in such a way only you can access it.

	WHY NOT ENCRYPT THE ENTIRE HOME DIRECTORY?
		Encrypted huge data tends to call attention, if someone can sniff you have something too big on any of your
		/etc/, /usr/share/, /usr/bin or /opt/, then your obfuscated data can be easily sniffed.

		Decryption isn't the problem, but finding the data to be decrypted is.
		If someone doesn't knows it exists, then it can never be decrypted.

		Therefore, use loop device for smaller unimportant data along with fscrypt directories,
		finally use a script to mount, decrypt and bind mounting everything in it's proper places.

		Never leaving a trace of where the script files are by disabling sudo, bash and zsh logging 
		and anything else that might log commands on the system

	ADVANCED OBFUSCATION
		As already mentioned, by using bind mounts one can have a fake user home directory,
		and one can also have a fake fscrypt directory containning keys to unlock the fake user-home directory.

		So if someone ever finds your lost laptop, steals your cellphone 
		and has enough knowledge on linux and EXT4 filesystem, once they break into the folder, 
		they'll think they're handling with the real important data.

		In fact, they're dealing with the unimportant fake data.
	
	PROTECTING THE FSCRYPT KEY ON A REMOVABLE DISK
		VERY IMPORTANT: the steps described in here is just a scratch plan(an idea),
	if you want actual steps on how to do things, considder reading the following
	topic: "ENCRYPTING DIRECTORIES WITH SECURITY KEY INSTEAD OF PASSWORD | 2024"

		It's possible to protect the fscrypt keys and signatures by removing it from local drive
	and saving it on an external drive.

	However, data can be compromised in case the device with the key is ever found.
	Question: How to protect the data in case of theft?

	Fscrypt can be used in conjunction with GPG Keys on the system.

	1st, move the /.fscrypt dir to a newly named directory
	remember, this new directory is the key for unlocking your current fscrypted directories.

	2nd, setup fscrypt again on the system, creating new protector and password as needed.
	^ new protector and password will only be created by fscrypt when encrypting a new directory.
	^ it'll look on /.fscrypt to see if it exists and ask the user if he wants to use the current protector on the system
	if it does exists. Both protector and passwords are stored on /.fscrypt directory.

	3rd, remember this new /.fscrypt directory CAN'T be used to unlock the current encrypted directories in the system.
	it'll only be used to lock/unlock the directory described on #1st step.

	4th, encrypt the old /.fscrypt directory that you renamed to a new dir on #1st step, 
	^ however, this has to be done in the following order:
		1 - Create new directory
		2 - Encrypt it with fscrypt
		3 - Copy & Paste the contents of the old renamed fscrypt directory
		4 - Very important: 
		Encrypting an already existing directory with files and data in it will perma-erase it's contents!
		make sure you're following steps #1 to #3 here!
	
	5th, finally backup your new /.fscrypt and save it somewhere you can remember on your hard-drive, pen-drive,
	etc.

	6th, Now you have 2 different /.fscrypt dirs: 
		1) the 1st one can unlock the current folders on the system
		2) the 2nd one has to be used to unlock the 1st fscrypt dir
		without this 2nd fscrypt dir, it'll not be possible to unlock the 1st fscrypt dir,
		therefore locking access from current system folders that have been encrypted.

	7th, Make sure to backup both of them somewhere and remember usernames and passwords.

	8th (Advice): Copy the 1st fscrypt folder to an USB Drive, 
	leaving the 2nd on the system since it can't unlock any other folder
	or vice-versa.

	9th, Whenever you need to unlock system folders: 
		1 - plug the pendrive onto your computer
		2 - mount it on /mnt/some_name
		3 - decrypt it with fscrypt
		4 - bind mount it to /.fscrypt

	10th, Now you can unlock system folders from removable USB Disk, once folders are unlocked 
	and USB Drive is umounted it can be safely removed from the computer.

	11th, With this, you finally have system dirs that can only be unlocked through removable media disk.

	12th, VERY IMPORTANT: Always have backups of 2 fscrypt dirs elsewhere, 
	or else you risk permanently losing your data. 

	13th, CONSIDERATIONS:
		It's possible to simplify the steps described in here,
	by using GPG Signing Keys to encrypt the /.fscrypt directory.

	HOWEVER, this will require removing the decrypted files after it's been used!
	so always decrypt them on /tmp/ and bind mount them on /.fscrypt.

Important links:
	https://firejail.wordpress.com/documentation-2/basic-usage/#basic-usage
	https://security.archlinux.org/				#Archlinux Security Track
	https://wiki.archlinux.org/title/Security		#Important linux security tips
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
USING A DIFFERENT MEMORY ALLOCATOR(MALLOC) FOR LAUNCHING PROGRAMS
	By default, linux uses a SLAB memory allocator that makes sure only minimum ammount of memory
is allocated when needed. However, allocating memory this way will make most programs run slower in a noticeable way.

However, the ammount of memory a system can have isn't a problem anymore.
If speed is desirable it's possible to use an alternate memory allocator for running programs:
		$ LD_PRELOAD="/usr/lib/libjemalloc.so" /usr/bin/firefox
			or
		$ LD_PRELOAD="/usr/lib/libmimalloc.so" /usr/bin/nvim

NOTE: ^ don't copy the '$' character!

more info:
	https://dustri.org/b/files/blackalps_2022.pdf		#Benchmark on different mallocs
								^ mimalloc is the fastest

	SCRIPT FOR RUNNING PROGRAMS WITH MIMALLOC
		Example for nvim:

		[file: /usr/bin/nvim-mimalloc]
			#!/bin/bash
			LD_PRELOAD="/usr/lib/libmimalloc.so" /usr/bin/nvim "$@"
		[/file]


		The "$@" ensures that user commandline arguments are passed through the bash script 
		and intO the program call.

		Make sure the script is marked as executable to 'other' users:
			#chmod o+x /usr/bin/nvim-mimalloc

		Also, some programs already use allocators of their own, so forcing a memory allocator
		with LD_PRELOAD will not work, examples: firefox uses jemalloc by default.

		VERY IMPORTANT: Beware, some websites may tell you to use this...:
			LD_PRELOAD=libmimalloc.so

		^ ...instead of LD_PRELOAD="/lib/libmimalloc.so", but the truth is libmimalloc might not be pre-loaded
		with some programs(like dosbox-x) using the above example and because of that always type the full-path 
		of the library to be pre-loaded.

	TROUBLESHOOTING PRELOAD
		32BIT APPLICATIONS DO NOT WORK WITH PRELOADED MIMALLOC
			In that case, you'll have to use an alternate malloc library that offers a 32bit version
		for running 32bit software:
			1) Download and install lib32-jemalloc
			2) Run 32 bit program:
				$ LD_PRELOAD="/usr/lib32/libjemalloc.so" wine
				^ this assumes you have both 32bit and 64bit wine, and 64bit wine is called 'wine64'

			3) if it works, then you have preloaded wine with jemalloc.


		FURTHEROVER, some programs may benefit from pre-loading from both libraries:
			$ ENABLE_VKBASALT=1 LD_PRELOAD="/usr/lib32/libjemalloc.so /usr/lib/libmimalloc.so" wine diablo1

		both wine and diablo1 are 32bit applications, however, some dependencies are 64bit like VKBASALT here.
		loading both libraries will benefit both parts: wine and vkbasalt.

		MAKING SURE APPLICATIONS ARE RUNNING WITH PRELOADED LIBRARY
			You should be able to see the library's name after preloading it into ldd,
		preloading it to 'ldd' will ensure that the program being called by it is preloaded with the same
		libraries:
			$ LD_PRELOAD="/usr/lib/libmimalloc.so" ldd -v /bin/nvim
				or
			$ env LD_PRELOAD="/usr/lib/libmimalloc.so" ldd -v /bin/nvim

		'ldd' however can only check elf(executable binary) files, so doing this to check if steam
		or firefox is running with preloaded libraries will not work.

		However, it's possible to append the '&'(Ampersand) character to see if it's loading:
			$ LD_PRELOAD="/usr/lib/libmimalloc.so" /bin/firefox&

		If the library failed as a preload, the user should get a message warning of such a thing on the terminal.

		Programs like firefox already use jemalloc, so preloading mimalloc(Microsoft Memory Allocator)
		may not work.

			ALTERNATE WAY TO CHECK APPLICATIONS ARE RUNNING WITH MIMALLOC
				This only works for mimalloc.

				It's possible to set an environment var. to check if libmimalloc is running,
			and running the desired program from a terminal:
				$ env MIMALLOC_VERBOSE=1 LD_PRELOAD=/usr/lib/libmimalloc.so nvim

			once the program is closed, it's possible to see libmimalloc's usage statistics.
			if you don't see any stats, that means mimalloc was never used.

			source: https://github.com/microsoft/mimalloc
		
		DO NOT WASTE TIME ON MISTAKES
		An unware user may think it's possible to load firefox with mimalloc by doing the following:
			1) openning a terminal emulator of your choice with the following command:
				$ LD_PRELOAD="/usr/lib/libmimalloc.so" terminology

			2) run firefox with & ampersand from this newly open terminal
				$ firefox&
				^ command is likely going to fail, but this time it'll give an error message

			3) Add misssing library:
				$ LD_PRELOAD="/usr/lib/libcurl.so" firefox&

		What happens here is that LD_PRELOAD is an environment variable; on this case, the user has just
		override the LD_PRELOAD="/usr/lib/libmimalloc.so" with a new different value, removing the libmimalloc.so.

		Thus, leading the user to think he has successfully preloaded.
		Doing the following will reveal the mistake:
			$ LD_PRELOAD="/usr/lib/libcurl.so ${LD_PRELOAD}" firefox&

	RUNNING STEAM GAMES WITH LD_PRELOAD
		These are just examples, don't use libraries if you don't know what they're doing:
			LD_PRELOAD="/usr/lib/libmimalloc.so" ENABLE_VKBASALT=1 %command%
							or
			LD_PRELOAD="/usr/lib/libmimalloc.so.2.1 ${LD_PRELOAD}" ENABLE_VKBASALT=1 %command%

		You can also try something different:
			LD_PRELOAD="/usr/lib/libpthread.so.0 /usr/lib/libGL.so.1 ${LD_PRELOAD}" __GL_THREADED_OPTIMIZATIONS=1 %command%
		Or even:

			LD_PRELOAD="/usr/lib/libmimalloc.so /usr/lib/libopenblas.so ${LD_PRELOAD}" ENABLE_VKBASALT=1 %command%

		Note: some games that consume too much RAM, like phasmophobia, may not benefit 
	from using non-standard mem. allocator(mimalloc). It causes way too many stutters when running out of RAM.
	The default SLAB memory allocator for linux is better in situations where the PC runs out of memory pretty fast.

		Note2: Not using ${LD_PRELOAD} will disable the steam overlay in-game.

		Very Important Note: 
			When using multiple environment vars, do not use ',' character, example:
				LD_PRELOAD="/usr/lib/libmimalloc.so ${LD_PRELOAD}" ENABLE_VKBASALT=1 %command%
				^ The above example is incorrect and libmimalloc.so will not be pre-loaded.

		RUNNING DISTROBOX WITH LD_PRELOAD
			Read more on 'RUNNING DISTROBOX WITH LD_PRELOAD'.

		USING OPENBLAS LIBRARY
			The BLAS (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. The Level 1 BLAS perform scalar, vector and vector-vector operations, the Level 2 BLAS perform matrix-vector operations, and the Level 3 BLAS perform matrix-matrix operations. Because the BLAS are efficient, portable, and widely available, they are commonly used in the development of high quality linear algebra software, LAPACK for example.

			The standard linux BLAS library is /lib/libblas.so,
			however OPEN Blas is an open-source performance BLAS library.
			^ source: https://www.netlib.org/blas/

			Setting the number of threads using environment variables
		you can use this environment vars when running programs(libopenblas not tested):
			export OPENBLAS_NUM_THREADS=4
			export GOTO_NUM_THREADS=4
			export OMP_NUM_THREADS=4

			If you compile this library with USE_OPENMP=1, 
			you should set the OMP_NUM_THREADS environment variable; 
			OpenBLAS ignores OPENBLAS_NUM_THREADS and GOTO_NUM_THREADS when compiled with USE_OPENMP=1.

			Source: https://github.com/OpenMathLib/OpenBLAS


			Note: Tested on Phasmophobia and it proved to be way much stable, I saw no stutters even when at the 
			truck in Brown High School.

	SECURING YOUR CODE/PROGRAM FROM LD_PRELOAD INJECTIONS
		Make sure not to allow your program to run upon LD_PRELOAD environment variable being set on the system.
	or at least, make sure it's run EMPTY.

	EXAMPLES OF KNOWN PROGRAMS THAT USE LD_PRELOAD
		
		a) eatmydata
			Allows programs to ignore data i/o on the disk, only use this if you know data
			isn't important!

		b) frida
			uses LD_PRELOAD to inject anything

		c) libnaw
			wrap and authenticates connect() and accept() calls

		d) faketime
			Fakes time and date to a given running program.

		e) fakeroot
			Gives the running program the impression that it is running as root, often used for example for building debian packages. This will let the build script create a directory tree which it believes is owned by root and then when that directory tree is packed by tar, tar will also see root as the owner and the .tar-archive will have root as the user/group for the files in the final debian package.

			Persona Note: if you really need debian packages, it's recommended to use distrobox.

		d) tsocks
			
		e) FlexLM
			Gives you a different MAC Address

		d) proxychains

--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
FASTER LOCATE, USE MLOCATE
	Use Faster Locate, like mlocate.
	https://wiki.archlinux.org/title/Locate

	1) Create Group 'mlocate':
		# groupadd mlocate

	2) Add user to mlocate:
		# usermod --append -G mlocate <username>

	3) Install 'mlocate':
		# pacman -S mlocate

	4) Start unit service:
		# systemctl start updatedb.timer

	
	Note: By default, mlocate excludes a few directories and devices from being read, including the devices
	mounted on /mnt, so if you wish performance in those dirs when searching for files/data, you can edit
	and remove them from the black/prune list: /etc/updatedb.conf

	To save time, updatedb can be (and by default is) configured to ignore certain filesystems and paths by editing 
	/etc/updatedb.conf; It is worth noting that among the paths ignored in the default configuration (PRUNEPATHS) 
	are /media and /mnt, so locate may not discover files on external devices. 
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
PAL VS NTSC GAMES
	Back in the old days, PAL games were written to be compatible with TVs found in Europe. These TVs updated at 50hz, or 50 times a second. In NTSC regions, TVs updated at 60hz. For the most part, a PAL game runs at 50fps or a number that can easily divide into 50, like 25, while NTSC games run at 60fps, or 30. PAL games can run almost 17% slower than NTSC games.

Concerning resolution, PAL signals were often higher resolution than an NTSC signal (576i vs 480i for example), but in my opinion, the higher frame rate is preferable.

Since the move to 720p and HD signals, all modern TVs operate on at least a 60hz signal. 
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
REVERTING DOWNGRADING NVIDIA DRIVERS ON ARCHLINUX || NVIDIA DRIVERS || TROUBLESHOOTING XORG
	Sometimes system upgrades may break video drivers from working,
to revert this situation, the following packages need to be reverted using the ALA Repository:

	As of  2024, these  are the full drivers:

	1) nvidia-utils: 
		https://archive.archlinux.org/packages/n/nvidia-utils/

	2) lib32-opencl-nvidia
		https://archive.archlinux.org/packages/l/lib32-opencl-nvidia/

	3) lib32-nvidia-utils: 
		https://archive.archlinux.org/packages/l/lib32-nvidia-utils/

	4) libxnvctrl: 
		https://archive.archlinux.org/packages/l/libxnvctrl/

	5) nvidia-dkms: 
		https://archive.archlinux.org/packages/n/nvidia-dkms/

	6) nvidia-settings: 
		https://archive.archlinux.org/packages/n/nvidia-settings/

	7) opencl-nvidia:
		https://archive.archlinux.org/packages/o/opencl-nvidia/
	
	8) libva-nvidia-driver
		https://archive.archlinux.org/packages/l/libva-nvidia-driver/

	9) egl-wayland
		( no  need to rever this )


VERY IMPORTANT: If you're troubleshooting drivers without being sure, it's important, if you're  also on xorg,
to install all related xorg packages beforehand, make sure  xorg is working properly:
	1)(OPTIONAL) Find out which xorg packages you have installed on your system
		$  pacman -sS xorg | grep -i -P "installed"

	2) INSTALL main xorg  packages:
		# pacman -S xorg xorg-drivers xorg-apps xorg-fonts
---
INSTALLING NVIDIA DRIVERS LOCALLY
To make sure these are the only needed drivers, search the database:
	A)(Optional) Type in:
		$ pacman -Sl nvidia | less

		A.2) Type in:
			/Installed

		note: Only packages using the following format needs to be downgrading, other packages can be ignored:
			example:
				550.54.14-2
		^ Packages have to be downgraded respecting the same version numbering
		^ the -2 number can be safely ingored.

	E) Upgrade packages as follows:
		$ cd <path_to_downloaded_packages>;
		# sudo pacman -U ./*.zst
			or
		$ cd ~/Downloads;
		# sudo pacman -U ./*.zst
		
	F) Finally, rebuild initramfs:
		# mkinitcpio -P
	
	G) Restart system!
		
ALA Repository Website: https://archive.archlinux.org/
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
LINUX FIREWALLING WITH IPTABLES | NFTABLES

	UPDATE INFO - VERY IMPORTANT - :
		source: 
			https://wiki.archlinux.org/title/Iptables
			https://en.wikipedia.org/wiki/Netfilter

		As of current date, 'iptables' is a legacy framework, 'nftables' aims to provide a modern replacement including 
		a compatibility layer for iptables.

		Note:  
			'nftables' allows the use of 'iptables-nft' package that works by providing implementations of the iptables 
			commands that actually create and act on the nftables rules. However, rules created using the old 
			iptables-legacy tools are separate objects, and iptables-nft will warn you if they are present.

			If you intend to use some of the things described here for iptables, it's advisable to install 'iptables-nft'.


BASIC CONCEPTS
	1) Is a port bound to a socket?
		A socket is one endpoint of a two-way communication link between two programs running on the network. A socket is bound to a port number so that the TCP layer can identify the application that data is destined to be sent to. An endpoint is a combination of an IP address and a port number.

OPTIONAL TOOLS FOR IPTABLES
	lsof,
	gufw, psad, ss, ip, ifconfig, netwatch, 
	watch, firetable, fwlogwatch, whois, host, wireshark.
	traceroute, networkmanager, netstat.
	
	very important: *don't use gufw if you're using iptables.
	very important2: read more about other tools on topic: "MONITORING TOOLS" and "EXAMPLES OF KNOWN PROGRAMS THAT USE LD_PRELOAD"
	very important3: beware, some tools like 'wireshark' do  not effectively detecting incoming/outcoming connections,
	it instead detects network traffic and it achieves this by sending broadcast packages and checking who's behind each accepting connection.

	0. lsof:
		$ lsof -i				#Lists all network connections along with 
							^ hostname, port and protocol
	0. ip:
		shows local ip address.
		example:
			$ip address
	1. ifconfig:
		same as ip, shows more details and allows disabling network interfaces.
		example:
			$ ifconfig -a			#Lists all network devices
			$ ifconfig lo down		#Disables 'lo' device
							^ lo is a loopback device.
							^ internet/local network will 
							^ not be accessed through this device.
							^ steam sets up a loopback device, don't disable it.

			$ ifconfig lo up			#Enables 'lo' device

	2. gufw:
		can be used a GUI Interface for iptables.

	3. psad:
		port scan attack detector that uses itpables.
	
	4. ss:
		helps monitoring new connections on the system and listing open doors and proccesses
		that use them.
		
		usage example: 
			# ss -s				##Displays summary statistics

			# ss -spatu			##Shows summary AND process names for TCP and UDP Connections
							^ shows UNCONNECTED, LISTENNING, etc.. and STABILISHED connections.
			# ss -spatun			##same as above, shows port numbers.
							ex.:
								# ss -spatun | grep -Pi "((listen)|(estab)).*firefox"

								^ shows all listenning and estabilished connections
								^ from firefox.

			# ss -sptu			##Same as above, but only shows estabilished connections.
			# ss -lptu			##Display only LISTENNING sockets for TCP and UDP
			# ss -lpntu			## same as above, shows port-number instead of human-readable values
			# ss -lspntu			##Shows summary(-s) and shows only listenning sockets TCP and UDP
			# ss -lsptu			##Shows only open sockets for LISTENNING with processes name
			
			# ss src 192.168.100.2		##Shows connections from a given source
			# ss dst 192.168.100.2		##Shows connection to a given destination
			# ss -t state listening		##Shows TCP sockets on the 'listenning' state
			# ss -tu state listening state established	##Shows TCP and UDP sockets on the 
									'listenning' & 'established' states
			# ss -tu state all		##Shows TCP and UDP sockets on ALL states
							^ same as 'ss -tua'
			# ss -tup state all		##Shows TCP and UDP sockets on ALL states with process names
							^ same as 'ss -ptua'

			-a		#Displays sockets in ALL available states
					^ this will override '-l' flag option.
					^ VERY IMPORTANT: doesn't includes none of the other types of connections
			-l		#Displays only sockets in LISTENNING & UNCONNECTED states
					^ VERY IMPORTANT: doesn't includes none of the other types of connections
			-s		#Prints summary statistics
					^ Displays summary of everythingn
					^ this summary doesn't care about other flags passed to 'ss'
			-n 		#Do not try to resolve service names. 
					^ Show exact bandwidth values, instead of human-readable.
					^ VERY USEFUL: This will show port numbers instead of service names.
			-p		#Shows processes name
			-t		#Shows TCP Sockets
			-u		#Shows UDP Sockets
			-d		#Shows DCCP Sockets
			-w		#Shows RAW Sockets
			-x		#Shows UNIX domain sockets
			-S		#Shows SCTP Sockets

	5. netwatch
		Netwatch examines all the packets travelling on an ethernet and analyses the IP packets. The information is tallied according to the source and destination hosts. An ncurses display indicates a dual-list status for all hosts. The left display refers to LOCAL hosts. The right list refers to REMOTE hosts. It is possible to examine statistics (counts) on number of packets, bytes, IP service type and last communication host for each host. Use the arrow keys (left and right) to change the display. 

		example:
			# netwatch -e eth0
				or
			# netwatch -e enp3s0

		side note: use 'ifconfig' to list your network devices, since netwatch requires one to be passed upon
		program call.


HOW DOES IPTABLES WORK | VERY IMPORTANT
	It's important to read "HOW DOES IPTABLES WORK || ADVANCED" Topic after reading this one!

	iptables and ip6tables need to be configured separately, that means when you see
	an instruction saying to use "iptables-restore", the same must be done for "ip6tables-restore".
	
	1) In order for ip6table and iptable use the same rule file the following must be done:
	rules that use IPV6 IP addresses need to use the --ipv6 suffix and ipv4 rules need to use --ipv4 suffix:
		# iptables --append INPUT --ipv4 --source 192.168.1.13 --jump DROP
		# iptables --append INPUT --ipv4 --destination 192.168.1.13 --jump DROP
			
	for IPV6 address range:
		# iptables -A INPUT --ipv6 -s 2606:50c0:8003::154/32 -j LOG_DROP
		# iptables -A INPUT --ipv6 -d 2606:50c0:8003::154/32 -j LOG_DROP
	
	keep in mind that IPV6 addresses in this case, they need to be loaded into ip6tables to take effect:
		1) # iptables-save | sudo tee /etc/iptables/your_filters.rules
		2) # ip6tables-restore < /etc/iptables/your_filters.rules
	
	the next time you restart your computer, you'll need to restore on both iptables and ip6tables the same way!

	If you check filter files around /etc/iptables/ you'll soon notice
	that rules are separated by a '\n'(newline) character.

	Filters set on the 1st line have higher priority than filters set on the 2nd line and so on.

	Because of this, if you have the following filter on the 1st line:
	ACCEPT     tcp  --  anywhere             anywhere            tcp dpt:http 

	Having the following filter on the 2nd line will have no effect:
	DROP       all  --  66.249.0.0/16        anywhere 


	it's important to keep the above in mind when making your filter files on /etc/iptables.
	sometimes when saving your first iptable filter file, it might be interesting checking to see if the file
	has any weaker filter on the first new lines, these weaker rules will override all rules set below them!

USEFUL COMMANDS || EXTENDED REGEX USING GREP
	1) Displays both estabilished and listenning connections for both firefox and opera
		# ss -spantu | grep -Ei "((listen)|(estab)).*((firefox)|(opera))"

		the '|' operator is an inclusive 'or'.

USEFUL LINKS
	https://portforward.com/ports/a/	#Contains PORT FORWARD number collection for several programs.
						^ Port Forwarding

LISTING ALL FILTERS CREATED FOR IPTABLES
	# iptables -L -v
		or
	# iptables --verbose --numeric --list --line-numbers

		and
	
	# ip6tables -L -v
		or
	# ip6tables --verbose --numeric --list --line-numbers

BLOCKING MULTIPLE PORTS
	# iptables -A INPUT -m multiport -p udp --sport 27015,27031:27036 -m comment --comment "PHASMO/GBVSR Port"
	# iptables -A INPUT -m multiport -p udp --dport 27015,27031:27036 -m comment --comment "PHASMO/GBVSR Port"

CREATING BASH SCRIPTS TO SIMPLIFY RULE CREATION
	This is a bash script and must be executed as sudo,
	it diverge/redirects all TCP/UDP/ICMP protocol INPUT connections on the ETH0 Network Device to LOGDROP
	for allowing individual logging description.

	It also uses environment variables to simplify settings on log limit and limit burst.

	VERY IMPORTANT: before using a script like the one below here, 
	it's necessary to empty all active current rules of the firewall using this:
		# iptables-restore < /etc/iptables/empty.rules
	
	it's important to backup any current rules you may have created before doing this, just in case you want to revert:
		# iptables-save | sudo tee /etc/iptables/my_backup.rules

	[file: /etc/iptables/script/my_script.sh]
		myIptables=("iptables" "ip6tables")
		LOGLIMIT="2/s"
		LOGLIMITBURST="10"

		for iptableStr in ${myIptables[@]}; do
			iptableStr"-restore" < /etc/iptables/empty.rules;
			iptableStr -N LOGDROP;
			iptableStr -A LOGDROP -p tcp -m limit --limit $LOGLIMIT --limit-burst $LOGLIMITBURST -j LOG --log-prefix "TCP LOGDROP: ";
			iptableStr -A LOGDROP -p udp -m limit --limit $LOGLIMIT --limit-burst $LOGLIMITBURST -j LOG --log-prefix "UDP LOGDROP: ";
			iptableStr -A LOGDROP -p icmp -m limit --limit $LOGLIMIT --limit-burst $LOGLIMITBURST -j LOG --log-prefix "ICMP LOGDROP: ";
			iptableStr -A LOGDROP -f -m limit --limit $LOGLIMIT --limit-burst $LOGLIMITBURST -j LOG --log-prefix "FRAGMENT LOGDROP: ";
			iptableStr -A LOGDROP -j DROP;

			iptableStr -A INPUT -p icmp -i eth0 -j LOGDROP;
			iptableStr -A INPUT -p tcp -i eth0 -j LOGDROP;
			iptableStr -A INPUT -p udp -i eth0 -j LOGDROP;
		done
  	[/file]	

	The script above is built so that the rules are applied to both iptables and ip6tables.

TESTING TO SEE IF A GIVEN PORT IS BLOCKED
	1) Open 2 terminals

	1.1) Type inside one terminal:
		# nc -l <port_number>
			or
		# nc -l 443

	1.2) Type inside another terminal:
		# nc <machine_ip_address> <port_number> 
			or
		# nc 192.168.1.8 443
		^ you can find your local ip address using 'ifconfig' tool

ADDING IP ADDRESSES TO IPTABLES FILTER: 
	# iptables --append <chain> --source <ip_address> --jump <rule> 
				or
	# iptables --append <chain> --destination <ip_address> --protocol <tcp | udp | icmp | ...> --dport <port> --jump <rule> 

	Examples:
		# iptables --append --ipv4 INPUT --source 192.168.1.13 --jump DROP
		# iptables --append --ipv4 INPUT --destination 192.168.1.13 --jump DROP
					or
		# iptables --append --ipv4 INPUT -s 192.168.1.13 --jump DROP
		# iptables --append --ipv4 INPUT -d 192.168.1.13 --jump DROP

	This blocks both incoming and outgoing packages for the given addresss.

IPTABLES LOGGING
	Iptables stores all logs as kernel messages that require to be accessed through either "dmesg" 
	or "journalctl -b-0"

	It's possible to create filters for an specific IP Address, Port, Chain or Rule:
	example:
		# iptables -A INPUT --ipv4 -s 192.168.11.0/24 -j LOG --log-prefix='[netfilter] '
			or
		# iptables -A INPUT --ipv6 -j LOG --log-prefix='[IPV6 ACCEPTING]'

	the command above will filter-log all INPUT packages coming from given IP Address and add
	a '[netfilter]' prefix to the message so that it can be easily read on DMESG and Journalctl.

	CONFIGURE SYSLOG
		as explained, iptables store it's logs into the kernel's message buffer,
	it's however possible to redirect it to a file by editing: /etc/syslog.conf
	or /etc/rsyslog.conf and appending the following content:

	[file: /etc/syslog.conf]
		kern.*    /var/log/iptables.log
	[/file]

		VERIFY SYSLOG LOG FILE
			#tail -f /var/log/iptables.log


	read more on the topic: "CREATING CHAINS TO FACILITATE RULE APPLICATIONS AND LOGGINGS"

FORMATTING IPTABLES LOG ON A HUMAN READABLE FORMAT
	This will format any log with the [IPV6 ACCEPTING] name in it.
# journalctl -b-0 | grep "IPV6" | sed -e 's/MAC=.*SRC/SRC/' -e 's/\[IPV6 ACCEPTING\]/\n\n\[IPV6 ACCEPTING\]\n/' -e 's/ /&\n/g' | less

DROPPING PACKETS USING IP ADDRESS RANGE, USE THE 'IPRANGE' MODULE
	# iptables -A INPUT --ipv4 -m iprange --src-range 192.168.1.100-192.168.1.200 -j DROP

DROPPING PACKAGES TO/FROM SPECIFIC PORTS
	# iptables -A <chain> -p <tcp | udp | icmp | ... > --dport <port> -j <rule>
			or
	# iptables -A INPUT --ipv4 -p tcp --dport 80 -j DROP

DELETING A RULE
	# iptables --delete <chain> <rule_line_number>

CREATING A NAT RULE TO TRANSLATE ALL TRAFFIC FROM THE `192.168.0.0/24` SUBNET TO THE HOST'S PUBLIC IP
    	example:
	    # iptables --table nat --append POSTROUTING --ipv4 --source 192.168.0.0/24 --jump MASQUERADE

DROPPING PACKAGES FROM UNWANTED SOURCES
	This is gonna drop all packages that aren't defined as an ACCEPT rule for iptables.
	# iptables -A INPUT -j DROP

BLOCKING ALL INCOMING AND ONGOING CONNECTIONS
	Make sure to use 'iptables-save' first before doing this in case you have any new rules pending to save.
	
	# iptables-restore < /etc/iptables/empty.rules
	# ip6tables-restore < /etc/iptables/empty.rules
	# iptables -A INPUT -j DROP
	# iptables -A OUTPUT -j DROP
	# iptables -A FORWARD -j DROP
	# ip6tables -A INPUT -j DROP
	# ip6tables -A OUTPUT -j DROP
	# ip6tables -A FORWARD -j DROP


	Rule file should look like this:
	[file: /etc/iptables/block_all]
		# Generated by iptables-save v1.8.10 (nf_tables) on Thu Mar  7 13:49:00 2024
		*filter
		:INPUT DROP [0:0]
		:FORWARD DROP [0:0]
		:OUTPUT DROP [0:0]
		# Completed on Thu Mar  7 13:49:00 2024

		#PIPEWIRE CONFIGURATION FOR ALLOWING MULTIPLE USERS TO HAVE ACCESS TO AUDIO AT THE SAME TIME
		-A INPUT --ipv4 -p tcp -s 127.0.0.1 --sport 4713 -j ACCEPT
		-A INPUT --ipv4 -p tcp -d 127.0.0.1 --dport 4713 -j ACCEPT
		-A OUTPUT --ipv4 -p tcp -s 127.0.0.1 --sport 4713 -j ACCEPT
		-A OUTPUT --ipv4 -p tcp -d 127.0.0.1 --dport 4713 -j ACCEPT

		-A INPUT -j DROP
		-A FORWARD -j DROP
		-A OUTPUT -j DROP

		COMMIT
	[/file]

SAVING AND PERSISTING RULES ON DISK FILE
	# iptables-save | sudo tee /etc/iptables/my_rules.v4

	Always make sure you have a backup file just in case:
		#cp /etc/iptables/my_rules.v4 /etc/iptables/my_rules.v4-backup
	
	You can also make sure rules haven't been changed on the system by backing it up elsewhere
	only you know and using 'diff' tool:
		#diff -rdq /etc/iptables/<rule_name_a> <path_to_backup_rule_name_a>
		
		^ it is recommended to use a loop-device for this.
		^ this is only for ensuring a hacker hasn't changed firewall permissions to make it easier
		for him to access the system.

RESTORING RULES FROM DISK FILE
	# iptables-restore < /etc/iptables/my_rules.v4
	^ VERY IMPORTANT: iptables-restore has to be used everytime you need to load rules into iptables.

CREATING CHAINS TO FACILITATE RULE APPLICATIONS AND LOGGINGS || VERY IMPORTANT
	The way chains work is embedded into the linnux kernel and as such changing chains priority isn't possible,
by typing "sudo iptables -L" you can see that INPUT, FORWARD and OUTPUT chains; that means to say 
rule filters set for the INPUT chain have higher priority, so if a rule filter allows and the OUTPUT one blocks
the same thing, the INPUT chain will allow the connection into the computer.

	HOW DOES IPTABLES WORK || ADVANCED
		When creating a CUSTOM chain, the same happens, since CUSTOM chains have the lesser of all priorities,
	it is however possible to DIVERGE connection from one given chain to another chain and therefore avoid this issue.

		Example:
			[file: /etc/iptables/custom_fail.rule]
				:INPUT ACCEPT [586845:3887889782]
				:FORWARD ACCEPT [0:0]
				:OUTPUT ACCEPT [550635:43563982]
				:LOG_DROP - [0:0]
				-A LOG_DROP -j LOG --log-prefix "[iptables LOG_DROP] "
				-A LOG_DROP -j DROP
				-A LOG_DROP --ipv4 -m iprange --src-range 192.168.0.0-192.168.253.253
				-A LOG_DROP --ipv4 -m iprange --dst-range 192.168.0.0-192.168.253.253
			[/file]

			The .rule file above will never allow address 192.168.X.Y to be blocked,
			simply becase INPUT chain IS ACCEPTING any connection on ports 586845 to 3887889782.

			Thus, the firewall won't be working as it should even tho the syntax is acceptable.
			to circumvent this do as follows:

			[file: /etc/iptables/custom.rule]
				:INPUT ACCEPT [586845:3887889782]
				:FORWARD ACCEPT [0:0]
				:OUTPUT ACCEPT [550635:43563982]
				:LOG_DROP - [0:0]
				-A LOG_DROP -j LOG --log-prefix "[iptables LOG_DROP] "
				-A LOG_DROP -j DROP
				-A INPUT --ipv4 -m iprange --src-range 192.168.0.0-192.168.253.253 -j LOG_DROP
				-A INPUT --ipv4 -m iprange --dst-range 192.168.0.0-192.168.253.253 -j LOG_DROP
			[/file]

			As you can see, each filter-line have a higher priority than the port-rule defined by the chain
			when the filter belongs to that same chain; as you can see the INPUT chain accepts ports between
			586845 to 3887889782, but the filters set for the INPUT chain will block connections coming from
			these ports on the 192.168.X.X addresses regardless of them being accepted by the chain.

			In this one example, the rule made for 192.168.X.Y now belongs to the INPUT chain 
			and is now DIVERGED from INPUT to LOG_DROP chain. This means that, once a connection
			falls into a chain, it can't go to another chain unless it's manually diverged.

			[OBS: when ping testing, make sure to block destination with either -d(when using CIDR notation) 
			or --dst-range(if using iprange module) so that the ping test below wont work,
			also keep in mind that servers like google.com use both IPV4 and IPV6,
			so blocking only IPV4 address might not suffice]

			Therefore the firewall is now active and working as it should,
			You can finally test it with ping command:
				#ping <blocked_ip_address>

			if the IP exists, 100% of the sent/received packets will be discarded.
			on the "custom_fail.rule" file above this will result in packets being sent and received
			which isn't what is desired from the firewall.

			So keep in mind when making CUSTOM and NON-CUSTOM chain rules.
		-----------------------------

	The following describes on how to create a firewall chain facilitating logging and rule applications:

	1. Create a new chain on iptables:
		# iptables -N <chain_name>
			or
		# iptables -N LOG_DROP

	3. Populate Rules on the newly created chain:
		# iptables -A LOG_DROP -j LOG --log-level 4 --log-prefix "[iptables:LOG_DROP] "
		# iptables -A LOG_DROP -j DROP
	
	4. Create IP Filters on the newly created chain:
		# iptables -A INPUT --ipv4 -m iprange --src-range 192.168.0.2-192.168.253.253 -j LOG_DROP
	
	Observation: all IP ADDRESSES inserted to the LOG_DROP chain will be automatically dropped and logged this way.
	Remember: iptable rule insertions have to be inserted in the specified order here,
	changing the order of the instructions described here will change the behavior of the firewall when 
	the filter is applied. for example: doing -j DROP instruction before -j LOG will prevent any logging from happening.
		always LOG before DROP. Edit the file and change the order if you need to /etc/iptable/.

	You may want to save settings at this point, read more on "SAVING AND PERSISTING RULES ON DISK FILE" 
	if you haven't done so yet.

	about --log-level, use "#dmesg --level 4" to read the firewall messages sent to the kernel message buffer.
	read more on "IPTABLES LOGGING"

HOW TO BLOCK BY LINK ADDRESS / HOW TO BLOCK BY MAC ADDRESS
	LINK and MAC Address are pretty much the same thing.
on iptables, it's possible to use the 'mac' module for blocking a MAC/LINK Address.

	# iptables -A FORWARD -m mac --mac-source cc:ed:21:87:02:50 -j LOG_DROP
	# iptables -A INPUT -m mac --mac-source cc:ed:21:87:02:50 -j LOG_DROP


HOW TO BLOCK IP BY WEBSITE / DOMAIN NAME
		The instructions in here require you to create new chain and steps described on 
	the topic "CREATING CHAINS TO FACILITATE RULE APPLICATIONS AND LOGGINGS".

	1. Find out IP Address of website
		$ host -ta www.google.com
		^ it'll output an IP Address

	2. Find out who this ip belongs to:
		$ whois 142.250.219.164
		^ fetch the CIDR IP Address

	3. Add it to iptables:
		# iptables -A LOG_DROP --ipv4 -s 142.250.0.0/15
		# iptables -A LOG_DROP --ipv4 -d 142.250.0.0/15
			or
		# iptables -A LOG_DROP -m iprange --src-range 142.250.0.0-142.250.255.255
		# iptables -A LOG_DROP -m iprange --dst-range 142.250.0.0-142.250.255.255

REGULATING INTERNET ACCESS THROUGH UDP/TCP CONNECTIONS BY TIME
	All UPD and TCP packets will be dropped only between at 12:00AM and 13PM
		# iptables -A INPUT -p tcp -m time --timestart 12:00 --timestop 13:00 -j DROP 
		# iptables -A INPUT -p udp -m time --timestart 12:00 --timestop 13:00 -j DROP
	
	As a reminder, ip6tables also need to be set and configured.

LIMITING CONNECTIONS FROM A SINGLE HOST
	If a single host makes more than 20 new connection through HTTP/HTTPS using the TCP protocol, reject
the connection.

		# iptables –A INPUT –p tcp –syn -m multiport -–dport http,https –m connlimit -–connlimit-above 20 –j REJECT -–reject-with-tcp-reset

BLOCKING ALL IP ADDRESSES IN THE LOCAL NETWORK
	If you're using an untrusted local network, you can block everyone in it, adding the local broadcast address
to the exception. The example in here follows up the topic "CREATING CHAINS TO FACILITATE RULE APPLICATIONS AND LOGGINGS".

	As a reminder, ip6tables also need to be set and configured.

	1. Find out your local ip address using:
		$ ifconfig
	
	2.(Optional/Skippable) Add the broadcast address to exception for both source and destination :
		# iptable -A INPUT --ipv4 -s 192.168.1.254 -j ACCEPT
		^ this is only required when you have a tool like ping for ping testing.

	4. Add entire IP Range for local network:
		# iptable -A INPUT --ipv4 -m iprange --src-range 192.168.0.0-192.168.255.255 -j LOG_DROP

		Very important: - DO NOT USE THIS BELOW -
			#iptable -A INPUT -m iprange --dst-range 192.168.0.0-192.168.255.255 -j LOG_DROP
			^ DO NOT USE --dst-range OR ELSE you won't receive any packets coming outside the
			network to your machine. Using this will force you to use:
				# iptable -A INPUT -d 192.168.1.<your_machine_ip> -j ACCEPT
			which will create a weak rule that will override all other blocking rules!!!
	
	VERY IMPORTANT: if you edited your .rule files located under /etc/iptables/ 
	you'll have to use iptables-restore to load the new configurations:
		example:
			# iptables-restore < /etc/iptables/my_custom_filters.rule

SEEING HOW MANY TIMES A RULE HAS BEEN HIT
	# iptables -L -vn --line-numbers
		or
	# iptables -nvL --line-numbers

	SHOWING SUMMARY ONLY
		# iptables -nvL | grep -v "0     0"
		^ there is no tabulation character, those are 5 white-space characters.
	
		CONSTANT MONITORING
			uses 'watch' to monitor every 5 seconds:
			# watch --interval=5 'iptables -nvL | grep -v "0     0"
				or
			# watch --interval=5 'iptables -nvL' | grep -v "0     0"

ASSURING AND VERIFYING A GIVEN IPTABLE MODULE HAS BEEN LOADED 
	 1. Verify that the module exists:
		 # lsmod | grep iprange

IP RANGE BLOCK USING CIDR IP ADDRESS NOTATION
	It's possible to use CIDR to block an entire IP Address range.

	# iptables -A OUTPUT --ipv4 –d 31.13.64.0/24 -j DROP
	^ the above rule will block 31.14.64.0 to 31.14.64.255


CUSTOM IPTABLES RULES
	In here i'll leave some custom .rules files as a reference for understanding iptables even further.
	index:
		{ALLOWING MINIMUM INTERNET WORK, MY OWN FILTERS, BLOCK_ALL} 

	ALLOWING MINIMUM INTERNET WORK
		The iptables rules need to allow the workstation to get an IP address, netmask, and other important information via DHCP (-p udp --dport 67:68 --sport 67:68). For remote management, the rules need to allow inbound SSH (--dport 22), outbound mail (--dport 25), DNS (--dport 53), outbound ping (-p icmp), Network Time Protocol (--dport 123 --sport 123), and outbound HTTP (--dport 80) and HTTPS (--dport 443).

		[file: /etc/iptables/minimum_work.rules]

			# Set a default policy of DROP
			*filter
			:INPUT DROP [0:0]
			:FORWARD DROP [0:0]
			:OUTPUT DROP [0:0]

			# Accept any related or established connections
			-I INPUT  1 -m state --state RELATED,ESTABLISHED -j ACCEPT
			-I OUTPUT 1 -m state --state RELATED,ESTABLISHED -j ACCEPT

			# Allow all traffic on the loopback interface
			-A INPUT -i lo -j ACCEPT
			-A OUTPUT -o lo -j ACCEPT

			# Allow outbound DHCP request
			-A OUTPUT –o eth0 -p udp --dport 67:68 --sport 67:68 -j ACCEPT

			# Allow inbound SSH
			-A INPUT -i eth0 -p tcp -m tcp --dport 22 -m state --state NEW  -j ACCEPT

			# Allow outbound email
			-A OUTPUT -i eth0 -p tcp -m tcp --dport 25 -m state --state NEW  -j ACCEPT

			# Outbound DNS lookups
			-A OUTPUT -o eth0 -p udp -m udp --dport 53 -j ACCEPT

			# Outbound PING requests
			-A OUTPUT –o eth0 -p icmp -j ACCEPT

			# Outbound Network Time Protocol (NTP) requests
			-A OUTPUT –o eth0 -p udp --dport 123 --sport 123 -j ACCEPT

			# Outbound HTTP
			-A OUTPUT -o eth0 -p tcp -m tcp --dport 80 -m state --state NEW -j ACCEPT
			-A OUTPUT -o eth0 -p tcp -m tcp --dport 443 -m state --state NEW -j ACCEPT

			COMMIT
		[/file]


	MY OWN FILTERS
		this .rule file logs all accepted connections,
	the log request must stay at the end of the file.

	It also uses a custom chain called LOG_DROP.
	The Local Network Broadcast Network Address is also blocked here, preventing external machines
	from finding out the computer. This however may break a few programs, like 'ping'.

	If you notice any further problems, you may allow the broadcast address.

	[file: /etc/iptables/my_iptable.rules]
		# Generated by iptables-save v1.8.10 (nf_tables) on Wed Feb 28 23:43:58 2024
		*filter
		:INPUT ACCEPT [586845:3887889782]
		:FORWARD DROP [0:0]
		:OUTPUT ACCEPT [550635:43563982]
		:LOG_ACCEPT - [0:0]
		:LOG_DROP - [0:0]
		:LOG_WATCH_DROP - [0:0]
		#-A LOG_ACCEPT -m limit --limit 2/min -j LOG --log-prefix "[iptables LOG_ACCEPT] "
		-A LOG_ACCEPT -j ACCEPT

		#LOGGING ON LOG_DROP CONNECTION CHAINS - UNCOMMENT THIS IF YOU NEED TO SEE WHICH IPs ARE BEING BLOCKED:
		#-A LOG_DROP  -m limit --limit 2/min -j LOG --log-prefix "[iptables LOG_DROP] "
		-A LOG_DROP -j DROP

		#LOG_WATCH_DROP IS MEANT TO ALLOW SEEING SPECIFIC IP ADDRESSES BEING BLOCKED:
		-A LOG_WATCH_DROP -m limit --limit 2/min -j LOG --log-prefix "[iptables LOG_WATCH_DROP] "
		-A LOG_WATCH_DROP -j LOG_DROP

		#PIPEWIRE CONFIGURATION FOR ALLOWING MULTIPLE USERS TO HAVE ACCESS TO AUDIO AT THE SAME TIME
		-A INPUT --ipv4 -p tcp -s 127.0.0.1 --sport 4713 -j ACCEPT
		-A INPUT --ipv4 -p tcp -d 127.0.0.1 --dport 4713 -j ACCEPT
		#-A INPUT --ipv6 -p tcp -s ::ffff:7f00:0001 --sport 4713 -j ACCEPT
		#-A INPUT --ipv6 -p tcp -d ::ffff:7f00:0001 --dport 4713 -j ACCEPT

		#SOME NEW IP ADDRESSES | Can't upload steam cloud without these
		#-A INPUT --ipv6 -s 2800:3f0::/32 -j LOG_DROP
		#-A INPUT --ipv6 -d 2800:3f0::/32 -j LOG_DROP

		#Phoronix website gets affected by these:
		#-A INPUT --ipv4 -s 142.250.0.0/15 -j LOG_DROP
		#-A INPUT --ipv4 -d 142.250.0.0/15 -j LOG_DROP
		##---

		-A INPUT --ipv4 -s 192.16.0.0/18 -j LOG_DROP
		-A INPUT --ipv4 -d 192.16.0.0/18 -j LOG_DROP

		##MY ISP KEEPS PINGING ME THROUGH THIS ADDRESS:
		-A INPUT --ipv6 -s 2804:0d49:44db:9300:0000:0000:0000:0001 -j LOG_DROP
		-A INPUT --ipv6 -d 2804:0d49:44db:9300:0000:0000:0000:0001 -j LOG_DROP

		## NokiaShangha_87??, JUST DROPPING IT - FIND OUT LATER:
		-A FORWARD -m mac --mac-source cc:ed:21:87:02:50 -j LOG_DROP
		-A INPUT -m mac --mac-source cc:ed:21:87:02:50 -j LOG_DROP
		#-A INPUT -m mac --ipv4 --mac-destination cc:ed:21:87:02:50 -j DROP
		#-A INPUT --ipv6 -s fe80::1 -j LOG_DROP
		#-A INPUT --ipv6 -d fe80::1 -j LOG_DROP

		##This has also been used as destination address after 1h on full-block.
		-A INPUT --ipv6 -s 2804:0d49:44db:9300:d472:070f:2346:6d9b -j LOG_DROP
		-A INPUT --ipv6 -d 2804:0d49:44db:9300:d472:070f:2346:6d9b -j LOG_DROP

		##Google Argentina | Same as 2800:3f0::/32, kept pinging after 1h on full-block, 
		##it was trying TCP Connection on port 80
		-A INPUT --ipv6 -s 2800:03f0:4001:0824:0000:0000:0000:2003 -j LOG_DROP
		-A INPUT --ipv6 -d 2800:03f0:4001:0824:0000:0000:0000:2003 -j LOG_DROP
		#
		##No idea what this is:
		-A INPUT --ipv6 -d ff02:0000:0000:0000:0000:0001:ff46:6d9b -j LOG_DROP
		#-

		#---Telemar/OI | don't disable:
		#-A INPUT --ipv6 -s 2804:d40::/28 -j DROP
		#W-A INPUT --ipv6 -d 2804:d40::/28 -j DROP
		#----


		# Accepts broadcast address on local network: 
		# Blocking this, also blocks pacman, yay from updating the system, also prevents 'ping' from working.
		# some online games also have problem when blocking this: gbvsr
		-A INPUT --ipv4 -s 192.168.1.254 -j ACCEPT

		# Blocks all hosts in the local network: 
		-A INPUT --ipv4 -m iprange --src-range 192.168.0.0-192.168.255.255 -j LOG_DROP

		##---PORTS CONNECTIONS:
		#Accepts Incoming HTTP connections
		-A INPUT -m tcp -p tcp --sport 443 -j LOG_ACCEPT

		#Accepts Phasmos connections:
		#Alternate Ports: 27000, 27001, 27002
		-A INPUT -m multiport -p tcp --sport 5055,5056,5058 -j LOG_ACCEPT
		-A INPUT -m multiport -p tcp --dport 5055,5056,5058 -j LOG_ACCEPT
		-A INPUT --ipv4 -s 34.64.0.0/10 -j LOG_DROP
		-A INPUT --ipv4 -d 34.64.0.0/10 -j LOG_DROP
		#--

		#VRCHAT
		-A INPUT -m multiport -p tcp --sport 5353,27015:27030,27036:27037 -j LOG_DROP
		-A INPUT -m multiport -p tcp --dport 5353,27015:27030,27036:27037 -j LOG_DROP
		-A INPUT -m multiport -p udp --sport 5353,4380,5055:5056,5058,27000:27100 -j LOG_ACCEPT
		-A INPUT -m multiport -p udp --dport 5353,4380,5055:5056,5058,27000:27100 -j LOG_ACCEPT
		#--

		#-Accepts GBVSR / USING 'ss' DOESN'T WORK | use 'LSOF' | changed: Does this work now?
		#Note: Usually source port should suffice for incoming connections
		-A INPUT -m multiport -p udp --sport 39113 -j LOG_ACCEPT
		-A INPUT -m multiport -p udp --dport 39113 -j LOG_ACCEPT
		-A INPUT -m multiport -p udp --sport 47895 -j LOG_ACCEPT
		-A INPUT -m multiport -p udp --dport 47895 -j LOG_ACCEPT
		-A INPUT -m multiport -p tcp --sport 57343 -j LOG_ACCEPT
		-A INPUT -m multiport -p tcp --dport 57343 -j LOG_ACCEPT
		-A INPUT -m multiport -p tcp --sport 4713 -j LOG_ACCEPT
		-A INPUT -m multiport -p tcp --dport 4713 -j LOG_ACCEPT
		#--

		#Accepts GBVSR, according to port-forward.com - Does this work?
		-A INPUT -m multiport -p tcp --sport 27015,27036 -j LOG_ACCEPT
		-A INPUT -m multiport -p tcp --dport 27015,27036 -j LOG_ACCEPT
		-A INPUT -m multiport -p udp --sport 27015,27031:27036 -j LOG_ACCEPT
		-A INPUT -m multiport -p udp --dport 27015,27031:27036 -j LOG_ACCEPT
		##--

		#--GITHUB Address called from Upscayl application:
		-A INPUT --ipv6 -s 2606:50c0:8003::154/32 -j LOG_DROP
		-A INPUT --ipv6 -d 2606:50c0:8003::154/32 -j LOG_DROP
		##--

		# This address is used for DHCP for IP Address attribution when no IP Address has been attributed yet:
		#-A INPUT --ipv4 -s 0.0.0.0 -j LOG_DROP
		#-A INPUT --ipv4 -d 0.0.0.0 -j LOG_DROP
		##source: https://en.wikipedia.org/wiki/0.0.0.0


		##------TWITCH.TV IP ADDRESSES, DON'T BLOCK THEM UNLESS YOU INTEND TO:
		##TWITCH WEBSITE ADDRESS:
		#-A INPUT --ipv4 -m iprange --src-range 151.101.0.0-151.101.255.255 -j LOG_DROP
		##STREAMING IP ADDRESS:
		#-A INPUT --ipv4 -m iprange --src-range 52.223.0.0-52.223.255.255 -j LOG_DROP
		#-A INPUT --ipv4 -m iprange --src-range 99.181.0.0-99.181.255.255 -j LOG_DROP

		##--Bans Fastly, Inc. (SKYCA-3)
		#---based on this IP: 151.101.178.167
		#-A INPUT --ipv4 -s 151.101.178.167 -j LOG_DROP
		#-A INPUT --ipv4 -d 151.101.178.167 -j LOG_DROP

		#---END OF THE VERY LIKELY TWITCH ADDRESSES

		##-----STEAM IP ADDRESS, DON'T BLOCK:
		#-A INPUT --ipv4 -m iprange --src-range 104.80.0.0-104.80.255.255 -j LOG_DROP ##STEAM USER PROFILE
		#-A INPUT --ipv4 -m iprange --src-range 155.133.0.0-155.133.255.255 -j LOG_DROP ##STEAM APP
		#-A INPUT --ipv4 -m iprange --src-range 23.37.0.0-23.37.255.255 -j LOG_DROP ##STEAM STORE
		#-A INPUT --ipv4 -m iprange --src-range 2.21.0.0-2.21.255.255 -j LOG_DROP ##AKAMAI TECHNOLOGIES
		#-A INPUT --ipv4 -m iprange --dst-range 2.21.0.0-2.21.255.255 -j LOG_DROP ##AKAMAI TECHNOLOGIES
		#---END OF THE THE STEAM ADDRESSES

		##----AUR(ARCHLINUX USER REPOSITORY) IP ADDRESS:
		##Some packages upgrades on AUR may require one of the IP to be unblocked:
		##yay-bin / Microsoft IP, Blocking it will make yay package manager stop working:
		##Also, checking for package updates doesn't work on yay if these are blocked.
		##This is GITHUB IP ADDRESS RANGE, so don't block:
		-A INPUT --ipv4 -m iprange --src-range 20.192.0.0-20.255.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 20.192.0.0-20.255.255.255 -j LOG_DROP

		#thunderbird-bin / Google IP:
		-A INPUT --ipv4 -m iprange --src-range 34.0.0.0-34.255.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 34.0.0.0-34.255.255.255 -j LOG_DROP
		#---END OF AUR IP ADDRESSES

		#GBVSR Servers/AMAZON:
		-A INPUT --ipv4 -m iprange --src-range 54.224.0.0-54.255.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 54.224.0.0-54.255.255.255 -j LOG_DROP
		-A INPUT --ipv4 -s 54.220.0.0/15 -j LOG_DROP
		-A INPUT --ipv4 -d 54.220.0.0/15 -j LOG_DROP
		-A INPUT --ipv4 -s 54.208.0.0/13 -j LOG_DROP
		-A INPUT --ipv4 -d 54.208.0.0/13 -j LOG_DROP
		-A INPUT --ipv4 -s 54.216.0.0/14 -j LOG_DROP
		-A INPUT --ipv4 -d 54.216.0.0/14 -j LOG_DROP
		-A INPUT --ipv4 -s 52.88.0.0/13 -j LOG_DROP
		-A INPUT --ipv4 -d 52.84.0.0/14 -j LOG_DROP
		-A INPUT --ipv4 -s 54.160.0.0/11 -j LOG_DROP
		-A INPUT --ipv4 -s 54.144.0.0/12 -j LOG_DROP
		-A INPUT --ipv4 -s 54.192.0.0/12 -j LOG_DROP
		-A INPUT --ipv4 -d 54.160.0.0/11 -j LOG_DROP
		-A INPUT --ipv4 -d 54.144.0.0/12 -j LOG_DROP
		-A INPUT --ipv4 -d 54.192.0.0/12 -j LOG_DROP
		-A INPUT --ipv4 -s 54.64.0.0/11 -j LOG_DROP
		-A INPUT --ipv4 -d 54.64.0.0/11 -j LOG_DROP
		##---END OF GBVSR Servers

		#Uknown SPAM Addresses:
		-A INPUT --ipv4 -m iprange --src-range 200.25.0.0-200.25.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 200.25.0.0-200.25.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 104.104.0.0-104.104.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 104.104.0.0-104.104.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 200.149.0.0-200.149.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 200.149.0.0-200.149.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 208.64.0.0-208.64.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 208.64.0.0-208.64.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 208.84.0.0-208.84.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 208.84.0.0-208.84.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 192.16.0.0-192.16.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 192.16.0.0-192.16.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 108.158.0.0-108.158.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 108.158.0.0-108.158.255.255 -j LOG_DROP

		##------------Banned on 05-mar-2024
		##Bans amazon IP Range:
		#based on this IP: 35.163.95.174
		#website: 
		-A INPUT --ipv4 -s 35.176.0.0/13 -j LOG_DROP
		-A INPUT --ipv4 -s 35.152.0.0/13 -j LOG_DROP
		-A INPUT --ipv4 -s 35.160.0.0/12 -j LOG_DROP
		-A INPUT --ipv4 -d 35.176.0.0/13 -j LOG_DROP
		-A INPUT --ipv4 -d 35.152.0.0/13 -j LOG_DROP
		-A INPUT --ipv4 -d 35.160.0.0/12 -j LOG_DROP
		##--------END OF BLOCK LIST 05-mar-2024



		##AMAZON ADDRESSES:
		-A INPUT --ipv4 -m iprange --src-range 44.241.0.0-44.241.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 44.241.0.0-44.241.255.255 -j LOG_DROP

		#UNITY GAME SERVICES IP is here?:
		-A INPUT --ipv4 -m iprange --src-range 35.82.0.0-35.82.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 35.82.0.0-35.82.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 52.46.0.0-52.46.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 52.46.0.0-52.46.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 23.160.0.0-23.160.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 23.160.0.0-23.160.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 239.255.0.0-239.255.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 239.255.0.0-239.255.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 224.0.0.0-224.0.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 224.0.0.0-224.0.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 18.173.0.0-18.173.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 18.173.0.0-18.173.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 208.84.0.0-208.84.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 208.84.0.0-208.84.255.255 -j LOG_DROP

		#Google Address, but nothing happens:
		-A INPUT --ipv4 -s 142.250.0.0/15 -j LOG_DROP
		-A INPUT --ipv4 -d 142.250.0.0/15 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --src-range 142.250.0.0-142.250.255.255 -j LOG_DROP
		-A INPUT --ipv4 -m iprange --dst-range 142.250.0.0-142.250.255.255 -j LOG_DROP

		#LOGGING ON ACCEPTED CONNECTIONS: - UNCOMMENT THIS FOR CHECKING ON INCOMING CONNECTIONS
		##Disabling this increases performance
		#-A INPUT -j LOG --ipv4 -m limit --limit 2/min --log-prefix "[IPV4 ACCEPTING]"
		#-A INPUT -j LOG --ipv6 -m limit --limit 2/min --log-prefix "[IPV6 ACCEPTING]"

		COMMIT
		# Completed on Wed Feb 28 23:43:58 2024

	[/file]

	BLOCK ALL CONNECTIONS
		Blocks all incoming and outgoing connections.

	[file: /etc/iptables/block_all.rules]
		# Generated by iptables-save v1.8.10 (nf_tables) on Thu Mar  7 13:49:00 2024
		*filter
		:INPUT DROP [0:0]
		:FORWARD DROP [0:0]
		:OUTPUT DROP [0:0]
		# Completed on Thu Mar  7 13:49:00 2024

		#PIPEWIRE CONFIGURATION FOR ALLOWING MULTIPLE USERS TO HAVE ACCESS TO AUDIO AT THE SAME TIME
		-A INPUT --ipv4 -p tcp -s 127.0.0.1 --sport 4713 -j ACCEPT
		-A INPUT --ipv4 -p tcp -d 127.0.0.1 --dport 4713 -j ACCEPT
		-A OUTPUT --ipv4 -p tcp -s 127.0.0.1 --sport 4713 -j ACCEPT
		-A OUTPUT --ipv4 -p tcp -d 127.0.0.1 --dport 4713 -j ACCEPT

		-A INPUT -j DROP
		-A FORWARD -j DROP
		-A OUTPUT -j DROP

		COMMIT

	[/file]

LISTING CONNECTED IP ADDRESSES AND CURRENT OPEN DOORS ON THE SYSTEM
	use 'ss' tool to look for IP Address to block.
		# ss -spantu
		    or
		# ss -lpntu

	note: incomplete, there are other network tools that allows doing 
	this that needs to be described here TOO AS WELL like 'netwatch':

		# netwatch -e eth0
			or
		# netwatch -e enp3s0
	
	Observation: if a given IP Address is still passing through the firewall,
	make sure you have the same rules blocking on both iptable (IPV4) and ip6table (IPV6).

	Server/services like google.com can still attempt connection through IPV6 when IPV4 fails
	due to firewalls.

ENABLING IPTABLES TO BOOT UP WITH THE SYSTEM
	# systemctl start iptables;
	# systemctl enable iptables;

	VERY IMPORTANT: use iptables-restore to load rules for iptables.

ENABLING LOOPBACK INTERFACE TO BE LISTENED FOR PACKAGES
	This is only important for virtual machines, distrobox, dosbox-x,
	and only if it's an actual problem: 

	# iptables -A INPUT -i lo -j ACCEPT

GOING INTO DETAILS
	Iptables relies on 3 important things: tables, chains and rules

Tables are usually pre-defined by iptables, each of the existing tables contains their own set of chains.
The rules defines what happens to the packet sent by a given IP Address on a specified port or non-specified
port.

As you had seen before on the topic "CREATING CHAINS TO FACILITATE RULE APPLICATIONS AND LOGGINGS",
it is possible to create custom chains to facilitate logging and blocking of IP Addresses.

Below a summary of tables and their respective chains:

	1. FILTER
		The Filter table is the most frequently used one. It acts as a bouncer, 
	deciding who gets in and out of your network. It has the following default chains:

		1.1 - INPUT 
		1.2 - OUTPUT 
		1.3 - FORWARD

	2. NAT
		This table contains NAT (Network Address Translation) rules for routing packets to networks 
	that cannot be accessed directly. When the destination or source of the packet has to be altered, 
	the NAT table is used. It includes the following chains:

		2.1 - OUTPUT
		2.2 - PREROUTING 
		2.3 - POSTROUTING

	3. MANGLE
		The Mangle table adjusts the IP header properties of packets. 
	The table has all the following chains from both NAT and FILTER tables.

		3.1 - INPUT
		3.2 - OUTPUT
		3.3 - FORWARD
		3.4 - PREROUTING
		3.5 - POSTROUTING 

	4. RAW
		The Raw table is used to exempt packets from connection tracking. 
	Not meant for INPUT(or receiving) packages.

		4.1 - OUTPUT
		4.2 - PREROUTING
		


Below a summary of each of the RULES that can be applied on IPTABLES:

	1. ACCEPT
		Accepts packages.

	2. DROP
		The dropped package is not matched against any further chain. 
	When Linux iptables drop an incoming connection to your server, 
	the machine trying to connect does not receive an error. 
	It appears as if they are trying to connect to a non-existing machine.

	3. RETURN
		This rule sends the packet back to the originating chain 
	so you can match it against other rules.
	
	4. REJECT
		The iptables firewall rejects a packet and sends an 
	error to the connecting device.

IMPORTANT: Some versions of Linux also use a Security table to manage special access rules. 
This table includes input, output, and forward chains, much like the filter table.

FIREWALLD VS IPTABLES
	Firewalld package isn't default to archlinux, but it is to another operating system, 
	it is therefore well maintained and supported there.
	however, on archlinux it hasn't been well-maintained in the past, so avoid using this.

	Firewall'd firewall-config GUI is too bloated and offers so many options it'll confuse even the most 
	advanced linux users. Troubleshooting firewalld isn't easy neither, since there's little 
	to no documentations on how to solve problems outside it's chosen default linux distro.

	In the other favorable hand, iptables can save rules to disks which can be used as a module for iptable;
	these modules can be used to ensure the firewall settings do not get changed by 3rd parties along it's lifetime 
	and also for allowing different rules to be applied at different times
	without universally modifying the settings. 

	The files can be saved to a different place on the disk, encrypted to read-only loop devices 
	to ensure no one can change them.

VERY IMPORTANT 0:
	iptables can't be used with firewalld, you need to complete disable it to prevent iptables from being turned off.

	# systemctl stop firewalld
	# systemctl disable firewalld
	# systemctl mask firewalld
	^ masking services simply create a symlink to /dev/null
	^ Output Message is: 
			Created symlink /etc/systemd/system/firewalld.service → /dev/null.
	
	If you ever need to revert this choice, use:
		# systemctl unmask firewalld;
		# systemctl enable firewalld;
	

VERY IMPORTANT 1:
iptables isn't compatible with firewalld and thus can't be used with firewall-config GUI, since that's a firewalld tool.
However, gufw works pretty well for iptables and can also save and export the same rules used by iptables.

If you set any rules while gufw is running on gufw itself, then make sure to SAVE those rules using iptables-save 
and reloading those rules manually after closing gufw.

	As a side note: 
		iptables-restore has to be used everytime you need to load iptables.

VERY IMPORTANT 2:
This is a very old info, on archlinux iptables is the same as iptables6, iptables6 doesn't exist on archlinux.
	"Important! iptables rules only apply to ipv4. If you want to set up a firewall for the ipv6 protocol, you will need to use ip6tables instead." - Quotation Source: the internet

VERY IMPORTANT 3:
Filtering packets based on their sources is crucial if you are using an intrusion detection and prevention system (IDS/IPS) like SURICATA. This tool monitors your VPS network and notifies you about malicious traffic.

IDS/IPS shows the malicious packets’ origins, which you can add to the iptables blocklist. 

VERY IMPORTANT 4:
	sshguard is a program that protects ssh servers that can automatically block IP Addresses using iptables
	in case someone tries to brute force into the ssh server.

VERY IMPORTANT 5:
	shorewall and shorewall6(WITH IPV6 Support) is an iptable-based firewall.
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
TODO LIST:

NOTE LIST:
Making packages will usually build all required tools that are compatible with the program that is going to be installed on the system.
This is good for testing if current system dependencies have an issue or not.

READ LATER:
#systemctl restart systemd-binfmt
--------------------
PERSONAL STUFF:
/etc/mkinitcpio.conf
MODULES=(vfio vfio_iommu_type1 vfio_pci vfio_virqfd virtio-gpu nvidia nvidia_modeset nvidia_drm nvidia_uvm wd719x v4l2loopback zram)

^ modules list from a not-so-old linux OS.
-------------------
-------------------
-------------------
-------------------
-------------------
-------------------
ADVANCED REGEX PART 1
Regex comes in 3 main flavors:
	1 - Extended Regexp (ERE)
	2 - Basic regexp (BRE)
	3 - Perl regexp (PCRE)

A LITTLE  HISTORY ON REGEX
source: https://thevaluable.dev/regular-expression-basics-vim-grep/ 	| date: 10-mar-2024

	There are many different regex engine (also called “regex flavors”) out there. As we saw above, these engines interpret the different metacharacters of a regex. That is, depending on the engine you’re using, you won’t have the same metacharacters available. The meaning of the different metacharacters can also be different from one engine to another. To make everything more complicated, regex engines can also have different versions, offering slightly take on their metacharacters.

But there are islands of hope in this lake of confusion: many regex flavors are based on the Perl regular expression engine. It’s not really a standard, but it’s considered as such by many. That’s why many resources out there will simplify everything by saying that the most used regex engines are all “Perl-style”, or PCRE, for Perl Compatible Regular Expression.

In fact, PCRE is simply a library written in C; many developers used it to create their own regex engine implementations in different tools or programming languages. That’s why it’s considered as one of the most common regex engine.

That said, all these “Perl-style” regex engines don’t necessarily have everything PCRE defines. In the case of Vim, its regex engine is not even based on PCRE, but it still has many metacharacters in common with PCRE. Yet, it also has some new, unique metacharacters, and weird quirks.

WHAT’S IMPORTANT TO REMEMBER: IT’S NOT BECAUSE A REGEX WORKS AS EXPECTED IN YOUR TOOL OF CHOICE THAT IT WILL WORK THE SAME WAY IN ANOTHER TOOL. LOOK AT THE SPECIFICS OF THE REGEX FLAVORS YOU’RE WORKING WITH, AND ADJUST YOUR REGEXES ACCORDINGLY.

In this article, we’ll mostly see the basics of regex in Vim. To show the differences with a “Perl-style” regex engine, all the solutions of the exercises are also using GNU Grep with its PCRE engine. It will give you the general knowledge for you to be able to work with most regex engines out there.

ADVANCED REGEX WITH GREP
	source: https://caspar.bgsu.edu/~courses/Stats/Labs/Handouts/grepadvanced.htm | date: 13-mar-2024

	GREP can be used as Extended(-E) or Perl(-P) regex commands. Both having different syntaxes and options.
	Perl's syntax require parenthesis to use '\' character as well as pipe '|' characters.
	Also in Perl, \w+ turns out into \\w+ for encasing whole words.
	'!' characters are used by terminal, so they also need to use '\' character before it.
	'<' also must be preceded by '\'
	
	MATCHING NULL CHARACTERS
		The grep engine used in previous versions of BBEdit was unable to search text that contained 
	null characters (ASCII value zero). This limitation is removed in BBEdit 6.5. Here's one way to match 
	a null using following regex pattern:
					\x{0}

	PERL-STYLE PATTERN EXTENSIONS
		(?:...)           Cluster-only parentheses, no capturing
		(?#...)           Comment, discard all text between the parens
		(?imsx-imsx)      Enable/disable pattern modifiers
		(?imsx-imsx:...)  Cluster-only parens with modifiers
		(?=...)           Positive lookahead assertion
		(?!...)           Negative lookahead assertion
		(?<=...)          Positive lookbehind assertion
		(?<!...)          Negative lookbehind assertion
		(?()...|...)      Match with if-then-else
		(?()...)          Match with if-then
		(?>...)           Match non-backtracking subpattern ("once-only")
		(?R)              Recursive pattern

		 i -- By default, BBEdit obeys the "Case Sensitive" checkbox in the Find dialog (or the corresponding property of the search options when using the scripting interface). The (?i) option overrides this setting.

		m -- By default, BBEdit's grep engine will match the ^ and $ metacharacters after and before returns, respectively. If you turn this option off with (?-m) , ^ will only match at the beginning of the document, and $ will only match at the end of the document. (If that's what you want, however, you should consider using the new \A, \Z, and \z metacharacters instead of ^ and $.)

		s -- By default, the magic dot metacharacter . matches any character except return (`\r'). If you turn this option on with (?s) , however, dot will match any character. Thus, the pattern (?s).+ will match an entire document.

		x -- When turned on, this option changes the meaning of most whitespace characters (notably, tabs and spaces) and #. Literal whitespace characters are ignored, and the # character starts a comment that extends until a literal return or the `\r' escape sequence is encountered. Ostensibly, this option intends to let you write more "readable" patterns.

		Perl programmers should already be familiar with these options, as they correspond directly to the -imsx options for Perl's m// and s/// operators. Unadorned, these options turn their corresponding behavior on; when preceded by a hyphen (-), they turn the behavior off. Setting and unsetting options can occur in the same set of parentheses. 


	LOOKAHEAD AND LOOKBEHIND ASSERTIONS

	lookahead assertions allow to filter-search for expressions by looking ahead of the expression 
	and applying it's filter, same for lookbehind assertions but it looks behind.

	lookahead assertions are defined by the following syntax:
		expression(?=filter)

	lookbehind assertions are defined by the following syntax:
		(?<=filter)expression

	Although the negation of these assertions exists, it simply works by negating the filter what it wants:

			negative lookahead: 
				expression(?\!filter)

			negative lookbehind:
				expression(?<\!filter)

			very important: '\!' is used instead '!' only because '!' is a reserved character 
			inside terminals.

		examples:
			echo "123 123bar bar123 bar" | grep -P "(?<=\d)bar"
			^ lookbehind assertion
			^ this will highlight the 'bar' word on '123bar'
			
			echo "123 123bar bar123 bar" | grep -P "(?<\!\d)bar"
			^ negative lookbehind assertion
			^ this will highlight all 'bar' words not preceded by a digit.

	SOME GREP EXAMPLES USING PCRE
		Leaving here some examples.

	$ echo "Hello Hello_ Hello pHello " | grep -P "(?<\!\w)Hello(?\!\w+)"
	^ uses negative lookbehindn and negative lookahead to ensure only the word Hello is returned
	^ highlights 1st 'Hello' and 3rd 'Hello' words

	$ echo "red king ate a dragon with the white queen inside the red queens room" | grep -E ((red|white) (king|queen))
	^ "red king", "white queen" and "red queen" will be selected/highlighted by grep.

	$ echo "sunday Sunday SuNdAy SaturDaY" | grep -P \(?i:sunday\)
	^ turns on case-insensitive

	$ echo "abc aB aBc abC ABc" | grep -P \(a\(?i\)b\|c\)
	^ turns on case-insensitive for 'b' and 'c', but not for 'a' character.
	^ because of that, AB is never selected/highlighted by grep.

	$ echo "abc aB aBc abC ABc" | grep -P \(a\(\(?i\)b\)\|c\)
	^ same as above, but turns on case-insensitive just for 'b' character.

	$ echo "foobar FOOBAR FOObar" | grep -P \(?i:foo\)\(?-i:bar\)
	^ turns off case-sensitive for 'bar' word.
	^ matches foobar and FOObar only.

	$ echo "text text2 text3 text2 text3" | grep -P \\w+\(?=3\)
	^ ?= is a positive lookahead
	^ fetches any word preceded by a '3', not including the '3' character.
	^ returns 2 results: text and text.
	^ ignores literals: text, text2 and text2

	$ echo "fooBAR FOObar foobar FoObAr" | grep -P \(\(?i\)foo\)\(?\!bar\)
	?! is a negative lookahead
	^ will match any 'foo' occurrence which isn't followed by 'bar'
	^ same as positive lookahead, but will not fetch the word/character when it's preceded by the lookahead search.


	$ echo "fooBAR FOObar foobar FoObAr" | grep -P \(?\!foo\)\(bar\)
	^ negative lookahead, fetches all occurrences of bar NOT PRECEDED by 'foo'
	^ returns FOObar and foobar, but only 'bar' is highlighted.

	$ echo "erqabc abcde abde erqabcdeq" | grep -P \(?\<=abc\)\\w{2}
	^ positive lookbehind
	^ looks for any 2 words that are preceded by 'abc' string
	^ does a search for the term abc and then fetches any 2 words followed by it.

	$ echo "123abcfoo 123abc999foo 123999foo" | grep -P \(?\<=\\d{3}...\)\(?\<\!999\)foo
			or
	$ echo "123abcfoo 123abc999foo 123999foo" | grep -P "(?<=\d{3}...)(?<\!999)foo"
	^ positive lookbehind + negative lookbehind
	^ the lookbehind assertions here look for 9 characters, in which 3 are digits, 
	  3 can be any characters and the final 3 ones can't be '999'
	^ only fetches 'foo' if it's preceeded by 3 digits + any other 3 characters, as long as the final 3 ones aren't
	999;
	^ returns 'foo' in 123abcfoo


	$ echo "foobarbaz barfoobaz baz buzbaz barbaz" | grep -P \(?\<=\(?\<\!foo\)bar\)baz
			or
	$ echo "foobarbaz barfoobaz baz buzbaz barbaz" | grep -P "(?<=(?<\!foo)bar)baz"
	^ nested lookbehind assertions
	^ looks for 'baz' word that is preceded by bar but is unpreceded by 'foo'
	^ highlights 'baz' on "barbaz".


	$ echo "123999foo 123567aaafoo 123456foo" | grep -P "(?<=\d{3}(?\!999)...)foo"
	^ looks for word 'foo' preceded by 6 digits and the final 3 digits isn't '999'
	^ highlights 'foo' on '12356aaafooo' and '123456foo'


	$ echo "123999foo 123567foo" | grep -P \(?\<=\\d{3}\(?\<\!999\)\)foo
			or
	$ echo "123999foo 123567foo" | grep -P "(?<=\d{3}(?<\!999))foo"
	^ does the same as above, but using negative lookbehind
	^ highlights 'foo' on '1234567foo'


	$ echo "2 is even | 3 is even | 3 is odd" | grep -P "\d+(?(?<=[13579]) is odd| is even)"
	^ CONDITIONAL SUBPATTERNS  
	^ becareful with blank-space on patterns, 
	^ ex: "is odd| is even" is different than "is odd | is even"
	^ syntax: 	if-then:       (?(condition)yes-pattern)
			if-then-else:  (?(condition)yes-pattern|no-pattern)
	
	^ will match '2 is even' and '3 is odd' and will ignore 3 is even.


	source: https://caspar.bgsu.edu/~courses/Stats/Labs/Handouts/grepadvanced.htm

	ONCE-ONLY SUBPATTERNS 
		once-only subpatterns allow for faster results, the below is a once-only subpattern,
	it uses (?>) pattern:

		$ echo "123456bar 1234567 12349121bar" | grep -P "(?>\d+)bar"
	
	the below is a slower version of the same regex that doesn't uses one-only subpattern finding:

		$ echo "123456bar 1234567 12349121bar" | grep -P "\d+bar"
	
	However, little to no difference can be seen due to it's simplicity.
	This example however will give drastically different results:
		$ echo "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" | grep -P "(\D+|<\d+>)*[\!?]"
		^ note that \! is used instead of ! because '!' is a special character on terminals

	The above regex pattern searches for non-digits and digits enclosed by '<' and '>' 
	which are followed by either '!' and '?'. However, it will perform very slowly and will
	likely output the following message: 
		"grep: (standard input): exceeded PCRE's backtracking limit"

	However, once-only subpartterns completely solve the issue:
		$ echo "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" | grep -P "((?>\D+)|<\d+>)*[\!?]"
	
	none of the patterns should return anything on this case.

		RECURSIVE PATTERNS
			not yet done.
		echo "(aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa()" | grep -P "(((?>[^()]+)|(?R))*)"

		this pattern has no results, it's only meant to show how slow it would be executed 
		without 'once-only' subppaterns


	GREP AND OTHER APPLICATIONS:
		In here, find is used to search for a given file and look for it's filetype using the 'file' tool,
		'sed' is used to ensure only fullpath is piped into 'awk' for use with 'grep'.
		Grep in turn parses through all text look for a specific string in the given files.

		--Using 'find' on text/binary files:
			Finding all 'password' entries in the text files on the current dir(recursive for all dirs):
		# find . -type f -exec file -N -i -- {} + | sed -n 's!: text/[^:]*$!!p' | awk '{system("grep --color -Hi \"password\" -- \""$0"\"")}'

			Finding all 'http' references in all text files in a given directory(recursive for all dirs):
		# find . -type f -exec file -N -i -- {} + | sed -n 's!: text/[^:]*$!!p' | awk '{system("grep --color -Hi \"http\" -- \""$0"\"")}'

			Finding all 'www' references in all application files in a given directory(recursive for all dirs):
		# find . -type f -exec file -N -i -- {} + | sed -n 's!: application/[^:]*$!!p' | awk '{system("grep --color -Hi \"www\" -- \""$0"\"")}'

----------------------------
----------------------------
----------------------------
ADVANCED REGEX PART 2
	WORD BOUNDARIES
		src: https://www.regular-expressions.info/wordboundaries.html
	
		$ echo "word wordx wordi word" | grep -P "\bword\b"
		^ fetches only the 1st and final 'word' occurrence.

	CHARACTER CLASS INTERSECTION
		src: https://www.regular-expressions.info/charclassintersect.html

		Example below isn't supported by grep:
		
		selects all characters a to z except vowel letters:
			[a-z&&[^aeiuo]]

	CHARACTER CLASS SUBTRACTION
		src: https://www.regular-expressions.info/charclasssubtract.html

		Matches letters except vowels:
			[a-z-[aeiuo]]

	ALTERNATION
		The subject here talks about pipes "|" used in regex. This has already been explained before.

	OPTIONAL ITEMS
		src: https://www.regular-expressions.info/optional.html

		The "?" is also called repetition operator or quantifier.
		It tells the engine to attempt to match the preceding token zero times or once.

		The "?" character allows to match both color and colour in the following example:

			$ echo "color and colour" | grep -P "colou?r"
			^ will highlight both color and colour

		The question mark is greedy, to turn off greediness add a second '?' character in front of the first.
		example:
			
			$ echo "This is 23 February" | grep -P "23(rd)??"
			^ without the 2nd '?', no value will be highlighted/returned

	REPETITION OPERATOR OR QUANTIFIER
		src: https://www.regular-expressions.info/repeat.html

		The '?' is a repetition/qantifier operator,
		The * and + is also another one.

		The '*' tells the engine to match the preceding token 0 or more times.
		The '+' tells the engine to match the preceding token 1 or more times.
		The '?' tells the engine to match the preceding token/pattrern 0 or 1 times.

		Example:
			$ echo "123456789" | grep -P "[0-9]*"
			^ will match number characters multiple times.

		REPETITION LIMITATION
			It's also possible to limit how much a token/pattern repeats.
		The syntax is {min,max}.

			example:
				$ echo "123456789" | grep -P "[0-9]{1,3}"
				^ example broken on grep, not working as it should.

		In other words, So {0,1} is the same as "?", {0,} is the same as "*", and {1,} is the same as "+".
	
	BACKREFERENCES
		src: https://www.regular-expressions.info/backref.html

		It allows to match the same text multiple times in a single regex search.

		example:
			$ echo "axaxax bxbxbx cxcxcx" | grep -P "([a-c])x\1x\1x"
			^ This will highlight the entire sentence
	
	BRANCH RESET GROUPS
		src: https://www.regular-expressions.info/branchreset.html

		Branch reset groups allow multiple tokens/patterns to be referenced as a single group.
		syntax: (?|regex)

		ex:
			echo "aa bb cc dd ee ff" | grep -P "(?|(a)|(b)|(c)|(d))\1"
			^ this will highlight "aa bb cc dd" only.
			^ if no backreference is made, then only "a b c d" will be highlighted.
	FREE SPACING MODE
		src: https://www.regular-expressions.info/freespacing.html
		white space character is ignored in regex patterns.
		syntax: (?x:)

		example:
			echo "car c a r" | grep -P "(?x: c a r)"
			^ highlights 'car' instead of 'c a r'

	MODE MODIFIERS
		src: https://www.regular-expressions.info/modifiers.html

		leaving just a few as example:
		(?i) enable case insensitive
		(?x) enable free spacing mode
		(?m) enable “multi-line mode” makes the caret and dollar match at the start and end of each line 
		in the subject string. In Ruby, (?m) makes the dot match all characters, without affecting the 
		caret and dollar which always match at the start and end of each line in Ruby. In Tcl, (?m) also 
		prevents the dot from matching line breaks.

	ATOMIC GROUPS
		src: https://www.regular-expressions.info/atomic.html

		Atomic groups allows the engine to immediatly give up a search upon failure,
		thus leading to improved results.

		example: 
			$ echo "integer insert in" | grep -P "\b(?>integer|insert|in)\b"
			^ \b allows searching for whole words
			^ highlights 'integer' 'insert' 'in'

		However, the use of ATOMIC GROUPS may lead a pattern search to fail:
			$ echo "integer insert in" | grep -P "\b(?>in|integer|insert)\b"
			^ it'll only highlight 'in' word

		The reason it fails, is because the engine gives up searching on the 1st word
		'integer', as soon as it gets on the 't' letter, the engine notices the 'in' word isn't a whole word
		after checking 't' is in place of \b.

		Removing \b from the regex pattern will result in the following:
			$ echo "integer insert in" | grep -P "(?>in|integer|insert)"
			^ all 'in' characters will be highlighted, but not 'integer' and nor 'insert'.

		Again the search fails because of the ATOMIC GROUP usage.

	RECURSION
		src: https://www.regular-expressions.info/recurse.html

		It works the same way as Recursive Functions does in Programming Languages.
		the (?R) makes the regex engine recurse through the entire regex expression from the beginning.
		On the example below, Once it reaches the final 'a' word, the recursive function returns to the
		caller function and 'b' will be found for 5 times
		syntax: (?R)
		example:
			$ echo "aaaaabbbbb" | grep -P "a(?R)?b"
			^ highlights all 'a' and 'b', "aaaaabbbbb"

			$ echo "aabbbbb" | grep -P "a(?R)?b"
			^ this time, only "aabb" is highlighted
			^ this happens because the recursive function is only called twice,
			^ so that means 'b' is only searched 2 times by the engine.
IMPORTANT LINKS:
	https://www.regular-expressions.info/index.html
	https://caspar.bgsu.edu/~courses/Stats/Labs/Handouts/grepadvanced.htm
	https://www.rexegg.com/regex-disambiguation.html#lookarounds
-------------------
-------------------
-------------------
-------------------
-------------------
-------------------
HOW TO USE LSOF
	DEFAULT COLUMNS
		The default columns in the lsof output are:

		    COMMAND - Refers to the command associated with the process that opened the file. 
		    PID - The process identification number of the process running the file.

		    TID - Represents a task identification number for the respective process. 
		    It is blank if a process, not a task, has opened the file.

		    TASKCMD - Refers to the command name in the first column. 
		    However, TASKCMD can differ when a task changes its command name.

		    USER - Names the user executing the process. The column contains the User ID or name.
		    FD - Is the file descriptor the process uses to associate with the file.
		    TYPE - Shows the type of file and its identification number.
		    DEVICE - Prints device numbers related to the file.
		    SIZE/OFF - Represents the value or the file taken during the runtime (size or offset).
		    NODE - The local file's node number or inode number of the directory/parent directory.
		    NAME - Shows the path or link to the file.

	SUMMARY:
			-a 				#Uses an 'AND' logical operator, by default it uses 'OR' in all 
			-b				#Suppresses kernel blocks.
			/ [file system] /		#Shows open files in a particular file system.
			/dev/tty*			#Displays files associated with the terminal.
			-u [username]			#Prints all files opened by a user.
			-u ^[username]			#Prints all files opened by everyone except a specific user.
			-c [process]			#Lists all files accessed by a particular process.
			-p [process ID]			#Shows all open files associated with a specific process ID.
			-p ^[process ID]		#Shows files opened by all other PIDs.
			-R				#Lists parent process IDs.
			+D [directory path]		#Prints all open files in a directory.
			-i				#Displays all files accessed by network connections.  
			-i [IP version number]		#Filters files based on their IP.
			-i [udp or tcp]			#Filters open files based on the connection type (TCP or UDP).
			-i :[port number]		#Finds processes running on a specific port.
			-i :[port range]		#Finds processes running on specific port ranges.
			-t [file name]			#Lists IDs of processes that have accessed a particular file.
			-d mem				#Shows all memory-mapped files.
							commands.

	EXAMPLES:
		# sudo lsof -p [PID] -R 			##Gets the PPID of a running process ID
								ex: sudo lsof -p 635 -R
								PPID - Parent Process Identification

		# kill -9 'lsof -t -u notsara'			##Kills all processes ran by 'sara'

		# lsof -p ^635					##Prints file opened by all processes except '635'
		# lsof -u ^saraz				##Prints processes opened by all users except saraz
		# lsof -b 					##Conceals Kernel Blocks
		# lsof +D [directory path]			##Shows files under a given directory
								ex: lsof +D /run/systemd/
		# lsof -i 4					##Lists IPV4 addresses
		# lsof -i 6					##Lists IPV6 addresses
		# lsof -t [filename]				##Lists PIDs of running processes holding the given filename
		# lsof -d mem					##Prints all memory mapped files
		# lsof [path] | grep deleted			##Display locked deleted files
		# lsof -u [username] -c [process]		##Lists files open by particular process OR user
		# lsof -u [username] -c [process] -a		##Lists files open by particular process AND user
		# lsof -i -u [username] -a			##Finds all network connections of a user
		# lsof -i -u [username] -a -r5			##Prints every 5 seconds all network connections of a user
		# lsof -i -a -p 606				##Lists files open by internet connection AND process id 606
		# lsof -i -a -c ssh				##Displaying files open by internet connection AND process 'ssh'
		# man lsof					##Opens the man page.

---------------
---------------
---------------
---------------
DIFFERENCE BETWEEN SYMLINK, HARDLINK AND REFLINK

---------------
---------------
---------------
---------------
RUNNING APPLICATIONS AS ANOTHER USER || MULTIPLE USERS
	An alternate method is to use 'gksudo'.

1) Disable Access Control through 'xhost':
	$ xhost +

2) Login to user from terminal:
	$ su - <username>
	^ requires knowing user password
		or
	$ sudo -iu <username>
	^ better option
	^ requires that the user calling 'sudo' is included in the /etc/sudoers file

3(Alternate Option) -  Login to user without knowing his password:
	3.1 - Login as root user:
		$ su - root
	3.2 - Login as the desired username from root:
		$ su - <username>

4 - Export Display:
	$ export DISPLAY=:0.0
	$ export XDG_RUNTIME_DIR=/run/user/1000
		or
	$ export DISPLAY=:0
	$ export XDG_RUNTIME_DIR=/run/user/1000


	note: some programs may require XDG_RUNTIME_DIR,
	if you have trouble with 1000 set as UID, you can find the user UID by typing 'id <username>'.

	If desirable, you can set these variables to be exported on either ~/.zshrc or ~/.bashrc
	for the user you want to login with.

5 - You can finally open any GUI program as another user.
	
	ALLOWING MULTIPLE LOGIN USERS TO USE SAME AUDIO DEVICE AT SAME TIME
		You need to read topic: 
			"SHARING AUDIO WITH MULTIPLE LOGGED USERS ON THE SYSTEM"

-------------------
-------------------
-------------------
-------------------
-------------------
-------------------
CUSTOMIZING ZPROFILE | ZSH PROFILE
	This is my customization for zsh, alsactl is here to restore general/global audio levels,
	easyeffects for applying audio equalization,
	tclock for outputting clock/date.

	[file: /etc/zsh/zprofile]
		alsactl --file /etc/alsa/asound.state restore
		(easyeffects --gapplication-service > /dev/null 2>&1 &)
		(disown %easyeffects > /dev/null 2>&1 &)
		echo -e "\n\033[1;34mWELCOME TO ARCHLINUX\n"
		calendar_output=$(cal -m); echo -e "\033[1;36m"$calendar_output
		echo "\n"
		tclock; echo "\n"
	[/file]

-------------------
-------------------
-------------------
-------------------
-------------------
-------------------
DISABLING HUGEPAGE FOR PERFORMANCE
If you don't have 32GB RAM, it's advisable to turn off this sh*t:

	[file: cat /etc/tmpfiles.d/local.conf]
		w /sys/kernel/mm/transparent_hugepage/enabled - - - - never
		w /sys/kernel/mm/transparent_hugepage/defrag - - - - never
	[/file]

	 Learn to set this to 0 on the local.conf file above:
		 /proc/sys/vm/compaction_proactiveness

Having huge pages will make your PC faster only if you can have enough free RAM Memory at all moments; it'll bring your system down once you start running out of it. This is a very bad option, specially if you play games.

By default, as of 2024.2, all linux kernels have this enabled, just disable this if you can't afford 32GB RAM.

You can make sure it's not running by checking:
	$ cat /sys/kernel/mm/transparent_hugepage/enabled
	$ cat /sys/kernel/mm/transparent_hugepage/defrag 


-------------------
-------------------
-------------------
-------------------
GETTING INFO ON MEMORY USAGE

	cat /proc/meminfo
-------------------
-------------------
-------------------
HOW TO FETCH FILE DESCRIPTORS
	
	1) Use 'pstree -p' to identify PID Number of a given process
		$ pstree -p
	
	2) With it's PID number in hand change directory to:
		$ cd /proc/<pid_number>/fd/
	
	3) On Linux, File descriptor /proc/PID/fd/0 is stdin, /proc/PID/fd/1 is stdout, and /proc/PID/fd/2 is stderr. 
	As a shortcut to these, any running process CAN ALSO ACCESS ITS OWN FILE DESCRIPTORS through the folders /PROC/SELF/FD and /DEV/FD.

-------------------
-------------------
-------------------
DISABLING OOM(OUT-OF-MEMORY) KILLER
	This is the command used for disabling OOM Killer on linux:
		# sysctl -w vm/overcommit_memory=0

		Information From reading 'man 5 proc':
			/proc/sys/vm/overcommit_memory
			       This file contains the kernel virtual memory accounting mode.
			       Values are:

				      0: heuristic overcommit (this is the default)
				      1: always overcommit, never check
				      2: always check, never overcommit

			       In mode 0, calls of mmap(2) with MAP_NORESERVE are not
			       checked, and the default check is very weak, leading to the
			       risk of getting a process "OOM-killed".

			       In mode 2 (available since Linux 2.6), the total virtual
			       address space that can be allocated (CommitLimit in /proc/mem‐
			       info) is calculated as

				   CommitLimit = (total_RAM - total_huge_TLB) *
						 overcommit_ratio / 100 + total_swap
-------------------
-------------------
-------------------
CLEANING SWAP MEMORY
	1 - Turn off swap for 30 secs:
		# swapoff -a
	2 - Turn swap back on:
		# swapon -a
-------------------
-------------------
-------------------
SETTING UP STRETCHED RESOLUTION FOR NVIDIA WITHOUT USING GAMESCOPE
	1 - Run nvidia settings
		$ nvidia-settings
	2 - Go to X Server Display Configuration Tab
	3 - Advanced Settings
	4 - Set 'ViewPortIn' and 'Panning' to the same and desired resolution:
		ex.: 1280x800
	5 - Set 'ViewPortOut' to desired stretched resolution:
		ex.: 1280x1024
	6 - These steps will  allow 1280x800 screen resolution to be stretched out as 1280x1024
	^ This allows many steam and non-steam games to run as full-screen without resolving to using Gamescope.
	^ Using gamescope to stretch out resolution may result in loss of image quality throughout the game.
	^ Gamescope can also incur performance issues.
	^ Some games might freeze/crash when used with gamescope, this happens due to anti-cheat protection some games might have.
	^ Some anti-cheat systems may also mistake gamescope as actual cheat.

	VERY IMPORTANT: Both resolutions must be supported by monitor and gpu.

-------------------
-------------------
-------------------
ADDING NEW SUPPORTED/UNSUPPORTED SCREEN RESOLUTION | NVIDIA ONLY
	INCOMPLETE: This will indeed add a new resolution mode, but i  wasn't able to  set it up from nvidia-settings panel.
	VERY IMPORTANT: BE SURE THE MONITOR AND GPU SUPPORTS THE RESOLUTION

	Note: For using the method described here, you'll have to disable 'EDID' feature on your /etc/X11/xorg.conf,
	add the following line to the DEVICE SECTION and restart your computer:
		
	0 -  Allow Non-EDID Modes for increasing the number of Available Screen Resolutions and Refresh Rate, and also for adding custom resolutions:
	add this:	Option         "ModeValidation" "AllowNonEdidModes"
		VERY IMPORTANT:  Follow  Step  #6 or Step #7 for correcting 'HorizSync' and 'VertRefresh' for extra refresh rates available:
		[file: /etc/X11/xorg.conf]
			Section "Monitor"

			    # Generated using https://tomverbeure.github.io/video_timings_calculator, Custom Modeline:
			    Identifier     "Monitor0"
			    VendorName     "Unknown"
			    ModelName      "DFP-1"
			    HorizSync       30.0 - 71.0
			    VertRefresh     56.0 - 75.0
			    #This is a custom resolution:
			    ModeLine       "1280x768_75n" 78.948 1280 1288 1320 1360 768 782 790 796 +hsync -vsync
			    #ModeLine       "1280x768_70n" 73.685 1280 1288 1320 1360 768 780 788 794 +hsync -vsync
			    Option         "DPMS"
			EndSection


			Section "Screen"

			    # Removed Option "metamodes" "1280x768_70n +0+0"
			    Identifier     "Screen0"
			    Device         "Device0"
			    Monitor        "Monitor0"
			    DefaultDepth    24
			    Option         "Coolbits" "28"
			    ##Don't use this option at all, prefer "ModeValidation" "AllowNonEdidModes":
			    #Option         "UseEDID" "FALSE"
			    #https://us.download.nvidia.com/XFree86/Linux-x86_64/384.69/README/xconfigoptions.html
			    ##Below is Very Dangerous use with care:
			    #Option "IgnoreEDIDChecksum" "true"
			    Option         "ModeValidation" "AllowNonEdidModes"
			    Option         "Stereo" "0"
			    Option         "nvidiaXineramaInfoOrder" "HDMI-0"
			    ##Leaving here for reference, but don't use them, they didn't work:
			    #Option         "metamodes" "1280x768_73 +0+0"
			    #Option         "metamodes" "VGA-0: nvidia-auto-select +0+0 {ForceCompositionPipeline=On}, HDMI-0: nvidia-auto-select +0+0 {ForceCompositionPipeline=On}"
			    #Option 	    "metamodes" "DFP-1: 1280x1024"
			    #Option 	    "metamodes" "DFP-1: nvidia-auto-select"
			    ##The below is optional, it's meant for an actual custom resolution on the 'Monitor Section'
			    Option         "metamodes" "DFP-1: 1280x768_75n +0+0"
			    Option         "SLI" "Off"
			    Option         "MultiGPU" "Off"
			    Option         "BaseMosaic" "off"
			    SubSection     "Display"
				Depth       24
			    EndSubSection
			EndSection
		[/file]

	1 - use 'cvt' for generating a mode line
		Very Important: CVT is quite old tool, you may  need to use a video timing calculator provided here:
			https://tomverbeure.github.io/video_timings_calculator

		$ cvt  1280 x 1024 60
		^ 'gvt' can also be used instead of 'cvt'
		ex.: Modeline "1280x1024_60.00"  109.00  1280 1368 1496 1712  1024 1027 1034 1063 -hsync +vsync

	2 - Add the modeline using 'xrandr':
		$ xrandr --newmode <Resolution_Name/Alias> <Specifications>
		Example:
			$ xrandr --newmode "1280x1024_60.00"  109.00  1280 1368 1496 1712  1024 1027 1034 1063 -hsync Interlaced
			^ Resolution Name/Alias can be set to anything, just becareful.
		
	3 - Verifying newly added resolution:
		$ xrandr
		^ This will output all available resolution
		^ This will output new Resolution ID in the hexadecimal format:
		Example - it's here for example, don't  actually use this value here:
			1024x800_60.00 (0x50c) 66.750MHz -HSync
			^ 0x50c is the ID in hexadecimal
	
	4 - Adding new resolution:
		$ xrandr --addmode <monitor_output> <resolution_alias/name>
		^ 2024 NOTE: "xrandr --addmode" doesn't work with nvidia drivers anymore - Needs to Check?
		^ use "nvidia-xconfig --mode"
		Example:
			$ xrandr --addmode HDMI-0 1024x800_60.00
				or
			$ xrandr --addmode HDMI-0 0x50c
			^ you can fetch resolution ID from Previous step #3.

	5 - Changing current resolution:
		$ xrandr --output HDMI-0 --mode 1024x768_79.92c

	6 - (Optional) Fetch Further EDID Information like 'HorizSync' and 'VertRefresh' for correcting 
	it on /etc/X11/xorg.conf:
		# di-edid-decode /sys/class/drm/card0-HDMI-A-1/edid

		VERY IMPORTANT: Correcting 'HorizSync' and 'VertRefresh' will increase number of available resolutions
		when  disabling EDID.  Check Step #0

	7 - (Optional) Fetch Further EDID Information like 'HorizSync' and 'VertRefresh' for correcting 
	it on /etc/X11/xorg.conf:
		# parse-edid < /sys/class/drm/card0-HDMI-A-1/edid

		VERY IMPORTANT: Correcting 'HorizSync' and 'VertRefresh' will increase number of available resolutions
		when  disabling EDID. Check Step #0

	8 - (Optional) Creating a Custom EDID (INCOMPLETE) - NVIDIA ONLY
		Add the following line to /etc/X11/xorg.conf under "Device" Section:
			Option       "CustomEDID" "DFP-0:/etc/X11/L2410NM-modified.edid"
			^ This section is incomplete, since it doesn't covers how to edit EDID files.
			^ This also requires either removing or setting to true the line mentioned on Step #0.

	8 - (Optional) Define a New Monitor:
		$ xrandr --setmonitor <name> <geometry> <outputs>
	
	9 - (Optional) Creating reduced blanking for higher resolution
		$ cvt 1280 1024 -r
		ex: Modeline "1280x1024R"   90.75  1280 1328 1360 1440  1024 1027 1034 1054 +hsync -vsync
		^ Reduced blanking can also reduce power consumption and EMI emissions.
	
	10 - (Optional) Adding new xconf mode?
		nvidia-xconfig --mode

RECOMMENDED READINGS:
	WHAT IS EDID (EXTENDED DISPLAY IDENTIFICATION DATA) AND DISPLAYID?

RECOMMENDED LINKS:
	https://download.nvidia.com/XFree86/Linux-x86_64/396.51/README/xconfigoptions.html
	https://tomverbeure.github.io/video_timings_calculator	- Allows creating your own custom resolution according to standards
	https://fpstest.org/refresh-rate-test/	- Tests Monitor Refresh Rate
-------------------
-------------------
-------------------
WHAT IS EDID (EXTENDED DISPLAY IDENTIFICATION DATA) AND DISPLAYID?
	Extended Display Identification Data (EDID) and Enhanced EDID (E-EDID) are metadata formats for display devices to describe their capabilities to a video source (e.g., graphics card or set-top box). The data format is defined by a standard published by the Video Electronics Standards Association (VESA).

	The EDID data structure includes manufacturer name and serial number, product type, phosphor or filter type (as chromaticity data), timings supported by the display, display size, luminance data and (for digital displays only) pixel mapping data.

	DISPLAYID is a VESA standard targeted to replace EDID and E-EDID extensions with a uniform format suited for both PC monitor and consumer electronics devices. 

	DisplayID is a VESA standard for metadata describing display device capabilities to the video source. It is designed to replace E-EDID standard and EDID structure v1.4.

	The DisplayID standard was initially released in December 2007. Version 1.1 was released in March 2009 and was followed by version 1.2 released in August 2011. Version 1.3 was released in June 2013[1] and current version 2.0 was released in September 2017. 

	^ source: https://en.wikipedia.org/wiki/Extended_display_identification_data

	WHERE IS THE EDID STORED?
		The EDID is often stored in the monitor in the firmware chip called serial EEPROM (electrically erasable programmable read-only memory)
	and is accessible via the I²C-bus at address 0x50. The EDID PROM can often be read by the host PC even if the display itself is turned off. 

	HOW TO READ EDID OR DISPLAYID?
		On linux you can use 'di-edid-decode' util that comes installed with 'libdisplay-info' package.
		Example:
			$ di-edid-decode /sys/class/drm/card0-HDMI-A-1/edid
			^ EDID/DISPLAYID will also contain information on Monitor Brand and Model.
			^ This is a way on 'how to fetch monitor model & monitor brand'
			^ One can also fix the 'HorizSync' and 'VertRefresh' values on /etc/X11/xorg.conf along with disabling
			^ EDID for access to more Resolutions and Refresh Rates. Read more on:
				ADDING NEW SUPPORTED/UNSUPPORTED SCREEN RESOLUTION | NVIDIA ONLY

	COORDINATED VIDEO TIMINGS (CVT)
		Coordinated Video Timings (CVT; VESA-2013-3 v1.2) is a standard by VESA which defines the timings of the component video signal. Initially intended for use by computer monitors and video cards, the standard made its way into consumer televisions.

		The parameters defined by standard include "horizontal blanking" and "vertical blanking intervals", "horizontal frequency" and "vertical frequency" (collectively, "pixel clock rate" or "video signal bandwidth"), and "horizontal/vertical sync polarity".

		The standard was adopted in 2002 and superseded the Generalized Timing Formula(GTF).

		source: https://en.wikipedia.org/wiki/Coordinated_Video_Timings

		VERY IMPORTANT: Both CVT and GTF can be called and used on linux under packages named "libxcvt" and "xorg-server" respectively.
		CVT being the newer standard.

	REDUCED BLANKING
		CVT timings include the necessary pauses in picture data (known as "blanking intervals") to allow CRT displays to reposition their electron beam at the end of each horizontal scan line, as well as the vertical repositioning necessary at the end of each frame. CVT also specifies a mode ("CVT-R") which significantly reduces these blanking intervals (to a period insufficient for CRT displays to work correctly) in the interests of saving video signal bandwidth when modern displays such as LCD monitors are being used, since such displays typically do not require these pauses in the picture data. This also allows for lower pixel clock rates and higher frame rates.

		In revision 1.2, released in 2013, a new "Reduced Blanking Timing Version 2" mode was added which further reduces the horizontal blanking interval from 160 to 80 pixels, increases pixel clock precision from ±0.25 MHz to ±0.001 MHz, and adds the option for a 1000/1001 modifier for ATSC/NTSC video-optimized timing modes (e.g. 59.94 Hz instead of 60.00 Hz or 23.976 Hz instead of 24.000).

		CEA-861-H introduced RBv3. RBv3 defines ways to specify different VBLANK and HBLANK duration formulae.

		CEA-861-I introduced "Optimized Video Timings" (OVT), a standard timing calculation that covers resolution/refresh rate combinations not supported by CVT.

		source: https://en.wikipedia.org/wiki/Coordinated_Video_Timings
-------------------
-------------------
-------------------
DIFFERENCE BETWEEN 'ENV' AND 'EXPORT'
	'ENV'
		'Env' executes the given command as a new process through replacement,
	it first starts itself as a process and then replaces itself with the one passed for the command.
		Example:
			$ env GLOVAL_ENV_VAR="NEW_VALUE" thunderbird

	Because of this, any current shell  environment variable called "GLOBAL_ENV_VAR" don't get affected by the new value.

	Using env is literally the same as executing the program with just the VAR=KEY name/values:
		"var=key <program>" is the same as "env var=key <program>"
		^ if no program is provided, then "var=key" will modify the current shell environment variable located under same name.
		^ 'env' in the other hand will not change current shell environment variables when no <program> is given.
		
		VERY IMPORTANT: If you do follow the below example, you'll notice that the "var=key" will  not be passed down/inherited
		to the executed process:
			$ MY_VAR="Test_111";
			$ xterm;
		^ when typing echo $MY_VAR there'll be no var set with 'Test_111' key as value.
	
	'EXPORT'
		Export is a stronger form of 'env' it will however modify the current shell environment variable to that of the export,
	In the other hand, any other program invoked through the current shell will inherit it's Environment Var and no need for re-typing
	the var will be required:
		
		Example:
			export USEFUL_GLOBAL_ENV_VAR="TWEAK_VALUE";
			nvim &;
			xterm &;

		^ In this case, both nvim and xterm will use the USEFUL_GLOBAL_ENV_VAR

-------------------
-------------------
-------------------
SUSPEND X HIBERNATE X HYBRID
	1 - Suspend writes the system state	to RAM, wasting a little bit more RAM. 
	2 - Hibernate writes system state do DISK, wasting the hard drive.

	SUSPENDING SYSTEM / SLEEP SYSTEM
		When placed in sleep mode, the computer cuts power to unneeded subsystems and places the RAM into a minimum power state, 
	just sufficient to retain its data. Because of the large power saving, most laptops automatically enter this mode when the computer
	is running on batteries and the lid is closed.

		$ run0 systemctl suspend

	HIBERNATING
		SAVES ALL COMPUTER OPERATIONAL DATA ON THE FIXED DISK BEFORE TURNING THE COMPUTER OFF COMPLETELY. 
	On switching the computer back on, the computer is restored to its state prior to hibernation, with all programs 
	and files open, and unsaved data intact. In contrast with standby mode, hibernation mode saves the computer's state on the hard disk, 
	which requires no power to maintain, whereas standby mode saves the computer's state in RAM , which requires a small amount of power to maintain.

		$ run0 systemctl hibernate

	HYBRID SUSPENDING AND HIBERNATING
		hybrid-sleep suspend and hibernate the system.
	'suspend' copies the contents to RAM  and then 'hibernate' copies the contents of the RAM to the DISK, 
	so that the power on is faster than powering on a computer and restoring the state manually from DISK.

		$ run0 systemctl hybrid-sleep
	
	SUSPEND THEN HIBERNATE
		Suspend the system and hibernate it WHEN THE BATTERY IS LOW or WHEN THE DELAY SPECIFIED IN SYSTEMD-SLEEP.CONF ELAPSED.
	This will trigger activation of the special target unit suspend-then-hibernate.target.

		$ run0 systemctl suspend-then-hibernate

	
	TROUBLESHOOTING NVIDIA GPUS
		source: https://wiki.archlinux.org/title/NVIDIA/Tips_and_tricks#Preserve_video_memory_after_suspend

	By default the NVIDIA Linux drivers save and restore only essential video memory allocations on system suspend and resume. Quoting NVIDIA:

	    The resulting loss of video memory contents is partially compensated for by the user-space NVIDIA drivers, and by some applications, but can lead to failures such as rendering corruption and application crashes upon exit from power management cycles.

	The "still experimental" interface enables saving all video memory (given enough space on disk or RAM).

	To save and restore all video memory contents, NVreg_PreserveVideoMemoryAllocations=1 kernel module parameter for the nvidia kernel module needs to be set. While NVIDIA does not set this by default, Arch Linux does for the supported drivers.

	You also need to have the services nvidia-suspend.service, nvidia-hibernate.service, and nvidia-resume.service enabled. Arch Linux enables them by default on supported drivers, as per upstream requirements. 

-------------------
-------------------
-------------------
ACCESSING THE UNDERLAYING BIND MOUNT POINT
	Suppose you have bind mount a random directory onto /home/username/,
how to access the underlaying shadowed mount point?

	$ run0 mkdir /mnt/root_test/;
	$ run0 mount -B / /mnt/root_test/;
	$ ls -la /mnt/root_test/home/username/;

VERY IMPORTANT: either -B or --bind will work, do not forget it or else filesystem might be corrupted!
-------------------
-------------------
-------------------
USING GAMECONQUEROR  ON GAMES
	1 - DISABLING ASLR
		Some and most games might require disabling ASLR, since it randomizes memory.
	The command below both saves and changes the current ASLR value to 0.

		1.1 - this: 
			$ VA_SPACE=$(sysctl -n kernel/randomize_va_space ) && run0 sysctl -w kernel/randomize_va_space=0;

		if something goes wrong, you can just use this other command, since using VA_SPACE will overwrite the original value:
			$ run0 sysctl -w kernel/randomize_va_space=0;
			
		Restarting the system should also set the ASLR to it's default normal value.
	
		1.2 - RESTORING ASLR
			$ run0 sysctl -w kernel/randomize_va_space=${VA_SPACE}
	
	2 - (Optional) - Disabling ASLR Individually:
		$ setarch `uname -m` -R /bin/<program_to_launch>
	
	3 - Launch gameconqueror before the game is launched (Not really mandatory).
	
	4 - Launch game.

	5 - Chose game process by it's PID or Name in gameconqueror.


-------------------
-------------------
-------------------
-------------------
-------------------
DISABLING IPV6 TEMPORARILY
	Some programs and games do better without IPV6 Enabled:
		$ run0 sysctl -w net/ipv6/conf/all/disable_ipv6=1


-------------------
-------------------
-------------------
-------------------
TCP OFFLOAD AND WHAT IT IS || NETWORK OFFLOADING
	TCP Offloading allows processing of the entire or part of the TCP/IP Stack to be processed to the Network Controller instead of the CPU.
This is very common on Gigabyte Network Cards.

Offloading improves CPU Performance, however it allows very old cards to introduce bugs on whatever is being offloaded and therefore perform wrong/worse.

An old NIC(Network Interface Card) may also contain bugs that are concern to security. 

Even if disabling offload on to the NIC impacts the CPU Performance, the gains from doing so on a modern PC CPU may so outperform the Controller Device 
of any NIC(Network Interface Card) performance when dealing with those tasks themselves, since they are not that all powerful. - VERY IMPORTANT.


Generally speaking, if the network traffic is too low, then there's little to no gains offloading to a NIC on modern CPUs.
When talking about servers with huge network traffic, there may be significal gains when offloading to NIC, which is the reason
Linux defaults to offloading.

If you're either at home or work, just go on with disabling offload.


	CHECKING CURRENT OFFLOAD
		$ run0 ethtool --show-offload enp3s0

	DISABLING OFFLOAD || PERFORMANCE ADVICE || SPEED
		$ run0 ethtool --offload enp3s0 rx off  tx off; 
		$ run0 ethtool -K enp3s0 gso off;
			or
		$ run0 ethtool --offload  enp3s0 rx off  tx off && run0 ethtool -K enp3s0 gso off
	
	ENABLING IT BACK
		$ run0 ethtool --offload  enp3s0 rx on tx on && run0 ethtool -K enp3s0 gso on
	
	PERMANENT SETTINGS
		TODO: still todo
-------------------
-------------------
-------------------
-------------------
WEECHAT HOTKEYS
	Source: https://weechat.org/files/doc/stable/weechat_user.en.html#environment_variables

	'esc' + <number>	- Go to Nth tab
	/help <commmand>	- Look for help pages for a given command
				ex.: /help  server
	/server add <server_alias> <server_address>	- Adds server to server list
	/connect <server_alias>	- Connects to previously added server
	ctrl+n 			- Goes to next window
	ctrl+p			- Goes to previous windows
	/list			- Lists all channels from current server and allows user to select one and also to see how many people are connected
	ctr+j			- When in /list window you can join channels by pressing this combination of keys

	ctrl + '+'		- Zoom In
	ctrl + '-'		- Zoom Out (Very useful for copying hyperlinks) - VERY IMPORTANT
	/autojoin add <channel>	- Adds given channel to the default autojoin list
				^ VERY IMPORTANT: You have to normally exit weechat for saving the autojoin list,
				^ or else it'll be lost!
	/allchan /autojoin add	- Adds all current channels to default autojoin ilist
	/autojoin join all	- Joins all channels in the autojoin list

	alt+m			- Toggles Mouse
	/cursor			- Toggles Cursor Mode

	ctrl+r			- Search for message / Enable Search mode

	/plugin load <plugin>	- Loads plugin
	/script load 		- Loads script

	TEXT INPUT:
		alt+enter	- Inserts New Line
		ctrl+u		- Deletes entire line
		alt+r		- Deletes entire Line
		alt+R		- Deletes entire commandline

	WHILE IN /LIST WINDOW - The bellow requires the user to press 'enter':
		$			- Refresh/Runs /list again; VERY IMPORTANT: doesn't resets filters!

		Existing Filters:
			*:*			- Resets all filters
			u:>25			- Show only channels with 25 or  MORE users
			u:<25			- Show only channels with 25 or LESS users
			n:<name>		- Show only channels with given name in it
			t:<topic>		- Show only channels with given topic in it

		s:<field1>,<filed2>	- Sort channels by field
						Example:
							s:name2		- Sorts channel by name - includes character in name(ascending)
							s:name		- Sorts channel by name (ascending)
							s:-users	- Sorts channel by user count (descending)
							s:users		- Sorts channel by user count (ascending - starting from 0 to Nth)

		j			- Join channel

	WHILE IN CURSOR MODE:
		alt+<arrow_keys>	- Moves cursor to an area
		q			- Quotes selected prefix + message
		Q			- Quotes selected message + prefix + time
		m			- Quote selected message
		enter			- Stop cursor mode
		WHEN ON NICKNAME AREA:
			k			- Kick user
			K			- Kick and Ban user
			b			- Ban user
			w			- /whois on nick (VERY IMPORTANT: Useful when someone has weird characters in nickname)
			q			- Query user
	
	
	COLOR  CODE:
		ctrl+c, b		- Inserts code for Bold text
		ctrl+c, c		- Inserts code for Colored Text
					^ Requires color number to be  added:
						C1text with color
							or
						C1,2text with background color
		ctrl+c, d		- Insert code for colored text (RGB color, as hexadecimal).
		ctrl+c, i		- Inserts code for Italic Text
		ctrl+c, o		- Inserts code for code reset
		ctrl+c, v		- Inserts code for reverse color
		ctrl+c, _		- Inserts code for underlined text


	TEXT SEARCH WHEN IN SEARCH MODE:
		ctrl+r			- Use regex strings
		alt+c			- Switch beetween exact case
		tab			- Alternates between pre|msg or both?


	ctrl+a		#returns to first character in the line(Carriage Return?) - CHECK
	ctrl+d		#deletes character ahead of the cursor
	ctrl+p		#Goes to next channel
	ctrl+n		#Goes to previous channel
-------------------
-------------------
-------------------
-------------------
HTOP HOTKEYS
	F1			- Display all hotkeys
	'*'			- Collapse/Uncollapse process tree
	'p' 			- Toggle Program Path/Program Name
	'm' 			- Toggle Merge Command
	'w'			- Warp process command in multiple lines
	'l'			- List open files by a given process (uses 'lsof')
	'x'			- List file locks of process
	's'			- Trace syscalls (uses 'strace')
	'>' or '.' or F6	- Sort processes by a specific field
	'I'			- Invert Sort Order

--------
TOP HOTKEYS
	Launch Options:
		top -E m	#Launches top with 'Mebibytes' value.
				^ 'Kilobytes' is used by default.
	Hotkeys:
		'f' - filter available fields
		'arrow' - navigate / selects current field
		'd' or "space" - toggles current field for displaying on screen
		"right arrow key" - allows to move current field to a more convenient location
		's' - sort by current field
		'q' - return top view

-------------------
-------------------
-------------------
-------------------
VIEWING PICTURES ON COMMANDLINE WITHOUT XORG USING MPV
	Use mpv ;)

	example:
		$ cd <dir_with_pictures_only>
		$ mpv --loop ./*

		you can finally use  'shift + <' and 'shift + >' to navigate.

USING MPV TO PLAY ANIMATED DESKTOP BACKGROUNDS VIDEOS ON X11
	You might or might not want to look at ~/.config/mpv/mpv.conf here inside this document before using this:

		$ mpv --wid=0 <filename>


USING MPLAYER TO PLAY ANIMATED DESKTOP BACKGROUNDS VIDEOS ON X11
	 First install 'mplayer'. On linux, this doesn't work for wayland.

	Using 'x11':
		$ mplayer -zoom -vo x11  -loop 0 -rootwin -vf expand=1280:768 <filename>
			or 
		$ mplayer -vo x11 -fs -vf expand=::::::1,scale=1280:768 -rootwin  <filename>

	Using opengl:
		$ mplayer -vo gl -fs -vf expand=1280:768 <filename>
	Using 'vdpau':
		$ mplayer -vo vdpau:hqscaling=1 -loop 0 -rootwin -fs -nokeepaspect -screenw 1280 -screenh 768 <filename>

	Using 'matrixview':
		$ mplayer -vo matrixview -loop 0 -rootwin -fs -nokeepaspect -screenw 1280 -screenh 768 <filename>

-------------------
-------------------
-------------------
FIREJAIL INTRODUCTION
       Firejail is a SUID sandbox program that reduces the risk of security breaches by restricting the running environment of untrusted applications using
       Linux  namespaces,  seccomp-bpf  and Linux capabilities.  It allows a process and all its descendants to have their own private view of the globally
       shared kernel resources, such as the network stack, process table, mount table.  Firejail can work in a SELinux or AppArmor environment, and  it  is
       integrated with Linux Control Groups.

       Written in C with virtually no dependencies, the software runs on any Linux computer with a 3.x kernel version or newer.  It can sandbox any type of
       processes: servers, graphical applications, and even user login sessions.

       Firejail  allows the user to manage application security using security profiles.  Each profile defines a set of permissions for a specific applica‐
       tion or group of applications. The software includes security profiles for a number  of  more  common  Linux  programs,  such  as  Mozilla  Firefox,
       Chromium, VLC, Transmission etc.

       Firejail  is  currently implemented as an SUID binary, which means that if a malicious or compromised user account manages to exploit a bug in Fire‐
       jail, that could ultimately lead to a privilege escalation to root.  To mitigate this, it is recommended to only allow trusted users to run firejail
       (see firejail-users(5) for details on how to achieve that).   For  more  details  on  the  security/usability  tradeoffs  of  Firejail,  see:  #4601
       ⟨https://github.com/netblue30/firejail/discussions/4601⟩

       Alternative  sandbox  technologies like snap (https://snapcraft.io/) and flatpak (https://flatpak.org/) are not supported. Snap and flatpak packages
       have their own native management tools and will not work when sandboxed with Firejail.

	BY DEFAULT, Without any options, the sandbox consists of a filesystem build in a new mount namespace, and new PID and UTS  namespaces.  
	IPC,  network  and  user namespaces  can be added using the command line options. The default Firejail filesystem is based on the host 
	filesystem with the main system directories mounted read-only. These directories are /etc, /var, /usr, /bin, /sbin, /lib, /lib32, /libx32
	and /lib64. Only /home and /tmp are writable.

	UPON EXECUTION Firejail first looks in ~/.config/firejail/ for a profile and if it doesn't find one, it looks in /etc/firejail/.  For profile  reso‐
	lution  detail  see https://github.com/netblue30/firejail/wiki/Creating-Profiles#locations-and-types.  If an appropriate profile is not found, Fire‐
	jail will use a default profile.  The default profile is quite restrictive. In case the application doesn't work, use --noprofile option to  disable
	it. For more information, please see SECURITY PROFILES section below.

IMPROVING PERFORMANCE BY FIREJAILING APPLICATIONS
	Sandboxing applications isolates them from the rest of the system, limiting access to the system helps programs to perform better.

FIREJAIL COMMANDS
	--list				#Lists all sandboxes
	--ls=<name|pid> <dir/filename>	#Lists files in sandbox container.
	--tree				#Prints tree of all sandboxed processes.
	--top				#Monitors CPU Intensive sandboxes.
	--netstats			#Monitor Network namespace statistics
	--shutdown=<name|pid>		#Shutdowns given sandbox by it's name or pid.
	--tab  				#Enable shell tab completion in sandboxes using private or whitelisted home directories.
					^ Use when creating a sandbox
	--tracelog 			#MONITOR - This option enables auditing blacklisted files and directories. 
					^ A message is sent to syslog in case the file or the directory is accessed.

	--join=<name|id> [program_name]			#Joins an already initialized Sandbox by it's name or pid
	--join-or-start=<name|id> [program_name]	#Joins or starts if sandbox is not initalized

	--nodbus			#Disables dbus access
	--nodvd				#Disables dvd access
	--noinput			#Disables input access

	--x11 --net=<network_interface>		#--x11 requires --net to be instantiated as an option
						^ EXAMPLE:
							$ firejail --x11 --net=enp3s0 firefox

	--read-write=<dir_name/file_name>	#Sets directory or file read-write.
	--read-only=<dir_name/file_name>	#Sets  directoryu or file as read-only
	--writable-run-user			#Disable the default blacklisting of /run/user/$UID/systemd and /run/user/$UID/gnupg.
	--writable-var				#Mount /var directory read-write.
	--writable-var-log			#Use the real /var/log directory, not a clone. By default, a tmpfs is mounted on top of /var/log directory,
						^ and a skeleton filesystem is  created based on the original /var/log.
	
	--join-network=<name|pid>	#Joins the network namespace of the given sandbox
					^ This command is only  available to root user
					Example: 
						# starting firejail firefox:
						$ firejail --net=eth0 --name=browser firefox &
						# verifying netfilter configuration:
						$ run0 firejail --join-network=browser /sbin/iptables -vL

	--ignore=<command>		#Ignores any of the existing commands for firejail from being initialized
					Example:
						$ firejail --ignore=nodvd <program_name>

					^ It's  also possible to add 'ignore nodvd' on any of the profiles located in /etc/firejail/*.conf files

	--keep-tmp(UNEXISTENT)		#(NON-EXISTANT) If you ever need /tmp/ to be untouched, you'll need to instantiate
					firejail with --ignore=private-tmp or add 'ignore private-tmp' to thue /etc/firejail/*.conf 
					profiles of your choice.

	--keep-config-pulse		#Doesn't touches ~/.config/pulse
	--keep-dev-shm			#Doesn't touches /dev/shm even if --private-dev is enabled
	--keep-fd=all			#Doesn't touches file descriptors, by default only descriptors 0, 1 and 2 are inherited to the sandbox
	--keep-fd=<fd1,fd2,fd3...>	#Doesn't touches given file descriptors of the given running process
					^ READ MORE ON "HOW TO FETCH FILE DESCRIPTORS"

	--keep-var-tmp			#Doesn't touches /var/tmp/


	--tmpfs=<dirname> 		#Mount a writable tmpfs filesystem on directory dirname. Directories outside user home or 
					^ not owned by the user  are  not  allowed.  Sandboxes running as root are exempt from these restrictions.
					^ Very useful to see which temporary files are created by a given application.

	--private 			#Mount new /root and /home/user directories in temporary filesystems. 
					^ All modifications are discarded when the sandbox is closed.

	--private-home			#Makes /home/<user>/ as temporary filesystem.
	--private-tmp			#Makes /tmp/ folder private only whitelisting X11 and PulseAudio Sockets
	--private-srv=<file|directory>	#Makes /srv/ private by only copying desired files/directories.
					^ The files and directories in the list must be expressed as relative to the current user's home directory.

	--private=<directory> 		#Use directory as user home.  --private and --private=directory cannot be used together.
					^ firejail --private=/home/netblue/firefox-home firefox

	--private-cwd=<directory>	#Sets CWD(Current Working Directory) of the sandbox.

	--private-bin=<file1,file2> 	#Build  a  new  /bin in a temporary filesystem, and copy the programs in the list.
					^ This is good to test if a program can be run withuout using any other dependency in /bin.
					^ The files in the list must be expressed as relative to the /bin, /sbin, /usr/bin, /usr/sbin,
					^ or /usr/local/bin directories.  If no listed files are found, /bin directory will be empty.
					^ The  same  di‐ rectory is also bind-mounted over /sbin, /usr/bin, /usr/sbin and /usr/local/bin.
					^ All modifications are discarded when the sandbox is closed. Multiple private-bin commands are allowed 
					^ and they accumulate.
					Example: 
						$ firejail --private-bin=bash,sed,ls,cat
	
	--private-cache 		#Mount an empty temporary filesystem on top of the .cache directory in user home. 
					^ All modifications are discarded when the sandbox is closed.

	--private-home=<file,directory>	#Build  a  new user home in a temporary filesystem, and OPTIONALLY copies the files and directories in the list 
					^ in the new home. The files and directories in the list must be expressed as relative to the current user's 
					^ home directory. All modifications are discarded when the sandbox is closed.

	--private-etc=<file,directory>	#Same as private-home, but builds a private /etc/ folder.

	--private-dev			#Creates a new /dev/ directory
					^ Only disc, dri, dvb, hidraw, null, full, zero, tty, pts, ptmx, random, snd, urandom, video, log, 
					^ shm and usb devices are available.  
					^ Use the options --no3d, --nodvd, --nosound, --notv, --nou2f and --novideo for additional restrictions.

	^ VERY IMPORTANT - these are also available as options: --private-lib, --private-opt,  --private-srv, --private-tmp!

	--profile=<filename_or_profilename>	#Loads a custom security profile for firejail to use in the sandbox
						^ profiles are located in /etc/firejail by default.
						Example:
							firejail --profile=myprofile

	--profile.print=<name|pid>		#Prints profile of the given sandbox by it's name or pid.

	--rlimit-cpu=<number>		#Sets maximum number of CPU allowed for the sandbox processes; When the limit is reached, proccesses
					^ Are then closed.
	--rlimit-fsize=<number>		#Sets maximum size of files that can be created inside the sandbox by processes.
	--rlimit-nofile=<number>	#Limits number of open files by a process running in the sandbox.
	--rlimit-nproc=<number>		#Limits the number of processes that can be executed inside the running sandbox.
	--rlimit-sigpending=<number>	#Sets the maximum number of pending signals by a process.

	--allow-debuggers		#Allow tools such as  strace and gdb inside the sandbox by whitelisting system calls ptrace and process_vm_readv.
	--all-users			#Makes all users in /home to be visible inside the sandbox.
	--appimage			#Sandboxes an AppImage application.
	--bandwidth=<name | pid>	#Set bandwidth limits to sandboxed application
					^ Name must be set when sandboxing
	--bind=filename1,filename2	#Bind mounts filename1 on top of filename2
					^ firejail --bind=/config/etc/passwd,/etc/passwd
	--blacklist=<dirname_or_filename>	#Blacklists dirname/filename.
	--whitelist=dirname_or_filename>	#Whitelist dirname/filename.
	--noblacklist=<dirname_or_filename>	#Whitelists dirname/filename.
	--build <application_name>	#Builds a relaxed profile. Profile is printed on screen
	--build=profile-file <application_name>	#Builds a relaxed profile and saves it  for future use.
	--caps				#Restricts what a process running as root can do. By default, root programs run with all capabilities enabled.
	--caps.drop=all			#Must-have option when running applications untrusted programs.
	--caps.drop=<capability1,capability2> <application_name>	#Drops specific capabilities for a given program
									^ defines a blacklist linux capability filter.
									^ firejail --caps.drop=net_broadcast,net_admin,net_raw
	--caps.keep=<capability1,capability2> <application_name>	#Keeps specific capabilities for a given program
									^ defines a whitelist linux capability filter.
	--caps.print<name|pid>						#Prints/lists the capability filter for the sandbox identified by pid or name.
									Example:
										firejail --name=mygame --caps.drop=all warzone2100 &
										firejail --caps.print=mygame
	--read-only=<dirname_or_filename>	#Sets directory or file as readonly

	--name=<sandbox_name>		#Names a given sandbox
	--hostname=<hostname>		#Sets the sandbox hostname
					^ Some programs can benefit from using a different hostname.
	--machine-id			#Spoofs id-number in /etc/machine-id; Breaks audio-support, only use if no audio is required.

	--cat=<name|pid> <filename>	#Prints content of a given file for a given sandbox name or pid.
	--get=<name|pid> <filename>	#Gets file from given sandbox | FILE TRANSFER
	--put=<name|pid> <filename>	#Puts file in the given sandbox | FILE TRANSFER

	--hosts-file=<filename>		#Uses file as /etc/hosts
					^ This is useful to use with unstrusted applications who might try to change the system's host file.
					^ Example:
						$ firejail --hosts-file=~/myhosts firefox

	--chroot=<dirname>		#Chroots the sandbox for a given application.
					Example: 
						firejail --chroot=/media/ubuntu warzone2100
	--cpu=<cpu-number1,cpu-number2>	#Sets the cpu affinity for the sandbox.
	--cpu.print=<name|pid>		#Prints the cpu affinity for a given sandboxed application
					Example:
					      	firejail --name=mygame --caps.drop=all warzone2100 &
					      	firejail --cpu.print=mygame
	--debug				#Print debug messages
	--disable-mnt			#Blacklist /mnt, /media, /run/mount and /run/media access
	--dns=<address>			#Set a DNS server for the sandbox. Up to three DNS servers can be defined.  
					^ Use this option if you don't trust the DNS setup on your network.
	--dns.print<name|pid>		#Prints the defined DNS address set for a sandbox by sandbox's name or pid.

	--scan 				#NETWORK MONITOR. Arp-Scan All  the  networks  from inside a network namespace.  
					^ This makes it possible to detect macvlan kernel device drivers running on the current host.
					Example:
						$ firejail --net=eth0 --scan
						


	--trace[=filename]		#MONITOR - Trace open, access and connect system calls. 
					^ If filename is specified, log trace output to filename, otherwise log to console.

	--tracelog 			#MONITOR - This option enables auditing blacklisted files and directories. 
					^ A message is sent to syslog in case the file or the directory is accessed.

	--snitrace[=name|pid] 		#Monitor  Server  Name Indication (TLS/SNI). The sandbox can be specified by name or pid. 
					^ Only networked sandboxes created with --net are sup‐ ported. This option is only available 
					^ when running the sandbox as root. Without a name/pid, Firejail will monitor the main system network namespace.


	--nettrace[=name|pid]		#NETWORK. MONITORS received  TCP.  UDP,  and ICMP traffic. The sandbox can be specified by name or pid. 
					^ Only networked sandboxes created with --net are supported. 
					^ This option is only available when running the sandbox as root.

              				^ Without a name/pid, Firejail will monitor the main system network namespace

	--dnstrace[=name|pid]		#MONITOR DNS NETWORK QUERIES. The sandbox can be specified by name or pid.
					^ Only networked sandboxes created with --net are supported. 
					^ This  option  is only available when running the sandbox as root.
					^ Without a name/pid, Firejail will monitor the main system network namespace.


	--icmptrace[=name|pid]		#MONITOR  ICMP NETWORK TRAFFIC. The sandbox can be specified by name or pid. 
					^ Only networked sandboxes created with --net are supported. 
					^ This option is only available when running the sandbox as root.

              				^ Without a name/pid, Firejail will monitor the main system network namespace.

	--env=name=<value>		#Sets environment value in the new sandbox
					Example:
						$ firejail --env=LD_LIBRARY_PATH=/opt/test/lib
						^ sets $LD_LIBRARY_PATH for the sandbox

	--rmenv=<name>			#Removes environment variable for a sandbox

	--ignore=<command>		#Ignore command in profile file.
					Example:
						$ firejail --ignore=seccomp --ignore=caps firefox
							or
						$ firejail --ignore="net eth0" firefox
	
	--net=<network_interface>	#Defines a network interface to be used in the sandbox
	--net=<bridge_interface>	#Enable a new network namespace and connect it to this bridge interface.
	--net=<wireless_interface>	#Enables a wireless interface. 
					Example:
						$ firejail --net=wlan0

	--net=<tap interface>		#Enable a new network namespace and connect it to this ethernet tap interface 
					^ using the standard Linux macvlan driver.
					Example: 
						$ firejail --net=tap0 --ip=10.10.20.80 --netmask=255.255.255.0 --defaultgw=10.10.20.1 firefox

	--net=none			#Disables network interface
	--net.print=<name|pid>		#Print network interface configuration for the sandbox specified by name or PID

	--netmask=<address> 		#Use this option when you want to assign an IP address in a new namespace and
					^ the parent interface specified by --net is not configured
					Example:
						$ firejail --ip=10.10.20.67 --netmask=255.255.255.0 --defaultgw=10.10.20.1
	--netns=name 			#Run the program in a named, persistent network namespace. 
					^ These can be created and configured using "ip netns".


	--mac=<address>			#Sets mac address. to the last network interface defined by --net option. 
					^ Option unsupported for wireless connections.

	--ip=dhcp			#Acquires IP Address, Gateway and DNS Server for the last interface defined in --net option.
					^ This option requires  ISC dhclient and DHCP client to be installed.
					Example:
						$ firejail --net=br0 --ip=dhcp
	
	--ip=<address> 			Assign IP addresses to the last network interface defined by a --net option.
					^ A default gateway is assigned by default.

					Example:
						$ firejail --net=eth0 --ip=10.10.20.56 firefox
	
	--ip=none 			#No  IP  address and no default gateway are configured for the last interface defined by a --net option. 
					^ Use this option in case you intend to start an external DHCP client in the sandbox.
					Example:
						$ firejail --net=eth0 --ip=none firefox
	
	^ VERY IMPORTANT - Same options are available for IPV6 through --ip6 !!!

	--iprange=address,address 	#Assign an IP address in the provided range to the last network interface defined by a --net option.
					^ A default gateway is assigned by default.

					Example:
						$ firejail --net=eth0 --iprange=192.168.1.100,192.168.1.150
	
	--ipc-namespace			#Enable a new IPC namespace if the sandbox was started as a regular user. 
					IPC namespace is enabled by default for sandboxes started as root.
	


	--mtu=number 			#Assign a MTU value to the last network interface defined by a --net option.

					Example:
					$ firejail --net=eth0 --mtu=1492
	


	--netfilter 			#Enable a default firewall if a new network namespace is created inside the sandbox.  
					^ This option has no effect for sandboxes using the system network namespace.
					^ (Blocks all incoming connections)
	

	--netfilter=<filename>		#Specifies an iptables configuration file to be used on the sandbox.
					Example:
						$ firejail --netfilter=/etc/firejail/webserver.net --net=eth0 \
						  /etc/init.d/apache2 start

	--netfilter.print=<name|pid>	#Prints iptables configuration file used by the sandbox.
	
	^ VERY IMPORTANT - Same options are available for IPV6 through --netfilter6 !!!


	--netlock			#Allows only the starter connections to remain connected to the sandbox. Any subsequent requests are dropped.
					^ Several type of programs (email clients, multiplayer games etc.) talk to a very small number of IP addresses.
					Example:
						$ firejail --net=eth0 --netlock \
						  --private=~/tor-browser_en-US ./start-tor-browser.desktop
USEFUL TOOLS FOR USING IN FIREJAIL
	lsof, pstree

CORE TECHNOLOGY LINUX NAMESPACES
	Linux 3.8 and higher defines 6 linux namespaces:

		1 - Mount Namespace
			Isolates file system mount points seen by a group of processes.

		2 - UTS Namespace
			Isolates two-system identities, nodename and domain name. 
			Allows containers to have different hostnames and domain names.

		3 - IPC Namespace
			Isolates Interprocess Communication(IPC) Resources

		4 - PID Namespace
			Isolates Processes in different PID(Process ID Number) Namespaces.

		5 - Network Namespace
			Isolates Network System Resources

		6 - User Namespaces
			Isolates the User and Group ID Number spaces
	

USEFUL FIREJAIL EXAMPLES
	BASIC EXAMPLES
1) Obfuscates the entire user home directory:
	$ firejail --whitelist=~/.config/weechat --whitelist=~/.cache/weechat weechat
		^  If ~/.cache/weechat is a SYMLINK, consider using the below option, 
		^ firejail only reads directories/files pointed by SYMLINKS not the symlink file itself:
			$ firejail --whitelist=~/.config/weechat --whitelist=~/Documents/Private/Apps/MORE/.cache/weechat weechat

Alternate Option A: 
	$ firejail --net=enp3s0 --mac=00:11:22:33:44:55 --whitelist=~/.cache/weechat --whitelist=~/.config/weechat weechat
		or
	$ firejail --net=enp3s0 --mac=00:11:22:33:44:55 --whitelist=~/.config/weechat --whitelist=~/Documents/Private/Apps/MORE/.cache/weechat  weechat

Alternate Option B:
	$ firejail --hostname=officepc --net=enp3s0 --mac=F2:91:FF:58:FB:9D  --whitelist=~/.cache/weechat --whitelist=~/.config/weechat weechat
		or
	$ firejail --hostname=officepc --net=enp3s0 --mac=F2:91:FF:58:FB:9D --whitelist=~/.config/weechat --whitelist=~/Documents/Private/Apps/MORE/.cache/weechat weechat

Custom Option / Weechat Custom / WEECHAT MAC ADDRESS:
	$ firejail --hostname=officepc3 --net=enp3s0 --mac=3a:01:76:36:1a:b4 --whitelist=~/.config/weechat --whitelist=~/Documents/Private/Apps/MORE/.cache/weechat weechat

	$ firejail --hostname=officepc3 --net=enp3s0 --mac=3a:01:76:36:1a:b4 --whitelist=${ROOTPATH}/home/${USER}/.config/weechat --whitelist=${ROOTPATH}/home/${USER}/.cache/weechat weechat

Valid Mac Address:
		----START----
	Mac Addresses can be either multicast or unicast addresses, this is a list of unicast addresses:

		3a:01:76:36:1a:b4
		68:72:F8:18:5E:0E
		F2:91:FF:58:FB:9D
		00:11:22:33:44:55

		ca:67:46:fa:76:60
		b6:b8:0d:4c:a3:1f
		52:0f:6b:35:04:46
		ae:97:bd:c2:a1:65
		ce:2a:96:a0:94:3a
		ee:1b:be:f4:f9:28
		2e:9c:a6:2c:df:72
		1a:53:be:36:5c:ec
		ca:e4:3e:d8:07:5c
		7e:04:ca:7c:30:fe
		86:17:d7:d2:e7:7a
		8a:69:2a:a0:9e:19
		12:e3:1e:e6:46:96
		9a:94:0c:fd:51:99
		9e:91:8a:2f:13:5e
		72:5e:23:06:15:f2
		4a:71:48:25:06:7f
		62:d4:eb:29:c8:3d
		ca:6d:01:3a:13:e3
		36:a3:9e:e6:0f:63
		0e:cb:0c:b6:47:7b
		d2:64:a7:35:d5:57
		f6:85:eb:79:5d:6a
		ae:4d:95:5a:0f:ff
		4a:a8:13:18:3c:34
		56:f5:c8:8d:e4:c4
		3e:b9:fe:7d:6f:c8
		2e:0d:1b:b2:14:ae
		56:3e:a7:57:fc:cd
		3a:0b:a9:16:14:82
		ae:64:53:04:ed:35
		02:3d:3b:b8:cf:34
		d6:e3:af:3e:64:f7
		92:9b:33:b4:1f:72
		ba:14:85:24:34:8f
		ca:9e:48:b0:12:49
		5a:17:88:6a:61:95
		4a:ac:c8:65:e7:00
		36:da:e8:03:ed:97
		06:66:77:73:68:c0
		76:ba:f7:fe:87:17
		52:b2:e6:f0:fe:66
		3a:46:f2:3c:a2:97
		76:ca:f8:e7:ed:b4
		f6:ef:17:38:08:18
		fe:ba:22:6a:bc:7d
		9e:38:8b:7c:ad:af
		ee:33:97:c5:90:35
		1a:a0:80:fc:d1:21
		56:1c:e9:a1:8d:04
		e6:c2:18:4d:d7:b8
		c6:73:0b:8c:a8:25
		ee:32:40:06:e3:5c
		ea:de:fc:36:a2:11
		b2:bd:e1:7c:50:1f
		ee:5c:4e:70:30:b4
		02:f6:46:5d:4c:bc
		4e:c0:ab:9b:a3:d8
		c6:e6:1e:82:c7:6b
		ca:be:5b:c5:c4:c7
		3a:29:68:3b:4d:a3
		0e:8c:09:52:cf:09
		ea:1a:d0:f9:0d:cb
		4a:88:29:f0:cc:1e
		36:36:e1:7c:b7:98
		9a:14:e5:8b:15:d3
		6e:69:33:f9:3a:35
		02:b5:3e:3b:be:b6
		aa:c7:dc:aa:2c:fc
		0a:2f:bb:d6:b5:9c
		ea:9f:f7:b9:fc:25
		fe:68:44:e8:d1:21
		da:f9:26:2c:56:57
		1a:2c:49:f5:87:4c
		d6:50:7c:e5:12:1e
		3a:98:5e:fd:1e:27
		52:3c:2b:8e:3c:74
		86:4e:50:64:18:96
		5e:1c:3b:4b:28:66
		6a:f1:e8:f3:36:23
		8e:f2:92:8d:37:72
		ee:61:d4:90:e6:e0
		6e:69:56:c6:30:cb
		fe:d8:9c:61:7b:11
		de:84:58:07:29:70
		ca:bc:53:d2:82:90
		b6:df:fa:29:d6:53
		76:c4:59:a1:e6:92
		5a:e6:19:49:9f:57
		96:59:c4:86:5c:90
		d6:4c:09:e8:84:33
		d2:99:32:4a:42:98
		7e:c1:f6:3b:af:f5
		96:6c:29:ae:28:cb
		ba:ec:f5:e4:68:41
		1e:97:fe:f6:ba:d4
		b6:45:ca:37:77:e9
		5a:19:c9:b5:6b:01
		4a:44:41:08:c2:55
		aa:6e:28:df:cc:d7

		----END----

MAC ADDRESS GENERATOR LINKS:
	https://macaddress.io/mac-address-generator					#Generates more addresses
	https://www.hellion.org.uk/cgi-bin/randmac.pl?scope=local&type=unicast		#MAC ADDRESS GENERATOR, you can choose to opt in for multicast or unicast.
	https://miniwebtool.com/mac-address-generator/					#MAC ADDRESS GENERATOR, can generate multicast/unicast without warnign the user

----
2) Initializes steam on a firejail sandbox, by using a  different Network IP Address, allowing PIPEWIRE'S PULSE SERVER to work:
	$ firejail --net=enp3s0 --ip=dhcp --ip6=dhcp  --env=PULSE_SERVER=192.168.1.67 steam

	^ The big advantage here is that you can have a restrictive firewall settings on the host machine, and use the sandbox with a low-profile
	^ firewall setup mostly for online games. READ MORE ON if you need to: 
				"SETTING UP FIREJAIL TO USE PIPEWIRE SERVER AUDIO DEVICE WITH A DIFFERENT NETWORK DEVICE"



3) 
firejail --allow-debuggers --ignore=seccomp <program>		#Some programs  may run into issues, disabling seccomp and allowing debuggers solves
								^ most of the issues.
4)
firejail --noprofile <program>					#Also should solve most problems, however uses no default profiles.

5)
firejail --x11 --net=enp3s0 weston				#This is how you run Xwayland applications with firejail
								^ Requires installing 'xorg-server-xephyr'
								^ READ "FIREJAIL GAMESCOPE WORKAROUND" for an alternate solution!

	USING FIREFOX ANONYMOUSLY WITH FIREJAIL
		By default, firefox  runs firejail by using your local profile, local mac address, etc.
	So it's not 100% anonymous, cookies, scripts and files still get stored on the local machine. The server can keep a track of who you are
	this way, so follow these  steps if you want to be 100% anonymous:

		1 - First, you'll need to use a Unicast MAC Address Generator, to  make sure some websites don't track it to your machine,
	this is useful for dodging server-side cookies who might remember who your machine is on their server:
		https://www.hellion.org.uk/cgi-bin/randmac.pl?scope=local&type=unicast

		2  - Create a  temporary directory that will represent your user home directory:
			$ mkdir /tmp/tmp_firefox

		3 - Run firefox by changing hostname, mac address and user home directory in --private each time:
			$ firejail --net=enp3s0 --hostname=robotuser --ip=dhcp --ip6=dhcp --mac=1c:8a:06:f5:68:05:95:62 --private=/tmp/highlander/ firefox 
			^ some servers can check IP Address, Hostname, Mac address and User Home  directory.

FIREJAIL GAMESCOPE WORKAROUND
	This is a workaround to get 'gamescope' working under FIREJAIL with wayland/xwayland:

	SOLUTION A:
		0) If you're using nvidia drivers make sure to read the following topic:
			"FIXING NVIDIA ISSUE ON FIREJAIL WHEN RUNNING SOME PROGRAMS LIKE STEAM"

		1) Disable 'noroot' option by removing(or commenting) it on /etc/firejail/steam.profile:
			[file: /etc/firejail/steam.profile]
				nogroups
				nonewprivs
				#noroot
			[/file]

		2) Add 'noblacklist /tmp/.wine-*' so that games can be run using gamescope.
			[file: /etc/firejail/steam.profile]
				include globals.local
				noblacklist /tmp/.wine-*
			[/file]

		3) You have to either initialize firejail steam using '--ignore=private-tmp'
		or add 'ignore private-tmp' on /etc/firejail/steam.conf
			[file: /etc/firejail/steam.profile]
				#private-tmp
				ignore private-tmp
			[/file]


	SOLUTION B:
		Solution B is Not desirable, will make gamescope games go slow, SOLUTION A is a better solution:

	0) Disable 'noroot' option by removing(or commenting) it on /etc/firejail/steam.profile:
		[file: /etc/firejail/steam.profile]
			#noroot
		[/file]

	1) Initialize FIREJAIL Steam
		firejail --caps.keep=chown,sys_admin,setgid,setuid steam
			or
		firejail --caps.keep=chown,sys_admin,setgid,setuid --caps.drop=linux_immutable  steam 
		^ VERY IMPORTANT: This step is probably making no different, check later.

	2) Look for the Process PID of steam under firejail:
		$ firejail --tree
		^ do not mistake steam's PID with Firejail's own PID.

	3) Mount /tmp/ as follows:
		mount -B /tmp/.wine-1000/ /proc/<pid_number>/root/tmp/.wine-1000/ && \
		mount -B /tmp/.X11-unix/ /proc/<pid_number>/root/tmp/.X11-unix/
			or
		mount -B /tmp/ /proc/<pid_number>/tmp/

		OBS: Using this doesn't work:
			mount -fo remount,ro /run/firejail/firejail.ro.dir/

FIXING NVIDIA ISSUE ON FIREJAIL WHEN RUNNING SOME PROGRAMS LIKE STEAM
	Some programs that may rely on NVIDIA Drivers may need to have drivers whitelisted upon sandbox instatiation:

		[file: /etc/firejail/<profile_name>]
			noblacklist /sys/module
			whitelist /sys/module/nvidia*
			read-only /sys/module/nvidia*
		[/file]

	source: https://bbs.archlinux.org/viewtopic.php?id=296675

	VERY IMPORTANT: Only use this solution, if you think you're being affected by this problem.

SETTING UP FIREJAIL TO USE PIPEWIRE SERVER AUDIO DEVICE WITH A DIFFERENT NETWORK DEVICE
	This only applies to pipewire audio settings that are running as PIPEWIRE SERVER.
	
	0) Fetch your Local IP Address
		$ ifconfig

	2) Either disable your firewall or allow connections coming from the Local Network IP Addresses.
	^ You'll need to go through reading and setting up IPTABLES on your linux system(or similar).

	3) Use the following command to initialize a sandbox with a new IP Address
		--net=<network_interface> --ip=dhcp --ip6=dhcp  --env=PULSE_SERVER=<ip_address> <program>

		Example:
			$ firejail --net=enp3s0 --ip=dhcp --ip6=dhcp  --env=PULSE_SERVER=192.168.1.67 steam
	
	4)(OPTIONAL) 

	
	
GETTING FILES/DIRECTORIES FROM FIREJAIL CONTAINERS
	Most of the time, firejail will initialize it's containers as read-only.

	Every firejail instantiation should have 2  PIDs, one for firejail and another for it's running child process,

	1) OPTION A: - Easy Way -
		1 - Easily find the PID by joinning the sandbox:
			$ firejail --join=<name|id>
			^ VERY IMPORTANT: 'id' and 'pid' are always different, never the same.
			^ the PID can only be obtained AFTER JOINNING, not when typing --join.
			^ The ID is only used internally by firejail for identifying each running proccess.

		2 - As soon as you have joined, you'll notice the sandbox's full-path being announced on the screen.
			Example:
				Switching to pid 65089, the first child process inside the sandbox
				Changing root to /proc/65089/root

		3 - Exit the sandbox, login as root, and 'cd' into /proc/65089/root/

		4 - Now you can finally copy the files you want.

	2) OPTION B: - Hard way - 
	you'll need the child process ID if you want to copy  files from the container in read-only mode:

		1 - Fetch the firejail child process PID:
			$ firejail --tree
		
		It'll look like something like this:
			97769:<username>::firejail firefox 
			  97770:<username>::/usr/bin/xdg-dbus-proxy --fd=4 --args=5 
			  97773:<username>::firejail firefox 

		^ 97773 is the child process

		2 - Once you've located the child process, login as root inside a terminal/shell:
			$ run0 
		
		3 - Change directory to:
			# cd /proc/<child_pid>/root/
				or
			# cd /proc/97773/root/ 
		
		4 - Now you can finally fetch the files you want normally using either  CP or  RSYNC tools.

-------------------
-------------------
-------------------
-------------------
INTRO TO FLATPAK
	Just like Firejail, Flatpak is used to containerize and run programs isolated from the main system, which in turn
boosts security. Another main feature is that flatpak allows programs to not  rely on current system's library,
relying on it's own packaged files and libraries. This allows the program to execute using different library versions than the ones
installed on the system, causing no issues with current system libraries located in /lib/ for example.

Because of this nature, flatpak also allows you to run applications that may not be available on your linux operating system yet.

Flatpack applications are usually located on "/var/lib/flatpak" for system wide apps, and ~/.local/share/flatpak for user installed apps.

	DEFAULT FLATPAK SETTINGS
		source: https://docs.flatpak.org/en/latest/sandbox-permissions.html

	INSTALLING FLATPAK APPS
		Installing apps is as easy as this:
			$ flatpak install --user https://flathub.org/repo/appstream/org.clementine_player.Clementine.flatpakref

	UPDATING INSTALLED FLATPAK APP
		Updating installed flatpack apps:
			$ flatpak --user update org.clementine_player.Clementine

	RUNNING FLATPAK PROGRAMS
		Running a flatpak app post-install:
			$ flatpak run org.clementine_player.Clementine
		
	UNINSTALLING FLATPAK PROGRAM
		Uninstalled already installed flatpak app:
			$ flatpak --user uninstall org.clementine_player.Clementine

	REMOVING  UNUSED DATA
		Removes unused contents that might've been installed but not currently in use by anything:
			$ flatpak remove --unused -y
	
	CHECKING DISK SPACE USED BY FLATPAK
		Disk space:
			$ du -h /var/lib/flatpak

	LISTING ALL FLATPAK APPLICATIONS INSTALLED
		Listing all flatpak apps installed on system:
			$ flatpak list

	SHARING THE HOST FILESYSTEM
		Sharing the host filesystem with a flatpak app container:
			# flatpak override <Application_ID> --filesystem=host
		
	GRANTING ACCESS TO A FILESYSTEM, FOLDER OR DIRECTORY
		Granting access to a given filesystem/folder/directory:
			# flatpak override <Application ID> --filesystem="<unusual_path_here>"
				or
			# flatpak override <Application ID> --filesystem="/path/to/data"


	GRANTING READ-ONLY ACCESS TO A FILESYSTEM, FOLDER OR DIRECTORY
		Granting read-only mode access to filesyste, folder or directory:
			# flatpak override <Application ID> --filesystem="/path/to/data":ro
		
	RESETING PERMISSIONS TO DEFAULT
		Reseting user set permissions to default:
			# flatpak override --reset <Application ID>
			^ If no application ID is given, then this will reset all flatpak Programs to default settings!
		
	BLOCKING ACCESS TO A FILESYSTEM, FOLDER/DIRECTORY
		Blocking permission access to a given folder, directory or filesystem:
			# flatpak override <Application ID> --nofilesystem="path/to/data"

	DISPLAYING ALL USER EDITED PERMISSIONS
		Lists all user permissions set for a given application:
			$ flatpak override --show <Application ID>

	DISPLAYS DEFAULT PERMISSIONS
		Displays all default permissions for a given installed flatpak application:
			$ flatpak info --show-permissions <Application ID>

MISC LINKS
	https://docs.flatpak.org/en/latest/sandbox-permissions.html		#More on Flatpak Permissions sets(VERY IMPORTANT)
		https://docs.flatpak.org/en/latest/sandbox-permissions.html#reserved-paths

FLATPACK SOURCES
	https://flathub.org/apps/org.clementine_player.Clementine
-------------------

FINDING DKMS MODULE STATUS
	# dkms status

FINDING OUT-OF-TREE LOADED KERNEL MODULES
	# grep \(.*\) /proc/modules

LOOKING IF A CERTAIN FEATURE IS ENABLED IN THE SYSTEM
	# lspci -vvv | grep "ASPM .*abled"

LIMITING SYSTEM RESOURCES ON A USER BASIS
	The file /etc/security/limits.conf Can be used to setup limits to system resources on a user basis, 
	READ MORE using man pages: 
		$ man pam_limits
			and
		$ man limits.conf

RUNNING PROGRAMS WITHIN AN ALREADY EXISTING WINE OR PROTON PREFIX
	Use  winetricks / protontricks to  explain how  to run  programs  inside them.
		1 - Execute the program or game as you normally would in order to create the wine/proton prefix 
		2 - Use either winetricks / protontricks and select the program that you have created the wineprefix/protonprefix for.
		3 - Now choose 'Select the default wine prefix'
		4 - Programs can now be installed using 'winecfg'
			4.1 - Go to 'Applications' tab and click 'add application, then finally select the .exe application.
			4.2 - You can also use the 
		5 (Optional to #4) - you can install/execute .exe, .msi or .msu file using the 'Run an arbitrary executable' option.
		6 - Programs can now be run through 'taskmgr', 'explorer' or 'Run wine cmd shell' options.

-------------------
-------------------
-------------------
-------------------
THINGS TO KNOW:
#1:
	pacman.conf has a few packages on ignore list, this list includes linux-hardened which shouldn't be updated to maintain system stability, pahole package is ignored to prevent issues when building the initramfs for nvidia-dkms drivers, qemu is ignored due to it's huge size, proton-custom-bin because updates can break stuff on steam.

https://github.com/boppreh/keyboard 	#Hooks and simulate global keyboard events on Windows and Linux

#2:
Removing gnome xorg-apps, xorg-drivers xorg once you have a working setup with the following command:
	$yay -Rnu xorg-apps xorg-drivers xorg gnome
		or
	#pacman -Rnu xorg-apps xorg-drivers xorg gnome

Helped improve performance quite a bit! -Rnu Removes packages without breaking dependency.
Try to remove gnome with -Rns argument to see which packages remain and remove the left-over of gnome's packages.

#3:
Running "#pacman -Rns $(pacman -Qtdq)" from time to time helps keeping the system clean from unused packages.

#4: TROUBLESHOOTING NVIDIA / BROKEN XORG 2023
nvidia-utils require glvnd to work properly, so make sure that all package files are working correctly:
	pacman -Qkk nvidia-utils nvidia-dkms libglvnd

if any of the files are modified/broken, just fix them with:
	pacman -Sy --overwrite '*' <package_name>

Something about libglvnd - glvnd and glx:

as of 2023, startx doesn't work with nvidia's glx module("/usr/lib/nvidia/xorg/libglxserver_nvidia.so.525.89.02"),
from an nvidia developer in 2019, nvidia-utils comes bundled with two versions: glx and non-glx versions  as follows:

#4
DO not enable Fast Boot on BIOS(Intel CPU Motherboard), it's only meant for Windows and only newer windows distro(s).

---
"Did you install the non-GLVND GLX libraries? I can’t tell from your log because glxinfo is not installed and your installer log is from an install that was aborted because an X server was running.

If you did, please try reinstalling and selecting the GLVND libraries instead." - By Nvidia Developer
^ source: https://forums.developer.nvidia.com/t/xorg-error-seg-fault-with-nvidia-driver-update/80711
---

However, as of 2023, there aren't two versions - requires checking - anylonger,
the only way to use Xorg manually without using SDDM or Wayland/Xwayland(really?) is to disable GLX Module(check man xorg.conf).

Anyways, the problem only happens because using nvidia-dkms requires xorg.conf to have this added:

	[file: /etc/X11/xorg.conf]
		Section "Files"
		    ModulePath      "/usr/lib/nvidia/xorg"
		    ModulePath      "/usr/lib/xorg/modules"
		EndSection
	[/file]

Not having this will result in SDDM failing to boot due to modules not being found.
making /usr/lib/nvidia/xorg/libglxserver_nvidia.so.525.89.02 a must have for SDDM to work.

However, removing the above file and creating a symlink to /usr/lib/libGL.so.1.7.0
made startx and xorg work without outputting any errors, however SDDM Fails!

BUT, restoring the files didn't fix the issue, because somehow libGL.so.1.7.0 was replaced for nvidia's libglxserver file.
Installing libglvnd using --overwrite '*' was the only way to fix it.

TODO #1:
Looking at /var/log/Xorg.0.log, the nvidia module is called "glxserver_nvidia"
Maybe it's possible to disable glxserver_nvidia just for xinit through the /etc/X11/xinit/xinitrc file!


The following below lists all nvidia-util files
	$pacman -Ql nvidia-utils

TODO #2:
	Talk about VCS Clients and VCS Source:

	VCSCLIENTS=('bzr::bzr'
		    'fossil::fossil'
		    'git::git'
		    'hg::mercurial'
		    'svn::subversion')

TODO #3:
	Explain pkgver and pkgrel on PKGBUILD files
		ex: <package_name> 1.3-2
		^ <package_name> <pkgver>-<pkgrel>
TODO #4:
	Write more about PKGBUILD files now that we have knowledge on how it looks like a MAKE file
	except that unlike MAKE Files, PKGBUILDS tell Archlinux Package Manager how to install packages a given package.
	
TODO #6:
	Explain how to Install devtools and how pkgctl works for cloning git repositories and fetching PKGBUILD scripts
	for archlinux.

TODO #7:
	Compiling ur own zen-kernel: 
		1. yay -S linux-zen-git
		2. Go to '~/.cache/yay/linux-zen-git/src/build' and type 'make menuconfig'
		3. if you don't want to 'make menuconfig' download this one and edit to allow 'IVYBRIDGE' and Disable
		   generic CPU Architecture: 
			3.4 https://raw.githubusercontent.com/archlinux/svntogit-packages/packages/linux-zen/trunk/config
		4. cd ../../ and type: $makepkg -si
			4.1 - If you need to re-install for any reason: $makepkg -i


TODO #8:
	Explain that pkgctl can be used to clone archlinux packages for building & compiling,
	using: $pkgctl repo clone --protocol https <package-name> and then editing PKGBUILD and invoking makepkg -si

TODO #10:
	Explain how to switch the system default C/C++ Compiler,
	[file: /etc/makepkg.conf]
		...
		export CC=clang
		export CXX=clang++
		...
	[/file]

TODO #11:
	Explain git-worktree, git-tag, git-gui, gitk and look into $man git

TODO #12:
	Explain make, cmake, cpack, cmake-gui, ninja
	$cat CPackSourceConfig.make | less
	cpack --help-variable-list | less
	cmake --list-presets
	ninja -t list

TODO #13:
	When compiling proton-ge-custom it's possible to enable ESYNC or FSYNC(Don't enable both!"), Futex is the most optimal.
	ESYNC was an earlier problematic version of FSYNC.
	[file: ~/.cache/yay/proton-ge-custom/]
	    export WINEESYNC=0
	    export WINEFSYNC=1
	[/file]

TODO #14:
	Why not to use scanf:
		https://www.youtube.com/watch?v=RAbwchpvO1s - Be Careful When Using scanf() in C | Channel: NeuralNine
TODO #15:
	Explain how 'pstree' can be used to find out init system on the current installed linux,
	answer: the first line of the process tree will display the name of the init system,
	it is the first process and it's PID equals 1(PID=1).
	
	Amongst the list of init systems, there are these: 
		systemd, Runit, OpenRC, sysVinit, etc.

TODO #16:
	Lecture about distrobox and bedrock.
distrobox requires podman instead of docker on archlinux.

distrobox create --nvidia -n <installation_name> -i <image>		#Creates a distrobox installation, nvidia drivers
distrobox create -c <existing_installation> <new_installation>		#Clones existing installation, useful
									^ for fixing issues.
distrobox list								#Lists all available installations
distrobox rm <installation_name>					#Deletes a distrobox installation
distrobox rename <installation_name> <new_installation_name>		#Renames a distrobox
distrobox enter <installation_name>					#Enters a given distrobox installation

-c 	#clones
-i	#installation image
-ap	#additional packages
	^ example:
		distrobox create -i archlinux:latest --init --additional-packages "systemd" -n test


	RUNNING DISTROBOX WITH LD_PRELOAD
		In order to pass LD_PRELOAD var to distrobox, you must install mimalloc package inside the given
	distro image you want to run, after that's done you need to find where the libmimalloc.so image is located,
	then finally you should exit distrobox and launch it as following:

		LD_PRELOAD="/usr/lib64/libmimalloc.so.2" distrobox enter <installation_name>
		^note that this is the case for fedora and libmimalloc could be located elsewhere in other distros.

		ALTERNATE SOLUTION:
			Load the distrobox image/distro and create directories and create symlink on "/usr/lib/libmimalloc.so"
			pointing to "/usr/lib/lib64/libmimalloc.so.2" that way you can pre-load distrobox and
			all programs running inside the distrobox. ex:
				#mkdir -p /usr/lib/; ln -s /usr/lib/lib64/libmimalloc.so.2 /usr/lib/libmimalloc.so

			HOWEVER, some programs might not run at all with libmimalloc. 
			If that's not a desirable behavior then it's possible to instantiate the distrobox 
			with an empty LD_PRELOAD environment variable :
				$distrobox enter fedora_36_2024 -- LD_PRELOAD= zsh -l
							or
				$distrobox enter fedora_36_2024 -- LD_PRELOAD= sh -l

			This way, only the distrobox is loaded with libmimalloc!


		Read more on 'USING A DIFFERENT MEMORY ALLOCATOR(MALLOC) FOR LAUNCHING PROGRAMS'.
TODO #17:
	Lecture about how 'terminology' can split terminals and create new windows
	using ^Page-Up, ^Page-Down and ^shift+t.

TODO #18:
	Lecture about podman/docker.
	
	podman uses ~/.local/share/containers/storage for image/container storage.
	docker uses /var/lib/docker/ for image/container storage.

	source: https://wiki.archlinux.org/title/Linux_Containers
		https://wiki.archlinux.org/title/docker
			https://en.wikipedia.org/wiki/OS-level_virtualization
			^ "Linux containers are all based on the virtualization, isolation, 
			^ and resource management mechanisms provided by the Linux kernel, 
			^ notably Linux namespaces and cgroups." - By Wikipedia


		---OVERHEAD:
				Operating-system-level virtualization usually imposes less overhead than full virtualization because programs in OS-level virtual partitions use the operating system's normal system call interface and do not need to be subjected to emulation or be run in an intermediate virtual machine, as is the case with full virtualization (such as VMware ESXi, QEMU, or Hyper-V) and paravirtualization (such as Xen or User-mode Linux). This form of virtualization also does not require hardware support for efficient performance. 

TODO #19:
	Explain SysV Runlevels
	source: https://wiki.archlinux.org/title/SysVinit#inittab

TODO #20:
	Using CRIU to save a process state and reloading it at a later time.
	https://criu.org/Usage_scenarios#Snapshots_of_apps


TODO #21:
	Does this works?

	PROTON:
	WINE_CPU_TOPOLOGY=2:0,1,2,3,4 %command%

TODO #22:
	Talk about Fedora AUR-like Repository and how to install it using: #dnf install copr-cli
	and how to use it.

TODO #23: - DONE
	Talk about how to use pacman -Ql to list file ownership of given a package, for example:
		$pacman -Ql clamav
	
TODO #24: - DONE
	Explain how to use clamav for doing anti-virus system scan.

TODO #25:
	Explain how to fetch keys from gpg in other installed linux distributions located in different disks
	usually 'arch-chroot' and 'su <username>', finally gpg --list-keys or gpg --list-secret-keys

	sudo gpg --export --armor 42221DF76869F2EC22C8D97BDF2F4C3D5231B78E

TODO #26:
	Explain what psi=1 does

TODO #27:
	Explain how to change linker 
	-fuse-ld=mold

TODO #28:
	Write about Intel Powerclamp and why it's important to enable it soon after installation 

TODO #29:
	Lecture about how to use lsmod to list enabled kernel modules in conjunction with 'less' command 
	after using modprobe to confirm that the module has been enabled.

TODO #30:
	Talk about fstrim and it's importance when creating virtual image sparse files for use with virtual machines
	or loop devices like qcow2 and raw format! However, fstrim might need to be run inside the running virtual machine,
	in this case the block device(disk) needs to support trim option. Same for the host, trimmage must be supported
	on the real actual disk in order for the unused space to be reclaimed by the host.

	if conditions are not met, then virt-sparsify might have to be used, however virt-sparsify can only be used after
	cleaning guest free unused spaces with 'zeroes'.

	this is why sparse files are very bad in general, removing the files will not reclaim space on the disk!!!
	this is issue most commonly happens on. 

	final attempt that works is to use gparted to reclaim unused disk space after deleting the actual file.

TODO #31:
	Backtracking Memory Issues
		Source: https://www.kernel.org/doc/html/next/admin-guide/blockdev/zram.html
			$cat /sys/block/zram0/io_stat
			^ first column: failed reads
			^ 2nd column: failed writes
			^ 3rd column: invalid I/O
	
			^ Setting up a zram device may be required!
		Source: https://www.kernel.org/doc/html/next/block/stat.html
			$cat /sys/block/zram0/stat
	
		Lists available compression algorithms:
			$cat /sys/block/zram0/comp_algorithm

		lz4 is faster, zstd compresses better.

TODO #32:
	Check later
		Filesystem features:
			/sys/fs/ext4/features/verity
		Block devices: 
			/sys/block/
TODO #33:
	For qemu:
		ekd_ovmf firmware
TODO #34:
	explain bcache, cachefs

TODO #35:
	explain /etc/fonts

TODO #36:
	Use winetricks to add japanese/asian fonts to wine.

TODO #37 - VERY IMPORTANT - :
	Write about the difference between having "discard" option as a 'Default mount option' rather than
	being a 'Filesystem feature' using dumpe2fs.

	The discard option as a mount option forces the user to manually trim the SSD Disk
	by using #fstrim --all.

	a discard feature as 'filesystem feature' allows trimmage to happen every single time a file is deleted,
	which kinda makes the system slower.

TODO #38:
	Explain /etc/issue, /etc/issue.net
	explain how you can customize logind messages using those.

	https://www.cyberciti.biz/faq/howto-change-login-message/

TODO #39(OPTIONAL):
	Explain 'filesystem' package and explain that it's installation 
	creates all base linux directories: /dev/, /boot/, /etc/, /home/, /var/ 
	along with it's sub-directories.

TODO #40:
	systemctl soft-reboot is userspace reboot, the kernel keeps running, the userspace is restarted.

	VERY IMPORTANT: it's still recommended to close applications before going for a soft-reboot.
	if applications are left running, things like containers and virtual machines may be left in a corrupted state.

	Only use this if the system is 100% okay and if it's not okay, do a full-system restart.


TODO #40 - DONE:
	Is switching wine prefix graphic's renderer from opengl to vulkan safe through winetricks?
	ANSWER: NO!

TODO #41:
	WHAT IS DXVK AND VKD3D?
		DXVK_HUD=full it's just used to show data on HUD about framerate and other stuff for DXVK games.
		the DXVK_HUD is only meant for displaying HUD Data on screen about DXVK.

		DXVK is a layer that translates Directx Calls into Vulkan API, the game is still running on Directx.
		there'll never be a performance improvement from using DXVK, this is only meant for OSes like linux
		to be able to run directx games. Mainly translates Directx 9-11 to Vulkan.


		VKD3D is the same thing as DXVK but translates Directx 12 to vulkan.
TODO #42:
	explain bootctl

TODO #43
	yay -P --stats


TODO #44:
	NEW HARD DRIVE TOOL:
	udiskctl can be used to mount removable medias, udiskie to automount them.
 	Udisks2 is a Daemon, tools and libraries to access and manipulate disks, storage devices and technologies

	It can also easily mount loop devices.

	#udisksctl monitor

	Completely turn off a HDD, not meant for SSDs:
		#hdparm -B 255 -S 0 /dev/sda
		^ AVOID USING THIS AS IT COULD PERMA DAMAGE YOUR HDD,
		If such thing happens, consider resetting the BIOS by removing the battery from motherboard.

	systemctl list-units --type=automount
	journalctl /usr/lib/udisks2/udisksd
		journalctl -u udisks2.service 
		journalctl -u sddm

	also look in /var/log:
		$ls -la /var/log/
	
	/var/log/boot.log usually holds information about boot proccess
 
	cat /usr/lib/udev/rules.d/80-udisks2.rules
	cat /usr/lib/systemd/system/udisks2.service
	
	TODO:
		How to open journal files?
			#journalctl --file /var/log/journal/1aff91d6b7cb44acb83ed4c3a6493abb/user-1000@36511ed2eb864c3493a1b08bd9dfa340-00000000003d879b-00060e0ef4f07d2b.journal

		How to find logs by date in case they get corrupted?
			ls -laR /var/log/journal/1aff91d6b7cb44acb83ed4c3a6493abb | grep "Jan..7"
TODO #45:
	Checking system crashes:
		journalctl -u <tab-key>

		usually systemd-coredump@ files can be checked in search for a possible system/program crashes.
TODO #46:
	Learn to use SED, example:
		$sed -i "s/\[\"api.alsa.period-size\"\] = [[:digit:]]*/\[\"api.alsa.period-size\"\] = 2024/" ~/.config/wireplumber/main.lua.d/50-alsa-config.conf
sed -i "s/\[\"api.alsa.headroom\"\] = [[:digit:]]*/\[\"api.alsa.headroom\"\] = 22192/" ~/.config/wireplumber/main.lua.d/50-alsa-config.conf
	^ Leaving example here, don't use 50-alsa-config.conf nor 50-alsa-config.lua, it'll cause audio stutters!

	sed will replace the default values with the given new values above in the given text file.

TODO #47:
As of 2023, There is a known systemd bug that automounts efi:
https://bbs.archlinux.org/viewtopic.php?id=287760 

^ To solve this, add this to the kernel parameters in /boot/grub/grub.cfg : systemd.gpt_auto=no

More Info on:
https://man.archlinux.org/man/core/systemd/systemd-gpt-auto-generator.8.en

/usr/share/dbus-1/session.conf			#Eavesdrop here isn't a problem at all
/run/systemd/generator.late/efi.autmount	#Why does it automounts? Probably a bug.


TODO #48:
	MOUNTING A BIN/ISO IMAGE
		Install fuseiso and do as follow:
			1) Mounting the image:
				$fuseiso <image_file> <mount_point>

			2) Umounting:
				$umount <mount_point>

TODO #49:
	Talk about how 'env' runs a program with modified environment variables.

TODO #50:
	Talk about $umount --lazy <mount_point> to force umount of filesystems, this requires an immediate system reboot
	to prevent issues.

TODO #51:
	dm-crypt allows loop-devices to be encrypted at block-device level.
	this is a good approach for encrypting certain parts of the system,
	source: https://wiki.archlinux.org/title/Security
	^ read data-at-rest encryption

TODO #52:
	lists open ports on the system:
		#ss -lpntu

TODO #53:
	Tell the difference between wine, wine-ge, proton, proton-ge, lutris-ge. 
	
	wine-ge is a fork of wine, it's meant to be a bleeding edge version of wine,
	for running windows programs and games alike, but unlike proton it doesn't applies patches/fixes
	to make the programs(or games) run; this in turn requires user manual intervention to make the program work.

	proton is a fork of wine meant to run steam gamesonly , it's able to automatically apply patches/fixes to 
	specific games that have issues running normally under wine. proton-ge is the bleeding edge version of proton, 
	containning most up-to-date proton feature patches and bug fixes.

	lutris-ge is a proton fork able to run non-steam windows games and windows programs. 
	Unlike wine and wine-ge, lutris-ge is capable of applying patches just like proton and proton-ge.
	

	Write about proton dir:
		/usr/share/steam/compatibilitytools.d/

	Write about lutris runtime dir:
		~/.local/share/lutris/runners/wine/

	Games folder for Lutris:
		~/Games/gog-galaxy/drive_c/Program Files (x86)/GOG Galaxy/Games/

	Games folder for steam:
		 ~/.steam/steam/steamapps/common/
	
TODO #54:
	Explain how to protect the /.fscrypt dir using GPG Signing Keys.
	make a reference to "ENCRYPTING DIRECTORIES WITH SECURITY KEY INSTEAD OF PASSWORD | 2024"
	
TODO #55:
	When writing scripts for unlocking key secured directories, tell the scripts
	to use a temporary directory like /tmp/. and before running the script copy or symlink the key MANUALLY.

	2 big benefits from doing this:
		1. This will avoid storing the key's location on the actual script.
		2. You can easily change the location of the actual key without changing the whole script. 

TODO #56:
	Update program version on the zicroram manual

TODO #57:
	Talk about iptables and how to use it instead of firewalld.

TODO #58:
	WHAT ARE THE BOOTPC, BOOTPS SYSTEM PORTS THAT STEAM USES?
	if you run 'ss -lsptu' on your linux system while running steam, you'll notice a bootpc port name
	and if you run 'ss -lsnptu' you'll see it's number.

	SO WHAT ARE THEY?
	"Sun Microsystems had announced that they would sell “diskless workstations” — computers that consisted of CPU, memory and display but needing no disk drive because of this crazy fast 10Mbps wired thingie sticking off the back called “Ethernet”. It was revolutionary.

OK, if you’d been paying attention, it was in point of fact NOT revolutionary — that was the whole point of the SUN research project at Stanford which “birthed” Sun Microsystems, and was dead-ass common in the world of distributed operating systems research where I lived at that point. But for something that someone not steeped in that world to just walk up and buy something like that? Absolutely revolutionary.

But it had, you know, issues.

Like… when the diskless system boots, how exactly does it DO that? I mean, there’s no disk drive with a copy of the OS…

Turned out, that was only part of the issue. Underneath that was “how do I even know what my network address is when I wake up”

So, this protocol — largely perfected by the legendary John Gilmore. based on Bill Croft’s work at Stanford — solved that problem. The querant system would beacon a broadcast request, asking what it’s IP address was, then using that IP address use TFTP to download it’s designated boot file.

Sounds obvious, but that’s only because we’ve been doing it for 40 years.

Note that to some extent this replaced the earlier RARP (reverse ARP) protocol, and the TFTP Boot protocol. And then by the time that booting over the network was no longer cool, that port was grabbed for the now-more-useful DHCP protocol which largely did the same thing: answered the “so what’s my IP address supposed to be” question.

The full lineage of this starts with RFC 903 (RARP) and RFC 906 (TFTP Boot) in 1984, then RFC 951 (BOOTP) in 1985, then RFC 2131 (DHCP) in 1997." SOURCE: https://www.quora.com/Why-are-port-67-and-68-registered-as-BOOTPS-and-not-DHCP | access: Feb 27 2024

TODO #59:
	Talk about syslog and rsyslog.

TODO #60:
	more find, awk and grep solutions

TODO #61:
	BlendOS as replacement for archlinux; is it good or just bloat tho?

TODO #62:
	Read and write something about 'homectl', 'man systemd-homed' and 'userdbctl'

TODO #63:
	Write something about:
		# cpupower frequency-set -g performance
			and
		$ cpupower frequency-info

---------------------------------------------
VERY IMPORTANT REMEMBER / WARNING #0:
	Remember if ever doing rm -r <dir> after umounting a given device may delete all files from that device
	in case the umount has failed, which means the device is still mounted and all data will be deleted.

REMEMBER #1:
	==> Before using VMware, you need to reboot or load vmw_vmci and vmmon kernel modules (in a terminal on root: modprobe -a vmw_vmci vmmon)
	==> You may also need to enable some of the following services:
	- vmware-networks: to have network access inside VMs
	- vmware-usbarbitrator: to connect USB devices inside VMs
	
REMEMBER #2:
	If you ever need to use arch-chroot or chroot and even fakechroot as a security measure for running a software,
	just go straight for distrobox, enough isolation will be provided there.

REMEMBER #3:
	GTK4 2.14 is coming with VULKAN renderer enabled by default,
	for now you can change it back to OpenGL for performance since GTK's Vulkan Renderer isn't optimized yet,
	by setting the following environment variable on /etc/environment
	[file: /etc/enviornment]
		GSK_RENDERER=gl
	[/file]

REMEMBER #4:
	util-linux coreutils install POSIX utilities on the system.
	^ source: https://wiki.archlinux.org/title/General_recommendations

REMEMBER #5:
	When creating executable scripts for automating tasks with bash scripts, always make sure to 'cd' into directories
	before executing it or else some programs may not work, for example:

[file: /bin/diablo_tchernobog]
	#!/bin/sh
	cd "~/Games/gog-galaxy/drive_c/Program Files (x86)/GOG Galaxy/Games/Diablo/tchernobog/"; 
	wine "~/Games/gog-galaxy/drive_c/Program Files (x86)/GOG Galaxy/Games/Diablo/tchernobog/Tchernobog.exe"
[/file]

REMEMBER #6:
	wineconsole allows the user to execute commands before running wine programs.

REMEMBER #10:
	__GL_SHARPEN_ENABLED=1		#for games/apps with OpenGL
	ENABLE_VKBASALT=1		#for games/apps running under Vulkan
	WINE_FULLSCREEN_FSR=1 WINE_FULLSCREEN_FSR_STRENGTH=5 PROTON_ENABLE_NVAPI=1 ENABLE_VKBASALT=1 gamescope -h 720 -H 1440 -S integer -- %command%

REMEMBER #11:
	When writing scripts for unlocking key secured directories, tell the scripts
	to use a temporary directory like /tmp/. and before running the script copy or symlink the key MANUALLY.

	2 big benefits from doing this:
		1. This will avoid storing the key's location on the actual script.
		2. You can easily change the location of the actual key without changing the whole script. 


-----------------------------------------------
VERY IMPORTANT:
	If your system crash or has some uknown issue, this gitlab tracks all archlinux packaging problems:
	https://gitlab.archlinux.org/groups/archlinux/packaging/packages/-/issues/?sort=created_date&state=opened&search=crash&first_page_size=20

LINKS TO CHECK:
	https://genr8rs.com/Generator/Fun/LeetSpeakGenerator		#L33t language generator | Leet language generator
	https://security.archlinux.org/					#Archlinux Security Track
	https://opensource.com/article/18/11/udev			#Introduction to udev
	https://wiki.archlinux.org/title/Security			#Important linux security tips
	https://wiki.archlinux.org/title/General_recommendations	#General Recommendations
	https://winworldpc.com/home					#Contains CD-Rom data and 
									^ information about windows and dos
	https://archive.org/details/windows-95-vmdk			#WIN95 VMDK Image
	https://www.dosgames.com/					#DOS Games list
	https://github.com/Hi-Angel/clock_gettime_override		#Game Hack | Useless

	https://www.grymoire.com/magic.html				#A website containing a collection 
									^ of useful incantations for wizards, 
									^ be you computer wizards, magicians, or whatever
	https://vimregex.com/						#VIM Regex
	https://www.shellcheck.net					#SH/BASH/ZSH Spellcheckker
	https://github.com/mikeroyal/PipeWire-Guide			#Pipewire guide with reference to all audio tools
									^ Available on Linux.

	FILE UPLOAD:
		https://gist.github.com/	#Instantly Share code, notes and snippets.
						^ This requires a GITHUB Account.

		https://www.uguu.se/		#File Upload(Date: 2023.1)
		https://pastes.io/		#Text Upload(Date: 2023.1)

	GAME LINKS:
		https://x.com/abinarybbs	#Archaic Binary BBS

	MISC LINKS:
		https://www.clickspeedtester.com/keyboard-latency-test/			#Keyboard Input Delay Test
		https://cpu.userbenchmark.com/Compare/Intel-Core-i7-3770-vs-Intel-Core-i5-2500K/1979vs619 - CPU BENCHMARKING
		https://github.com/BaRRaKudaRain/PCem-ROMs
		https://makefiletutorial.com/#what-alternatives-are-there-to-make
	
	STUDY LINKS:
		https://en.wikipedia.org/wiki/Percent-encoding	#Percent Encoding
		https://en.wikipedia.org/wiki/Alt_code		#Alt Code
		https://en.wikipedia.org/wiki/Character_encoding	#Code Pages, Code Units, Code Points
		https://en.wikipedia.org/wiki/Code_point		#Code Point
		https://en.wikipedia.org/wiki/Wide_character		#Wide Char
		https://stackoverflow.com/questions/367633/what-are-all-the-common-undefined-behaviours-that-a-c-programmer-should-know-a	#List of C/C++ Undefined Behavior
		https://en.cppreference.com/w/cpp/language/ub	#More C/C++ Undefined Behavior
		https://en.cppreference.com/w/cpp/types/size_t	#Size_T data type can be used on very big loop iterations
		https://en.cppreference.com/w/cpp/types/climits	#C/C++ Numeric Limits Interface
		https://en.cppreference.com/w/cpp/types/integer	#Fixed width integer types

MISC STUFF:
AI / ARTIFICIAL INTELLIGENCE: 
	https://stablediffusionweb.com/ - Generates images from text models, it can create photo-realistic images based on text-data.
	https://civitai.com/ - AI Database, contains both models that can be fed to AI engines.
-------
-------
tmp:
cd <Folder_Containning_HDImages>;
ls ./ | awk '{system("file="$0"; rm ../${file%%.*}.*")}'

---------------------
---------------------
---------------------
---------------------
HOW TO USE THE 'LESS' COMMANDLINE TOOL
	It's a very common thing to pipe linux commands to 'less', this is how you use it:

		1 - press 'q' to exit, 
		2 - ctrl+f for forward page scroll
		3 - cltr+u for upwards page scroll 
		4 - shift+g for moving to the end of the page
		5 - press 'g' twice for moving to the top of the page
		6 - press '/' for querying a regex search towards the end of the page from the current page.
		7 - press '?' for querying a regex search towards the beginning of the page from the current page.
		7 - press 'n' to go for next occurrence of the queried search.
		8 - press shift+n to go for previous occurrence of the queried search.
---------------------
---------------------
---------------------
---------------------
FRAME INTERPOLATION
	Frame Interpolation / Motion interpolation uses AI to generate +1 frame between 2 different frames.
This increases perceived frame-rate when playing videos with lower framerate than the one offered by the monitor.

	A Display capable of 60Hz(60 Frames per second) will run 30fps content(games/movies/videos) at 30 framerate per second,
Frame interpolation / motion interpolation would then insert an extra frame for every existing frame, allowing it to run at display capable framerate instead (on this case).

	Example: 1) Nvidia Smooth Motion.
			Meant for DX11 and DX12 games only.
		 2) Elise-ng
		 	https://fly.elise-ng.net/news/fly-elise-ng-6-2-1-software-release/
		 3) SVP Project
		 	Meant for video play.
			On archlinux, you can install it under 'svp-bin'.

---------------------
---------------------
---------------------
---------------------
MIPMAPPING
	Mimapping allows textures to be changed depending on far the player is from the obejct. 
This way objects that are 'far away' will load a different texture that looks better from far away and also uses less resources.
When this same object is closer, then the texture is replaced with a higher quality one, to make it look better upclose.

Mimapping has to be built on a per game basis.
---------------------
---------------------
---------------------
---------------------
USING RAMDISK UTILITY IN LINUX | ZICRORAM | [PERFORMANCE]
	Zicroram is available via github for those under GNU Linux distributions :
		https://github.com/srmfx/Zicroram
	
			
	0) After downloading it, install the program using the following command:
		$ zicroram --install-wizard

	1) Read the Manual Page for the program, it contains helpful info and extra commands:
		$ man zicroram

		Note: always read the manual before using the program.

	2) Listing full commands from the program:
		$ zicroram --help

	3) Adding program/directories is as simple as:
		$ zicroram --add <path_to_data>
			or
		$ zicroram -a <path_to_data>

		Note: this will create a symlink to the program in /usr/bin/, read man pages carefully.
	
	4) Removing programs from RAMDISK can be simply done as this:
		$ zicroram --remove
			or
		$ zicroram --remove <some_program>
			or
		$ zicroram -r
	
	5) Data Backups can be done from RAM to DISK using the following:
		$ zicroram --mbackup
			or
		$ zicroram -mb
	
	6) Automatic Data Backups can be done using the following:
		$ zicroram -mtbackup
			or
		$ zicroram -mtbackup <program_name>
			or
		$ zicroram -mt
	
	7) Restoring data back to disk after power blackouts or any forced shutdown:
		$ zicroram --restore

		Note: This will simply move the data back to it's original place/name in disk.

	8) Re-adds data to RAM after power blackouts or any forced shutdown (if you haven't used --restore yet):
		$ zicroram --re-adds


	CREATING .ZICRORAM FILES
		source: https://github.com/srmfx/Zicroram?tab=readme-ov-file#creating-zicroram-files
		
		A file with a .zicroram extension can be optionally created to make it easy adding programs and/or group of programs into ram 
		in such a way the user doesn't have to worry about creating a shell(bash/zsh) script.

		To do this, simply create a file with an extension called '.zicroram', then make sure each line corresponds to a directory or 
		file of your choice; you can then add as much data(ex.: pictures, songs, etc..) as you want in this single file. example:

			[file: ~/my_programs.zicroram]
			    #GIMP - Image Editor:
				/usr/share/gimp
				
			    #GVIM - Text Editor:
				/usr/share/gvim

			    #Personal Files/Documents:
				~/Music
				~/Pictures    #My Pictures
				~/Documents    #My Documents
			[/file]
			Note: you SHOULD NOT have to use the notation for file that is located between brackets: '[' and ']'. 
			The brackets are only meant so that the user understands those are the pertaining contents of a given file.

		Now you can add all data using a single command:
			$ ./zicroram --add ~/my_programs.zicroram

		RULES FOR IDENTATION AND COMMENTS IN .ZICRORAM FILES
			source: https://github.com/srmfx/Zicroram?tab=readme-ov-file#rules-for-identation-and-comments-in-zicroram-files

			As you have seen in the example above, you can add comments to lines, create comment lines and make use of text-ident
			for better readability and maintainability of the .zicroram files; for doing this you need to follow these simple rules below.

			1 - You can create text identation using Tabulation(Tab) Key or white(blank) space key.

			2 - For creating a comment line, you can add '#' character at the start of a line.

			3 - To create a comment line at the same line as an existing data, you can use Tabulation(Tab) Key and '#' character.

	ADDING PROGRAMS/DATA TO RAM DURING SYSTEM STARTUP USING ZICRORAM
		source: https://github.com/srmfx/Zicroram?tab=readme-ov-file#adding-programsdata-to-ram-during-system-startup

		Adding programs into ram in this specific case can be achieved by adding 'zicroram --boots ..' inside your .bash_profile( for bash users ) 
		or .zprofile ( for zsh users ) file:

			Note: The system-winde configuration files will make sure all users under the specific shell,
			while user configuration files will make sure programs are added to ram for that one user under the specific shell.

			for ZSH users:
				A) system-wide configuration file:
					[file: /etc/zsh/zprofile]
						zicroram --boots ~/my_programs.zicroram
					[/file]

				B) user configuration file:
					[file: ~/.zprofile]
						zicroram --boots ~/my_programs.zicroram
					[/file]

			for BASH users:
				A) system-wide configuration file:
					[file: /etc/profile]
						zicroram --boots ~/my_programs.zicroram
					[/file]

				B) user configuration file:
					[file: ~/.bash_profile]
						zicroram --boots ~/my_programs.zicroram
					[/file]

			Note that you do not have to use the notation for file that is located between brackets: '[' and ']'. 
			The brackets are only meant here so that the user understands this is the contents of a file.

		The --boots instruction outputs to screen a copy-to-ram progress and makes the user wait for the whole process to finish, 
		however depending on the program and/or how fast your hard drive is, it's possible to bypass this step and avoid all the 'waiting' during system startup; 
		if such is the case, then use --bootsc instead of --boots - However, currently it's not possible to check copy progress after bypassing it with such option.

		Check the manual after installing for more information on --boots and --bootsc:

			$ man zicroram
			    or
			$ zicroram --help-program

		Note for non-bash and non-zsh users:
			If you use something other than 'bash' and 'zsh', like 'tcsh' you'll have to search for it's system/user configuration file(s) 
			and do the same described here.

	WORKAROUND FOR FIREFOX IN RAMDISK
		After adding mozilla firefox using:
			$ zicroram --add /usr/lib/firefox

		Firefox for some reason either creates or uses an alternate empty profile after being moved/copied to Ramdisk and executed from there.
	The workaround is to execute firefox using the following argument from commandline and then selecting the profile you want to use:
				$ firefox -P
	
---------------------
---------------------
---------------------
---------------------
USING CDPARANOIA TO EXTRACT AUDIO FROM .BIN .CUE FILES | EXTRACTING AUDIO FROM CD-ROM CONTAINNING CDDA LAYER
	This only works if you own the .cue file for the .bin image and the .bin file also contains audio tracks 
under a CDDA(Compact Disc Digital Audio) layer.

	1 (Optional) - (Mandatory if you're working  with .bin .cue files): 
		First, you'll need to mount the .cue file you own, by following the instructions mentioned on following topic:
			MOUNTING .CUE AND .BIN FILES USING CDEMU

	2 (Optional) Install 'cdparanoia' if you don't own it
		# pacman -S cdparanoia

	3 - Make a new folder for extracting audio, change directory to it and execute 'cdparanoia'
		$ mkdir ~/audio_extract/ && cd ~/audio_extract && cdparanoia -B	
			or
		$ mkdir ~/audio_extract/ && cd ~/audio_extract && cdparanoia -d /dev/sr0 -B

---------------------
---------------------
---------------------
---------------------
HOW TO USE KEXEC FOR EXECUTING A FAST SYSTEM REBOOT
	WARNING: It's advisable not to use this, unless you're using a virtual machine and you're trying to buy time for testing stuff.
	SOURCES: https://wiki.archlinux.org/title/Kexec ; https://access.redhat.com/solutions/7046293

	Note: Read 'Very Important' sections below before using any of this:
		Requirement:
			'kexec-tools' must be installed on your system.

		Steps:
			# systemctl stop dbus-broker.service
			# kexec -l /boot/vmlinuz-linux --initrd=/boot/initramfs-linux.img --reuse-cmdline
			# systemctl kexec.target

		Optional:
			Read 'VERY IMPORTANT' notes below before using this:
				Reboot with: # kexec -e

		To simply reboot on same kernel after a kernel update, you might type, as long as kexec -l has not been used:
			# systemctl start kexec

VERY IMPORTANT 2: 
	A) Warning: Running kexec -e directly will not unmount active filesystems or terminate any running services gracefully.
	B) Using KEXEC is not very recommended, because disks may fail to unmount causing data corruption.

	C) kexec skips all the firmware and bootloader stuff that might have been upgraded with the system, 
	which is what takes most of the time during boot proccess.

	D) if /boot/ is on a different filesystem, it may not be mounted/unmounted first,
	causing data corruption.

	E) As a recommendation, avoid using this, do a full-system reboot after each kernel update.
	if you still need a fast reboot, go for systemctl soft-reboot which restarts only userspace and poses
	no danger.

	D) NO GPU: The graphics driver needs to be unloaded before a kexec, or the next kernel will not be able to gain exclusive control of the device. 
	This is difficult to achieve manually because any programs which need exclusive control over the GPU (Xorg, display managers) must not be running. 

---------------------
---------------------
---------------------
---------------------
DIFFERENCE BETWEEN GNU TAR, 7Z, BZIP2, GZ, ZIP, COMPRESS, BZZ AND PACK
	GNU TAR, 7Z, BZIP2 and ZIP are both Archiving and Compression tools.
While TAR originally was only meant as an archiving tool, GNU TAR allows it to use compression algorithms like 'bz' and 'gz'.

GZ, Compress, Bzz and Pack in the other hand are 'compression algorithms' that can be both used as compression tool and compression algorithm for other
programs like GNU Tar for example.

However, 'Compression Algorithms/Tools' lack the ability that 'Archiving Tools' have for Archiving and Compressing multiple files together into one single file.
In the other hand, Archiving Tools like the ones described above have the ability to use several compression algorithms to achieve the goal of compressing data 
when needed.


	COMPRESSION ALGORITHM TOOLS
		VERY IMPORTANT: If you want to archive multiple files and be able to extract them separately, use DAR or TAR in conjunction with one of the 
		compression algorithms! GNU TAR is capable of using these compression algorithm tools on it's own, so you likely don't have to use this step.
	
	However, it's possible to use different arguments than the default ones that GNU Tar uses, so using these is still an option,
	for example, 'xz' tool allows using many different options for compression on .tar files: $ man tar

	NOTE: Reminder that any person who wants to uncompress any of the compressed files created in this section must have the related capable tools 
	installed on their system.

	VERY IMPORTANT NOTE: 
		All of the compression tools listed here aren't capable of retrieving any kind of metadata directly from the files,
	because of this, filename, filename extension and other meta-data about compressed files can't be retrieved/restored upon extraction.

		The way most of these tools recover meta-data like filename and filename extension is simply by renaming the source file when
	extracting/decompressing the file.

	It's highly advisable that for storing filename, filename extension and other meta-data, it's important to use an archiving tool like TAR  or DAR
	in conjunction with a Compression Algorithm like the ones described here for compression! READ TOPIC: 
			MANUAL TAR COMPRESSION 

		GZ CHEAT SHEAT
			'GZ' has the best compatibility when it comes to running support on different operating system. On windows, it can be compressed/decompressed
			by WINZIP and 7Z.

			List of Tools: 
				gzip, gunzip, gzexe, zcat, zmore, zless, zcmp, zdiff, zforce and znew.

			1 - Compressing files to a single compressed .gz file:
				NOTE: This will concatenate multiple files INTO ONE SINGLE FILE:
					$ gzip -c ./file\ 1 ./file\ 2 >> file.gz;
						or
					$ gzip -c ./file\ 1 >> file.gz;
					$ gzip -c ./file\ 2 >> file.gz;


				VERY IMPORTANT: 
					READ TOPIC: 
						4 - Decompressing/Extracting file from compressed .gz file into the current folder

			2 - Integrity Test for compressed .gz file:
				$ gzip -tv ./file.gz

			3 - Compressing directory into a single compressed .gz file :
				NOTE: This will concatenate all files from a given directory into ONE SINGLE FILE.
					$ gzip -rc ./TEST\ FOLDER >> ./file.gz;

			4 - Decompressing/Extracting file from compressed .gz file into the current folder:
				Note: Unless --stdout or -o is specified, by default files are written to a new file whose name is derived from the source file name.
				this means that if the compressed file wasn't named/renamed properly, it's filename extension will not be recovered upon extraction.

				NOTE: the word 'gunzip' comes from the conjunction of 'g' letter and 'unzip', therefore 'gunzip' works the same way as 'unzip' does for 'zip'
				files.

				Reminder: Extraction/Decompression happens at current folder by default.

					Example - Descompressing .mp3 file from compressed .gz file - good example:
						This will rename file.mp3.gz to 'file.mp3' upon extraction:
							$ gzip -dkf ./file.mp3.gz
								or
							$ gunzip -f ./file.mp3.gz
								or
							$ gunzip --stdout -f ./file.mp3.gz >> ./file.mp3

					Example - Decompressing .mp3 file from file.gz - bad example:
						As a result, the filename extension will be lost in all 2 examples.
							$ zstd -dkf ./file.gz
								or
							$ gunzip -f ./file.gz

						NOTE: It's still possible to recover the file if you know the filename extension by head:
							$ gunzip --stdout -f ./file.gz >> ./file.mp3

					VERY IMPORTANT: For storing filename, filename extension and other meta-data, it's important to use an archiving tool like TAR or DAR
					in conjunction with a Compression Algorithm like the one described here for compression!

			5 - Listing contents from compressed .gz file:
				$ gzip -lv ./file.gz

			6 - Directly Reading file from compressed .gz file:
				NOTE: this will output the contents of a compressed file to the screen.
					$ zcat ./file.gz

					Practical example:
						$ zcat /proc/config.gz

			7 - Using zless for reading file from compressed .gz file: 
				NOTE: this will output the contents of a compressed file to the screen.
					$ zless /file.gz

					Practical Example:
						$ zless /proc/config.gz

			8 - Comparing .gz files:
				$ zcmp ./file\ 1.gz ./file\ 2.gz

			9 - Displaying differences between two compressed .gz files:
				$ zdiff ./file\ 1.gz ./file\ 2.gz

			10 - RUNNING/EXECUTING .GZ COMPRESSED BINARY FILES - [PERFORMANCE]:
				NOTE: 'gzexe' utility allows you to compress executables in place and have them automatically uncompress and execute when you run them.
				NOTE: At current state of modern systems and linux, this will give you a performance boost.

					# gzexe /usr/bin/your\ program

					Practical Example:
						# gzexe /usr/bin/firefox

						Note: The original uncompressed file will be renamed to /usr/bin/firefox~

					READ MORE ON:
						https://en.wikipedia.org/wiki/Executable_compression

		--------
		--------
		XZ CHEAT SHEAT
			VERY IMPORTANT: Some people advocate against the use of 'XZ', due to XZ not being future proof, which means some XZ compressed files might not be 
			recoverable/decompresed when using future versions of the tools available for 'XZ'.

			List of Tools:
				xz, xzcat, xzcmp, xzdiff, xzdec, xzegrep, xzfgrep, xzgrep, xzless and xzmore

			1 - Compressing file into a single .xz file in the current folder:
				NOTE: This will compress 'file 1' and 'file 2' into 'file 1.xz' and 'file 2.xz' respectively.
				NOTE: -k option will prevent original uncompressed files from being deleted after compression.
				NOTE: -f option will overwrite any compressed .xz file if they already exist

					$ xz -k ./file\ 1 ./file\ 2
						or
					$ xz ./file\ 1 ./file\ 2

				NOTE: See 'xz -lv' below

				VERY IMPORTANT: 
					READ TOPIC: 
						3 - Decompressing/Extracting file from .xz compressed file

				1.1 - Compressing file using Compression level:
					NOTE: -k option will prevent original files from being deleted after compression.
					NOTE: -f option will overwrite any compressed .xz file if they already exist

					1.1.1 - Fastest Compression level:
						$ xz -kf0 ./file\ 1
							or
						$ xz -0 -fk ./file\ 1

					1.1.2 - Slowest & Best Compression:
						$ xz -kf9 ./file\ 1
							or
						$ xz -9 -fk ./file\ 1

			2 - Integrity Test of Compressed .xz file:
				$ xz -t ./file.xz 

			3 - Decompressing/Extracting file from .xz compressed file:
				NOTE: use 'cd' to change directory if you want to extract in a specific directory of your choice.
				NOTE: -k option will prevent original files from being deleted after decompression/extraction.
				NOTE: -f forces file overwritting when it already exists.
				NOTE: -d decompresses.

				Note: Unless --stdout or -o is specified, by default files are written to a new file whose name is derived from the source file name.
				this means that if the compressed file wasn't named/renamed properly, it's filename extension will not be recovered upon extraction.

				Reminder: Extraction/Decompression happens at current folder by default.

					Example - Descompressing .mp3 file from compressed .xz file - good example:
						This will rename file.mp3.xz to 'file.mp3' upon extraction:
							$ xz -dkf ./file.mp3.xz
								or
							$ xz -dkf ./file.mp3.xz -o /tmp/file.mp3
								or
							$ xzdec -f ./file.mp3.xz >> /tmp/file.mp3

					Example - Decompressing .mp3 file from file.xz - bad example:
						As a result, the filename extension will be lost in all 3 examples.
							$ xz -dkf ./file.xz
								or
							$ xz -dkf ./file.xz -o /tmp/extracted_content
								or
							$ xzdec -f ./file.xz >> /tmp/extracted_content

						Note: if you know the filename extension, you can still recover by just renaming upon extraction:
							$ xzdec -f ./file.xz  >> ./file.mp3
				
					VERY IMPORTANT: For storing filename, filename extension and other meta-data, it's important to use an archiving tool like TAR or DAR
					in conjunction with a Compression Algorithm like the one described here for compression!

			4 - Listing contents from compressed .xz file:
				NOTE: this will output the contents of a compressed file to the screen.
					$ xz -lv ./file.xz

			5 - Directly reading file from .xz file:
				$ xzcat ./file.xz

			6 - Using xzless for reading file from compressed .xz file: 
				NOTE: this will output the contents of a compressed file to the screen.
				$ xzless ./file.xz

			7 - Comparing content of two different compressed .xz files:
				$ xzcmp ./file\ 1.xz ./file\ 2.xz

			8 - Displaying content differences between two compressed .xz files:
				$ xzdiff ./file\ 1.xz ./file\ 2.xz

		--------
		--------
		ZSTD CHEAT SHEET
			ZSTD has support for multi-threading for compression and decompression; this allows it to take advantage of most modern CPUs 
			for both speed and compression.

			List of Tools:
				zstd, zstdcat, zstdgrep, zstdless and zstdmt.

			1 - Compressing file into a single .zst file in the current folder:
				NOTE: This will compress 'file 1' and 'file 2' into 'file 1.zst' and 'file 2.zst' respectively.
				NOTE: -f option will overwrite any compressed .zst file if they already exist

					$ zstd -zf ./file\ 1 ./file\ 2
						or
					$ zstd -z ./file\ 1 ./file\ 2

				VERY IMPORTANT: 
					READ TOPIC: 
						3 - Decompressing/Extracting file from .zst compressed file

				1.1 - Compressing into a different directory:
					$ mkdir ./newfolder;  zstd -zf ./file\ 1 -o ./newfolder/file1.zst

				1.2 - Compressing file using Compression level:
					Just listing a few of the many options available in the 'zstd' man page.
					Check 'ADVANCED COMPRESSION OPTIONS' on zstd's man page for more options.

					NOTE: Compression Level defaults to '3' whenever unspecified.
					NOTE: -f option will overwrite any compressed .zst file if they already exist
					NOTE: -T0 allows to use multi-thread, '0' automatically attempts to detect how many physical CPU cores are available.
					NOTE: Ultra allows to increase Compression level beyond 19(level 20 to level 22).
					NOTE: --auto-threads={physical,logical} only applied when using -T0, it allows to detect available CPU physical or logical cores
					NOTE: --adapt[=min=#,max=#] allows zstd to adapt compression levels to perceived I/O conditions. Compression Level adaptation can
					be observed with '-v' passed as argument. This works only with multi-threading and '--long'. (check 'man' page)


					1.2.1 - Fastest Compression level:
						$ zstd -zf -1 ./file\ 1
							or
						$ zstd -1zf ./file\ 1

						1.3.1.1 - Using Multiple Cores:
							$ zstd -zf -1 -T0 ./file\ 1
								or
							$ zstd -1zfT0 ./file\ 1

					1.2.2 - Slowest & Best Compression:
						$ zstd -zf -19 ./file\ 1
							or
						$ zstd -19zf ./file\ 1

						1.2.2.1 - Using Multiple Physical Cores:
							$ zstd -zf -19 -T0 ./file\ 1
								or
							$ zstd -19zfT0 ./file\ 1

					1.2.3 - Using Ultra Compression:
						$ zstd -zf --ultra -22 ./file\ 1
							or
						$ zstd -22zf --ultra ./file\ 1 
							
						1.2.3.1 - Using Multiple Physical cores:
							$ zstd -zf --ultra -22 -T0 ./file\ 1
								or
							$ zstd -22zfT0 --ultra ./file\ 1

					1.2.4 - Using Available Logical Cores:
						$ zstd -zf -19 --auto-threads=logical -T0 ./file\ 1
							or
						$ zstd -19zfT0 --auto-threads=logical ./file\ 1
							or
						$ zstd --ultra -22zfT0 --auto-threads=logical ./file\ 1

			2 - Integrity Test of Compressed .zst file:
				$ zstd -t ./file.zst

			3 - Decompressing/Extracting file from .zst compressed file:
				Note: Unless --stdout or -o is specified, by default files are written to a new file whose name is derived from the source file name.
				this means that if the compressed file wasn't named/renamed properly, it's filename extension will not be recovered upon extraction.

				Reminder: Extraction/Decompression happens at current folder by default.

					Example - Descompressing .mp3 file from compressed .zst file - good example:
						This will rename file.mp3.zst to 'file.mp3' upon extraction:
							$ zstd -df ./file.mp3.zst
								or
							$ zstd -df ./file.mp3.zst -o /tmp/file.mp3
								or
							$ unzstd -f ./file.mp3.zst -o /tmp/file.mp3

					Example - Decompressing .mp3 file from file.zst - bad example:
						As a result, the filename extension will be lost in all 3 examples.
							$ zstd -df ./file.zst 
								or
							$ zstd -df ./file.zst -o /tmp/extracted_content
								or
							$ unzstd -f ./file.zst -o /tmp/extracted_content

						Note: if you know the filename extension, you can still recover by just renaming upon extraction:
							$ unzstd -f ./file.zst -o /tmp/file.mp3
				
					VERY IMPORTANT: For storing filename, filename extension and other meta-data, it's important to use an archiving tool like TAR or DAR
					in conjunction with a Compression Algorithm like the one described here for compression!

			4 - Listing contents from compressed .zst file:
				NOTE: this will output the contents of a compressed file to the screen.
					$ zstd -lv ./file.zst
						or
					$ zstd -l ./file.zst

			5 - Directly reading file from .zst file:
				$ zstdcat ./file.zst
					or
				$ zstd -dcf ./file.zst

			6 - Using xzless for reading file from compressed .zst file: 
				NOTE: this will output the contents of a compressed file to the screen.
					$ zstdless ./file.zst

			7 - Comparing content of two different compressed .zst files:
				$ cmp ./file\ 1.zst ./file\ 2.zst

			8 - Displaying content differences between two compressed .zst files:
				$ diff ./file\ 1.zst ./file\ 2.zst
		--------
		--------
		BROTLI CHEAT SHEET
			'Brotli' offers very fast Decompression Times, however Compression Times are very slow.
			It's currently in use by modern web browsers - date: 03/04/2025.

			List of Tools:
				brotli.

			1 - Compressing files to a single compressed .br file:
				NOTE: This will compress 'file 1' and 'file 2' into 'file 1.br' and 'file 2.br' respectively.
				NOTE: -f option will overwrite any compressed .br file if they already exist
					$ brotli -f ./file\ 1 ./file\ 2

				VERY IMPORTANT: 
					READ TOPIC: 
						3 - Decompressing/Extracting file from .br compressed file into the current folder

				1.1 - Compressing file using Compression level:
					NOTE: -f option will overwrite any compressed .xz file if they already exist.
					NOTE: -q defines compression level.
					NOTE: -w sets LZ77 Window Size. -w 0 sets window size to automatic!

					1.1.1 - Fastest Compression level:
						$ brotli -fq 0 -w 10 ./file\ 1

					1.1.2 - Slowest & Best Compression:
						$ brotli -fq 10 -w 24 ./file\ 1

			2 - Integrity Test for compressed .br file:
				$ brotli --test -v ./file\ 1.br

			3 - Decompressing/Extracting file from .br compressed file into the current folder:
				Note: Unless --stdout or -o is specified, by default files are written to a new file whose name is derived from the source file name.
				this means that if the compressed file wasn't named/renamed properly, it's filename extension will not be recovered upon extraction.

				Reminder: Extraction/Decompression happens at current folder by default.

					Example - Decompressing .mp3 file from compressed .br file - good example:
						This will rename 'file.mp3.br' to 'file.mp3' upon extraction:
							$ brtoli -df ./file.mp3.br
								or
							$ brotli -df ./file.mp3.br -o /tmp/file.mp3

					Example - Decompressing .mp3 file from file .br - bad example:
						As a result, the filename extension will be lost in all 2 examples.
							$ brotli -df ./file.br
								or
							$ brotli -df ./file.zst -o /tmp/extracted_content

						Note: if you know the filename extension, you can still recover by just renaming upon extraction:
							$ brotli -df ./file.br -o /tmp/file.mp3
				

				VERY IMPORTANT: For storing filename, filename extension and other meta-data, it's important to use an archiving tool like TAR or DAR
				in conjunction with a Compression Algorithm like the one described here for compression!

			4 - Listing contents from compressed .br file:
				$ brotli --test -v ./file1.br     

			5 - Comparing content of two different compressed .br files:
				NOTE: Currently, brotli doesn't have any tools/options for comparing compressed .br files
					$ cmp ./file\ 1.br ./file\ 2.br

			6 - Displaying content differences between two compressed .br files:
				NOTE: Currently, brotli doesn't have any tools/options for sorting differences on compressed .br files
					$ diff ./file\ 1.br ./file\ 2.br

		--------
		--------
		LZ4 CHEAT SHEET
			- TODO - 
		--------
		--------
		OTHER COMPRESSION ALGORITHM TOOLS:
			LZ4
			BZIP2
			LZIP
			PBZIP ( Multi-threaded version of BZIP2 )
			PLZIP ( Multi-threaded version of LZIP )
			PXZ ( Multi-threaded version of XZ )
			PIGZ ( multi-threaded version of GZIP )
			PACK
			DEFLATE (libdeflate) / rarely used
			COMPRESS

	ARCHIVING TOOLS / ARCHIVE TOOLS
		TAR CHEAT SHEET
			TAR DATA COMPRESSION
				Tar has the ability to use filename extension to determine the compression algorithm to be used by passing the 'a' argument.
				Tar also has the ability to store file/data permission and ownership.

				Syntax:
					$ tar caf <filename>.tar.<compression_algorithm> <path_to_data1> <path_to_data2> <path_to_data3>

				RECOMMENDED TOPICS: 
					[INDEX: COMPRESSING AND UNCOMPRESSING FILES & DATA]

				NOTE:
					Using one of the compression algorithms/formats will require you to have it's tools installed on your system,
					GNU TAR will usually complain when the tools fail to exist.

				MANUAL TAR COMPRESSION
					USING ZSTD 
						The command below will archive the directory 'my folder' as 'my folder.tar',
						zstd will in turn compress the archive 'my folder.tar' and then name it 'my folder.tar.zst':
							$ tar cf ~/my\ folder.tar ~/my\ folder && zstd --ultra -22zfT0 ~/my\ folder.tar

				AUTOMATIC TAR COMPRESSION
					Practical example - using long name extensions:
						$ tar caf ~/my\ filename.tar.xz ~/{file1,file2,file3}
							or
						$ tar caf ~/my\ filename.tar.lz ~/{file1,file2,file3}
							or
						$ tar caf ~/my\ filename.tar.zst ~/{file1,file2,file3}
							or
						$ tar caf ~/my\ filename.tar.gz ~/{file1,file2,file3}
					
					Practical example - using short name extensions:
						$ tar caf ~/my\ filename.txz ~/{file1,file2,file3}
							or
						$ tar caf ~/my\ filename.tlz ~/{file1,file2,file3}
							or
						$ tar caf ~/my\ filename.tzst ~/{file1,file2,file3}
							or
						$ tar caf ~/my\ filename.tgz ~/{file1,file2,file3}

			Extracting:
				If you intend to preserve file permission and ownership when extracting files, you need to run 'tar' as root:
					# tar xvf <file_to_extract>.tar --directory=<destination_directory>

				If running 'tar' as a regular user when extracting the files, then 'tar' will extract files using default system permission/ownership
				when extracting them:
					$ tar xvf <file_to_extract>.tar --directory=<destination_directory>

				VERY IMPORTANT: Most of the time, when a program is distributed in .tar format, it's distributed this way to preserve data permission and 
				ownership. Because of this, it's recommended to always extract .tar files as root.

			Listing Contents:
				$ tar vtf <file_to_extract>.tar
					or
				$ tar tvf <file_to_extract>.tar
				
		------------------------
		DAR CHEAT SHEET
			DAR is a more powerful version of TAR, it is meant as a disk/data backup utility, but can be used as an Archival Tool
		in the same fashion as TAR.

			LIST OF DAR TOOLS
				dar, dar_slave, dar_split, dar_cp, dar_manager, dar_xform, parchive(install par2cmdline - par2, par2create, par2verify and par2repair) and avfs.

				NOTE: While archiving can be used to keep multiple backup copies of the original data,
				it's not possible to guarantee that the backup copies might not get damaged. One way of achieving this is to use Parchive
				to ensure aditional integrity of the backed up data.

				NOTE2: AVFS is virtual file system layer for transparently accessing the content of archives and remote directories just like local files.
				Currently it can access natively tar, rar, zip, ar, gzip, bzip2, ftp, http, dav and some other. Although, this only works for read-only.

			READ THIS BEFORE USING DAR
				1 - When in doubt, remember to check on these things:
					1.1 - $ man dar
					1.2 - $ dar --help

				2 - When compressing the chosen data, unlike other archiving tools, dar requires '-g' to be used;
				WHEN NOT passing '-g', dar defaults to executing a full system backup.
					example:
							$ dar -c <archive_filename_without_extension> -g <data_to_archive> 

				3 - Unlike TAR and Other archiving tools, when using compression, DAR compresses each file in the archive instead of compressing 
				the entire archive. DAR does the same when encrypting data, instead of encrypting the entire archive, it encrypts each of the file/data 
				inside the archive. therefore, files like 'my archive.dar.zst' will not exist like it does in TAR and other archiving tools, unless
				this is manually done by the user using compression tools on his own.

				4 - When working with archiving of any kind, there's always the chance the system might hang, freeze, crash for unexpected reasons like
				a power blackout during compression, decompression, encrypting, listing, etc; This may cause archive data structure to corrupt, leading 
				data unrecoverable even if they've kept integrity inside the archive. Because of this, it's important to isolate the archive catalogue 
				using -C option so that you can repair it with -y at some later point. check 'man page' and '--help' for more info on this.

				By isolating the catalogue, you won't need a backup/copy of the archive, just the catalogue is enough.
		
			LIST OF DAR FEATURES
				SOURCE: http://dar.linux.free.fr/doc/Features.html

				A) Unlike TAR, DAR HAS SUPPORT FOR HARD LINKS. 

				B) DAR HAS DEFAULT SUPPORT FOR SPARSE FILES and will remember and re-create it when extracting data to a Disk with Sparse File support. 
				When the data being stored in a DAR Archive is met with a long sequence of zero bytes, DAR stores the data as Sparse and remembers how big 
				that file was for re-creation when extracting and copying back to disk. 
				[ --sparse-file-min-size, -ah ]

				C) EXTENDED ATTRIBUTES [ -u -U -am -ae --alter=list-ea  ]

				D) FILESYSTEM SPECIFIC ATTRIBUTES ( FSA ) [ --fsa-family ]

				E) DIRTY FILES:  At backup time, dar checks whether each saved file had not changed at the time it was read. If a file has changed in that 
				situation, dar retries saving it up to three times (by default) and if it is still changing, is flagged as "dirty" in the backup, and handled 
				differently from other files at restoration time. The dirty file handling is either to warn the user before restoring, to ignore and avoid 
				restoring them, or to ignore the dirty flag and restore them normally.
				[ --dirty-behavior , --retry-on-change ]  

				F) FILTERS:  dar is able to backup from a total file system to a single file, thanks to its filter mechanism. 
				This one is dual headed: The first head let one decide which part of a directory tree to consider for the operation (backup, restoration, etc.) 
				while the second head defines which type of file to consider (filter only based on filename, like for example the extension of the file).

				For backup operation, files and directories can also be filtered out if they have been set with a given user defined EA. 
				[ -I -X -P -g -[ -] -am --exclude-by-ea ]

				G) NODUMP Flag: Many filesystems, like ext2/3/4 filesystems provide for each inodes a set of flags, among which is the "nodump" flag. 
				You can instruct dar to avoid saving files that have this flag set, as does the so-called dump backup program.  
				[ --nodump ]

				H) ONE-FILESYSTEM: DAR can stay in one single filesystem, without stepping into the boundary of other mounted filesystems 
				[ -M ]

				I) CACHE DIRECTORY TAGGING STANDARD: Many software use cache directories (mozilla web browser for example), directories where is stored 
				temporaneous data that is not interesting to backup. The Cache Directory Tagging Standard provides a standard way for software applications 
				to identify this type of data, which let dar (like some other backup softwares) ignore cache data designated as such by other applications.
				[ --cache-directory-tagging ]

				J) DIFFERENTIAL BACKUP: a differential backup, saves only files that have changed since a given reference backup. Additionally, files that 
				existed in the reference backup and which do no more exist at the time of the differential backup are recorded in the backup as "been removed". 
				At recovery time, (unless you deactivate it), restoring a differential backup will update changed files and new files, but also remove files 
				that have been recorded as "been removed".

				Note that the reference backup can be a full backup or another differential backup (this second method is usually designed as 
				incremental backup). This way you can make a first full backup, then many incremental backups, each taking as reference the last backup made, 
				for example. 
				[ -A ]

				K) DECREMENTAL BACKUP:  As opposed to incremental backups, where the older one is a full backup and each subsequent backup contains only the 
				changes from the previous backup, a decremental backup let the full backup be the more recent while the older ones only contain changes compared 
				to the just more recent one.

				This has the advantage of providing a single backup to use to restore a whole system in its latest known state, while reducing the overall 
				amount of data to retain older versions of files (same amount required as with differential backup). It has also the advantage to not have 
				to keep several set of backup as you just need to delete the oldest backup when you need storage space. However it has the default to require 
				at each new cycle the creation of a full backup, then the transformation of the previous full backup into a so-called decremental backup. 
				Yes, everything has a cost!
				[ -+ -ad ]


				L) DELTA BINARY / DELTA TRANSFER [ --delta sig, --include-delta-sig, --exclude-delta-sig, --delta-sig-min-size, --delta no-patch ]

				M) PREVENTING ROOTKITS AND OTHER MALWARE: At backup time when a differential, incremental or decremental backup is done, dar compares 
				the status of inode on the filesystem to the status they had at the time of the last backup. If the ctime of a file has changed while 
				no other inode field changed, dar issues a warning considering that file as suspicious. This does not mean that your system has been 
				compromised but you are strongly advised to check whether the concerned file has recently been updated (Some package manager may lead 
				to that situation) or has its Extended Attributes changed since last backup was made. In normal situation this type of warning does not 
				show often (false positive are rare but possible). However in case your system has been infected by a virus, compromised by a rootkit or 
				by a trojan, dar will signal the problem if the intruder tried to hide its forfait.  
				[ -asecu ]

				N) DIRECTORY TREE SNAPSHOT:  Dar can make a snapshot of a directory tree and files or even of a whole system, recording the inode status of 
				each files. This may be used to detect changes in filesystem, by "diffing" the resulting snapshot with the filesystem at a later time. 
				The resulting snapshot can also be used as reference to save files that have changed since the snapshot has been done.

				A snapshot is just a special dar backup, that is very small compared to the corresponding full backup but of course, it cannot be 
				used to restore any data. As a dar backup, it can be created using compressed, slices, encryption... 
				[ -A + ]

				P) SLICES : Given the size, dar will split the archive/backup in several files (called SLICES), eventually pausing before creating the next one, 
				and/or allowing the user to automate any action (like un/mount a medium, burn the file on CD-R, send it to the cloud, and so on)

					Additionally, the size of the first slice can be specified separately, if for example you want first to fulfill a partially filled 
				disk before starting using empty ones. Last, at restoration time, dar will just pause and prompt the user asking a slice only if it is missing, 
				and allowing here too user to automate any particular action (dowloading the slice from the cloud, mount/unmounting a removable media and so on).

				You can choose to have either more than one slice per medium without penalty from dar (no extra user interaction than asking the user to change 
				the removable media when it has been read), or just one slice per medium or even a backup without slice, which is a single file, 
				depending on your need. 
				[ -s -S -p -aSI -abinary -b ]

				Q) COMPRESSION: dar can use compression. Actually gzip, bzip2, lzo, xz/lzma, zstd, lz4 algorithms are available, and there is still room 
				available for any other compression algorithm. Note that, compression is made before slicing, which means that using compression together 
				with slices, will not make slices smaller, but will probably make less slices in the backup. 
				[ -z ]

				O) SELECTIVE COMPRESSION: dar can be given a special filter that determines which files will be compressed or not. This way you can speed up 
				the backup operation by not trying to compress *.mp3, *.mpg, *.zip, *.gz and other already compressed files, for example. Moreover another 
				mechanism allows you to say that files below a given size (whatever their name is) will not be compressed. 
				[ -Y -Z -m -am ]

				R) STRONG ENCRYPTION: Dar can use blowfish, twofish, aes256, serpent256 and camellia256 algorithms to encrypt the whole backup. 
				Two "elastic buffers" are inserted and encrypted with the rest of the data, one at the beginning and one at the end of the archive 
				to prevent a clear text attack or codebook attack.

				For symmetric key encryption several Key Derivation Functions are available, from the legacy PBKDF2 (PKCS#5 v2) to the modern Argon2 algorithm. 
				The user has the possibility to set the hash algorithm for the first and the interation count for both algorithms. 
				[  -K -J -# -* blowfish, twofish, aes256, serpent256, camellia256, --kdf-param ]

				S) PUBLIC KEY ENCRYPTION: Encryption based on GPG public key is available. A given backup can be encrypted for a recipient 
				(or several recipients without visible overhead) using its public key. Only the recipient(s) will be able to read such encrypted backup.

				The advantage over ciphering the backup as a whole is that you don't have to uncipher it all to extract a particular file or set of file, 
				which brings a huge gain of CPU usage and execution time.
				[ -K, --key-length ]

				T) PRIVATE KEY SIGNATURE: When using encryption with public key it is possible in addition to sign an archive with your own private key(s). 
				Your recipients can then be sure the archive has been generated by you, dar will check the signature validity against the corresponding public 
				key(s) each time the archive is used (restoration, testing, etc.) and a warning is issued if signature does not match or key is missing to 
				verify the signature. You can also have the list of signatories of the archive while listing the archive content. 
				[ --sign ]

				U) SLICE HASHING:  When creating a backup, dar can compute an md5, sha1 or sha512 hash before the backup is even written to disk and produce 
				a small file compatible with md5sum, sha1sum or sha512sum that let verify that the medium has not corrupted the slices of the backup.
				[ --hash, md5, sha1, sha512 ]


				V) DATA PROTECTION:  Dar is able to detect corruption in any part of a dar backup, but it cannot fix it on it's own.

				Instead Dar relies on the Parchive program for data protection against media errors. Thanks to dar's ability to run user command or script 
				and thanks to the ad hoc provided scripts, dar can use Parchive as simply as adding a word (par2) on command-line. Depending on the context 
				(backup, restoration, testing, ...), dar will by this mean create parity data for each slice, verify and if necessary repair the archive slices.

				Without Parchive, dar can workaround a corruption, skipping the concerned file and restoring all others. Dar also offers a 'lax' mode, in which
				the user will be asked several questions to help dar recover very corrupted archives.
				[ -al ]

				W) TRUNCATED ARCHIVE/BACKUP REPARATION: Since version 2.6.0 an truncated archive (due to lack of disk space, power outage, or any other reason) 
				can be repaired. A truncated archive lacks a table of content which is located at the end of the archive, without it you cannot know what file 
				is saved and where to fetch its data from, unless you use the sequential reading mode which is slow as it implies reading the whole archive even 
				for restoring just one file. To allow sequential reading of an archive, which is suitable for tape media, some metadata is by default inserted 
				all along the archive. This metadata is globally the same information that should contain the missing table fo content, but spread by pieces all 
				along the archive. Reparing an archive consists of gathering this inlined metadata and adding it at the end of the repaired archive to allow 
				direct access mode (default mode) which is fast and efficient. 
				[ -y ]

				X) DIRECT DATA ACCESS:  Even using compression and/or encryption dar has not to read the whole backup to extract one file. 
				This way if you just want to restore one file from a huge backup, the process will be very quick. Dar first reads the catalogue 
				(i.e. the contents of the backup), then it goes directly to the location of the saved file(s) you want to restore and then proceeds to 
				restoration. In particular using slices, dar will ask only for the slice(s) containing the file(s) to restore.

				Since version 2.6.0 dar can also read a backup from a remote host by mean of FTP or SFTP. Here too dar can leverage its direct access 
				ability to only download the necessary stuff in order to restore some files from a large backup, or list the backup content and even compare 
				a set of file with the live filesystem.
				[ -g ]

				Y) SEQUENTIAL ACCESS FOR TAPES: By default, DAR uses 'Random Access' meant for disks, however it's still possible to use Sequential Access 
				meant for tapes.
				[ -sequential-read, -at ]

				Z) MULTI-VOLUME TAPES: The independant dar_split program provides a mean to output dar but also tar archives to several tapes. 
				If takes care of splitting the archive when writing to tapes and gather pieces of archive from several tapes for dar/tar to work as 
				if it was a single pieced archive. 
				[ --sequential-read ]

				AA) ARCHIVE/BACKUP TESTING:  thanks to CRC (cyclic redundancy checks), dar is able to detect data corruption in a backup. 
				Only the file where data corruption occurred will not be possible to restore, but dar will restore the others even when compression 
				or encryption (or both) is used.
				[ -t ]
	

				AB) ISOLATION:  The catalogue (i.e.: the contents of a backup), can be extracted as a copy (this operation is called isolation) to a small file,
				that can in turn be used as reference for differential backup and as rescue of the internal catalogue (in case of backup corruption).

				There is then no need to provide a backup to be able to create a differential backup based on it, just its isolated catalogue can be used 
				instead. Such an isolated catalogue 
				[  -C -A -@ ]

				AC) FLAT RESTORATION: It is possible to restore any file without restoring the directories and subdirectories it was in at the time of the 
				backup. If this option is activated, all files will be restored in the (-R) root directory whatever their real position is recorded inside 
				the backup. 
				[ -f ]

				AD) USER COMMAND BETWEEN SLICES: several hooks are provided for dar to call a given command once a slice has been written or before reading 
				a slice. Several macros allow the user command or script to know the requested slice number, path and backup basename. 
				[ -E -F -~ ]
				
				AE) USER COMMAND BEFORE AND AFTER SAVING A DIRECTORY OR A FILE:
				It is possible to define a set of file that will have a command executed before dar starts saving them and once dar has completed saved them. 
				Before entering a directory dar will call the specified user command, then it will proceed to the backup of that directory. 

				Once the whole directory has been saved, dar will call again the same user command again (with slightly different arguments) and then continue 
				the backup process. Such user command may for example run a particular command which output will be redirected to a file of that directory, 
				suitable for backup. Another purpose is to force auto-mounting filesystems that else would not be visible and thus not saved. 
				[  -< -> -= ]
				
				AF) CONFIGURATION FILE: 
				dar can read parameters from file. This is a way to extends the command-line limited length input. A configuration file can ask dar to read 
				(or to include) other configuration files. A simple but efficient mechanism forbids a file to include itself directly or not, and there is 
				no limitation in the degree of recursion for the inclusion of configuration files.

				Two special configuration files $HOME/.darrc and /etc/darrc are read if they exist. They share the same syntax as any configuration file 
				which is the syntax used on the command-line, eventually completed by newlines and comments.

				Any configuration file can also receive conditional statements, which describe which options are to be used in different conditions. 
				Conditions are: "extract", "listing", "test", "diff", "create", "isolate", "merge", "reference", "auxiliary", "all", "default" (which may 
				be useful in case or recursive inclusion of files) ... more about their meaning and use cases in dar man page. 
				[ -B ]

				AG) REMOTE OPERATIONS: dar is able to read and write a backup to a remote server.
				[ -i -o - -afile-auth ]

				AH) DRY-RUN EXECUTION:  You can run any feature without effectively performing the action. Dar will report any problem but will not create, 
				remove or modify any file. 
				[ -e ]
				
				AI) ARCHIVE/BACKUP MERGING:
				From version 2.3.0, dar supports the merging of two existing archives into a single one. This merging operation is assorted by the 
				same filtering mechanism used for archive creation. This let the user define which file will be part of the resulting archive.

				By extension, archive merging can also take as single source archive as input. This may sound a bit strange at first, but this let you 
				make a subset of a given archive without having to extract any file to disk. In particular, if your filesystem does not support Extended 
				Attributes (EA), thanks to this feature you can still cleanup an archive from files you do not want to keep anymore without loosing any EA or 
				performing any change to standard file attributes (like modification dates for example) of files that will stay in the resulting archive.

				Last, this merging feature give you also the opportunity to change the compression level or algorithm used as well as the encryption 
				algorithm and passphrase. Of course, from a pair of source archive you can do all these sub features at the same time: filtering out files you 
				do not want in the resulting archive, use a different compression level and algorithm or encryption password and algorithm than the source 
				archive(s), you may also have a different archive slicing or no slicing at all (well dar_xform is more efficient for this feature only, 
				see above "RE-SHAPE SLICES OF AN EXISTING ARCHIVE/BACKUP" for details). 
				[ -+ -ak -A -@ ]

				AJ) ARCHIVE SUBSETTING: As seen above under the "archive merging" feature description, it is possible to define a subset of files from 
				an archive and put them into a new archive without having to really extract these files to disk. To speed up the process, it is also possible 
				to avoid uncompressing/recompressing files that are kept in the resulting archive or change their compression, as well change the encryption 
				scheme used. Last, you may manipulate this way files and their EA while you don't have EA support available on your system. 
				[ -+ -ak ] 

				AK) ARCHIVE/BACKUP USER COMMENTS: The backup header can hold a message from the user. This message is never ciphered nor compressed and always 
				available to any one listing the archive summary (-l and -q options). Several macro are available to add more confort using this option, 
				like the current date, uid and gid, hostname, and command-line used at backup creation.
				[ --user-comment, -l -v, -l -q ] 

				AL) MINIMUM DIGITS / PADDED ZEROS TO SLICE NUMBER: For example, with 3 as minimum digit, the slice name would become: 
				archive.001.dar, archive.002.dar, ... archive.010.dar. 
				[ --min-digits ]

				AM) MULTI-THREADING: Since release 2.7.0, compression can use several threads when the new compression per block is used (by opposition to the 
				streaming compression used so far, which is still available). Encryption can also be processed with multiple threads even for old backups 
				(no change at encryption level). The user defines the number of threads he wants for each process, compression/decompression as well as 
				ciphering/deciphering. 
				[ -m, --multi-thread ]

			BASIC DAR EXAMPLES:
				1) ADDING DATA TO DAR ARCHIVE:
					Common Options:
					'-va' outputs verbose mode.
					'-b' turns on 'beep' on the terminal for noticing the end of a given task. Only works on beep capable/enabled terminals.
					'-m <size>' data below the specified size will not be compressed - only works with '-z' - default value is 100.(size = 1K, 1M, 1G)
					'-m 0' compresses all files.
					'-' argument tells dar to either produce the archive or output results on the standard output(stdout).
					this allows the resulting output to be piped into another program/utility or to be sent directly to a writable media.

					IMPORTANT: When not using '-' DAR automatically creates filename extension in the following format <archive_filename>.<slice>.dar, 
					all DAR Archives must end in <slice>.dar for it to be detected by dar. So don't modify it!
					When using '-' you'll need to name the file manually by adding '.1.dar' at the end of the filename, see 1.1 example below.

					1.1 - USING DAR ARCHIVING WITHOUT COMPRESSION:
						$ dar -c <archive_filename_without_extension> -g <data_to_archive> -g <data_to_archive>
								or
						$ dar -c - -g <data_to_archive> > ./<archive_filename>.1.dar

					1.2 - GENERATING HASH FILE FOR DAR ARCHIVE DURING CREATION
						It's possible to choose between "md5", "sha1" and "sha512" algorithms. By default, no hash file is generated.
					this option is available when creating, isolating catalogue, merging or repairing an archive.

					NOTE: This is different than generating a hash file after the archive has been created,
					in the case where the storage media is faulty - ex.: old hard disk - there's a chance that the archive may get corrupt
					as soon as it is created; generating a hash file in this case will not allow you to test for data corruption.
						

						$ dar --hash md5 -c <archive_filename_without_extension> -g <data_to_archive>
								or
						$ dar -c - -g <data_to_archive> > ./<archive_filename>.1.dar
						
						NOTE: If multiple slices of a given size are created (-s <size> ) then a new hash will 
						automatically be created per slice.

						1.2.1 -- Practical example - Testing with a hash file --

							1.2.1.1 - Generate hash during archive creation
								$ dar --hash md5 -c ./archive -g ./data\ to\ archive
									or
								$ dar -3 sha512 -c ./archive -g ./data\ to\ archive

							1.2.1.2 - Testing the archive with the generated hash
								$ md5sum -c archive.1.dar.md5
									or
								$ sha512sum -c archive.1.dar.sha512
						
						1.2.2 -- Practical example - Testing hash for multiple slices  -
							First, this will create an archive and slice it in sizes of 700 bytes,	
							note that this will create as many files as needed. 

							NOTE: In this example, The Data to be archived needs to be bigger than 700 bytes due to slices being created (-s ).

							1.2.2.1 - Generate hash during archive creation
								$ dar --hash md5 -c ./<archive_name_with_no_extension> -g ./<data_to_archive> -s 700
									or
								$ dar -3 sha512 -c ./<archive_name_with_no_extension> -g ./<data_to_archive> -s 700

							1.2.2.2 - Testing all hashed slices
								$ md5sum -c ./<archive_name>*.md5 | grep -P -v 'OK' 
									or
								$ sha512sum -c ./<archive_name>*.sha512 | grep -P -v 'OK'

						NOTE: If you just need the return value of these tests for a script, you have the choice of not using 'grep' at all.
						you can get the return value straight from system call or from the environmental '$?' you can test with:
							$ echo $?
								

						RECOMMENDED READINGS:
							INTEGRITY TEST FOR DAR ARCHIVE
							COMPARING DAR ARCHIVES

					1.3 - USING STANDARD DAR COMPRESSION
						'-m0' is used to compress files of all sizes.

						NOTE: DAR uses level 9 gzip compression by default.
							$ dar -c <archive_filename_without_extension> -m0 -z -g <data_to_archive> 
									or
							$ dar -c - -g <data_to_archive> -m0 -z -g > ./<archive_filename>.1.dar

						1.3.1 - USING DAR WITH ZSTD COMPRESSION
							$ dar -c <archive_filename_without_extension> -m0 -zzstd:22 -g <data_to_archive> 
									or
							$ dar -c - -g <data_to_archive> -m0 -zzstd:22 -g > ./<archive_filename>.1.dar

						1.3.2 - USING DAR WITH LZ4 COMPRESSION
							$ dar -c <archive_filename_without_extension> -m0 -zlz4 -g <data_to_archive> 
									or
							$ dar -c - -g <data_to_archive> -m0 -zlz4 -g > ./<archive_filename>.1.dar

					1.4 - USING MULTITHREADING DAR COMPRESSION 
						Common Option(s) when Adding Data to Archive:
							-m <size>	#Sets up a Minimum size for compression, files under this size will not be compressed.
									Default value is '100' bytes.
									example: - argument for not compressing files under 1MB - 
										-m 1M

						NOTE: -G and <block_size> must be passed when using multi-thread.
						<block_size> defines how much data each thread will take care when compressing.
						NOTE 2: -G, --multi-thread { <num_of_threads> | <crypto_threads>,<compression_threads> }
						with this, it's possible to pass how many threads and/or also specify how many crypto and compression threads
						you want.

						Syntax:
							$ dar -c <archive_filename_without_extension> -m0 -G -zzstd:22:<block-size> -g <data_to_archive> 

						Practical Example:
							$ dar -c <archive_filename_without_extension> -m0 -G -zzstd:22:2M -g <data_to_archive> 

						Practical Example 2:
							$ dar -c <archive_filename_without_extension> -m0 -G3 -zzstd:22:2M -g <data_to_archive> 
						
					1.5 - CREATING DAR ARCHIVE WITH ENCRYPTION AND PASSWORD
						From 'man page': Available ciphers are "blowfish" (alias "bf"), "aes", "twofish", "serpent" and "camellia" for
						strong encryption and "scrambling" (alias "scram") for a very weak encryption.

						Syntax:
							$ dar -c <archive_filename_without_extension> -K <algorithm>:<password/cipher> -g <data_to_archive>
								or
							$ dar -c<archive_filename_without_extension> -K<algorithm>:<password/cipher> -g<data_to_archive>
								or
							$ dar -c <archive_filename_without_extension> --key <algorithm>:<password/cipher> -g <data_to_archive>

						Practical Example - Using SCRAMBLE Algorithm - :
							$ dar -c ./data\ archive -K scram:asdf -g ./data\ folder/
								or
							$ dar -c./data\ archive -Kscram:asdf -g./data\ folder/

						READ TOPIC:
							EXTRACTING DAR ARCHIVE USING PASSWORD

					1.6 - SPLITTING/SLICING ARCHIVE INTO SEVERAL FILES
						From 'man page': If the number is appended by k (or K), M, G, T, P, E, Z, Y, R or Q 
						the size is in kilobytes, megabytes, gigabytes, terabytes, petabytes, exabytes, zettabytes, yottabytes, ronnabytes 
						or quettabytes respectively. 

						Syntax:
							$ dar -c <archive_filename_without_extension> -s <slice_size> -g <data_to_archive>
								or
							$ dar -c<archive_filename_without_extension> -s<slice_size> -g<data_to_archive>

						Practical Example:
							$ dar -c ./data\ archive -s 2M -g ./data\ folder/
								or
							$ dar -c./data\ archive -s2M -g./data\ folder/

						1.6.1 - PAUSING ARCHIVING PROCESS FOR EACH SLICE CREATED
							In dar, it's possible to pause archiving for each slice created using the '-p' argument.
						This allows each slice to be copied into a different disk, before creating a new slice.

						This is very important when creating full system backups and there isn't enough disk space available 
						for all of the slices to be created. It can also be used when the Backup Media isn't big enough and needs
						to be switched between each slice creation.

						Practical Example:
							$ dar -c ./data\ archive -p -s 2M -g ./data\ folder/

							1.6.1.2 - PAUSING ARCHIVING PROCESS AFTER A GIVEN NUMBER OF SLICES HAVE BEEN CREATED
								This can be used for the same purpose; in this example, the archiving
							will pause after 3 slices of 2MB have been created.

								$ dar -c ./data\ archive -p 3 -s 2M -g ./data\ folder/
									or
								$ dar -c ./data\ archive -p3 -s 2M -g ./data\ folder/

					1.7 - ADDING/EXCLUDING DATA FROM DAR ARCHIVING USING A LISTING FILE
						During archive creation, it's possible to add or exclude data using a text file for listing which data is included
						or excluded from the archive during creation.

						The '-[' and '-]' arguments are similar to '-g' with minor differences:
							1.7.1 - No WildCard is supported
							1.7.2 - '-[' adds only the given data the given data to archive, not it's files and sub-directories
							1.7.3 - '-]' does the same, but excludes given files and directories from being added to archive
							1.7.4 - Unlike '-g' you need to create a listing file for it

						NOTE: since '[' is a reserved special character used by the terminal, you need to use '-\['
						for passing the '-[' argument. same for ']'

						Practical Example:
							Just for this example, you need to follow 3 different steps:

							-- Creating Dummy Folders and Files:
								$ mkdir -p ./test\ folder/folder\ B;
								$ folder=(./test\ folder ./test\ folder/folder\ B);
								$ touch {$folder[1],$folder[2]}/{file\ a,file\ b};

							-- Creating a Listing File:
								$ find ./test\ folder > ./listing\ file
									or
								$ find ./test\ folder > ./listing\ file.txt

								Note: At this point, you can open the listing file and edit it on your favorite text editor,
								applying any final changes by hand if desired.

							-- Creating Archive from Listing File:
								$ dar -c ./data\ archive -\[ ./listing\ file
							
							With everything done, you can now check the archive that has just been created:
								$ dar -l ./data\ archive
							
							Finally, you can remove everything after you're done with it.
								$ rm -r ./{test\ folder,data\ archive}

						RECOMMENDED TOPIC READING:
							SOME FIND COMMANDS
							FIND, AWK GREP AND UGREP


				2) LISTING DAR FILES
					Common Options to use when listing data (-l):
						-T              tree output format
						-as             only list files saved in the archive

					NOTE: DAR works different from other Archiving/Compressing tools, filename extension DOESN'T has to be passed for dar.
					Example: If your archive name is called 'archive.zst.1.dar', then you only need to pass 'archive.zst' for DAR.

						$ dar -l ./<archive_filename>

				3) MERGING DIFFERENT DAR ARCHIVES
					Syntax:
						$ dar -+ ./<New Merged Archive> -A ./<Archive 1> -@ ./<Archive\ 2>
						Note: Check the 'Examples' section below

					Read this before merging - VERY IMPORTANT:
						'DAR' by default replaces files when they're under the same filename, from the archive passed in '-@'
						with the one passed in '-A' (see example 3.1). 'Dar' always consider the archive passed in '-A' to have
						the most recent data when data under same filename exists in the archive passed in '-@'. 

						In this case, only the data/file on the archive passed in argument '-A' is copied/created on the merged archive,
						instead of the data in the archive passed in the '-@' argument.

						Everything else is merged.

					Common Options - VERY IMPORTANT -:
						A) '-ad' treats the whole merging as a DECREMENTAL BACKUP, the rules are the same as when doing a regular merge. 

						to put it simple: existing files and directories in the archive passed by '-@' are deleted in the final merge if they
						do not exist in the archive passed by '-A' argument.

						be warned delta binary feature (-8, --delta sig ) is not supported with '-ad'.
							(Read more on DECREMENTAL BACKUP)

						B) '-ak' for merging and repairing operations this means keeping files compressed. Both archives must either use 
						the same compression method or one of them mustn't use compression at all for this to work.
						Options -z, -Z, -Y, -m are ignored when using this.

						C) '-ah' for merging and repairing operations, the sparse file detection mechanism is disabled by default.
						this option enables it.

					Mandatory Arguments:
						A) '-+' initiates a merging operation, allowing to pass the name of the new merged archive.

						B) '-A' specifies the archive to use as reference, which is mandatory for ARCHIVE ISOLATION (-C option) 
						and MERGING OPERATION ( -+ ).  Else  it  specifies  the rescue catalogue to use when restoring/extracting (-x ), 
						testing (-t ) or comparing (-d ) an archive.
						
						C) '-@' specifies  an  auxiliary  archive  of reference (merging context) or the name of the on-fly isolated catalogue 
						(creation context). This option is thus only available with -+ option (merging) and -c option (archive creation). 
						Note that --aux and --on-fly-isolate are really aliases to the same  option, this is the context of use 
						(archive creation or merging) which lead it to behave a way or another.


					Examples:
						3.1 - MERGING TWO DIFFERENT DAR ARCHIVES
							$ dar -+ ./merged\ archive -A ./archive\ 1 -@ ./archive\ 2 

						3.2 - MERGING A THIRD DAR ARCHIVE
							For merging a 3rd dar archive, you need to do step 3.1 above, then follow this step:

								$ dar -+ ./merged\ archive -A ./merged\ archive -@ ./archive\ 3

				4) MAKING CHANGES TO AN EXISTING DAR ARCHIVE
					Common Options - VERY IMPORTANT -:
						'-ak' can be used to keep files compressed using the same algorithm as the previous archive.

					4.1 - CHANGING COMPRESSION ALGORITHM FROM EXISTING DAR ARCHIVE
						Syntax:
							$ dar -+ ./<new_archive> -A <old_archive> -z<algorithm>

						Practical Example:
							$ dar -+ ./new\ archive -A ./old\ archive -zzstd

					4.2 - CHANGING ENCRYPTION ALGORITHM AND PASSWORD FROM EXISTING DAR ARCHIVE
						Syntax:
							$ dar -+ ./<new_archive> -A <old_archive> -K<algorithm>:<password> -ak

						Practical Example:
							$ dar -+ ./new\ archive -A ./old\ archive -Kscram:asdf -ak

					4.3 - CREATING BACKUP SLICES FROM AN EXISTING DAR ARCHIVE
						'DAR' can create slices from an existing dar file:

							Syntax: 
								$ dar -+ ./<new_archive> -A <old_archive> -s <slice_size> -ak

							Practical Example:
								$ dar -+ ./<new_archive> -A <old_archive> -s 1M -ak

					4.4 - ADDING NEW FILES AND CONTENTS TO AN EXISTING DAR ARCHIVE
						For adding new files, you'll have to go through topic and sub-topics:
							MERGING DIFFERENT DAR ARCHIVES
								MERGING TWO DIFFERENT DAR ARCHIVES
								MERGING A THIRD DAR ARCHIVE
					
					4.5 - MAKING OTHER TYPES OF CHANGES TO EXISTING DAR ARCHIVE
						Most of the sub-topics covered in the topic right below can be applied to any already existing 'dar' archive:
											ADDING DATA TO DAR ARCHIVE

				5) INTEGRITY TEST FOR DAR ARCHIVE
					This integrity test only tests the archive structure and not the contents of the archive:
						$ dar -t ./<archive_filename>

						RECOMMENDED READINGS:
							GENERATING HASH FILE FOR DAR ARCHIVE DURING CREATION
							COMPARING DAR ARCHIVES

				6) REPAIRING A DAMAGED/CORRUPTED DAR ARCHIVE
					DAR makes it possible to isolate the index catalogue from an archive, this allows for quick repairing the catalogue
					in case the archive gets damaged by a power blackout or system crash while you're accessing the archive for listing,
					comparing, extracting or even copying/cloning the archive.

						6.1) ISOLATING THE CATALOGUE
							When the catalogue is damaged, you'll be unable to extract the archive or do any other available operations in it,
							for this reason it's important to isolate the catalogue.

							This merely makes a copy of the catalogue called 'Archive Catalogue', allowing to isolate it:
								$ dar -C ./Archive\ Catalogue -A ./Dar\ Archive
									or
								$ dar - -A ./Dar\ Archive > ./Archive\ Catalogue.1.dar

						6.2) REPAIRING DAR ARCHIVE
							Repairing the archive creates a complete new copy of it by completely overwritting the catalogue as 
							the new repaired archive, it doesn't removes the damaged archive nor replaces it: 
								$ dar -y ./Archive\ Catalogue -A ./Dar\ Archive
							
							After fixing the archive, the "Archive Catalogue" will be the repaired archive.
							
							6.3) LISTING REPAIRED FILES FROM DAR ARCHIVE
								With the repaired archive in hands, you can now list to see which files have been saved by the repair:
									$ dar -l ./Archive\ Catalogue

								Files that have been "recovered" will have been tagged as 'Saved'.

				7) COMPARING DAR ARCHIVES

					7.1 - COMPARING TWO DIFFERENT DAR ARCHIVES
						This will compare 2 different archives, both must share the same catalogue.

							$ dar -d ./Archive\ 1 -A ./Archive\ 2
						
					7.2 - COMPARING ARCHIVE DATA WITH DATA ON THE FILESYSTEM
						DAR also makes it possible to compare the archived data with the data located on the filesystem.
						This will quickly test all files under the filesystem belonging to the dar archive.
						
						Note 1: Just for this case, it'd be wise to archive data as root - during archive creation (using sudo or run0) -
						to ensure the filesystem properties for each archived file are copied as is from the disk to the archive.
						In this case, The same must be done when comparing archived, to ensure all properties are tested.
						archiving as regular user might not copy all filesystem properties, and comparing as regular user will
						not check for filesystem property differences.

						Note 2: This can be useful to see whether data has been properly extracted inheriting all of it's archived 
						filesystem properties (User, group, permissions, FSA, EA, etc) or not as well as checking which files have been removed
						or added/created in the filesystem.

							# dar -d ./Archive\ Name
								or
							# dar -va -d ./Archive\ Name

					RECOMMENDED READINGS:
						GENERATING HASH FILE FOR DAR ARCHIVE DURING CREATION
						INTEGRITY TEST FOR DAR ARCHIVE

				8) EXTRACTING DATA FROM DAR ARCHIVE
					Common Options to use when extracting (-x):
						-k              #Do not remove files destroyed since the reference backup
						-r              #Do not restore file older than those on filesystem
						-f              #Do not restore directory structure

					There is 2 ways of extracting DATA from a DAR Archive:
						A) As regular user
						B) As root

					Extracting data as root will allow you to extract metadata with permissions, ACLs and other filesystem settings
					that has been stored in the archive.

					NOTE: DAR works different from other Archiving/Compressing tools, filename extension DOESN'T has to be passed for dar.
					Example: If your archive name is called 'archive.zst.1.dar', then you only need to pass 'archive.zst' for DAR.
					you can list existing contents using the command on Step #2( $ dar -l )

					8.1 - EXTRACTING ALL CONTENTS FROM DAR ARCHIVE INTO CURRENT FOLDER

							Syntax:
								$ dar -x ./<archive_filename>
									or
								$ dar -x./<archive_filename>

							Practical Example:
								$ dar -x ./archive


					8.2 - EXTRACTING SINGLE CONTENT FROM DAR ARCHIVE INTO CURRENT FOLDER
						NOTE: The <existing_dar_content> must exist within the DAR Archive passed as <filename>.dar .

							Syntax:
								$ dar -x ./<archive_filename> -g <existing_content>

							Practical Example:
								$ dar -x ./archive -g ./existing_content

					8.3 - EXTRACTING DAR ARCHIVE USING PASSWORD
						From 'man page': Available ciphers are "blowfish" (alias "bf"), "aes", "twofish", "serpent" and "camellia" for
						strong encryption and "scrambling" (alias "scram") for a very weak encryption.

						Syntax:
							$ dar -x ./<archive_filename> -K <algo>:<password> 
								or
							$ dar -x./<archive_filename> -K<algo>:<password> 
								or
							$ dar -x./<archive_filename> --key <algo>:<password> 

						Practical Example:
							$ dar -x./archive -K scram:asdf
								or
							$ dar -x./archive -Kscram:asdf
								or
							$ dar -x./archive --key scram:asdf


			SOME DAR LINKS:
				http://www.boomerangsworld.de/cms/avfs/extfs.html
				hhttp://dar.linux.free.fr/doc/benchmark.html
				http://dar.linux.free.fr/doc/Features.html
				http://dar.linux.free.fr/doc/Tutorial.html
				http://dar.linux.free.fr/doc/mini-howto/dar-differential-backup-mini-howto.en.html#simple-dar-usage
				https://wiki.archlinux.org/title/Parchive

		------------------------
		7Z CHEAT SHEET
			NOTE: While '7z' does support the creation and extraction of .tar archives, it's not possible to extract - under a single command - .tar files that 
		have been compressed under compression algorithms (ex.: tar.gz, tar.zst, tar.lz, ... ); this task has to be done programmatically using multiple commands.

		The same has to be done if you want to create a .tar archive and compress it using a 'compression algorithm', this task has to be done programmatically
		See 'Practical Example 3' for both Compression and Extracting below:

			RECOMMENDED TOPICS: 
				[INDEX: COMPRESSING AND UNCOMPRESSING FILES & DATA]

			Compressing:
				Syntax:
					$ 7z a <filename>.7z <data_1> <data_2> <data_3>

				Practical Example 1:
					$ 7z a ~/My\ Filename.7z ~/{data_1,data_2,data_3}
				
				Practical Example 2 - Creating .tar Archive - :
					$ 7z a -ttar ~/My\ Filename.tar ~/{data_1,data_2,data_3}

				Practical Example 3 - Creating .tar.gz archive - :
					$ 7z a -ttar ~/My\ Filename.tar ~/{data_1,data_2,data_3} &&
					$ 7z a -tgzip ~/My\ Filename.tar.gz ~/My\ Filename.tar &&
					$ rm ~/My\ Filename.tar

				Practical Example 5 - Appending Multiple files - :
					$ 7z a ~/My\ Filename.7z ~/file\ 1;
					$ 7z a ~/My\ Filename.7z ~/file\ 2;
					$ 7z a ~/My\ Filename.7z ~/file\ 3;

				Practical Example 6 - Adding Header Encryption -:
					Header Encryption prevents compressed files from being listed:
						$ 7z a -mhe=on archive.7z ~/{data_1,data_2,data_3}

					ex. typing this command will not work:
						$ 7z l archive.7z

			       Practical Example 7 - Protecting compressed archive with Password -:
					$ 7z a -p<password> <archive>.7z <data_to_compress>
					
					Example:
						$ 7z a -pmy_password archive.7z ~/{data_1,data_2,data_3}

				Practical Example 8 - Adding both Header Encryption and Password - :
					$ 7z a -mhe=on -p<password> <archive>.7z <data_to_compress>
			
					Example:
						$ 7z a -mhe=on -pmy_password archive.7z ~/{data_1,data_2,data_3}

			Extracting:
				$ 7z x <file_to_extract>.7z -o<destination_directory>

				Practical Example 1 - Extract Into Current Directory - :
					$ 7z x File.7z -o./Folder\ for\ extraction

				Practical Example 2 - Extracting .tar Archive - :
					$ 7z x File.tar -o./Folder\ for\ extraction

				Practical Example 3 - Extracting .tar.gz Archive - :
					$ 7z x File.tar.gz -o./Folder\ for\ extraction/ &&
					$ 7z x File.tar -o./Folder\ for\ extraction/decompressed/;

			Listing Contents:
				$ 7z l <file_to_extract>
		
		------------------------
		ZIP CHEAT SHEET
			Extracting:
				$ unzip <file_to_extract>.zip -d <destination_directory>

			Listing contents:
				$ unzip -l <file_to_extract>.zip
					or
				$ zip -sf <file_to_extract.zip>

			

---------------------
---------------------
---------------------
---------------------
USING CUPS FOR HANDLING PRINTERS AND PRINTING DOCUMENTS ON LINUX
	CUPS - THE COMMON UNIX SYSTEM PRINTER
		Most recent printers support driverless usage by implementing 'AirPrint' and 'IPP Everywhere' technology.
		CUPS today Relies on AirPrint and 'IPP Everywhere' which requires 'avahi-daemon.service' to be running on the system.

		"IPP Everywhere" allows computers and mobile devices to find and print to networked and USB printers without 
		using vendor-specific software.

		"AirPrint" is a feature in Apple Inc.'s macOS and iOS operating systems for printing without installing printer-specific 
		drivers. 

	ENABLING CUPS TO FIND OUT AVAILABLE PRINTERS FOR USE | ENABLING AND STARTING AVAHI-DAEMON.SERVICE
		Enabling and starting is required for CUPS to find out available printers by using AirPrint and/or IPP_Everywhere.

		1 - For permanently enabling 'avahi-daemon.service' throughout multiple system boots:
			# systemctl enable avahi-daemon.service

		2 - Starting the service:
			# systemctl start avahi-daemon.service

		3 - (Alternative) One in all solution:
			# systemctl enable avahi-daemon.service && systemctl start avahi-daemon.service

		4 - (Optional) Disabling Avahi Service Completely:
			# systemctl disable avahi-daemon.service && systemctl disable avahi-daemon.socket
			# systemctl stop avahi-daemon.service && systemctl stop avahi-daemon.socket

	INSTALLING AND ENABLING 'CUPS' AND 'CUPS-PDF'

		1 - If you're on archlinux:
			# pacman -S cups cups-pdf

			Note: cups-pdf allows the user to create PDF files by printing documents to a virtual printer.

		2 - Enable and Start cups.service:
			# systemctl enable cups.service && systemctl start cups.service

			Note: Enabling a service will persist it throughout multiple system boots.
		
		3 - (Alternative) If you don't want CUPS running as a service, you can use 'Socket Activation' instead:
			Socket Activation will allow CUPS to be initialized only when needed by printer progams:

				3.1 - If you previously enabled and start cups.service, you'll have to disable and stop it from running:
					# systemctl disable cups.service && systemctl stop cups.service

				3.2 - Enable cups.socket, but do not start it:
					# systemctl enable cups.socket

				Note: When an application needs CUPS for printing, it'll 'start' cups.socket and 'stop' it on it's own.

		 4 - (Alternative) Running CUPS as a process using 'cupsd'
		 	It's possible to run 'cupsd' to make it run as a daemon process
				4.1 - Running 'cupsd' as background process:
					# cupsd
					
					Note: you want to kill 'cupsd' once you've finished all your tasks using the bellow command:
						# killall -r cupsd

				4.2(Alternative) Running 'cupsd' as foreground process:
					This will allow 'cupsd' to take over the terminal as a running process,
					press ctrl+c to finish and kill it afterwards:
						# cupsd -f

				
	NOTE: After doing either Step #2 or Step #4, you may want to access 'http://localhost:631' for more information.

	VERY IMPORTANT: Some of the mentioned tools below here may require either Step #2 or Step #4 to be done, they won't work with Step #3.
	
	VERIFYING YOUR USB PRINTER IS CONNECTED
		$ lsusb

		output example:
			Bus 002 Device 007: ID 9b0:300 Hewlett-Packard DeskJet
		
	VERIFYING YOUR PARALLEL PORT PRINTER IS CONNECTED
		Using a printer connected on the parallel port will require the 'lp', 'parport' and 'parport_pc' kernel modules to be installed.
	You'll need to check the 'IMPORTANT LINKS' section if you're completely clueless on how to set them up.

			1 - Querying for parallel port printers:
				# dmesg | grep -i parport

			output example:
				parport0: Printer, Hewlett-Packard HP LaserJet
				lp0: using parport0 (polling)

	ADDING A NETWORK PRINTER
		You'll need to use 'lpadmin' for adding a printer on the network.
			# lpadmin -p <printer_name> -E -v "<device_URI>" -m <driver>
						
		Example:
			# lpadmin -p myprinter -E -v "ipp://11.22.33.44/ipp/print" -m everywhere

	ADDING A LEGACY PRINTER
		As explained before, since 2010 manufactured printers have support for 'IPP Everywhere' and 'AirPrint'.

		Legacy(old) printers are supported by CUPS using PPD (PostScript Printer Description) files that describe printer capabilities 
		and driver programs needed for each printer. CUPS includes several sample PPD files for common legacy printers,
		type in :
			$ lpinfo -m | less

			Read topic if you don't know how to use the 'less' tool:
				"HOW TO USE THE 'LESS' COMMANDLINE TOOL"

		Once you have the Device URI and Driver Name you can add the printer using:
			# lpadmin -p <PRINTER_ALIAS> -E -v "<DEVICE_URI>" -m <DRIVER_NAME>

		MORE PPDS FILES:
			1 - The Linux Foundation's OpenPrinting workgroup's foomatic provides PPDs for many printer drivers, 
			here's a small list of packages available on Archlinux: 
					'foomatic-db-ppds' 'foomatic-db-nonfree-ppds'

				You can install them using:
					# pacman -S foomatic-db-ppds foomatic-db-nonfree-ppds
				
			2 - The 'Guttenprint Project' also provides ppds files for use with CUPS and GIMP:
			Install them:
				 # pacman -S gutenprint and foomatic-db-gutenprint-ppds

				VERY IMPORTANT: When the 'gutenprint' or 'foomatic-db-gutenprint-ppds' packages gets updated  post-installation,
				you'll need to to run the following command:
					# cups-genupdate
					# systemctl restart cups.service

			3 - Many printer manufacturers provides their own Linux Drivers,
			you can look for it in the AUR by having YAY Installed and set on your system, read more on:
				"INSTALLING YAY / AUR REPOSITORY"

			IMPORTANT: Remember, only install 'ppds' files if you own a legacy printer device with no support 
			to 'AirPrint' and 'IPP Everywhere'.


	CUPS MANAGEMENT THROUGH WEB INTERFACE
		CUPS allow complete management through it's web interface:	
				http://localhost:631
				http://localhost:631/jobs/
				http://localhost:631/printers/
				http://localhost:631/classes/
				http://localhost:631/admin

	VIEWING PRINTER INK LEVELS
		Some printers provide support for viewing how much ink remains on the printer device,
		if you're on archlinux, install 'ink' package from the AUR:
			# yay -S ink

			Read more on if you need:
				"INSTALLING YAY / AUR REPOSITORY"

			Very Important: you'll need to add yourself to the 'lp' group and then login again:
				# usermod -aG lp <username>
				$ sudo -iu <username>

		 Examples:
			1 - queries the first usb device for ink levels:
				$ ink -p usb

			2 - queries the second usb device for ink levels:
				$ ink -p usb -n 1

			3 - prints usb devices that are below 37% ink:
				$ ink -p usb -t 37 	

	SETTING UP DEFAULT PAGE SIZE FOR CUPS
		1 - List all available paper size
			$ paper --no-size --all
		
		2 - Once you've chosen the desired paper size, you can add it to /etc/papersize:
			# nvim /etc/papersize

	SETTING UP DEFAULT PRINTER FOR CUPS USING 'LPADMIN'
		$ lpadmin -d <printer_name>

		Note: if you're using 'zsh' you can press TAB Key for listing available printers.
			$ lpadmin -d <tab-key>

	DISPLAYING DEFAULT SYSTEM PRINTER USING 'LPSTAT'
		$ lpstat -d
	
	DISPLAYING ALL AVAILABLE SYSTEM PRINTERS USING 'LPSTAT'
		$ lpstat -p
			or
		$ lpstat -v
	
	LISTING ALL QUEUED PRINT JOBS USING 'LPSTAT'
		$ lpstat -u
		
		Note: Only shows print jobs queued by the current user.
	
	CANCEL ALL QUEUED PRINT JOBS USING 'LPRM'
		$ lprm -

	CANCEL ONLY CURRENT QUEUED JOB USING 'LPRM'
		$ lprm

	USING 'LP' FOR PRINTING DOCUMENTS ON COMMANDLINE
		This tool relies on an already set default printer on the system, 
		CUPS provides many ways for setting up default printers, 'lpadmin' can be used to do this, read more on topic:
			"SETTING UP DEFAULT PRINTER FOR CUPS"
			
		PRINTING A FILE
			$ lp <path_to_file>
				or
			$ lp ~/Documents/Document_A.pdf
		
		PRINTING FILE USING A KNOWN PRINTER
			$ lp -d <printer_name> <path_to_file>
				or
			$ lp -d lp0 ~/Documents/Document_A.pdf

			Note: if you're using 'zsh' you can auto-complete the printer's name using the TAB Key on your keyboard
				$ lp -d <TAB-KEY> <path_to_file>

		PRINTING A KNOWN NUMBER OF COPIES USING THE DEFAULT PRINTER
			$ lp -n <number> <path_to_file>
				or
			$ lp -n 5 ~/Documents/Document_A.pdf

		PRINTING SPECIFIC PAGES USING THE SYSTEM DEFAULT PRINTER
			This will print only page 1, page 3 to 5 and page 8:
				$ lp -P 1,3-5,8

		PRINTING OUTPUT OF A COMMAND
			This will send any output of a given command to the printer as text:
				$ <command> | lp
					or
				$ cal -y | lp

		IMPORTANT LINKS
			https://wiki.archlinux.org/title/CUPS
			https://wiki.archlinux.org/title/Kernel_module
			https://github.com/OpenPrinting/cups/

			These links will only work if you have either 'cupsd' or 'cups.service' running:
				http://localhost:631
				http://localhost:631/jobs/
				http://localhost:631/printers/
				http://localhost:631/classes/
				http://localhost:631/admin

---------------------
---------------------
---------------------
---------------------
THE VERY QUICK INTRODUCTION TO GIMP
	Hint: Always save image files as .xcf when working with selection and masks.
	Hint2: When working with 'Layer grouping', applying effects on 'layer group' instead of an individual layer works differently,
	it allows to apply the choosen effect for all effects of every single layer in the group.
	
	CHOOSING TMPFS DIRECTORY AS SWAP FOR GIMP
		If you know you have more than enough RAM Memory, you can do this:

		1 - Click 'Edit' tab -> Go to 'Preferences' -> Click on 'Folder' -> Select /tmp/ as swap directory
		2 - Click 'Edit' tab -> Go to 'Preferences' -> Click on 'Folder' -> Select /tmp/ as temporary folder.

		Note: By default, swap directory is set to a folder on your hard disk.
		By default, temporary directory is set inside /tmp/ folder on linux.

	SAVING FILES WITH BEST COMPRESSION
		Click on 'File' tab -> Click on 'Save File as' -> Click on 'Save this .XCF file with better but slower compression' -> 
		Click 'Select File Type by Extension' -> Now choose 'xz archive' (xcf.xz or xcfxz) -> Click Save

	USING THE PATH TOOL FOR CREATING MASKS AND CHANNELS
		The 'paths tool' can be used to better create and handle 'selection', 'masks' and 'channels'.
	With the 'paths tool' you can easily modify(edit/update) an existing 'mask' or 'channel'

		1 - First create a path using the 'Paths Tool'

		2 - Create a new Selection from Path
			Click on 'Select' tab -> then click on 'Selection from Paths'

		3 - Create a new Channel based on the current selection
			Click on 'Select' tab -> now click on 'Save to Channel'

		4 - Create a new Path
			Click on 'Select' tab -> click on 'To Path'


		CONVERTING A CHANNEL AND PATHS TO SELECTION FOR EDITING
			This is useful for creating a new selection based on an already existing one, given that you have saved it 
		previously as either 'Channel' or 'Path'.

			1 - Select 'Windows' Tab -> Hover over the mouse on 'Dockable Dialogs' -> Click on either 'Path' or 'Channels'

			2 - Select a 'Path' or 'Channel' -> Right-Click on the choosen Path/Channel -> 
			Click on 'Path to Selection' or 'Channel to Selection'

			3 - Note: Besides creating a new 'Selection' from an existing path/channel that it' also possible to Add, Subtract or Intersect
			with an already existing 'Selection'.

		EDITING AN ALREADY EXISTING PATH

			1 - Select 'Windows' Tab -> Hover over the mouse on 'Dockable Dialogs' -> Click on either 'Path'.

			2 - Select a 'Path' -> Right-Click on the choosen Path -> Click on 'Edit Path'.

			3 - Note: This will allow you to modify an existing path, which can later be converted into a new selection,
			also allowing you to create a new channel if necessary.

	SAVING SELECTION AS A NEW CHANNEL
		A Channel allows to save a selection for creating new masks:
			Right-Click (after making selection) > Select > Save to Channel

		MAKING/LOADING SELECTION FROM EXISTING CHANNEL
			If a selection has already been saved/converted to channel: 

				1 - Select 'Windows' Tab -> Hover over the mouse on 'Dockable Dialogs' -> click on 'Channels'

				2 - Select a 'Channel' -> Right-Click on the choosen 'Channel' -> Click on 'Channel to Selection'

				3 - Note: Besides creating a new 'Selection' from an existing channel that it' also possible to Add, Subtract or Intersect
				with an already existing 'Selection'.

	CREATING A LAYER MASK | APPLYING LAYER MASK | APPLYING MASK TO LAYER
		For this to work, a 'Channel' needs to be created using 'Selection' tools; in turn a 'Selection' can be made from either an
		existing 'Channel' or 'Path; Only one single mask can be a applied per existing layer:

			1 - Click on 'Windows' tab -> Hover over the mouse on 'Dockable Dialogs' -> Click on 'Layers'

			2 - Choose the desired layer and then duplicate it:
				2.1 - Click on 'Layer' tab -> Click on 'Duplicate Layers'.

			3 - Now right click the new duplicated layer -> Click on 'Add Layer Masks' :
				-> Click on 'Selection' to Initialize Layer Mask from what's been already selected.
					or
				-> Click on 'Channel' to Initialize Layer Mask from an existing channel.

			4 - Finally, Right-click the layer on #4 again -> Click on 'Apply Layer Masks'
			

	SELECTION EDITOR
		It might be useful to edit a selection when making an either incomplete or bad 'LAYER MASK', 'CHANNEL' or 'PATH'
			1 (Optional) - This will assist you in making a new selection:
				Click on 'Select' tab -> Click on 'Selection Editor' 

			2 - Select either a 'Channel' or 'Path':
				Read topic on: 
					'CONVERTING A CHANNEL AND PATHS TO SELECTION FOR EDITING'

			3 - Either 'select', 'add', 'subtract' or 'intersect' by Right-Clicking the Channel/Path.

			4 - Happy Editing!

	DUPLICATING LAYERS
		Useful when you need to work on multiple Layer Masks:
			Select Layer -> Right-Click the image -> Layer -> 'Duplicate Layers'
				or
			Select Layer -> Click on 'Layer' Tab -> 'Duplicate Layers'

		DELETING A LAYER MASK
			Useful after duplicating layers that contain a mask:
				Select Layer -> Right click on 'Delete Mask'	
					or
				Select Layer -> Click on 'Layer' tab -> 'Delete Layer'

	APPLYING MASK
		Allows the Mask to be used as a Regular Layer,
	Duplicating Layers might be useful before applying a mask:

		Select Layer -> Right-Click  'Apply Layer Mask'
			or
		Select Layer -> Click on Layer tab -> Mask -> Apply Layer Mask

	CREATE NEW LAYER GROUP
		Useful for grouping together Masks that have been applied as a new layer,
	allowing these layers to be easily duplicated for applying and testing different effects:

		Click on 'Layer' tab -> New Layer Group

	TRANSFORM TOOLS
		Changing image positioning, angle and visualization:
			Click 'Tools' Tab -> Click on 'Transform Tools' -> Choose a transform:
				1 - Unified Transform
				2 - Handle Transform
				3 - Perspective
				4 - Shear
				5 - Scale
				6 - Cage Transform
				7 - Warp Transform


	BETTER FREEDOM WHEN COLORIZING
		Recommended when applying colorization to masks:
			1.1- Colors > Exposure > Set exposure until it allows to reveal more color, either by clearing or darkenning the image.
				1.1.2 - You can also use 'Blending Options' in the 'Exposure' effect, by choosing an opacity/transparency effect,
				- NOTE: Colorization also has opacity options! - 
				here are some good Blending Modes:

					1.1.2.1 - Screen
						Useful when trying to use Light Colors but it doesn't blends too well.

					1.1.2.2 - 'Burn' and 'Linear Burn'
						Burns vivid color into a dark one. Useful when a choosen color is too bright/too light, 
					it helps enhancing the dark spots by controlling 'Opacity' option. 

					1.1.2.3 - Darken Only
						Helps applying Dark Color from a Vivid Color, by controlling saturation, lightness and
					opacity.

					1.1.2.4 - Subtract
						Subtracts the choosen color, acting like a color removal filter for the mask/selection.

					1.1.2.5 - Grain Extract
						Similar to Subtract.

					1.1.2.6 - Hard Mix
						Helps in applying dark colors.

					1.1.2.7 - Vivid light
						Helps applying vivid light colors by balancing 'Opacity', 'Saturation' and 'Lighteness'.

						Note: The 'Dodge' effect may help in situations where 'Vivid Light' can't and vice-versa.

					1.1.2.8 - Dodge
						Helps applying vivid color by controlling Saturation, Lighteness and Opacity.
						'vivid light' might still be a better option.

						Note: However, Dodge may help in situations where 'Vivid Light' can't and vice-versa.

			1.2 - Colors > Colorize > Set whatever color you want.

			1.3 - Colors > Colorize > Click to select color you want > Click on 'Wheel' tab > Click on 'HSV' Option
			next to 'LCh', on the  'H' letter you can switch colors using the same saturation and color value.

	SOME GOOD FILTERS
		Apply filters to image:
			1.1 - Click on 'Filters' tab:
				1.1.1 - 'Filters' Tab -> Click on 'Distort'
					1.1.1.1 - 'Apply Lens'
					1.1.1.2 - 'Emboss'
					1.1.1.3 - 'Engrave'
					1.1.1.4 - 'Lens Distortion'
					1.1.1.5 - 'Kaleidoscope'
					1.1.1.6 - 'Mosaic'
						Makes image look from old magazine.
					1.1.1.7 - 'Newsprint'
						Same as mosaic.
					1.1.1.8 - 'Polar Coordinates'
						Similar to Kaleidoscope.
					1.1.1.9 - 'Ripple'
						Adds Water ripple effect.
					1.1.1.10 - 'Shift'
						Adds shifting pixels effect to image.
					1.1.1.11 - 'Spherize'
						Similar to 'Polar Coordinates'.
					1.1.1.12 - 'Video Degradation'
						Adds video degradation effect to image.
					1.1.1.13 - 'Waves'
						Water drop effect.
					1.1.1.14 - 'Whirl and Pinch'
						Whirlwind effect.
					1.1.1.15 - 'Wind'
						Adds Wind effect to the image.
					1.1.1.16 - 'Blinds'
						Adds blind stripes to the image.
					1.1.1.17 - 'Curve Bend'
						Allows bending the image.
					1.1.1.18 - 'Page Curl'
						Turning over page effect.
												
				1.1.2 - 'Filters' Tab -> Click on 'Blur'
					1.1.2.1 - 'Zoom Motion blur'
					1.1.2.2 - 'Linear Motion blur'
						Adds directional motion blur, like god's ray effect or volumetric lights.
					1.1.2.3 - 'Selective Gaussian Blur'
					1.1.2.4 - 'Median Blur'
					1.1.2.5 - 'Mean Curvate Blur'
					1.1.2.6 - 'Lens Blur'

				1.1.3 - 'Filters' Tab -> Click on 'Enhance'
					1.1.3.1 - 'High Pass'
					1.1.3.2 - 'Noise Reduction'
					1.1.3.3 - 'Red Eye Removal'
					1.1.3.4 - 'Sharpen'
						Looks very good after 'Noise Reduction' has been applied.
					1.1.3.5 - 'Despeckle'
					

				1.1.4 - 'Filters' Tab -> Click on 'Lights and Shadow'
					1.1.4.1 - 'Bloom'
					1.1.4.2 - 'Lens Flare'
					1.1.4.3 - 'Gradient Flare'
					1.1.4.4 - 'Lightning Effect'
					1.1.4.5 - 'Vignete'
					
				1.1.5 - 'Filters' Tab -> Click on 'Noise'
					1.1.5.1 - 'CIE Ich noise'
					1.1.5.2 - 'HSV Noise'
					1.1.5.3 - 'Hurl'
					1.1.5.4 - 'Pick'

				1.1.6 - 'Filters' Tab -> Click on 'Edge-Detect'
					All of the filters

				1.1.8 - 'Filters' Tab -> Click on 'Generic'
					1.1.8.1 - 'Distance Map'
					1.1.8.2 - 'Normal Map'
					1.1.8.3 - 'Erode'
						Very good in enhancing images

				1.1.9 - 'Filters' Tab -> Click on 'Combine'
					1.1.9.1 - 'Filmstrip'
						Creates a filmstrip from multiple images.

				1.1.10 - 'Filters' Tab -> Click on 'Decor'
					1.1.10.1 - 'Fog'
					1.1.10.2 - 'Stain'
					1.1.10.3 - 'Fuzzy Border'
					1.1.10.4 - 'Old Photo'
					1.1.10.5 - 'Round Corners'
						Creates border for better displaying on web page.
					1.1.10.6 - 'Add Border'
						
				1.1.11 - 'Filters' Tab -> Click on 'Artistic'
					1.1.11.1 - 'Apply Canvas'
					1.1.11.2 - 'Cartoon'
					1.1.11.3 - 'Cubism'
					1.1.11.4 - 'Glass Tile'
					1.1.11.5 - 'Oilify'
					1.1.11.6 - 'Photocopy'
					1.1.11.7 - 'Simple Linear Clustering'
						Very good.
					1.1.11.8 - 'Soft Glow'
						Very good.
					1.1.11.9 - 'Waterpixels'
						Similar to 'Cartoon'
					1.1.11.10 - 'Clothify'
						Makes image looks like it's been drawn on a piece of cloth.
					1.1.11.11 - 'GIMPressionist'
						Applies several effects not listed.
					1.1.11.12 - 'Weave'
						Makes image looks like it's been drawn on weave.

				1.1.12 - 'Filters' Tab -> Click on 'Map'
					1.1.12.1 - 'Fractal Trace'
					1.1.12.2 - 'Illusion'
					1.1.12.3 - 'Little Planet'
					1.1.12.4 - 'Panorama Projection'
					1.1.12.5 - 'Paper Tile'
					1.1.12.6 - 'Tile Seamless'
					1.1.12.7 - 'Small tiles'
					1.1.12.8 - 'Wrap'

	-----
	MORE OPTIONS:
		Some of these options are applied on a layer basis, which means you'll be required to chose one layer before applying these.

		INCREASING SHADOW HIGHLIGHTS:
			Increasing shadow highlights can help reduce the perceived light reflection in an image.

				Click 'Color' tab -> Click 'Shadows-Highlight'

		POSTERIZE:
			Adds a cool posterize effect:
				Click 'Color' tab -> Click 'Posterize'
		
		COLOR TO ALPHA:
			Can be used to improve 'Colorize' options, only use when you feel you've got the right color, 
		this will add color to transparency/oppacity effects:
				
				Click 'Color' tab -> Click 'Color to Alpha'

		ROTATE COLOR:
			Rotates a chosen color into a new color on the selected layer:
				Click 'Color' tab -> Click 'Map' -> Rotate Colors

		CHANNEL MIXER:
			This will change the current color RGB Values, likely making it hard to apply other colors:
				Click 'Color' tab -> Click 'Components' -> Channel Mixer

		RGB CLIP:
			Same as 'color to alpha', but can help in shading the color of the image/mask instead of brigthness/contrast.

		OTHER EFFECTS:
				Click 'Color' tab -> Invert
				Click 'Color' tab -> Linear Invert
				Click 'Color' tab -> Value Invert
				Click 'Color' tab -> Click 'Map' -> Alien Mapping
				Click 'Color' tab -> Click 'Tone Mapping' -> Destripe
				Click 'Color' tab -> Click 'Tone Mapping' -> Retinex
				Click 'Image' tab -> Click 'Mode' -> Grayscale



OTHER OPTIONS ON GIMP'S GUI ( OLD ): 
	1 - Select > By Color (Shift+O)
	2 - Colors > Colorize
	3 - Working with Layers:
		Layer > Duplicate Layer
		then Select > Select Invert
		finally Press <Del>
	4 - Zoom IN/OUT: 
		Press <+> or <->
	5 - Magic Wand/Fuzzy Selection Tools:
		Tools > Selection Tools 
		> Fuzzy Select

	Note.: You can hold down ALT key and press the first letter to each of those options(Faster-Productive)
---------------------
---------------------
---------------------
---------------------
USING PIPEWIRE AS AUDIO RECORDER
	This will only work if you're using pipewire.

	0) Find out if you're on pipewire:
		$ pactl info | grep -P -i "pipewire"

		0.1) ( Alternative) Try running 'pw-top', if you're on pipewire, pw-top should work:
			$ pw-top

		NOTE: If you're not on pipewire, use something else instead like 'ffmpeg' or 'sox'.

	USING PIPEWIRE FOR RECORDING MICROPHONE AUDIO  
		This will use the input device for recording audio:

			$ pw-record ~/output.flac
				or
			$ pw-record --volume 1.25 ~/output.flac
				or
			$ pw-record --rate 48000 ~/output.flac

	USING PIPEWIRE TO RECORD DESKTOP AUDIO
		If you're using pipewire, you can use this to capture and record desktop audio:
			$ pw-record -P '{ stream.capture.sink=true }' ~/test.flac   

	USING PIPEWIRE TO RECORD AUDIO FROM SPECIFIC APPLICATION
		This will record audio from one single specific application:
		VERY IMPORTANT: This method will only work if the chosen program is already running and sending audio as output.
		
		1) List all pipewire objects to find the one program you want:
			$ pw-cli list-objects | grep node.name
		
			1.1) Alternative solution, if you're using an old pipewire version you'll have to fetch the object.serial 
			of the active program:
				1.1.1) using 'wpctl' to fetch object ID:
					$ wpctl status
						or
					$ wpctl status | less

				1.1.2) use 'pw-cli' to find object.serial of a given ID:
					$ pw-cli <Object_ID>
						or
					$ pw-cli info 486

				1.1.3)(Alternative) Fetching object serial of a given ID using grep:
					$ pw-cli info <Object_ID> | grep -P -i "object.serial"
						or
					$ pw-cli info 486 | grep -P -i "object.serial"

		2) This will record audio from a single program:
			$ pw-record --target < node.name | object.serial > ~/audio.flac
				or
			$ pw-record --target "Firefox" test.flac   
				or
			$ pw-record --target=Firefox test.flac   
				or
			$ pw-record --target="Firefox" test.flac   
				or
			$ pw-record --target=10805 test.flac
			
	USING PIPEWIRE FOR TESTING AUDIO FILE 
		This will playback your audio file:
			$ pw-play ~/output.flac

---------------------
---------------------
---------------------
---------------------
USING FFMPEG - THE UNIVERSAL MEDIA CONVERTER

	RECORDING MICROPHONE AUDIO USING FFMPEG
		VERY IMPORTANT: If you have PIPEWIRE, it's better to use 'pw-record' for recording audio from microphone,
	Only follow these instructions if you're not using pipewire:

		0) Find out if you're on pipewire:
			$ pactl info | grep -P -i "pipewire"

			0.1) ( Alternative) Try running 'pw-top', if you're on pipewire, pw-top should work:
				$ pw-top

			NOTE: If it works, this means you're using pipewire, use 'pw-record' instead.

		1) Find out your sink/source audio device:
			$ pactl info
			
			output example:
				Default Sink: alsa_output.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.iec958-stereo
				Default Source: alsa_input.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.mono-fallback

				NOTE: 'Sink' is the Speaker/Headphone, 
				'Source' is the microphone or any other input device.

		2) Record audio using the following:
			This will record audio. 
				$ ffmpeg -f pulse -i <monitor_source_name> < audio_filename.<audio_format> >
						or
				$ ffmpeg -f pulse -i alsa_input.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.mono-fallback ~/audio_output.wav

		3) Test audio with MPV:
			This requires installing 'mpv'.
				$ mpv <audio_filename>
					or
				$ mpv ~/audio_output.wav

	EXTRACTING AUDIO FROM VIDEO
		Note, you can choose whichever 'audio format' you want:
			$ ffmpeg -i <path_to_video> -vn < path_to_sound.<audio_format> >
					or
			$ ffmpeg -i ./video.mp4 -vn ./audio_output.mp3

	TRIM A VIDEO FROM ONE POINT TO ANOTHER
		Note, you can choose whichever 'video format' you want:
			$ ffmpeg -i <path_to_video> -ss <start_time> -to <end_time> -c copy < output_video.<video_format> >
					or
			$ ffmpeg -i ./video.mp4 -ss 00:10 -to 0:30 -c copy ./video_output.mp4


	CONVERTS IMAGES INTO EITHER ANIMATED .GIF OR .MPG
		This will convert all filenames starting with 'image_' and ending on a number:
			$ ffmpeg -i ./image_%d.* -f image2 ./video.mpg
				or
			$ ffmpeg -i ./image_%d.* -f image2 ./video.gif
	
	SAVE VIDEO AS AN ANIMATED .GIF
		This will covnert video to .gif, -r will set the framerate to 15:
			$ ffmpeg -i <path_to_video> -vf 'scale=-1:1000' -r <framerate> ./output.gif
				or
			$ ffmpeg -i ./video.mp4 -vf 'scale=-1:1000' -r 15 ./output.gif

	RECORDING SCREEN USING FFMPEG
		NOTE: This is a xorg server only solution

		I'd recommend using 'OBS STUDIO' instead of using 'fmmpeg', however
	it's still possible to do this task using 'ffmpeg':

		$ ffmpeg -f x11grab -video_size <screen_resolution> -framerate <frame> -i :0.0+0,0 <video_filename.<video_format>>
				or
		$ ffmpeg -f x11grab -video_size 1280x768 -framerate 25 -i :0.0+0,0 ~/output.mp4

		RECORDING SCREEN ALONG WITH AUDIO INPUT/SOURCE DEVICE
			This full example will require you to follow both steps:

				1 - RECORDING SCREEN USING FFMPEG
				2 - RECORDING MICROPHONE AUDIO USING FFMPEG

			Example:
				$ ffmpeg -f x11grab -video_size 1280x768 -framerate 25 -i :0.0+0,0 -f pulse -i alsa_input.usb-C-Media_Electronics_Inc._USB_Audio_Device-00.mono-fallback ~/output.mp4

	CONVERTING VIDEOS INTO A MORE COMPRESSED FORMAT
		AV1 is a compression format, not a file format, file format must remain the same.

		This will convert video to AV1 format using 'libsvtav1' codec:
			$ ffmpeg -i <input_video>.mp4 -c:v libsvtav1  <output_video>-2.mp4

	CONVERTING AUDIO INTO A MORE COMPRESSED FORMAT
		OPUS is an audio compression format, not a file format, file format must remain the same.
	Opus however resamples any audio to 48 kHz, so some audio files may actually get bigger in size after the resample.

		This will convert audio to OPUS format:
			$ ffmpeg -i <input_audio>.mp3 -c:a libopus <output_audio>-2.mp3
	
	CONVERTING BOTH VIDEO AND AUDIO TO MORE COMRPESSED FORMAT
		This will convert audio and video to a more compressed format:
			$ ffmpeg -i <input_video>.mp4 -c:a libopus -c:v libsvtav1  <output_video>-2.mp4
			
	
	BATCH CONVERSION OF MULTIPLE VIDEOS INTO A MORE COMPRESSED FORMAT
		This will convert all video files in the current directory only into a folder named ./COMPRESSED
	it will require the use of both 'find' and 'awk' tool:

			1 (Optional) - You can run this to see how the commands would be executed, before executing it on the step #2:
				$ find -maxdepth 1 -type f -printf '%f\n' |
				awk '{system("echo ffmpeg -i \""$0"\" -c:v libsvtav1 \"./COMPRESSED/"$0"\"");}'

			2 - Use this to make the conversion:
				$ mkdir ./COMPRESSED && find -maxdepth 1 -type f -printf '%f\n' |
				awk '{system("ffmpeg -i \""$0"\" -c:v libsvtav1 \"./COMPRESSED/"$0"\"");}'
			
			3 (Optional) - Does the same conversion on step #2, 
			but limits conversion to files greater than 150MB:
				$ mkdir ./COMPRESSED && find -maxdepth 1 -size +150M -type f -printf '%f\n' |
				awk '{system("echo ffmpeg -i \""$0"\" -c:v libsvtav1 \"./COMPRESSED/"$0"\"");}'
			
	MORE LINKS
		https://www.ffmpeg.org/ffmpeg-codecs.html
---------------------
---------------------
---------------------
---------------------
THE ONION WEB / TOR WEB
	HOW TO USE TOR
		1 - On linux, download and install 'torbrowser-launcher' and 'tor' packages.
		2 - launch 'tor' as root if needed, this will connect you to the tor proxy server.
		3 - launch 'torbrowser-launcher'
		4 - have fun
	
	LINKS
		http://deepmlzxkh7tpnuiv32nzzg6oxza4nvpd6b7ukujwxzgxj2f33johuqd.onion/ - Deep Link Guide
		http://torlinksge6enmcyyuxjpjkoouw4oorgdgeo7ftnq3zodj7g2zxi3kyd.onion/ - Tor Links
		http://libraryfyuybp7oyidyya3ah5xvwgyx6weauoini7zyz555litmmumad.onion/ - Just Another Library (Books)
		http://ovgl57qc3a5abwqgdhdtssvmydr6f6mjz6ey23thwy63pmbxqmi45iid.onion/ - Flashligt 2.0

		http://mail2torjgmxgexntbrmhvgluavhj7ouul5yar6ylbvjkxwqf6ixkwyd.onion/ - Mail2Tor
		http://nv3x2jozywh63fkohn5mwp2d73vasusjixn3im3ueof52fmbjsigw6ad.onion/ -  Comic Book Library
		https://protonmailrmez3lotccipshtkleegetolb73fuirgj7r4o4vfu7ozyd.onion/ - Proton Mail
		http://archiveiya74codqgiixo33q62qlrqtkgmcitqx5u2oeqnmn5bpcbiyd.onion/ - Archive  today
		http://zqktlwiuavvvqqt4ybvgvi7tyo4hjl5xgfuvpdf6otjiycgwqbym2qad.onion/ -  The Hidden Wikia
			http://zqktlwiuavvvqqt4ybvgvi7tyo4hjl5xgfuvpdf6otjiycgwqbym2qad.onion/wiki/index.php/Main_Page

		http://w7m432cocr665kf5tlpcxojwldajr3njd2etcxwhpbrt44eemuxhp7ad.onion/ - 8kun imageboard

		http://3bbad7fauom4d6sgppalyqddsqbf5u5p56b5k5uk2zxsy3d6ey2jobad.onion/	- Tor Search
		http://torbox36ijlcevujx7mjb4oiusvwgvmue7jfn2cvutwa6kl6to3uyqad.onion/welcome - TorBox Onion Mail
		http://2fd6cemt4gmccflhm6imvdfvli3nf7zn6rfrwpsy7uhxrgbypvwf5fad.onion/ - Excavator Search
---------------------
---------------------
---------------------
---------------------
---SOME WINDOWS STUFF
	SOME WINDOWS UTILITIES
		These tools are located under C:\Windows\System32\

		perfmon.msc			#Performance Monitor
		tpm.msc				#TPM Manager
		certlmgr.msc			#System Certification Manager
		certmgr.msc			#Local User Certification Manager
		comexp.msc			#Component Services
		compmgmt.msc			#Computer Management
		devmgmt.msc			#Device Manager
		DevModeRunAsUserConfig.msc
		diskmgmt.msc			#Disk Manager
		fsmgmt.msc			#File Share Manager
		gpedit.msc			#Group Policy Editor
		lusrmgr.msc			#Local Users and Groups Manager
		printmanagement.msc		#Printer Manager
		rsop.msc			#Resultant Set of Policy (RSoP) - group policies applied to a user. 
		secpol.msc			#Local Security Policy
		taskschd.msc			#Task Scheduler
		WF.msc				#Windows Firewall
		WmiMgmt.msc			#WMI Management
		eventvwr.msc			#System Event Log Viewer
		services.msc			#Services 

		appwiz.cpl			#Application Wizard - Uninstall programs
		desk.cpl			#Monitor Settings
		Firewall.cpl			#Firewall Manager (Same as WF.msc)
		hdwwiz.cpl			#Hardware Wizard - Device Manager
		inetcpl.cpl			#Internet Control Panel - Internet Properties
						Note: This is only meant for 'Internet Explorer' web browser.
		intl.cpl			#International Settings and Language
		joy.cpl				#Joystick/Gamepad Settings
		main.cpl			#Mouse Settings
		mmsys.cpl			#Audio Settings
		ncpa.cpl			#Network Control Panel Applet
		powercfg.cpl			#Power Configuration
		RTSnMg64.cpl			#RealTek Sound Manager
		timedate.cpl			#System Clock/Date
		irprops.cpl			#Infrared Control Panel
		bthprops.cpl  			#Bluetooth Control Panel

		cmd.exe
		chkdsk.exe			#Check Disk Utility
		chkntfs.exe			#Can be used to set up automatic disk verification during boot
		doskey.exe			#Configure aliases for Command Prompt
		regedit.exe			#Register Editor
		regedt32.exe			#Same as Regedt32.exe
		msconfig.exe			#Services initialization, Program initialization, System initialization
		OptionalFeatures.exe		#Enable/Disable Windows Features
		dxdiag.exe			#Direct X Diagnostics - Provides Basic System Diagnostics
		winget.exe			#Windows Commandline Package Manager
		runas.exe			#Allows running programs/commands as a different user.
						example: 
							$ runas /user:Administrator "cmd"
		msinfo32.exe			#Some system resources: physical/virtual memory

		SystemPropertiesAdvanced.exe	#Manages Environment Variable and Virtual Memory
		sysdm.cpl			#Same as above
		tasklist.exe				#Lists all running processes
		taskkill.exe /im <program_name> /t /f	#Kills runnning process by it's name
		taskkill.exe /pid <prorgam_pid> /t /f	#Kills runnning process by it's PID


	RUNNINNG COMMAND AS ADMINISTRATOR
		$ runas.exe /user:Administrator "cmd"
	
	SOME WINDOWS 11 HOTKEYS
		system_key + tab		#Virtual Desktops
		system_key + I			#Access to system settings, developer mode, etc
		system_key + r			#Execute commands

	SOME WSL CONFIGURATION FILES | WSL SETTINGS
		Reference: https://learn.microsoft.com/en-us/windows/wsl/wsl-config#wslconf

		WSL offers a few different configuration files, each of them containnning different settings and configurations:

			1) In this file you can set up Swap Space, Total Ammount of RAM, Default username,
			Number of Logical Processors and also set a path for custom kernel for all WSL Distributions:
				C:\Users\<YOUR_USERNAME>\.wslconfig needs to be created by the user

				EXAMPLE:
					[file: C:\Users\<USERNAME>\.wslconfig ]
						[wsl2]
						kernel="C:\\Users\\User\\.wsl-kernel\\bzImage"
						swap=0
						memory=9GB
					[/file]

			2) WSL also creates some configuration files after installing any linux distribution:
				These files contain different settings:

				2.1 - This file containns Network, Interop, Username created on the first run, Boot, GPU and Time settings:
					/etc/wsl.conf is created by default inside your WSL Linux Distro
						EXAMPLE:
							[file: /etc/wsl.conf]
								[boot]
								systemd=true

								[gpu]
								enabled=true

								[system-distro-env]
								WESTON_RDP_MONITOR_REFRESH_RATE=75

							[/file]

				2.2 - This file contians hostname
					/etc/wsl-distribution.conf is created by default inside your WSL Linux distro

					EXAMPLE:
						[file: /etc/wsl-distribution.conf ]
							[oobe]
							command = /usr/lib/wsl/first-setup.sh
							defaultName = archlinux

							[shortcut]
							icon = /usr/lib/wsl/archlinux.ico

						[/file]

			3) WSL2 is installed by default with WSLg which allows Graphical User Interface to be executed by the user.
				C:\ProgramData\Microsoft\WSL\.wslgconfig needs to be created by the user
						
					EXAMPLE - Setting up default refresh rate of 75Hz - :
						[file: C:\ProgramData\Microsoft\WSL\.wslgconfig ]
							[system-distro-env]
							WESTON_RDP_MONITOR_REFRESH_RATE=75
						[/file]


	TEST YOUR MONITOR'S REFRESH RATE
		Due to the nature of WSL and WSLg at the current state, it may not display Higher than 60Hz Refresh Rates:

		ONLINE TESTS:
			https://refresh-rate-visualizer.vercel.app/
			https://frameratetest.com/refresh-rate-tester/

	MANAGING WINDOWS ACCOUNTS
		system_key + r >> Type in 'netplwiz'

		WINDOWS REMOTE ACCOUNTS
			Windows remote accounts can only be be accessed with an internet connection;
		that means being offline will render you without access to your PC.

		You can tell you're on a remote account when your email address is listed in it.

		LOCAL WINDOWS ACCOUNTS
			Local Windows accounts can be accessed anytime.

		CONVERTING REMOTE USER ACCOUNT INTO LOCAL USER ACCOUNT
			It'd be wise - if you're not using one-drive at all to store your local data, then
		convert the user account to a local account to prevent being locked away from your PC.

			system_key + i >> click on 'view information'/'user information' >> Click on 'Enter with your Local Acount'.
	
	SOME WINDOWS PERFORMANCE IMPROVEMENT
		ENABLING DRIVERS FOR YOUR GPU
			Type 'devmgmt.msc' >> Video Adapters >> Right-Click on your GPU >> Go to 'Driver' Tab >> Click on 'Driver Details'
			>> Click on 'Update Drivers' >> Click on 'Manually choose a driver on your PC' >> 
			click on 'Choose your driver from a list of available drivers' >> Choose the newer driver  

			NOTE: Always Choose the newest driver. You have to install your GPU Drivers first.

		DISABLING XBOX GAME BAR
			system_key + I	>> Games >> Xbox Game Bar >> disable it

		ENABLING GAME MODE
			system_key + I	>> Games >> Game Mode >> Enable it

		DISABLING GAME DVR
			1) Open regedit and edit:
				Computer\HKEY_CURRENT_USER\System\GameConfigStore >> set GameDVR_Enabled to '0'


			2) Open regedit and edit:
				Computer\HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\PolicyManager\default\ApplicationManagement\AllowGameDVR
				>> set value to '0'

		INSTALLING DIRECT-X
			https://www.microsoft.com/en-us/download/details.aspx?id=35

		DISABLE ENERGY ECONOMY
			system_key + I >> System >> Disable Energy Economy

		ENABLE DIRECT PLAY
			Type OptionalFeatures.exe >> Legacy Components >> Direct Play

		DISABLE HYPER-V
			If you're sure that you might not need 'Hyper-V', you can disable it:

				Type OptionalFeatures.exe >> Uncheck 'Hyper-V' option.
			
			Note: Hyper-v is only meant for virtual machines(VMs) for better interfacing Storage Devices for better performance.
			However, Hyper-V has huge impact on GPU, Memory and other hardware devices performances for programs/software that don't need Hyper-V.
			
		DISABLE CORTANA AND CO-PILOT
			No instructions.

		DISABLING PAGE FILE
			Only disable the Page File / Disk Swap Cache if you have more than enough Physical RAM, otherwise you'll get BSODs often.
				run 'SystemPropertiesAdvanced.exe' as administrator by typing on search box and right-clicking for to launch it with admin rights >> ...
				... >> Go to "Advanced" tab >> go to "Performance" and click on "Settings" >> Click on "Advanced" >> ...
				... >> Disable by choosing 'No Page File' and then apply it.

	SOME NVIDIA PERFORMANCE IMPROVEMENT
		1 - Go to Nvidia Settings >> 3D Settings >> Managing 3D Settings >> Global 3D Settings:
			1.1 - Set Energy Management to: Maximum Performance
			1.2 - Anti-Aliasing: Disabled
			1.3 - Cache Shader: Disabled
				Enabling it will allow faster loading, however it will "burn" your hard disk overtime, since lots of Disk I/O will be done in it.
				If you have a good CPU and large ammounts of RAM, disable this for the best.
				Hard Disks aren't faster than RAM, while disk caching might allow for some faster load times, it's no guarantee that execution will be faster. 
				Whenever programs need access to disk cache(when available), they'll read data and/or copy it to RAM(for faster reads) at hard-disk speeds,
				therefore creating a bottleneck.

		2 - Go to Nvidia Settings >> Image Settings >> set to maximum Performance instead of quality.


	SOME LIFE QUALITY IMPROVEMENTS ON WINDOWS

		ENABLING HDR(HIGH DYNAMIC RANGE) ON WINDOWS
			You need an HDR Enabled Monitor for this:
				There are two ways for doing this:
					A) Using windows:
						System_Key + I >> Click on Display / Monitor >> HDR >> Enable it from there

					B) When windows doesn't shows this option itself, you may have to enable it by hand by using the buttons on the monitor itself.
						This should work the same, even if windows can't see it enabled.

			Personal Note: In short words, HDR is a feature that enhances light parts of an image with a wider range of limunosity and dark spots of an image with less,
			allowing image features that couldn't be well observed to be evenly seen.

		ENABLING DSR(DYNAMIC SUPER RESOLUTION) FOR NVIDIA CARDS ON WINDOWS
			If you're using a low/mid-end monitor, DSR allows you to use higher resolutions than your monitor can support.
			It achieves this by using your display's native resolution and upscaling it internally with the GPU.

			DSR also allows using unsupported 'Refresh Rates' at Higher Resolutions when your monitor can't support higher refresh rates outside the supported Native Resolution.
			Example: Some monitor may allow 75Hz refresh rate at 720p resolutions but only allow 60Hz refresh rate at 1080p. HOWEVER, by using Nvidia's DSR this can be changed.
			
			Pretty much any Nvidia GPU beyond 400 series can support DSR.
			The only trade cost is FPS(Frames Per Second) depending on your GPU some software might run slightly slower when running at higher resolutions.

		
			1 - Go to Nvidia Settings >> 3D Settings >> Managing 3D Settings >> Global 3D Settings:
				1.1 - Go to 'DSR - Factor' >> Enable it >> Choose 1.20 Scaling for testing
				1.2 - Go to 'Monitor' >> click 'Change Resolution' >> Choose one of the 'Super Dynamic Resolutions' and apply it.

---SOME INTRODUCTION TO WSL AND ARCHLINUX
	INSTALLING ARCHLINUX DISTRIBUTION IN WSL
		Note: You might need to install and update WSL depending on your windows version.

		1) Install WSL without a distribution
			$ wsl --install --no-distribution

		2) Update WSL
			$ wsl --update --pre-release

		4) Install archlinux
			$ wsl --install archlinux --name archlinux_dist

		5) Use the distro post-install
			$ wsl -d archlinux_dist

		WSL DEFAULT WINDOWS LOCATION FOR LINUX DISTROS
			The Default Location for WSL Linux Distros is:
				C:\Users\<YOUR_USERNAME>\AppData\Local\wsl\

			Always make a backup from time to time to avoid data loss upon image getting corrupted.
			Also read about WSL filesystem corruption on this topic:
				BARE MOUNTS

		LISTING ALL DISTRIBUTIONS INSTALLED IN WSL
			Inside a windows power shell session, type: 
				$ Get-ChildItem "HKCU:\Software\Microsoft\Windows\CurrentVersion\Lxss" -Recurse

	REMOVING A WSL DISTRIBUTION
		Do not use '--uninstall' as it'll completely erase WSL from existence instead.
			$ wsl --unregister <distro_name> 
	
	ADDING AND USING A NEW REGULAR USER
		
		1) Start your already installed distro:
			$ wsl -d archlinux_dist

		2) Install 'zsh' and 'sudo':
			# pacman -S zsh sudo

		3) Create new user:
			# useradd --shell /usr/bin/zsh --create-home <user_name>

		4) Change password for user:
			# passwd <user_name>

		5) Log into user
			# sudo -iu <user_name>

			Note: you can use 'run0' and 'su' too instead of 'sudo'.

		6) Type 'exit' if you want to exit the user shell
			$ exit
		
		ADDING A NEW USER TO SUDOERS LIST
			
			1) Install 'visudo'
				# pacman -S vi sudo

			2) Run 'visudo'
				# visudo

			3) How to Use VI - For Complete Beginners - 
				3.1 - Press 'ESC' to ensure you're in normal-mode, useful for navigating through text file and also for 
				selecting other available vi modes: insert-mode, command-mode, visual-mode and select-mode.
				3.2 - In normal mode, you can navigate using h, k, j, l / left, down, up, right
				3.3 - In normal mode, You can also use CTRL+F to scroll forward and CTRL+U to scroll up the screen.
				3.4 - you can edit text using 'i' to go into insert-mode, press 'esc' to go back to normal-mode.
				3.6 - To save data in visudo, press ':' and then type 'w!' and press enter.
				needless to say, pressing ':' sends you into command-mode, press 'ESC' to leave command-mode.
				3.7 - you can quit by going into command-mode and then pressing 'q' or 'q!

			4) Use the instructions from Step #3 to Find the line that says:
				[file: /etc/sudoers ]
					##
					## User privilege specification
					##
					root ALL=(ALL:ALL) ALL
				[/file]

			5) Use instructions from step #3 to add a single new 
			   line right below the 'root ALL=(ALL:ALL) ALL' text like this:

				[file: /etc/sudoers ]
					##
					## User privilege specification
					##
					root ALL=(ALL:ALL) ALL
					YOUR_USERNAME ALL=(ALL:ALL) ALL
				[/file]

			6) Create group 'wheel' and 'sudo'
				Note: 'sudo' group should already exist due to 'sudo' being installed on step #1.

				# groupadd wheel & groupadd sudo
			
				6.1 - You can check to see if groups have been succesfully created:
					
			7) Modify your user to participate in 'wheel' and 'sudo' groups:
				# usermod -aG wheel,sudo <YOUR_USERNAME>

			8) Log into your user:
				# sudo -iu <YOUR_USERNAME>

			9) Test it by installing texinfo through pacman:
				$ sudo pacman -S texinfo

				Note: if installation succeeds, then you've successfully added yourself to sudoers list.


	LOGIN AS ANOTHER USER IN A WSL LINUX DISTRO
		NOTE: Using machinectl is important for setting XDG_RUNTIME_DIR and some other WSL env. vars, just in case you ever need it.
		
		# machinectl shell --uid=<username>
			or
		$ sudo -iu <username>


	BASIC USER AND GROUP MANAGEMENT

			CREATING A NEW USER
				1) First create the user:
					Note: Choose 'A' or 'B'

					A) Creating user using the default home directory /home/<USERNAME>/ :
						# useradd --shell /usr/bin/zsh --create-home <user_name>
							or
						# useradd -s /usr/bin/zsh -m <user_name>

					B) Creating user in a specific home directory
						# useradd --shell /usr/bin/zsh --create-home --home-dir <path_to_home> <USERNAME>
							or
						# useradd -s /usr/bin/zsh -m -d <path_to_home> <USERNAME>
							or
						# useradd -s /usr/bin/zsh -md <path_to_home> <USERNAME>

						EXAMPLE:
							# useradd -s /usr/bin/zsh -md /home/NEW_USER NEW_USER
						

				2) Change password for the user:
					# passwd <USERNAME>

				3) Login as another user:
					$ su - <username>
						or
					$ sudo -iu <username>
						or
					# machinectl shell --uid=<username>

				4) (Optional) Run command or program as another user:
					$ sudo -iu <username> <command>

					example:
						$ sudo -iu spamtla firefox

				5) (Optional) Run command as another group:
					$ sudo -ig <groupname> <command>

				6) (Optional) Run command as different user and different group:
					$ sudo -iu <username> -ig <groupname> <command>

				7) (Optional) Running command as root/admin:
					$ sudo <command>

				8) (Optional) Running a she session as a different user:
					$ sudo -iu <username>
						or
					$ sudo -iu <username> /usr/bin/zsh

				9) (Optional) Running a program as different group:
					$ sudo -g <username> <program_name>

				NOTE: You can check the veracity of these optional commands by using 'pgrep' on a different terminal tab after running them:
					$ pgrep --list-name -G <Groupname>
						or
					$ pgrep --list-name -U <Username>

				HINT: you can use 'userdbctl' and 'lslogins' to list all users on a system.
				'getent groups' to list all groups.
				
					RECOMMENDED READINGS:
						ADDING A NEW USER TO SUDOERS LIST
						LISTING USER AND GROUP ENTITIES ON THE LINUX DISTRO
						A FEW LINUX BEGINNER COMMANDS

			MODIFYING EXISTING USER
				A) Adding user to an existing group
					# usermod -aG group1 group2 <USERNAME>
						or
					# gpasswd -a <USERNAME> group1
					# gpasswd -a <USERNAME> group2

				B) Removing user from groups
					# usermod -rG group1 group2 <USERNAME>
						or
					# gpasswd -d <USERNAME> group1
					# gpasswd -d <USERNAME> group2

				C) Changing a user's shell:
					# usermod --shell <SHELL> <USERNAME>
						or
					# usermod -s <SHELL> <USERNAME>

					example:
						# usermod --shell /usr/bin/zsh <USERNAME>

				D) Moving user's home to a new directory:
					# usermod --move-home --home <PATH_TO_NEW_HOME> <USERNAME>
						or
					# usermod -m -d <PATH_TO_NEW_HOME> <USERNAME>
						or
					# usermod -md <PATH_TO_NEW_HOME> <USERNAME>

			CREATING A NEW GROUP
				A) Creating a regular group:
					# groupadd <GROUPNAME>

				B) Creating a system group:
					# groupadd --system <GROUPNAME>

			MODIFYING AN EXISTING GROUP
				A) Changing a group's name:
					# groupmod --new-name <NEW_GROUP_NAME> <CURRENT_GROUP_NAME>
						or
					# groupmod -n <NEW_GROUP_NAME> <CURRENT_GROUP_NAME>

				B) Adding users to group list:
					Note: This is the same as using 'usermod -aG'

					# groupmod -aU group1 group2 <USERNAME>


	SETTING UP SEATD ON YOUR ARCHLINUX SYSTEM | PERFORMANCE
		'seatd', a compositor-agnostic session management daemon, can contribute to better performance on Linux systems, particularly in multi-seat or virtualized environments, 
		by optimizing resource allocation and access to hardware. It allows for fine-grained control over which user has access to which hardware devices, reducing overhead and potential conflicts:

		1) Install seatd
			# pacman -S seatd
		
		2) Enable the service:
			# systemctl enable seatd
			# systemctl start seatd

		2) Add user to both 'seat' and 'video' group:
			# gpasswd -a <username> video
			# gpasswd -a <username> seat
				or
			# usermod -aG video seat spamtla
			 
		3) Restart the service:
			# systemctl restart seatd

		4) restart your WSL Virtual Machine from scratch:
			Save all your work before you progress with the following step.
				From a Windows Power Shell session, type:
					$ wsl --shutdown
				

	SETTING UP TIME AND DATE ON THE LINUX SYSTEM
		If you're using Linux on WSL(Windows Subsystem for Linux) you do not have to setup time/date.
		However, i'm still leaving this here for reference for those who might wish to do something.

		Read more on:
			ADJUSTING HARDWARE & SYSTEM CLOCK USING HWCLOCK

	SETTING UP SYSTEM LOCALE IN A WSL LINUX DISTRO
		From this point forward you'll have to look for things on your own:
			SETTING UP SYSTEM LANGUAGE | SYSTEM LOCALE (NOT MANDATORY FOR KEYBOARD LAYOUT)
	
	SETTING UP TEXT FONT FAMILIES IN LINUX SYSTEM
		'fontconfig' comes installed by default and provides these lists of tools:

		1) Scans system for new fonts and sets them up:
			# fc-cache

		1) Listing all available fontconfig tools:
			$ pacman -Ql fontconfig | grep -i "/usr/bin/..*"

		2) Lists all processed (+) and ignored (-) fonts on the system:
			$ fc-conflist

		3) Lists fonts and styles available on the system for applications using Fontconfig:
			$ fc-list

			Example 1: Listing all available fonts for the enlgish language
				$ fc-list :lang=us

			Example 2: Listing all available fonts for the japanese language
				$ fc-list :lang=ja
		
		LOOKING FOR AND FINDING NEW FONT FAMILIES AND FONT TYPES FOR INSTALLATION
			Note: you'll probably have to install YAY Package Manager for the AUR(Archlinux User Repository),
			since standard package manager will not find any in the default archlinux repository.

				$ yay -sS font | grep -i -P "[ot]tf|font" | less
					or
				$ pacman -sS font | grep -i -P "[ot]tf|font" | less

			LOOKING FOR HANDWRITTEN FONTS FOR INSTALLATION
			
				$ yay -sS font | grep -i -B2 -A1 -P "handw" | less
					or
				$ pacman -sS font | grep -i -B2 -A1 -P "handw" | less

		INSTALLING NEW FONTS ON THE SYSTEM
			1) Installing a few known fonts:
				# pacman -S noto-fonts noto-fonts-extra noto-fonts-emoji fonts-noto-hinted nerd-fonts
					or
				$ sudo pacman -S noto-fonts noto-fonts-extra noto-fonts-emoji fonts-noto-hinted nerd-fonts

				NOTE - Read More on Topic:
					INSTALLING FONTS

			2) Updating the fonts cache:
				# fc-cache
					or
				$ sudo fc-cache

		LISTING USER INSTALLED FONTS ON THE SYSTEM
			While fc-list can display all available fonts, 
		there's a way for displaying only user installed fonts:
				# pacman -Ss font | grep -i -B2 -A1 -P "installed"
					or
				$ yay -Ss font | grep -i -B2 -A1 -P "installed"

	LISTING USER AND GROUP ENTITIES ON THE LINUX DISTRO
		1) Listing all users on the system:
			# compgen -u 
				or
			# getent passwd
				or
			# getent shadow

			# userdbctl

			# lslogins

		2) Listing all groups on the system:
			# getent group
	
	A FEW LINUX BEGINNER COMMANDS
		Aside from a few of the commands already mentioned in the other topics,
		there are still a few ones worth mentioning:

		1) Cheats sheet for any command(s) in linux ( Requires installing package 'tldr' ) 
			$ tldr <command_name>
			
			Example:
				$ tldr gpg

		2) Listing all logins in the system
			2.1) Listing current logins in the system:
				$ lslogins
			
			2.2) Listing all users in the system:
				$ userdbctl

		3) Listing file descriptors
			Lists all files hold by running or zombie processes:
				$ lsfd
				   or
				$ lsof

		4) Listing files in a directory
			$ lsa
			  or
			$ ls -la
		
		5) Listing files and directory in tree style (Requires installing package 'tree')
			$ tree

		6) Commandline File Explorer ( requires installing package 'broot')
			This program uses vi modular keys - This requires knowing VI / VIM / NVIM a bit:
				$ broot

		7) Listing Block Devices mounted/unmounted
			$ lsblk

		8) Listing current file locks in the filesystem
			$ lslocks

		9) Listing extended attributes of data from EXT Filesystems
			$ lsattr

		10) Setting access control list for file/directories
			$ setfacl

		11) Getting Access Control List(ACLs) for File/Directories
			$ getfacl

		12) Fetching info on CPU
			$ lscpu

		13) Fetching short info on memory
			$ lsmem
			  or
			$ free -h
			  or
			$ zramctl

		14) Listing enabled and disabled kernel features:
			$ zless /proc/config.gz
				or
			$ zcat /proc/config.gz | less

		15) Searching for specific kernel features:
			Note: Options for zgrep and grep are the same, so use 'man grep' for access to more options.
			$ zgrep -i 'encrypt' /proc/config.gz
				and
			$ zgrep -i 'hyperv' /proc/config.gz

				OR

			$ zcat /proc/config.gz | grep -i "encrypt"
				and
			$ zcat /proc/config.gz | grep -i "hyperv"
			
		16) Listing all available Kernel Modules:
			This is not available on WSL(Windows Subsystem for Linux) 
				# lsmod

		17) OpenPGP encryption and signing tool (GNU PGP - gpg)
			Manages public and private OpenPGP Encryption keys on the system.
				$ gpg

		18) Archiving Tools ( require installing both )
			$ tar
			  and
			$ dar
			
		19) Advanced Text Editor ( requires installing neovim )
			$ nvim
		
		20) Reads Information on Executable and Linkable files:
			$ readelf --all /bin/cp
				and
			$ readelf --file-header /bin/cp

		21) Displays all shared objects/libraries required for a running program:
			$ ldd -v /bin/cp

		22) Read user input from commandline
			Useful if you ever need to create user shell scripts.
			Syntax:
				$ read <environment_variable>

			Example:
				This code asks, reads the user password from the keyboard, then displays it back to the user and clears 
				the password variable:
					$ echo "Please type your password: " && read -s password && 
					  echo "\nYour password is:\n\t${password}\n\n" && password=''
			
		23) Make pipes appear in your terminal ( Requires installing it from AUR - pipes.sh package ):
			$ pipes.sh

		24) Using pgrep to list processes by USER name or GROUP name:
			24.1 - Lists all procceses executed by Group video:
				$ pgrep --list-name -G video 

		25) Make a cow say something on commandline:
			$ cowsay -y -W 12 "Archlinux is NOT for everyone"

			Resulting output will be:
				 ____________
				/ Archlinux  \
				| is NOT for |
				\ everyone   /
				 ------------
					\   ^__^
					 \  (..)\_______
					    (__)\       )\/\
						||----w |
						||     ||

	SETTING UP NEW DEFAULT STARTER USER FOR WSL DISTRO
		This is optional.

		Most often this breaks some stuff like when launching GUI Programs,
		Note: I'll check later how to fix it, but for now avoid this:
			$ wsl --manage <distro> --set-default-user <username>

		IMPORTANT: It's advisable to just use 'sudo' to log into another user, for now.
		
	RECOMMENDED ARCHLINUX PACKAGES FOR WSL
		NOTE: Depending on how long this list has been updated, some packages might be unavailable at the time.
		since archlinux is a rolling release distro, some packages might just not work anylonger while some others might just be too old.

		Note: You'll need to double check on what you need and what you don't need AND also check on what works and what doesn't works.

		These are merely options, you can either opt in or out of one or more of these packages depending on your needs:

		A) ARCHLINUX REPOSITORY
			1) INSTALLING UTILITIES
				# pacman -S wget curl man-pages base coreutils util-linux binutils zsh iptables-nft \
				  vi neovim gvim par2cmdline ntfs-3g netwatch texinfo whowatch xnviewmp xsel xclip \
				  smartmontools pcmanfm-gtk3 mpv vlc kdenlive yt-dlp distrobox broot pavucontrol \
				  sudo fscrypt htop konsole git cdparanoia firejail google-chrome firefox vivaldi opera \
				  clamav clamtk arch-install-scripts apparmor wireshark-qt screen testdisk \
				  clock-tui cool-retro-term dosbox podman gimp glances torbrowser-launcher systemd-ui \
				  scrot ugrep ripgrep sed memtester lsof hwinfo cowsay gdb cmus ibus flatpak \
				  calc libreoffice icu xfce4 glxinfo mesa-demos seatd atop
 
		
				Note: When installing firefox, choose 'wireplumber' for audio support.

			2) INSTALLING FONTS
				# pacman -S noto-fonts noto-fonts-extra noto-fonts-emoji fonts-noto-hinted nerd-fonts \
				  gnu-free-fonts ttf-arphic-uming ttf-indic-otf  

				Note: Everytime you install new fonts, update the cache using:
					# fc-cache

		B) AUR(ARCHLINUX USER REPOSITORY)
			This requires yay package manager to be installed on the system:
				$ yay -S oh-my-zsh zsh-theme-powerlevel10k archey4 ddd xnview-xnsketch-bin \
				  scc-bin dbgl pipes.sh 

			This requires compilation, so doing them individually is the best:
				$ yay -S dosbox-x-sdl2 qmplay2 memstat pcmanx-gtk2 syncterm cheat svp-bin ssed dar


		BASICS ON HOW TO USE PACMAN
			Just a few tips and hints for new archlinux users.

			RECOMMENDED READING:
				PACMAN, THE MAIN ARCHLINUX PACKAGE MANAGER

			note 1 - display remote information about a package:
				$ pacman -Si sed
					and
				$ yay -Si pcmanx-gtk2

			note 2 - you may want to check which files belong to a certain package:
				$ pacman -Ql systemd-ui
					and
				$ yay -Ql syncterm

			note 3 - if you don't know how to execute a certain utility belonging to a certain package:
				$ pacman -Ql binutils | grep "/bin/"
					and
				$ yay -Ql syncterm | grep "/bin/"

			note 4 - checking package integrity
				# pacman -Qqk
					and
				$ yay -Qqk
		
			note 5 - listing all user installed packages
				# pacman -Qe
					and
				$ yay -Qe

			note 6 - Uprading packages:
				# pacman -Syu
					and
				$ yay -Syu

			note 7 - Upgrading only AUR packages:
				$ yay -Syua


	STARTING DESKTOP ENVIRONMENT FROM WSL USING ARCHLINUX
		NOTE: At the current state, WSL defaults to wayland, so DEs that require Xorg/startx might not work at all.

		Starting xfce4 directly without Using a Display Manager:
			$ xfce4-session

			Note: You can simply logout to shutdown the desktop environment,
			

	RUNNING CLAMAV ANTI-VIRUS IN WSL
		From this point forward you'll have to look for things on your own:
			CLAMAV ANTI-VIRUS
					ALLOW FILE DESCRIPTOR PASS | ALLOWING CLAMAV TO SCAN DIFFERENT FILE SYSTEM TREES

	SETTING UP A FIREWALL WITH IPTABLES/NFTABLES IN WSL
		From this point forward you'll have to look for things on your own:
			LINUX FIREWALLING WITH IPTABLES | NFTABLES

	ENABLING DBUS-BROKER IN WSL
		Most up-to-date linux distributions are now using dbus-broker, however there's always the chance you might 
	still be on a linux system without dbus-broker.

		Install:
			# pacman -Syu dbus-broker

		Enable it:
			# systemctl --global enable dbus-broker.service && systemctl start dbus-broker.service

		Verify:
			# systemctl status dbus-broker


	ENABLING CLIPBOARD FOR NVIM EDITOR
		Recommended reading:
			SETTING UP NVIM CLIPBOARD FOR COPY AND PASTING IN 2025

	MOUNTING DEVICES IN WSL
		FETCHING/LISTING ALL HARD DISKS AVAILABLE USING WINDOWS POWER SHELL
			$ GET-CimInstance -query "SELECT * from Win32_DiskDrive"

		MOUNTING DEVICES ON WSL
			1) Fetch devices using:
				$ GET-CimInstance -query "SELECT * from Win32_DiskDrive"

			2) Mount device using:
				Note: Hard Disks with multiple partition need to specify the partition to be mounted as below.

				$ wsl --mount \\.\PHYSICALDRIVE0 --partition 2
			

			BARE MOUNTS
				Bare mounts allows to load a storage device without actually mounting it,
				this is important for running 'fsck' on Power Blackouts; 

				NOTE: WSL at the current state simply doesn't care about running 'fsck' on partitions 
				for doing filesystem checks during boot.

				VERY IMPORTANT: It's very important to run 'fsck' on partitions/disks that have been abruptly umounted,
				or else you'll end up with a damaged disk/image/partition, rendering impossible any kind of access or data
				backup.
				
				HINT: Besides running 'fsck' on a bare mounted device, it's also possible to use 'dd_rescue'/'dddrescue', 
				'testdisk' and 'smartctl'.

				BARE MOUNTING DEVICES
					You can bare mount a partition as followed:
						note: the '#' and '$' are merely reference to root and user,
						it's not necessary to type them in your windows shell/session.

						1) Run a windows power shell session as administrator:
							You'll be unable to mount devices otherwise.

						2) Fetch available devices using:
							# GET-CimInstance -query "SELECT * from Win32_DiskDrive"

						3) Inside a Windows Power Shell session, type: 
							VERY IMPORTANT:
								MAKE SURE NO OTHER VM(Virtual Machine) OR WSL INSTANCE IS 
								RUNNING THE DISK BEFORE MOUNTING!

							# wsl --mount \\.\PHYSICALDRIVE0 --bare

						4) Go into another linux distro and finallly list and mount it from there:
							4.1 - Listing
								$ lsblk
								# blkid

							4.2 - (Optional - EXT4 Filesystems only) If you so desire, you can run 'fsck' to 
							perform a filesystem check and fix:
								# fsck.ext4 -Dfy /dev/sdX2

							4.3 - Mounting
								# mount --mkdir /dev/sdX2 /mnt/my_linux_mount_point
									or
								# mount --mkdir -t auto /dev/sdX2 /mnt/my_linux_mount_point

				BARE MOUNTING WSL .VHDX IMAGES 
					Mounting WSL .VHDX is useful for running 'fsck'.
					
					1) Run a windows power shell session as administrator:
						You'll be unable to mount devices otherwise.

					2) Listing available WSL .VHDX Images on your system:
						# Get-ChildItem "HKCU:\Software\Microsoft\Windows\CurrentVersion\Lxss" -Recurse

					3) Bare mounting a .VHDX image:
							VERY IMPORTANT:
								MAKE SURE NO OTHER VM(Virtual Machine) OR WSL INSTANCE IS 
								RUNNING THE IMAGE BEFORE MOUNTING!

						# wsl --mount --vhd --bare \
						  "C:\Users\sebas\AppData\Local\wsl\{44cec59d-a5b3-4326-8171-f9218b2ae285}\ext4.vhdx"

					4) Go into another linux distro and finallly list and mount it from there:
						4.1 - Listing
							$ lsblk
							# blkid

						4.2 - (Optional - EXT4 Filesystems only) If you so desire, you can run 'fsck' to 
						perform a filesystem check and fix:
							# fsck.ext4 -Dfy /dev/sdX2

						4.3 - Mounting
							# mount --mkdir /dev/sdX2 /mnt/my_linux_mount_point
								or
							# mount --mkdir -t auto /dev/sdX2 /mnt/my_linux_mount_point


		COPYING AND MOVING FILES FROM WITHIN WSL
			1) First, start your wsl distro:
				$ wsl -d <distro_name>

			2) Access the following directory to get access to your system's drive:
				$ cd /mnt/c/<username>
					or
				$ cd /mnt/c/

			3) Copy using 'cp' or 'mv' for moving:
				This will copy the Pictures folder from the windows directory into the running WSL Distro.
					$ cp -R /mnt/c/<username>/Pictures ~/

		INSTALLING ZSH AND POWERLEVEL10K FOR WSL DISTROS | PERFORMANCE | RECOMMENDED
			It is recommended to install 'zsh' and 'zsh-theme-powerlevel10k', the default 'bash' shell is too slow and 
		unoptimized. Read topic:
				USING BOTH POWERLEVEL10K AND OH-MY-ZSH FOR EXTRA FEATURES | PERFORMANCE | RECOMMENDED

		CREATING AN ENVIRONMENT VARIABLE
			You can create both 'user environment variables' and 'system wide environment variables'; 
			for WSL however I'm only covering 'system wide environment'.

			You can use either '3A - Easy' or '3B - Intermediate/Advanced method', but if you choose '3A - Easy Method' 
			you'll first need to use your root/admin account in linux:

				1) Using sudo
					Use your own user password if you have added yourself on the /etc/sudoers file:
						$ sudo -iu root

				2) (alternate option) Using su
					Use your root/admin password instead:
						$ su -- root

			Option 3A - EASY Method:
				3A.1) - Use vi, vim or nvim to edit the /etc/environment file: 
					# vi /etc/environment
						or
					# vim /etc/environment
						or
					# nvim /etc/environment
					
				3A.2) - Create the following environment variables by adding them, replace <USERNAME> with your windows
				username:
					[file: /etc/environment ]
						WIN_USERNAME="<USERNAME>"
						WUSER_PATH="/mnt/c/Users/<USERNAME>"
						WIN_USERS_PATH="/mnt/c/Users"
						WDIR_PATH="/mnt/c"
						WPROGRAM_FILES="/mnt/c/Program Files"
						WPROGRAM_FILES_X86="/mnt/c/Program Files (x86)"
						WSYSTEM="/mnt/c/Windows/System"
						WSYSTEM32="/mnt/c/Windows/System32"
						WSL_STORAGE="/mnt/c/Users/<USERNAME>/AppData/Local/wsl"
						WAPP_DATA="/mnt/c/Users/<USERNAME>/AppData"
					[/file]

			Option 3B - INTERMEDIATE/ADVANCED method:
				This method uses both 'printf' and 'tee' to add a new environment var,
			the only disadvantage is that you'll need to use the EASY Method to make corrections if you make any mistake; 
			there's always the option of using 'SED' to correct mistakes as well, but i'm not going to cover it here.

				VERY IMPORTANT: Check 'PRACTICAL EXAMPLE 2' for the full script!

				3B.1)  - First, create an env. var for your windows username, replace <USERNAME> with 
				your windows username:
					# printf "WIN_USERNAME=\"%s\"" "<USERNAME>" | tee -a /etc/environment

				3B.2) - Second, create a windows_user variable, no need to do anything else here:
					# printf "WUSER=\"/mnt/c/${WIN_USERNAME}\"" | tee -a /etc/environment
					  printf "WDIR=/mnt/c/" | tee -a /etc/environment

				Then whole script would be:
					# printf "WIN_USERNAME=\"%s\"\n" "<USERNAME>" | tee -a /etc/environment &&
					  printf "WUSER_PATH=\"/mnt/c/${WIN_USERNAME}\"\n" | tee -a /etc/environment &&
					  printf "WDIR_PATH=\"/mnt/c/\"\n" | tee -a /etc/environment

			Personal Notes:
				In my honest opnion, the '3B - Intermediate/Advanced method' is better, because there is less 
			steps involved; on the other hand '3A - Easy method' requires manually going into root/admin mode,
			manually openning the file, and manually copying, pasting and saving it.
				
				The '3B' Method is better despite being harder to implement, because one can simply run the entire 
			thing as script and it will do the entire work; Therefore it's faster than '3A'. 

			---- PRACTICAL EXAMPLE 1 - STORING THE SCRIPT AS A STRING IN A ENV. VARIABLE ----
				Note: In this example, i've made a few modifications to allow the script to be stored as text
			by simply replacing 'printf's  arguments with '' instead of "" characters

				You can store a full script in an environment variable as a string, 
			then executing it with 'zsh -c $SCRIPT' as follows:

				$ export MINI_SCRIPT="printf 'WIN_USERNAME=\"%s\"\n' '<USERNAME>' | tee -a /etc/environment &&
						    printf 'WUSER_PATH=\"/mnt/c/${WIN_USERNAME}\"\n' | tee -a /etc/environment &&
						    printf 'WDIR_PATH=\"/mnt/c/\"\n' | tee -a /etc/environment"
				  sudo zsh -c $MINI_SCRIPT

			---- PRACTICAL EXAMPLE 2 - POPULATING /ETC/ENVIRONMENT USING SHELL FUNCTION/METHOD -----
				While it is easier to just write a script and then execute it as root/admin,
			I'll still be describing this special example here that resolves around using Environment variables only!

				1) First, Create the function:
					Note: Replace only <USERNAME> with your actual username!

					$ cmd_funct(){  
						export WIN_USERNAME="<USERNAME>";

						tee_funct(){
							tee -a /etc/environment;
						};
						printf "WIN_USERNAME=\"%s\"\n" "${WIN_USERNAME}" | tee_funct &&
						printf "WUSER_PATH=\"/mnt/c/Users/${WIN_USERNAME}\"\n" | tee_funct &&
						printf "WIN_USERS_PATH=\"/mnt/c/Users\"\n" | tee_funct &&
						printf "WDIR_PATH=\"/mnt/c\"\n" | tee -a /etc/environment &&
						printf "WPROGRAM_FILES=\"/mnt/c/Program Files\"\n" | tee_funct &&
						printf "WPROGRAM_FILES_X86=\"/mnt/c/Program Files (x86)\"\n" | tee_funct &&
						printf "WSYSTEM=\"/mnt/c/Windows/System\"\n" | tee_funct &&
						printf "WSYSTEM32=\"/mnt/c/Windows/System32\"\n" | tee_funct &&
						printf "WSL_STORAGE=\"/mnt/c/Users/${WIN_USERNAME}/AppData/Local/wsl\"\n" | tee_funct &&
						printf "WAPP_DATA=\"/mnt/c/Users/${WIN_USERNAME}/AppData\"\n" | tee_funct;

					  };

				2) Store function declaration in an environment variable:
					Note: Unlike 'environment variables', Functions aren't exported by default when initializing
				a new shell session for running the script we created as root/admin.

					$ FUNC=$(declare -f cmd_funct);

				3) Finally use 'sudo' to run it as root/admin:
					Note: it's not possible to run a function/method directly as root using 'sudo', 
				at least not without creating a file for it first. 

					HOWEVER, it's possible to tell sudo to execute 'zsh -c' or 'bash -c' 
				which will in turn execute a script:

					$ sudo zsh -c "${FUNC}; cmd_funct" 

				NOTE: Don't forget to read the RECOMMENDED TOPIC right below the full script!

				---FULL SCRIPT 1-1:
					$ cmd_funct(){  
						export WIN_USERNAME="<USERNAME>";

						tee_funct(){
							tee -a /etc/environment;
						};
						printf "WIN_USERNAME=\"%s\"\n" "${WIN_USERNAME}" | tee_funct &&
						printf "WUSER_PATH=\"/mnt/c/Users/${WIN_USERNAME}\"\n" | tee_funct &&
						printf "WIN_USERS_PATH=\"/mnt/c/Users\"\n" | tee_funct &&
						printf "WDIR_PATH=\"/mnt/c\"\n" | tee -a /etc/environment &&
						printf "WPROGRAM_FILES=\"/mnt/c/Program Files\"\n" | tee_funct &&
						printf "WPROGRAM_FILES_X86=\"/mnt/c/Program Files (x86)\"\n" | tee_funct &&
						printf "WSYSTEM=\"/mnt/c/Windows/System\"\n" | tee_funct &&
						printf "WSYSTEM32=\"/mnt/c/Windows/System32\"\n" | tee_funct &&
						printf "WSL_STORAGE=\"/mnt/c/Users/${WIN_USERNAME}/AppData/Local/wsl\"\n" | tee_funct &&
						printf "WAPP_DATA=\"/mnt/c/Users/${WIN_USERNAME}/AppData\"\n" | tee_funct;

					  };
					  FUNC=$(declare -f cmd_funct);
					  sudo zsh -c "${FUNC}; cmd_funct";


				RECOMMENDED TOPIC READINGS:
					LOADING THE NEWLY CREATED ENVIRONMENT VARIABLES

				---FINAL CONSIDERATIONS:
					While it's harduous to make a script for automating simple tasks,
				it pays off in speed.
				
				The only downside is that, depending on the script, it might take tons of time getting 
				everything right and correct in place until it starts working 100%.

			4 - LOADING THE NEWLY CREATED ENVIRONMENT VARIABLES
				You can either start a new shell environment or use the 'source' command:
					$ zsh 
					  or
					$ source /etc/environment
					  and
					$ zsh -s /etc/environment

			5 - TESTING THE NEWLY ADDED VAR
				$ echo $WINDOWS_USER
					or
				$ echo ${WINDOWS_USER}

	ALLOWING AUDIO TO BE USED BY USERS OTHER THAN ROOT

		Add this to your /etc/environment file:
			[file: /etc/environment ]
				PULSE_SERVER=unix:/mnt/wslg/PulseServer
			[/file]

	THE VERY BASICS ABOUT FILESYSTEM USER AND GROUP OWNERSHIP
		Any data on your hard disk contains meta-data about who and which group it belongs to and what they 
		can both do with the given data; it also contains information regarding what other users can do 
		with the data.

		Note: While just the basics are given here, know that you can use 'find', 'awk' and even 'sed' to do
		more things regarding ownership and permissions.

		LISTING OWNERSHIP AND PERMISSIONS OF A GIVEN FILE OR DIRECTORY USING 'GETFACL'
			SYNTAX:
				$ getfacl ./<path_to_directory_or_file>

			Example 1: - Simple Use - :
				$ getfacl ./path/to/file

			Example 2: - Retrieving Numeric IDs for user and group instead of names - :
				$ getfacl --numeric ./path/to/file

		LISTING USER PERMISSIONS, MASK AND OTHERS USING 'LS'
			Using 'lsa' or 'ls -la' will display data in the following format:
				$ lsa
				  or
				$ ls -la

			Output:
				drwxr-xr-x   2 user_name group_name 4.0K Jul 31  2023  'Directory Name'

			
			1 - 'Jul 31 2023' refers to the mdate, which is the Modification Date for given directory/file. 
			On linux systems, Modification Date can't be changed and is created during file creation.
			It's the actual birth date or creation date of the file/directory.

			2 - Interpreting 'drwxr-xr-x' from left to right: 
				2.1 - 'd' refers to directory.
				2.2 - 'rwx' refers to 'user permission/rights' over the given directory.
				2.3 - 'r-x' usually refers to group; HOWEVER, when a MASK exists, it always refer to the 
				       mask permission set instead of 'group'. (Very Important)
				2.4 - 'r-x' refers to 'other' permissions
			

		CHANGING USER-OWNER AND GROUP-OWNER OF A GIVEN DATA USING 'CHOWN'
			It's possible to use 'chown' to change both group and user owner of a data.

			A) Changing user and group of a given directory and it's files:
				This will change the user-owner and group-owner of the file/directory.

				SYNTAX:
					# chown <username>:<groupname> -R ./<directory_name>

				Example:
					# chown another_username:another_group -R ./my_directory

			B) Changing group alone for a given directory and files:
				In here, just ommiting the <username> will suffice to change the group name:
				SYNTAX:
					# chown :<groupname> ./<directory_name>
				Example:
					# chown -R :another_group ./my_directory


			C) Changing user alone for a given directory and files:
				SYNTAX:
					# chown another_username: ./<directory_name>
				Example:
					# chown -R another_username: ./my_directory

		CHANGING FILE MODE BITS | CHANGING PERMISSIONS/RIGHTS FOR A GIVEN DATA USING 'CHMOD'
			It's possible to change what user-owner, group-owner and 'other users' can do with the file:
			
				SYNTAX:
					# chmod <[u|g|o|a]<[+|-]>[r|w|x]> -R ./<directory_name>

			A) Changing what user-owner can do with a given directory and it's files:

				Example 1 - Concatenating 'write' permission for user-owner - :
					# chmod u+w -R ./my_directory

				Example 2 - Removing all 'read', 'write', 'execute' permissions for 'others' - :
					# chmod o-rwx -R ./my_directory

				Example 3 - Changing Permission set for group-owner, setting it to 'read and execute' only -:
					# chmod g=r-x -R ./my_directory

				Example 4 - Adding (read, write and execute) Permission for all 3 User-owner, 
					    group-owner and others in one single line - :
							# chmod a+rwx -R ./my_directory
								or
							# chmod u+rwx,g+rwx,o+rwx -R ./my_directory

		GIVING EXTRA PERMISSION FOR OTHER USERS AND GROUPS IN A FILE/DIRECTORY USING 'SETFACL'
			It's possible to add new owners and new groups for a given data without modifying the actual user-owner,
		group-owner or yet giving/taking permissions from 'others'.

		By creating using the ACL(access control list) feature, it's super easy managing new onwers, groups and adding
		new permissions/rights to a given data.

		NOTE 1: 'setfacl' can also be used to change current owner/group permission rights
			instead of 'chmod', the only requirement is that the user must be the actual owner, or else
			a new ACL Entry will be created instead. Note that 'chgrp' and 'chown' may still have to be used
			to change the actual primary ownerships about user-owner and group-owner.

		NOTE 2: In practice, the only tools you need are these:
			2.1 - 'chown' for changing primary user and group ownership. 'chgrp' not really needed.
			2.2 - 'getfacl' for retrieving ACL List on a given data in the filesystem.
			2.3 - 'setfacl' for either changing or setting a new ACL entry of a given data in the filesystem
			2.4 - Even tho 'chmod' can be completely replaced by 'setfacl', 'setfacl' doesn't uses 'a' character 
			to setup 'user-owner', 'group-owner' and 'others' in one go as in mentioned in the previous 'example 4'.


			PRACTICAL 'SETFACL' EXAMPLES
				SYNTAX:
					$ setfacl -m <[u|g|o|m]>:[<username|groupname>]:<[r|w|x]> -R ./<my_directory>

					VERY IMPORTANT: -n or --no-mask allows new ACLs to be created without
					changing/updating the current mask. This is very important on environments
					with pre-set or already existing ACLs, since custom masks might already exist.
						
						For an example, read more on: 
							SHARING SINGLE DIRECTORY WITH ANOTHER USER | RESTRICTING SOME PERMISSIONS WHILE GIVING ACCESS | RECOMMENDED

					
				A) ADDING NEW USER-OWNER AND GROUP-OWNER TO A GIVEN FILE/DIRECTORY
					Note, the new user must already exist on the system, or else this will fail.

					Example 1 - Adding new user-onwer to a given directory along with all - :
					it's belonging files:
						This will give 'read' and 'execute' permissions for 'another_username'
							$ setfacl -m u:another_username:rx -R ./my_directory
								or
							$ setfacl -m u:another_username:r-x -R ./my_directory


					Example 2 - Adding new group-owner to a given directory along it's files - :
						This will give 'read' and 'execute' permissions, but take away 'write' rights
					for 'another_username'.
							$ setfacl -m g:another_groupname:r-x -R ./my_directory
						
				B) MODIFYING CURRENT USER-OWNER AND GROUP-OWNER TO A GIVEN FILE/DIRECTORY

					Example 1 - Changing just the User-owner of a given directory along it's given file - :
						$ setfacl -m u:another_username -R ./my_directory
						
					Example 1 - Changing just the Group of a given directory along it's given files - :
						$ setfacl -m g:another_groupname -R ./my_directory

				C) REMOVING AN EXISTING ACL

					Example 1 - Removing ACL Entry for a given username - :
						$ setfacl -x u:another_username	./my_directory


				D) MODIFYING MASK OF A GIVEN/DIRECTORY FILE
					Masks allows limiting what new ACLs can do with a given data,
				without actually changing the ACL Entries themselves.

				NOTE 1: By default, a Mask is either added or modified whenever a new ACL Entry is created.
				In this case, the mask is always set to the mode bit of the ACL that requires more
				permission/rights over the data.

				NOTE 2: The masks do not give rights/permissions, they only are used for limitting access
				for new ACL Entries and group. For example, when a mask is created with 'rw' permissions,
				it then limits 'eXecute' permissions, restricting users from 'x' mode bit permission.
				
				NOTE 3: Masks have no effect against the current user-owner of the data.
				Masks also apply no effect against 'other' users. Read 'note 2' again.
				Masks are only applied to extra newly user-owners and group-owners and the current group-owner.

					SYNTAX:
						$ setfacl -m <[u|g|o|m]>:[<username|groupname>]:<[r|w|x]> -R ./<my_directory>

					Example 1 - Clearing all mode bits, taking away all permissions/rights using Mask - :
						$ setfacl -m m::--- -R ./my_directory
							or
						$ setfacl -m m:--- -R ./my_directory

					Example 2 - Removing Mask from ACL -
						$ setfacl -x m -R ./my_directory
							or
						$ setfacl -x m:: -R ./my_directory
							or
						$ setfacl -x m: -R ./my_directory

						
					Example 3 - Creating a Mask - 
						$ setfacl -m m::rw- -R ./my_directory	
							or
						$ setfacl -m m:rw- -R ./my_directory	

						
					
		SHARING SINGLE DIRECTORY WITH ANOTHER USER | RESTRICTING SOME PERMISSIONS WHILE GIVING ACCESS | RECOMMENDED
			For giving a user directory access, you'll need to follow these steps.

				1) Pretend we want 'User_B' to access a single 'User_A' directory while restricting access
				to all other directories, 

					$ /home/User_A/Documents/pdf\ files/User\ B/

					Note: the only directory 'User_B' should ever have access is the 'User A' 
					folder/directory. None of the other directories should ever allow listing 
					or lay any other type of access for 'User B'.

				2) Now add the 'User_B' ACL for these directories:
					$ setfacl -m u:User_B:r-x /home/{User_A,User_A/Documents,User_A/Documents/pdf\ files}

				3) Now create a mask that only allows 'read' for all directories, 
				except 'User B' dir.
					$ setfacl -m m::--x /home/{User_A,User_A/Documents,User_A/Documents/pdf\ files}

					Note: This mask is going to prevent 'User_B' from reading any of the contents, but still
					allow path access for his shared user directory named 'User B'

					Very Important: Masks will be reset whenever creating a new ACL for a new User or 
					group when using 'setfacl', unless the option -n or --no-mask is given.

				4) Finally, create an ACL that allows 'read' and 'execute' mode bits for the shared directory:
					$ setfacl -m u:User_B:r-x -R /home/User_A/Documents/pdf\ files/User\ B/

					Note: 
					4.1 - 'x' allows 'User_B' to execute tools/aliases like 'cd' for changing directory,
					4.2 - 'lsa' ( ls -la ) for listing directory contents.
					4.3 - 'r' allows 'User_B' to open contents inside a directory for reading, 
					      it's not possible to use programs like 'cat' without 'r' permission.
					4.4 - without 'w' mode bit the user can't overwrite any of the files or append data 
					      to any of the file contents, not even create directories. 
					      HOWEVER, 'w' mode bit can be given if so desired.

				5) (OPTIONAL STEP) - Since 'User_B' can't create/edit any of the files, 'User_B' will have to rely
				on 'User_A' for adding, editing and creating files in this case. However, depenting on the program
				being used to create/edit/copy files used by 'User_A', 'User_B' may still end up being unable 
				to access them due to the file(or other sub-directories) permissions not allowing 'User_B' to 
				'Read', 'Write' or 'eXecute' them when desired. 
				
				In this case, there are two solutions that can be choosen here:

					A) Either Re-apply or change permissions recursively
						$ setfacl -m u:User_B:r-x -R ./home/User_A/Documents/pdf\ files/User\ B/
						
						Note: this will re-apply 'r-x' mode bit to all new files created, allowing 'User_B'
						to execute programs and read them. 'w' can be added at will if so desired.

					B) Applying or changing permissions to new files only:
						$ setfacl -m u:User_B:rwx -R ./home/User_A/Documents/pdf\ files/User\ B/{.*,*}

						Note: this will apply 'rwx' mode bits to files, sub-directories, hidden files and 
						hidden sub-directories only.

						Important: In this case, 'User B' directory will remain untouched and unchanged
						if you so desire to set 'w' mode bit without changing the permissions of 'User_B' dir.

				6) (RECOMMENDED) Adding extra protection:
					You should create a mask with '--x' mode bit, for your user files and dir:
						$ cd ~ && setfacl -m m::--x ./{,*,.*}
							or
						$ cd ~ && setfacl -m m::--x ./ ./{*,.*}

					Note: all arguments do the exact same thing here.

				7) Create a symlink for allowing 'User_B' for easily accessing the directory
					$ ln -sf /home/User_A/Documents/pdf\ files/User\ B /home/User_B/Desktop/Shared\ User\ A\ Dir/

				8) (Optional) Creating an environment variable for easily accessing the directory
					You can also create environment variables for User_B, however i'm not covering this here.
					You can also create env. var for User_A, since he'll also be the one copying/sharing the files 
					with 'User_B'.


				FINAL CONSIDERATIONS:
					As you can see, using ACL makes this task much simplier than actually changing permissions 
				for 'others' or adding new 'groups'. The '--x' mask prevents 'User_B' from listing any other directory,
				but still allowing him for accessing any 'r-x' directories.

				This adds up flexibility when creating/sharing new directories not only for 'User_B' but also for other
				users, you can easily have different sharing permissions without resorting to creating new groups for
				each new user.

				NOTE: If you so desire, you can revert the whole example and share a User_B directory so that 
				User_A can copy his own stuff for User_B, allowing him full flexibility. However, if User_B messes up
				permission settings, masks on his end, User_A will be unable to share or fix anything on his own 
				without administrative rights.


	COMPILING WSL KERNEL
		source: https://github.com/microsoft/WSL2-Linux-Kernel

		1 - First, Install and update WSL:
			$ wsl --install --no-distribution

		2 - Restart the computer if necessary, then update it to latest version:
			$ wsl --update --pre-release

		3 - Compilation requires an ubuntu installation:
			$ wsl --install Ubuntu

		4 - Once installation is complete, start the distro:
			$ wsl -d ubuntu

		6 - Install required packages as needed:
			# apt install build-essential flex bison libssl-dev libelf-dev bc python3 pahole cpio libncurses-dev pkg-config

			6.1 - Alternate packages(may be old):
				# apt install build-essential flex bison dwarves libssl-dev libelf-dev cpio qemu-utils
					or
				$ sudo apt install build-essential flex bison dwarves libssl-dev libelf-dev cpio qemu-utils

			VERY IMPORTANT: Required packages might change from time to time, so checking on github might be necessary:
				https://github.com/microsoft/WSL2-Linux-Kernel

		7 - Clone the git repository:
			$ git clone https://github.com/microsoft/WSL2-Linux-Kernel

			7.1 - (Alternate Option) Git cloning a specific an kernel version:
				This isn't recommended, always download the newest one,
			Older kernel versions may contain bugs and problems that can permanently cause damage.

				$ git clone https://github.com/microsoft/WSL2-Linux-Kernel.git --depth=1 -b linux-msft-wsl-6.X.Y

				HOW TO SELECT THE RIGHT KERNEL VERSION FOR YOU:
					Let's pretend a new Kernel just came out 1 month ago: Kernel 6.0.0

					If a kernel is new and too 'young', it's deemed as immature and not good for use, 
				and only meant for testing, example: Kernel 6.1.6 or Kernel 6.1.12
				Reason is that new 'young' kernels may contain bugs and problems that haven't been found yet
				by the devs. Some of these bugs can break your system and even wreck your hard drive completely.

				When a kernel is too old and not from an LTS (Long Term Stability) Branch, there is a bigger chances
				bugs have never been fixed. For Example: Kernel 3.2

				A Kernel is good for use when it's both new and mature, for example(in this case): Kernel 5.9.0.

				If you still want to use 6.X.Y version, it's recommended to wait until it has matured and reached
				around 6.5.0 or 6.6.0, at this point most of the features meant to be on the new 6.0 Kernel have 
				both been implemented and tested, completely mitigating the chances of something going wrong in your
				system. The more it mature, the lesser the chances you're going to have problems with it.

		8 - Change directory into the git repository you just cloned:
			$ cd ~/WSL2-Linux-Kernel

		9 - Select Kernel options to be included in the compilation:
			$ make menuconfig KCONFIG_CONFIG=Microsoft/config-wsl

		10 - Compile using this:
			$ make -j$(nproc) KCONFIG_CONFIG=Microsoft/config-wsl && make INSTALL_MOD_PATH="$PWD/modules" modules_install

		11 - Once it's finished, Copy the compiled kernel to your windows directory located in /mnt/c/ on WSL:
			11.1) First create a new directory in your user directory:
				$ mkdir -p /mnt/c/Users/<username>/.wsl-kernels/

			11.2) Copy the compiled kernel to the newly created directory:
				$ cp arch/x86/boot/bzImage /mnt/c/Users/<username>/.wsl-kernels/
			
			NOTE: Replace <username> with an existing username.

		12 - Create the file '.wslconfig' in your user directory:
			At this point, if you wish to do so, you can rename the kernel name from 'bzImage' to anything you'd like.

			[file: C:\Users\USER\<username>\.wslconfig ]
				[wsl2] 
				kernel=C:\\Users\\<username>\\.wsl-kernels\\bzImage
			[/file]

			NOTE: Replace <username> with an existing username.

		13 - Now terminate/shutdown ALL running WSL distributions:
			$ wsl --shutdown
				or
			$ wsl --terminate <distribution>

		14 - Now you can finally use any distribution you want with your new custom compiled kernel:
			$ wsl -d archlinux

		15 - Verify Kernel version 
			$ uname -rv


---SOME CMD(COMMAND PROMPT) AND WINDOWS POWER SHELL STUFF
	MANAGING ALIASES IN WINDOWS POWER SHELL
		Source: https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/set-alias?view=powershell-7.5

		By default, aliases aren't permanently set when created.

		LISTING ALL AVAILABLE ALIASES
			$ Get-Alias -Name *

		CREATING AN ALIAS IN WINDOWS POWER SHELL
			This creates an alias called 'np':
				$ Set-Alias -Name np -Value C:\Windows\notepad.exe

			NOTE: It's also possible to use 'New-Alias' for creating an alias, difference is that New-Alias will not create
		an override an alias with new values when it already exists, whereas 'Set-Alias' will update and change 
		pre-existing aliases.

		CREATING ALIAS FOR A COMMAND
			This will create a function called 'CD32', then 'set-alias' will create an alias 'go' that calls on that function:
				$ function CD32 {Set-Location -Path C:\Windows\System32}
				$ Set-Alias -Name Go -Value CD32

		UPDATING ALIASES
			This will update an alias and set it to 'readonly' mode, further changes and updates will have to be 
			made using the 'Force' parameter; the 'private' option makes it only available under the current scope.
				$ Set-Alias -Name Go -Option ReadOnly, Private

		
		HOW TO PERMANENTLY SET ALIASES
			You'll need to choose a user/system profile and create your aliases there

			LISTING ALL PROFILES AVAILABLE
				$ $profile | select *

			CREATE THE FILE USING NVIM OR NOTEPAD
				If you choose an administration profile, then create and/or edit the profiles:

				$ runas /user:Administrator "nvim C:\Windows\System32\WindowsPowerShell\v1.0\profile.ps1"
					or
				$ runas /user:Administrator "notepad C:\Windows\System32\WindowsPowerShell\v1.0\profile.ps1"
			
			
				example:
					[file: C:\Windows\System32\WindowsPowerShell\v1.0\profile.ps1]
						Set-Alias -Name dar -Value "C:\Program Files\DAR\dar.exe"
						Set-Alias -Name dar_slave -Value "C:\Program Files\DAR\dar_slave.exe"
						Set-Alias -Name dar_cp -Value "C:\Program Files\DAR\dar_cp.exe"
						Set-Alias -Name dar_manager -Value "C:\Program Files\DAR\dar_manager.exe"
						Set-Alias -Name dar_split -Value "C:\Program Files\DAR\dar_split.exe"
						Set-Alias -Name dar_xform -Value "C:\Program Files\DAR\dar_xform.exe"
						Set-Alias -Name par2 -Value "C:\Program Files\PAR2\par2.exe"

						function sudo_f {
							param (
								[Parameter(Mandatory=$true, ValueFromPipeline=$true)]
								[string]$exec="",
								[switch]$askpass
							)

							if($askpass){
								<# HOW TO USE: $ sudo -askpass   OR $ sudo -askpass $true
								 This has no practical use here, Im just adding this here for fun #>
								param (
									[Parameter(Mandatory=$true, ValueFromPipeline=$true)]
									[string]$password = $( Read-Host "Input password, please" )
								) 
							}

							C:\Windows\System32\runas.exe /user:Administrator $exec
						}

						Set-Alias -Name sudo -Value sudo_f
					[/file]

			ALLOWING SCRIPTS TO BE EXECUTED BY POWERSHELL
				$ Set-ExecutionPolicy RemoteSigned -Scope CurrentUser

				You can then disable if you don't want it:
					$ Set-ExecutionPolicy Restricted
	

---------------------
---------------------
---------------------
---------------------
ADVANCED REGEX PART 3
DIFFERENCE BETWEEN SED AND GREP
	GREP can be used to select lines that match a given regex pattern, 
	with GREP it's also possible to select matched text with the -o option.
	GREP has support for PERL REGEXP(PCRE) engine as well as EXTENDED REGEXP(ERE) and BASIC REGEXP(BRE)
	SED is used for string substitution, modifying & replacing texts that have matched a given regex text.
	SED doesn't supports PCRE engine, only supports EXTENDED REGEXP(ERE) and BASIC REGEXP(BRE).
	Things like 'lookaround assertions' don't exist in SED.
	
ALTERNATE OPTION FOR SED
	For those still wishing to use PCRE(Perl Regex Extended) with sed,
	there's a tool called 'ssed' that includes the PCRE engine.

	example:
		$ echo "Hello World\! Hello World\!\!\! Hello World\! Hello World\!\!?" \
		| ssed -R -e 's/Hello World(?=(\!\!\!|\!\!\?))/Hello My World\!/g'

		will change "Hello World" to "Hello My World" adding an extra '!' character
		without changing any of the other characters, 
		the substitution only happens when the "Hello World" sentence is succeeded by either '!!!' or '!!?'
		in the lookahead expression.

		This is the actual output result:
			Hello World! Hello My World!!!! Hello World! Hello My World!!!?


SED SYNTAX
	Sed syntax is as follow:
		$ sed -e 's/<regex_pattern>/<substitution_string>/[option(s)]'
			or
		$ sed -e 's#<regex_pattern>#<substitution_string>#[option(s)]'

	Special terminal characters have to be inserted with '\' character.

	THE 'G' OPTION
		By default, 'sed' isn't greedy and stops at the first occurrence of a regex pattern.
		example:
			$ echo "hello1; hello2; hello3;" | sed -e 's/;/ ---/'
			^ this will output the following formatted text:
				hello1 --- hello2; hello3;

		the 'g' option allows for all ';' characters to be changed into '---' as follows:
			$ echo "hello1; hello2; hello3;" | sed -e 's/;/ ---/g'
			^ output: hello1 --- hello2 --- hello3 --- 

SOME SED COMMANDS
	FETCHING SAME-LINE FIELDS
		The regex pattern below allows to get an attribute for any of the existing inline fields:
			$ echo "name: my_name my_middle_name; age: 15; height: 1.7m" | sed -e 's/.*age:[ ]//' -e 's/;.*//'

		This regex pattern allows to retrieve both attribute and field name:
			$ echo "name: my_name my_middle_name; age: 15; height: 1.7m" | sed -e 's/.*age:/age:/' -e 's/;.*//'

		The pattern below allows to format a given field before display:
			$ echo "name: Bill Gates; age: 15; height: 1.7m" | sed -e 's/name:[ ]/\n\nFull Name: /' \
			-e 's/age:[ ]/\n\tAge: /' -e 's/height:[ ]/\n\tHeight: /' -e 's/;//g'

		Pattern for formatting several people at once:
			$ echo "name: Bill Gates; age: 55; height: 1.7m; name: Steve Jobs; age: 41; height: 1.8m" | \
			sed -e 's/name:[ ]/\n\nFull Name: /g' -e 's/age:[ ]/\n\tAge: /g' -e \
			's/height:[ ]/\n\tHeight: /g' -e 's/;//g'

FIND, AWK GREP AND UGREP || GOOD AND BAD EXAMPLES

	FINDING ALL 'PASSWORD' OCCURRENCES IN ALL FILES IN A GIVEN DIRECTORY
		The following examples will demonstrate common mistakes done when using grep.

	# find . -type f | awk '{system("ugrep -C 3 -P \"pass|wd|word\" -- \""$0"\"")}'
	^ returns 'pass', 'password', 'passwd', but could also return "wd" and "word" alone which is not intended.
	^ BAD EXAMPLE, leaving here for reference.

	# find . -type f | awk '{system("grep -C 3 -P \"\\bpass\\b|pass([word].*)\" -- \""$0"\"")}'
	^ actual command is: grep -C 3 -P "\bpass\b|pass([word].*)" -- <filename>
	^ will find exact 'pass' word, but not 'pass:', will also find all pass + [word] combinations,
	^ may however return 'passo', which isn't intended.
	^ SLIGHTLY BETTER, BUT STILL BAD EXAMPLE. Leaving for reference

	# find . -type f | awk '{system("grep -C 3 -P \"pass(?=(:|word|wd|d))|\\bpass\\b\" -- \""$0"\"")}'
	^ actual command is: grep -P "pass(?=(:|word|wd|d))|\bpass\b"
				or
			     ugrep -P "pass(?=(:|word|wd|d))|\bpass\b"
	^ will find 'pass' followed by ':' or 'word' or 'wd' or 'd' by using lookeahead assertion
	^ also finds the exact 'pass' word by using \bpass\b
	^ CORRECT FORM, GOOD EXAMPLE.

	By using 'wc -l' you can ensure there's more lines found using the correct form than the incorrect ones.
	even if the incorrect methods provide more line counts, that's because they're returning wrong and bad findings.

		^ Example:
		# find . -type f | awk '{system("grep -C 3 -P \"pass(?=(:|word|wd|d))|\\bpass\\b\" -- \""$0"\"")}' | wc -l

	
	LOOKING FOR SIMILAR PACKAGES ON THE PACKAGE MANAGER REPOSITORY
		Meant for archlinux only, but can likely be adapted for others
			$ pacman -sS awk | ugrep -C 3 -P -i "\bawk\b|\bawk.*"
			^ looks for packages named 'awk', then greps/ugreps for alternative options of the same package
			^ GOOD FORM. May return uwanted but still related results.

			$ pacman -sS grep | ugrep -P -i -C 3 "\bgrep\b|\bgrep.*|(?<=\w)grep\b"
			^ BETTER REGEX for searching related packages.
			^ install 'yay' package manager for greater results.

	FORMATTING IPTABLES LOG ON A  HUMAN READABLE FORMAT
		This will format any log with the [IPV6 ACCEPTING] name in it.
# journalctl -b-0 | grep "IPV6" | sed -e 's/MAC=.*SRC/SRC/' -e 's/\[IPV6 ACCEPTING\]/\n\n\[IPV6 ACCEPTING\]\n/' -e 's/ /&\n/g' | less


STRIPPING OUT TEXT WITH TR

1) Strips out all printable characters from a file:
	$ tr -cd "[:print:]" < path/to/file
------
NSCDE HOTKEYS
Hint: use 'alt' for pages, and 'system_key' for workspaces

alt+tab			#Move/Cycle Between Pages
system_key+tab		#Move/Cycle Between Workspaces

alt+space		#Change Page of current window
system_key+space	#Change Workspace of current window

shift+arrow_keys	#Change between existing windows in the current workspace
------
---------
Steam Doesn't crashes when doing this if there's any faulty library:
	$ DEBUGGER=gdb steam
	(gdb) run
	[...]
	(gdb) quit
----
----
Smaller TODO list:

	1 - systemd-run can execute programs as service on the background.
	2 - machinectl login executes a login shell by initiating a new pts
	3 - study run levels and targets on linux
	4 - explain how 'systemd-analyze blame' works by reading the man-page:
		it works by timing how long each service took to initiate.
		it doesn't time how long it took for finishing.
	5 - explain how setuid, setgid and sticky bit works:
		'setuid' on a program allows the user to run the program as the owner of that.
		allowing the user to inherit any permissions for the given user.

		if the program is a terminal, trying to execute any other 'root' commands will result in failure.
			# chmod u+s ./file

		'setguid' works similarly to 'setuid', but allows the user to run the program
		with the group permissions of the file owner.
			# chmod g+s ./file

		'sticky bit' on a file protects it from being deleted by users other than the owner.
			# chmod u+t ./file

	6 - /etc/security/limits.d/username.conf can help setting user limits on the system
	7 - Mount usb device automatically on plug-in
		https://unix.stackexchange.com/questions/134797/how-to-automatically-mount-an-usb-device-on-plugin-time-on-an-already-running-sy 



---------------
---------------
---------------
---------------
SECURITY CONCERN
	Remember this settings on /usr/share/dbus-1/session.conf
		<policy context="default">
			<!-- Allow everything to be sent -->
			<allow send_destination="*" eavesdrop="false"/>
			<!-- Allow everything to be received -->
			<allow eavesdrop="false"/>
			<!-- Allow anyone to own anything -->
			<allow own="*"/>
		</policy>
---------------
---------------
---------------
---------------
FROM TCSETATTR MAN PAGES
	If you want to read more: man tcsetattr, stty, termios

ctrl+q			#Start Signal - VSTART
ctrl+s 			#Stops Output on Terminal until START Signal is Sent - VSTOP
ctrl+u			#Erases entire line on terminal - VKILL
ctrl+w			#Erases single word -  VWERASE
ctrl+h			#Erase a single character - VERASE


TEMPORARY:
Using setcap and getcap
sudo setcap 'cap_sys_nice=eip' <application>
getcap <application>

eip means: effective, inheritable, permitted
https://stackoverflow.com/questions/7635515/how-to-set-cap-sys-nice-capability-to-a-linux-user


strace: trace system calls of a program
seccomp: CONFIG_SECCOMP_FILTER needs to be enabled for it to work with firejail.
capsh use capsh --print for printing current capabilities.


TEMPORARY 2:
1 - Save easyeffects configuration files
2 - List all nvidia required packages
	libva-nvidia-driver: Allows VA-API Hardware Video Acceleration
3 - Explain how to setup custom startup resolution using nvidia
	check on /etc/X11/xorg.conf
4 - Explain how to apply 'clock.force-quantum' and 'clock.force-rate' on an application basis and also on a device basis:
	check on /etc/pipewire/pipewire-pulse.conf
5 - Explain how to use openssl and gpg for encrypt/decrypting archives and normal files
	encrypting:
		$ tar cz ./path\ to\ data  | openssl enc -aes-256-cbc -p<password> -iter 100000 -e > <filename>.tar.gz.enc
			or
		$ openssl aes-256-cbc -a -salt -p<password> -in <filename>.txt -out <filename>.txt.enc

	decrypting:
		$ openssl enc -aes-256-cbc -d -in <filename>.tar.gz.enc | tar xz
			or
		$ openssl aes-256-cbc -d -a -p<password> -in <filename>.txt.enc -out <filename>.txt.new

6 - Write something about POLKIT
	https://wiki.archlinux.org/title/Polkit

---
VERY IMPORTANT/TASK  LOGS:

Feb-07-2025:
not using 'load_video' nor 'set gfxpayload=keep' in /etc/grub/grub.cfg anylonger, but those  options  aren't needed and were added to try to make my NonEDID Resolutions work, however another solution was found and kept, therefore discarding these options. - OPTIONS REMOVED / DONE - The solution was to enable and allow NonEDID resolutions instead of completely disabling EDID Resolutions in /etc/X11/xorg.conf.

Adding 'nvidia_fbset.modeset=0' to grub, in an attempt to avoid  crashing issues wasn't a solution. - OPTION REMOVED / DONE

on /etc/modprobe.d/nvidia.conf file i just disabled Nvidia GSP Firmware, since 'journalctl' likely pointed to GSP probably being the source of the problem,
, but this was not the case. Turning the GSP option back to '1'.

Furtherover, the reason you shouldn't completely disable EDID is because TTY only supports EDID resolutions. 
-----
----
Temporary 3:
Custom Modeline 70hz:
"1280x768_70n" 73.685 1280 1288 1320 1360 768 780 788 794 +HSync -VSync
----
